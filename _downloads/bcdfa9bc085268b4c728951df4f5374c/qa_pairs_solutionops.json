[
    {
        "user_prompt": "Please explain how TestCraft's AI technology enhances the test automation process.",
        "output_prompt": "TestCraft's AI technology enhances the test automation process by allowing testers to create automated test scenarios without the need to write code. The AI analyzes the application under test and helps in maintaining the tests by adapting to changes in the UI, thus significantly reducing maintenance overhead. This facilitates faster test creation, seamless execution, and aligns with continuous integration and deployment practices for quick feedback and improved digital product quality.",
        "context": "Robot Framework Website\n\n3. TestCraft\n\nTestCraft is a codeless Selenium test automation platform. Its revolutionary AI technology and unique visual modeling allow for faster test creation and execution while eliminating test maintenance overhead.\n\nThe testers create fully automated test scenarios without coding. Customers find bugs faster, release more frequently, integrate with the CI\/CD approach and improve the overall quality of their digital products. This all creates a complete end-to-end testing experience.\n\nPerfecto (TestCraft) Website or get it  from the Visual Studio Marketplace\n\n4. Ranorex Studio",
        "chunk_id": "chunk81_10",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/README.md",
        "root_chunk_id": "chunk81"
    },
    {
        "user_prompt": "How does Prompt flow facilitate local experimentation and support integration tests?",
        "output_prompt": "Prompt flow enables local experimentation by providing SDK and CLI for executing LLMOps flows either locally or in the cloud with both single input records or batches. Outputs from every run are accessible, which allows the execution of evaluation flows using a run id. For integration tests, you can spin up a development service using a single CLI command, making it important to implement these tests even on a local machine.",
        "context": "Let\u2019s look at the challenges from above and see how Prompt flow can solve them:\n\nLocal experimentation environment: Prompt flow supports SDK and CLI to execute LLMOps flows locally or in the cloud using a single input record or a batch. In both cases, you have access to output from every run of your experiment and can use it to execute an evaluation flow just by providing a run id. Moreover, using a single line in CLI you can spin up a development service to serve your flow. It\u2019s important to implement integration tests even on a local computer.\n\nSharable components: Prompt flow flows can be published as Azure ML components, and it\u2019s possible to reuse them across the organization thanks to Azure ML Registries. Moreover, each flow step can be represented as a reusable tool",
        "chunk_id": "chunk275_21",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/mlops-in-openai\/llmops.md",
        "root_chunk_id": "chunk275"
    },
    {
        "user_prompt": "Based on the given context, how should I structure the Results section in my report to effectively present the evaluation of multiple solutions?",
        "output_prompt": "The Results section should include a comparison table where each solution is evaluated against the defined criteria. The table should list all solutions in the first column, with subsequent columns representing different evaluation criteria. Within the table, use qualitative descriptions for each solution's performance against the criteria, such as good, satisfactory, or poor. Additionally, apply color coding to the cells\u2014green for good, yellow for fair, and red for bad\u2014to provide a visual representation of each solution's effectiveness. This will help to clearly communicate which solutions were most effective and support your decision-making process.",
        "context": "Metrics collected while testing the prototype\n\nDocumentation that indicates the solution can provide the desired capabilities\n\nNOTE: Evidence is not required for every capability, metric, or constraint for the design to be considered done. Instead, focus on presenting evidence that is most relevant and impactful towards supporting or eliminating the hypothesis.\n\n{Solution 2}\n\n...\n\n{Solution N}\n\n...\n\nResults\n\nThis section should contain a table that has each solution rated against each of the evaluation criteria:\n\nSolution Evaluation Criteria 1 Evaluation Criteria 2 ... Evaluation Criteria N Solution 1 Solution 2 ... Solution M\n\nNote: The formatting of the table can change. In the past, we have had success with qualitative descriptions\nin the table entries and color coding the cells to represent good, fair, bad.\n\nDecision",
        "chunk_id": "chunk47_10",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/trade-studies\/template.md",
        "root_chunk_id": "chunk47"
    },
    {
        "user_prompt": "What are some essential aspects to consider when monitoring generative AI applications?",
        "output_prompt": "When monitoring generative AI applications, one should consider data quality, model performance, output variability, system efficiency, and user experience. A structured approach to telemetry and monitoring can help in tracking these aspects effectively.",
        "context": "rings:\n  - internal\n\nMonitoring generative AI applications\n\nAs the adoption of generative AI applications continues to grow, so does the\nnecessity for robust monitoring. These applications, powered by intricate\ndata models and algorithms, aren't exempt from the challenges faced by any\nother software system. Yet, their unique nature makes their monitoring\nrequirements distinctive. generative AI apps interact with a vast array of\ndata, generate varied outputs, and often operate under tight performance\nconstraints. The quality, performance, and efficiency of these applications\ndirectly impact user experience and operational costs. Therefore, a structured\napproach to monitoring and telemetry isn't only beneficial but critical.",
        "chunk_id": "chunk294_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/monitoring\/monitoring.md",
        "root_chunk_id": "chunk294"
    },
    {
        "user_prompt": "How should I interpret if my server's Available MBs counter drops below 10%?",
        "output_prompt": "If the Available MBs on your server drops below 10%, it indicates that your server is running low on memory. This can induce Page Faults, which increases CPU load as it swaps memory to and from the disk. To prevent potential performance degradation or system instability, it's advisable to add more memory to your server.",
        "context": "Counter Description Available MBs This counter represents the amount of memory that is available to applications that are executing. Low memory can trigger Page Faults, whereby additional pressure is put on the CPU to swap memory to and from the disk. if the amount of available memory dips below 10%, more memory should be obtained. Pages\/sec This is actually the sum of \"Pages Input\/sec\" and \"Pages Output\/sec\" counters which is the rate at which pages are being read and written as a result of pages faults. Small spikes with this value do not mean there is an issue but sustained values of greater than 50 can mean that system memory is a bottleneck. Paging File(_Total)\\% Usage The percentage of the system page file that is currently in use. This is not directly related to performance, but you can run into serious application issues if the page file does become completely full and additional",
        "chunk_id": "chunk69_19",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/performance-testing\/README.md",
        "root_chunk_id": "chunk69"
    },
    {
        "user_prompt": "How can the sprint goal be effectively integrated into the Agile process, particularly during sprints and sprint reviews?",
        "output_prompt": "The sprint goal can be effectively integrated into the Agile process by including it at the end of daily standup meetings as a reminder, assessing its achievement during sprint reviews, and defining it as a named User Story in the sprint backlog to provide clarity and focus on the sprint's objectives.",
        "context": "The sprint goal can be used :\n\nAt the end of each stand up meeting, to remember the north star for the Sprint and help everyone taking a step back\n\n*During the sprint review (\"was the goal achieved?\", \"If not, why?\")\n\nNote: A simple way to define a sprint goal, is to create a User Story in each sprint backlog and name it \"Sprint XX goal\". You can add the bullet points in the description.\n\nStories:\n\nExample 1 - Preparing in advance:\n\nThe dev lead and product owner plan time to prepare the sprint backlog ahead of sprint planning.\n\nThe dev lead uses their experience (past and on the current project) and the estimation made for these stories to gauge how many should be in the sprint.",
        "chunk_id": "chunk170_9",
        "source": "..\/data\/docs\/code-with-engineering\/agile-development\/core-expectations\/README.md",
        "root_chunk_id": "chunk170"
    },
    {
        "user_prompt": "Which automation framework should I use if I need consistent cross-platform support and a syntax based on Markdown?",
        "output_prompt": "You should use Gauge Framework, as it offers simple, flexible syntax based on Markdown and ensures consistent cross-platform\/language support for writing test code.",
        "context": "Simple, flexible and rich syntax based on Markdown.\n\nConsistent cross-platform\/language support for writing test code.\n\nA modular architecture with plugins support.\n\nSupports data driven execution and external data sources.\n\nHelps you create maintainable test suites.\n\nSupports Visual Studio Code, Intellij IDEA, IDE Support.\n\nSupports html, json and XML reporting.\n\nGauge Framework Website\n\n2. Robot Framework\n\nRobot Framework is a generic open source automation framework. The framework has easy syntax, utilizing human-readable keywords. Its capabilities can be extended by libraries implemented with Python or Java.",
        "chunk_id": "chunk81_8",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/README.md",
        "root_chunk_id": "chunk81"
    },
    {
        "user_prompt": "How can you effectively support high consumer concurrency when using LLM requests on Azure OpenAI?",
        "output_prompt": "To effectively support high consumer concurrency when using LLMs on Azure OpenAI, it is recommended to segregate consumers into different regions to exploit regional quotas and concurrently operate. Using a gateway like GenAI for load balancing can distribute requests across various regions to manage the load. Additionally, implementing region affinity can reduce latency by routing requests to the closest regional deployment. Benchmarking under high and normal loads also helps to identify the best performing OpenAI instances.",
        "context": "Supporting High Consumer Concurrency: To accommodate numerous consumers making LLM requests, it is advisable\nto segregate these consumers into distinct regions. Since Azure OpenAI\nquotas are enforced at the regional level, deploying in multiple regions\nallows these consumers to operate concurrently. The GenAI gateway can\nfacilitate load balancing by distributing their requests across various\nregions. However, supporting cross-region deployments might introduce\nlatency issues for consumers. This can be partially mitigated by\nimplementing region affinity, whereby the GenAI gateway routes consumer\nrequests to the nearest regional deployment to the requestor or the\nregions can be identified by performing benchmarking. In benchmarking,\nit is ideal to mimic high, normal loads from requestor and can evaluate\nwhich of the OpenAI instances are working well.",
        "chunk_id": "chunk345_1",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/genai-gateway\/scalability.md",
        "root_chunk_id": "chunk345"
    },
    {
        "user_prompt": "How do I format Python code using autopep8 or yapf?",
        "output_prompt": "To format Python code with autopep8, first install it using 'pip install autopep8'. Then run 'autopep8 [file\/folder] --in-place' in your terminal. For yapf, install it using 'pip install yapf', then execute 'yapf [file\/folder] --in-place' to format your code. Both commands will format the code in place. If you're using VS Code, you can also install the Python extension which offers formatting features.",
        "context": "{% raw %}\n\nbash\npip install autopep8\n\n{% endraw %}\n\nFormat python code\n\n{% raw %}\n\nbash\nautopep8 [file\/folder] --in-place\n\n{% endraw %}\n\nyapf\n\nyapf Yet Another Python Formatter is a python formatter from Google based on ideas from gofmt.  This is also more configurable, and a good option for automatic code formatting.\n\n{% raw %}\n\nbash\npip install yapf\n\n{% endraw %}\n\nFormat python code\n\n{% raw %}\n\nbash\nyapf [file\/folder] --in-place\n\n{% endraw %}\n\nVS Code Extensions\n\nPython",
        "chunk_id": "chunk16_4",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/python.md",
        "root_chunk_id": "chunk16"
    },
    {
        "user_prompt": "What are these script and task snippets used for in a build pipeline?",
        "output_prompt": "These snippets are part of a build pipeline configuration, likely for a CI\/CD system such as Azure DevOps. The 'go vet' script is used for static analysis of Go code to find any suspicious constructs. The Go task is used to build the Go application. The CopyFiles task copies files to a specified target folder, which is likely the artifact staging directory. Lastly, the PublishBuildArtifacts task is for publishing the build artifacts to a server, with 'drop' as the artifact name, making them available for deployment or further stages in the pipeline.",
        "context": "script: go vet\n  workingDirectory: $(System.DefaultWorkingDirectory)\/scripts\n  displayName: 'Run go vet'\n\ntask: Go@0\n  inputs:\n    command: 'build'\n    workingDirectory: '$(System.DefaultWorkingDirectory)'\n\ntask: CopyFiles@2\n  inputs:\n    TargetFolder: '$(Build.ArtifactStagingDirectory)'\n\ntask: PublishBuildArtifacts@1\n  inputs:\n     artifactName: drop",
        "chunk_id": "chunk15_9",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/go.md",
        "root_chunk_id": "chunk15"
    },
    {
        "user_prompt": "Which online stores are currently supported by Feast, and are there limitations on the data sources per namespace?",
        "output_prompt": "Feast currently supports the following online stores: SQLite (locally), Redis, Datastore, DynamoDB, Azure SQL DB, Azure Database for MySQL, Amazon Aurora (MySQL-compatible), and Amazon RDS MySQL. As of April 2022, Redis is no longer supported. Feast only supports one data source per namespace within a feature store definition. While you can have multiple namespaces, each FeaSt SDK instance can access only one namespace at a time. It's possible to mix multiple instances and merge data in your code, but this may lead to performance issues.",
        "context": "tables - ADLS (Azure Data Lake Storage) - Azure Storage Blob - Amazon S3 - Delta Lake ADLS (Azure Data Lake Storage) Supported Online Stores Currently supported online stores (Online stores - Feast) - SQLite (locally) - Redis - Datastore - DynamoDB Managed SQL services from Azure\/AWS are options: - Azure SQL DB - Azure Database for MySQL - Amazon Aurora (MySQL-compatible) - Amazon RDS MySQL As of April 2022, Redis is not supported. - Redis - CosmosDB - SQL Redis Read from Multiple Data Sources FeaSt only supports one data source per namespace. For one feature store definition you might have multiple namespaces, but can access one namespace at a time per FeaSt SDK instance. You can mix multiple instances and merge data in your code, but there might be some associated performance",
        "chunk_id": "chunk327_10",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/model-development\/feature-management\/feature-store-comparison.md",
        "root_chunk_id": "chunk327"
    },
    {
        "user_prompt": "Can you explain the difference between LLMOps and DevOps when it comes to backend operations?",
        "output_prompt": "The primary difference between LLMOps and DevOps for backend operations is that LLMOps is more focused on integrating machine learning models into the deployment pipeline. It involves tasks such as model training, versioning, and serving, and requires a deep understanding of machine learning principles. On the other hand, general DevOps is concerned with the continuous integration and delivery of software, ensuring that the code is properly integrated, tested, and ready for production deployment. Both share common principles of automation, monitoring, and collaboration, however, LLMOps additionally emphasizes the lifecycle management of machine learning models.",
        "context": "LLMOps vs DevOps for Common Scenarios\n\nIn this section we will focus on LLMOps for the backend. Let\u2019s address the question on whether there really any difference between LLMOps and for this kind of backends general DevOps is. Let\u2019s look at a few prompt engineering tasks:\n\nPrepare a list of samples that can be used in few-shot learning: for example, you would like to use OpenAI to convert a raw user input into a specific format that satisfies your organization requirements. Obviously, OpenAI models will not be able to understand your requirements if you don't supply a few samples. The quality of the responses will depend on the sample set provided. It is an iterative process to pick an initial set of few-shot samples and tune them until they produce the desired result.",
        "chunk_id": "chunk275_1",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/mlops-in-openai\/llmops.md",
        "root_chunk_id": "chunk275"
    },
    {
        "user_prompt": "What are the potential solutions to mitigate the downsides of sharing secrets among developers using Postman?",
        "output_prompt": "To mitigate the downsides of sharing secrets, one could use environment variables within Postman, integrate with a secrets manager like Azure Key Vault, employ CI\/CD pipelines to handle secret distribution securely, and automate the update process of Postman Collections and environment files to ensure all developers have the latest version without exposing sensitive data.",
        "context": "Grants developers a common experience via Postman IDE features.\n\nEnding with this approach has the following downsides:\n\nPromotes unsafe sharing of secrets. Credentials needed to request JWT token in the prefetch script are being manually shared.\n\nSecrets may happen to get exposed in the git commit history for various reasons (ex. Sharing the exported Postman Environment files).\n\nCollections can only be used locally to hit APIs (local or deployed). Not CI based.\n\nEach developer has to keep both their Postman Collection and Postman environment file(s) updated in order to keep up with latest changes to deployed services.\n\nUse Case - Hands-on Functional Testing Of Endpoints with Azure Key Vault and Azure App Config",
        "chunk_id": "chunk85_10",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/recipes\/postman-testing.md",
        "root_chunk_id": "chunk85"
    },
    {
        "user_prompt": "How could AI help in optimizing manufacturing processes to identify bottlenecks?",
        "output_prompt": "AI can assist in manufacturing optimization by analyzing data from various sources such as sensors and surveillance footage to identify patterns and pinpoint bottlenecks. Machine learning algorithms can then suggest process improvements or automate decisions for more efficient workflows.",
        "context": "manufacturing to identify bottlenecks and automate decisions : CCH \u2022 ASSETS : Process and validate videos from surveillance cameras to detect suspicious activity : Sony NATURAL LANGUAGE TO CODE AI assisted code generation. May include domain specific code or protocols This is a developer scenario. \u2022 ASSETS : Generate PLC code that uses Ladder Logic : ROCKWELL \u2022 ASSETS : Natural language to SQL : Honeywell \u2022 ASSETS : Natural Language to Python : Cognite \u2022 ASSETS : AI assisted prompt generation : CVS, Novartis, HSBC AI-AUGMENTED DATA CURATION AI assisted data classification and labeling. Includes generating synthetic data from sample set or profile \u2022 ASSETS : data contextualization for various energy related analysis : P&G \u2022 PEOPLE : Generate synthetic data based on customer criteria : LSEG \u2022 PEOPLE : AI assisted corpus curation of",
        "chunk_id": "chunk293_6",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/use-cases\/use-cases.md",
        "root_chunk_id": "chunk293"
    },
    {
        "user_prompt": "Can you explain what the '.order' file is used for in a documentation project?",
        "output_prompt": "Certainly! The '.order' file is typically used to specify the sequence in which files and directories should be displayed or processed. In the context of documentation, it likely defines the order in which the documentation pages should be presented to the reader, ensuring that the content is structured and easy to navigate.",
        "context": "\u2502\n\u2502   \u251c\u2500\u2500 .order                                \/\/ Providing a fixed order of files and directories\n\u2502   \u251c\u2500\u2500 index.md                          \/\/ Landing page documentation",
        "chunk_id": "chunk110_21",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/using-docfx-and-tools.md",
        "root_chunk_id": "chunk110"
    },
    {
        "user_prompt": "What are the three pillars of observability in software development and how do they contribute to reducing production failures?",
        "output_prompt": "The three pillars of observability are logging, metrics, and tracing. Logging provides a record of events that have occurred within the application, which can be helpful for debugging issues after they have happened. Metrics offer aggregated data over time, which helps in understanding the performance trends and spotting anomalies. Tracing tracks the journey of requests through the application, making it easier to identify where failures or bottlenecks are occurring. Together, these pillars allow developers to monitor applications in real-time and quickly diagnose problems, leading to fewer and less severe production failures.",
        "context": "Observability\n\nObservability alleviates unforeseen challenges for the developer in a complex distributed system. It identifies project bottlenecks quicker and with more precision, enhancing performance as the developer seeks to deploy code changes. Adding observability improves the experience when identifying and resolving bugs or broken code. This results in fewer or less severe current and future production failures.\n\nThere are many observability strategies a developer can use alongside best engineering practices. These resources improve the DevEx by ensuring a shared view of the complex system throughout the entire lifecycle. Observability in code via logging, exception handling and exposing of relevant application metrics for example, promotes the consistent visibility of real time performance. The observability pillars, logging, metrics, and tracing, detail when to enable each of the three specific types of observability.\n\nMinimize the Number of Repositories",
        "chunk_id": "chunk245_15",
        "source": "..\/data\/docs\/code-with-engineering\/developer-experience\/README.md",
        "root_chunk_id": "chunk245"
    },
    {
        "user_prompt": "Could you explain the difference between a fake and other test doubles in software testing?",
        "output_prompt": "Certainly! In software testing, test doubles are objects that stand in for real objects for testing purposes. A fake is a type of test double that has a working implementation, but not the same as the production one. It's more complex than other test doubles like stubs or mocks, which don't have full implementations. The advantage of a fake is that it can be tested in isolation, increasing confidence in its reliability when used as a part of other tests.",
        "context": "Writing a fake does take more time than other test doubles, because they are full implementations, and can have\ntheir own suite of unit tests. In this sense though, they increase confidence in your code even more because your test\ndouble has been thoroughly tested for bugs before you even use it as a downstream dependency.",
        "chunk_id": "chunk88_10",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/unit-testing\/mocking.md",
        "root_chunk_id": "chunk88"
    },
    {
        "user_prompt": "Can you explain the characteristics of the image file 'image2.raw' with the provided metadata?",
        "output_prompt": "The 'image2.raw' file has an 8-bit depth, follows the Bayer format, and uses RGB color. This means it stores color images in a mosaic pattern of red, green, and blue pixels, and has 256 levels of intensity per channel.",
        "context": "\"bitDepth\": 8,\n            \"format\": \"Bayer\",\n            \"additionalProperties\": {\n              \"color\": \"rgb\"\n            }\n          }\n        },\n        {\n          \"fileName\": \"inputs\/testcase1\/image2.raw\",\n          \"inputProperties\": {",
        "chunk_id": "chunk256_35",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "How can different branches be used effectively during the ML pipeline development process to avoid conflicts?",
        "output_prompt": "To manage multiple branches effectively in the ML pipeline development process, incorporating the branch name into the naming of experiments, models, and pipeline names is recommended. This practice helps in differentiating and tracking the artifacts related to each feature branch, thus avoiding conflicts and making collaborative work more manageable.",
        "context": "Use branch names and build IDs to enable developers modifying the pipeline simultaneously (see below).\n\nBranch name utilization\n\nThe biggest challenge of the ML pipeline development process is how to modify and test the same pipeline from different branches. If we use fixed names for all experiments, models, and pipeline names, it will be hard to differentiate your own artifacts working in a large team. To make sure we can locate all feature branch-related experiments, we recommend using the feature branch name to mark all pipelines. Also, use the branch name in experiments and artifact names. This way, it will be possible to differentiate pipelines from different branches and help data scientists to log various feature-branch runs under the same name.",
        "chunk_id": "chunk326_14",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/model-development\/ml-pipelines\/index.md",
        "root_chunk_id": "chunk326"
    },
    {
        "user_prompt": "What are the two forms of authentication supported by Azure OpenAI?",
        "output_prompt": "Azure OpenAI supports Microsoft Entra authentication with managed identities and Entra application-based authentication.",
        "context": "Security and Data Integrity\n\n1. Authentication\n\nAzure OpenAI (AOAI) supports two forms of authentication.\n\nMicrosoft Entra ID: AOAI supports Microsoft Entra\n    authentication with managed identities or Entra\n    application.\n    Using a managed identity, it is possible to achieve a keyless authentication between the consumer and the AOAI service. In\n    case of Entra application based authentication, each consumer will\n    need to maintain client id and secret for the Entra app having\n    access to AOAI resource.",
        "chunk_id": "chunk347_0",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/genai-gateway\/security-and-data-integrity.md",
        "root_chunk_id": "chunk347"
    },
    {
        "user_prompt": "How do I upload my bundle to a specific repository?",
        "output_prompt": "To upload your bundle to a specific repository, you need to follow the Repository \/ Bundle format by specifying the slot, project slug, and bundle slug in the format '<slot>\/<project-slug>\/<bundle-slug>'. If your bundle is for the 'water-glass-detection' project under the 'slot' configuration service, and your bundle is named 'ok-nok', you would use the identifier 'slot\/water-glass-detection\/ok-nok'. Make sure your scope map is created in the slot namespace before attempting to register your bundle. To register it, simply add your bundle to the scope map allowing you to upload it to that specific repository. If the repository doesn't exist yet, it will be created upon bundle registration.",
        "context": "Format: <slot>\/<project-slug>\n\nExample: aa\/cogwheel-quality\n\nRepository \/ Bundle\n\nFormat: <slot>\/<project-slug>\/<bundle-slug> or <scope-map>\/<bundle-slug>\n\nExample: slot\/water-glass-detection\/ok-nok\n\nScope maps and tokens will be managed by each slot's configuration\nservice. Scope maps will be created in\na slot namespace. Bundles will be registered by adding them to a scope map,\nallowing users to upload bundles to that specific repository (or create it if it\ndoesn't exist).\n\nBundle Status",
        "chunk_id": "chunk257_7",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-registry.md",
        "root_chunk_id": "chunk257"
    },
    {
        "user_prompt": "Why is it important to abstract third-party API calls in unit testing, and what is one of the best practices for handling them?",
        "output_prompt": "It is important to abstract third-party API calls in unit testing because we cannot be sure of the internals of these libraries, and they can introduce potential points of failure that are outside the scope of the unit being tested. One best practice is to wrap these API calls in interfaces or other structures that can be easily mocked or stubbed during testing, avoiding direct calls to the third-party services in the test environment, which ensures that unit tests are focused solely on the codebase.",
        "context": "Calling third-party APIs - When you do not control a third-party library that you are calling into, it's impossible to\n  know for sure what that is doing, and it is best to abstract it out. Otherwise, you may be making REST calls or other\n  potential areas of failure without directly writing the code for it. This is also generally a sign that the design of\n  the system is not entirely testable. It is best to wrap third party API calls in interfaces or other structures so\n  that they do not get invoked in unit tests. For more information see the page on mocking.\n\nUnit Testing Frameworks and Tools\n\nTest Frameworks",
        "chunk_id": "chunk89_17",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/unit-testing\/README.md",
        "root_chunk_id": "chunk89"
    },
    {
        "user_prompt": "What are the recommended settings for the third camera given the provided context?",
        "output_prompt": "For the third camera, it is recommended to use an aperture of 5.6, an orientation from the top, a resolution of 1280x720, and a shutter speed of 1\/250 as suggested.",
        "context": "\"aperture\": \"5.6\",\n        \"orientation\": \"top\",\n        \"resolution\": \"1280x720\",\n        \"shutterSpeed\": {\n          \"max\": \"1\/125\",\n          \"min\": \"1\/500\",\n          \"suggested\": \"1\/250\"\n        }\n      }\n    },\n    {\n      \"name\": \"image3\",\n      \"description\": \"Third camera\",",
        "chunk_id": "chunk256_29",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Can you explain the differences between metrics, logs, and traces in monitoring systems?",
        "output_prompt": "Certainly! Metrics are used to track quantitative data such as event occurrences, item counts, time durations, and resource values like CPU and memory usage. Logs are meant for recording detailed information about events that can include errors, warnings, or any notable occurrences. Traces are utilized to follow the path of a request across various services within a microservices architecture, with each trace having a unique identifier to ensure traceability.",
        "context": "Parameters passed.\n\nHow deep into the function the user could get.\n\nUsage Guidance\n\nWhen to use metric or log data to track a particular piece of telemetry can be summarized with the following points:\n\nUse metrics to track the occurrence of an event, counting of items, the time taken to perform an action or to report the current value of a resource (CPU, memory, etc.)\n\nUse logs to track detailed information about an event also monitored by a metric, particularly errors, warnings or other exceptional situations.\n\nA trace provides visibility into how a request is processed across multiple services in a microservices environment. Every trace needs to have a unique identifier associated with it.",
        "chunk_id": "chunk183_3",
        "source": "..\/data\/docs\/code-with-engineering\/observability\/log-vs-metric-vs-trace.md",
        "root_chunk_id": "chunk183"
    },
    {
        "user_prompt": "Can you explain how automated penetration testing tools, like OWASP ZAP, can help improve my organization's security posture?",
        "output_prompt": "Automated penetration testing tools, such as the OWASP Zed Attack Proxy (ZAP), are designed to identify security vulnerabilities in web applications. These tools simulate attacks on your systems to detect issues like unsanitized inputs that might be vulnerable to code injection attacks. By doing so, they help organizations uncover weaknesses that a manual penetration test might miss due to the sheer volume of new vulnerabilities that are found daily. The insights from these tests can inform security teams on how to update their Web Application Firewall (WAF) policies and patch identified vulnerabilities, thereby enhancing the overall security posture of the organization.",
        "context": "Applying Penetration Testing\n\nMany organizations perform manual penetration testing. But new vulnerabilities found every day. Therefore, it is a good practice to have an automated penetration testing performed.\nTo achieve this automation use penetration testing tools to uncover vulnerabilities, such as unsanitized inputs that are susceptible to code injection attacks.\nInsights provided by the penetration test can then be used to fine-tune your WAF security policies and patch detected vulnerabilities.\n\nPenetration Testing Frameworks and Tools\n\nOWASP Zed Attack Proxy (ZAP) - OWASP penetration testing tool for web applications.\n\nConclusion",
        "chunk_id": "chunk240_1",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-integration\/dev-sec-ops\/penetration-testing\/README.md",
        "root_chunk_id": "chunk240"
    },
    {
        "user_prompt": "Can you demonstrate how to declare a variable in Python?",
        "output_prompt": "Certainly! In Python, you can declare a variable by assigning a value to a name. For example: my_variable = 42",
        "context": "Other examples in static context formatting can be used in code generation applications. Here we can see the importance of using the correct comment character for the language that you want to generate.\nPrompt:\n\/* Create a javascript dictionary with 5 cities and their location *\/\nCompletion:\njavascript\n var cities = {\n'Los Angeles': [34.0522, -118.2437],\n'New York': [40.7128, -74.0059],\n'San Francisco': [37.7749, -122.4194],\n'Chicago': [41.8781, -87.6298],\n'Houston': [29.7604, -95.3698]\n};",
        "chunk_id": "chunk284_5",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/prompt-engineering\/prompt-engineering.md",
        "root_chunk_id": "chunk284"
    },
    {
        "user_prompt": "How do I ensure that a specific variable in a GitHub Actions workflow is set based on a value mentioned in my commit message?",
        "output_prompt": "To conditionally set a variable in your GitHub Actions workflow based on the presence of a value in your commit message, you can use environment variables combined with conditional statements. You can read the commit message using the `git log -1 --pretty=%B` command and then set up a conditional step to check if the commit message contains the specific variable pattern you are interested in. If it does, you can set the desired environment variable to true or any other specified value.",
        "context": "```\n\n{% endraw %}\n\nThis triggers the workflow (as will any push). As the [commit var] is in the commit message, the ${COMMIT_VAR} variable in the workflow will be set to true and result in the following:\n\nNot Including the Variable\n\nPush to branch master:\n\n{% raw %}\n\n```cmd\n\ngit add.\ngit commit -m \"Running GitHub Actions Test\"\ngit push",
        "chunk_id": "chunk130_11",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/devops-provider-recipes\/github-actions\/runtime-variables\/README.md",
        "root_chunk_id": "chunk130"
    },
    {
        "user_prompt": "Can you suggest how I should structure a database schema to track the different versions of my models, their descriptions, labels, and an audit trail, considering each new training will create a new model id?",
        "output_prompt": "Certainly! You can create a 'models' table with fields for the model id, description, and labels. Then, have a 'model_versions' table that references the 'models' table, with fields for each version's unique model id, creation date, and any other specific metadata. Lastly, an 'audit_trail' table can be linked to the 'model_versions' table to keep records of the changes made over time. Make sure to set up the necessary foreign keys to maintain relational integrity between the tables.",
        "context": "As a user, you need to create a custom solution to track your model ids. The model id is a UUID, thus it would be beneficial to track additional information as part of your solution. For example, a description of the model, readable labels, an audit trail of how the model came to exist, and previous versions of the model. One thing to note, FR does not support retraining an existing model, instead it would generate a new model with a new model id for each training request. Each version of your model would have a different model id to be tracked.",
        "chunk_id": "chunk338_5",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/deployment\/model-release.md",
        "root_chunk_id": "chunk338"
    },
    {
        "user_prompt": "Please explain how to access the Azurite blob storage service securely using the given configuration.",
        "output_prompt": "To access the Azurite blob storage service securely, you would need to connect to the service using HTTPS. The configuration snippet provided specifies that Azurite should be started with TLS\/SSL encryption, using the certificate and key located in '.\/cert\/azurite\/'. To access the service, you will need to make requests to 'https:\/\/azurite:10000' or 'https:\/\/localhost:10000'. Ensure your client supports SSL and accepts or trusts the certificate used by Azurite. Authentication is set to basic OAuth, so you should also provide the necessary credentials if prompted.",
        "context": "{% raw %}\n\nyaml\nversion: '3.4'\nservices:\n  azurite:\n    image: mcr.microsoft.com\/azure-storage\/azurite\n    hostname: azurite\n    volumes:\n      - .\/cert\/azurite:\/data\n    command: \"azurite-blob --blobHost 0.0.0.0 -l \/data --cert \/data\/127.0.0.1.pem --key \/data\/127.0.0.1-key.pem --oauth basic\"\n    ports:\n      - \"10000:10000\"\n      - \"10001:10001\"",
        "chunk_id": "chunk72_5",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/tech-specific-samples\/blobstorage-unit-tests\/README.md",
        "root_chunk_id": "chunk72"
    },
    {
        "user_prompt": "Could you explain the significance of using different data formats when fine-tuning language models?",
        "output_prompt": "Certainly! Different data formats can significantly influence the performance of a fine-tuned language model. By using formats tailored to the specific task, such as classifying texts or responding to queries, you can guide the model to discern the structure of the input and the desired output more effectively. For instance, inserting special tokens or patterns can help the model recognize the beginning and the end of different segments, which can lead to more accurate and relevant responses. It's essential to experiment with various formats to determine which one works best for your particular application.",
        "context": "Best practices for fine-tuning\n\nHere are some best practices that can help improve the efficiency and effectiveness of fine-tuning LLMs for various applications:\n\nTry different data formats: Depending on the task, different data formats can have different impacts on the model\u2019s performance. For example, for a classification task, you can use a format that separates the prompt and the completion with a special token, such as {\"prompt\": \"Paris##\\n\", \"completion\": \" city\\n###\\n\"}. Be sure to use formats suitable for your application.",
        "chunk_id": "chunk289_4",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/fine-tuning\/fine-tuning-recommend.md",
        "root_chunk_id": "chunk289"
    },
    {
        "user_prompt": "What is data sovereignty and why is it important for businesses?",
        "output_prompt": "Data sovereignty refers to the concept that data is subject to the laws and governance structures of the country in which it is located. For businesses, ensuring data sovereignty is crucial for compliance with local data protection laws to avoid legal consequences and fines. This involves storing and processing the data within the required geographic boundaries.",
        "context": "3. Data Sovereignty\n\nData sovereignty in the context of AOAI refers to the legal and\nregulatory requirements related to the storage and processing of data\nwithin the geographic boundaries of a specific country or region.\nPlanning for data sovereignty is critical for a business to avoid\nnon-compliance with local data protection laws resulting in hefty fines.",
        "chunk_id": "chunk347_7",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/genai-gateway\/security-and-data-integrity.md",
        "root_chunk_id": "chunk347"
    },
    {
        "user_prompt": "Can you explain how to use the \/predict endpoint with the CVT Bundle Interface?",
        "output_prompt": "Certainly! To use the \/predict endpoint, you'll need to send a POST request to the CVT Bundle Interface with the appropriate image data. The endpoint will then process the image using the model or computer vision code contained in the bundle to provide you with the prediction results.",
        "context": "CVT Bundle Interface\n\nThis library contains building blocks to make it easier to build consistent APIs using Python and Bundle interface.\n\nThe Bundle format has a REST API endpoint to interface with the underlying prediction service.. CVT Edge modules will use this endpoint to call the model\/traditional computer vision code and get the predictions. This REST API endpoint provides endpoints:\n\n\/predict: This endpoint is used to call the model\/traditional computer vision code and get the predictions.\n\n\/metadata: This endpoint is used to get the metadata of the model\/traditional computer vision code and bundle.\n\n\/testdata: This endpoint is used to expose sample images and expected results in as a zip file.\n\n\/health\/live: This endpoint is used to check the status of the bundle.",
        "chunk_id": "chunk256_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "What are the differences between Category1 and Category2?",
        "output_prompt": "Category1 typically refers to a group of items or concepts that share common characteristics or attributes distinct from those in Category2. Category2, on the other hand, is another group with its own unique set of features that set it apart from Category1. To give you a more accurate answer, I would need to know the specific context or items you're referring to within these categories.",
        "context": "{\n                            \"field_name\": \"Category1\"\n                        },\n                        {\n                            \"field_name\": \"Category2\"\n                        },",
        "chunk_id": "chunk315_33",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/search\/index.md",
        "root_chunk_id": "chunk315"
    },
    {
        "user_prompt": "What does it mean when `private_cluster_public_fqdn_enabled` is set to false in a cloud infrastructure configuration?",
        "output_prompt": "When `private_cluster_public_fqdn_enabled` is set to false, it means that the cluster's public fully qualified domain name (FQDN) is not enabled. Consequently, the cluster can only be accessed through its private FQDN within the private network, enhancing security by not exposing it to the public internet.",
        "context": "+ portal_fqdn                         = (known after apply)\n      + private_cluster_enabled             = (known after apply)\n      + private_cluster_public_fqdn_enabled = false\n      + private_dns_zone_id                 = (known after apply)\n      + private_fqdn                        = (known after apply)",
        "chunk_id": "chunk133_21",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/recipes\/terraform\/share-common-variables-naming-conventions.md",
        "root_chunk_id": "chunk133"
    },
    {
        "user_prompt": "What are the recommended best practices for the Envisioning phase of an ML project?",
        "output_prompt": "In the Envisioning phase of an ML project, it is recommended to thoroughly understand the problem by consulting domain experts and stakeholders, clearly define customer goals, objectives, and key success metrics, perform a preliminary study of the available data sources, and align the project's expected outcomes with the business strategy.",
        "context": "For specific aspects of an ML project, refer to the guidelines for different project phases.\n\nML Project phases\n\nThe diagram below shows different phases in an ideal ML project. Due to practical constraints and requirements, it might not always be possible to have a project structured in such a manner, however best practices should be followed for each individual phase.\n\nEnvisioning: Initial problem understanding, customer goals and objectives.\n\nFeasibility Study: Assess whether the problem in question is feasible to solve satisfactorily using ML with the available data.\n\nModel Milestone: There is a basic model that is achieving the minimum required performance, both in terms of ML performance and system performance. Using the knowledge gathered to this milestone, define the scope, objectives, high-level architecture, definition of done and plan for the entire project.",
        "chunk_id": "chunk215_1",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/README.md",
        "root_chunk_id": "chunk215"
    },
    {
        "user_prompt": "Can Power Virtual Agents be integrated with any custom applications?",
        "output_prompt": "Yes, Power Virtual Agents can use the Direct Line channel to integrate with custom applications. There are examples of integrating with a generic Android application using Java, a Furhat robot using Kotlin, and a Misty robot using C# in the repository at https:\/\/github.com\/microsoft\/PowerVirtualAgentRobotsSamples.",
        "context": "Channels\n\nWith Power Virtual Agents, you can publish bots to engage with your customers on multiple platforms or channels. These include live websites, mobile apps, and messaging platforms like Microsoft Teams and Facebook.\n\nCustom Applications\n\nPower Virtual Agent bots can use the Direct Line channel to integrate with custom applications.\n\nThis repository: https:\/\/github.com\/microsoft\/PowerVirtualAgentRobotsSamples contains examples of connecting a Power Virtual Agent to three types of custom applications:\n\nA generic Android application, with Java\n\nA \"Furhat\" robot (from https:\/\/furhat.io, using Kotlin\n\nA \"Misty\" robot (from https:\/\/mistyrobotics.com, using C#",
        "chunk_id": "chunk255_1",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/conversational-ai\/index.md",
        "root_chunk_id": "chunk255"
    },
    {
        "user_prompt": "Who typically collaborates on feasibility studies according to the given context?",
        "output_prompt": "In the context provided, collaboration on feasibility studies typically includes data scientists, data engineers, software engineers, project managers (PMs), human experience researchers, and domain experts.",
        "context": "Who collaborates on feasibility studies?\n\nCollaboration from individuals with diverse skill sets is desired at this stage, including data scientists, data engineers, software engineers, PMs, human experience researchers, and domain experts. It embraces the use of engineering fundamentals, with some flexibility. For example, not all experimentation requires full test coverage and code review. Experimentation is typically not part of a CI\/CD pipeline. Artifacts may live in the main branch as a folder excluded from the CI\/CD pipeline, or as a separate experimental branch, depending on customer\/team preferences.\n\nWhat do feasibility studies entail?\n\nProblem definition and desired outcome\n\nEnsure that the problem is complex enough that coding rules or manual scaling is unrealistic\n\nClear definition of the problem from business and technical perspectives\n\nDeep contextual understanding",
        "chunk_id": "chunk218_2",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-feasibility-study.md",
        "root_chunk_id": "chunk218"
    },
    {
        "user_prompt": "Can you summarize the guidelines for managing documentation related to Microsoft docs links, lists, and images?",
        "output_prompt": "Of course! When managing documentation, ensure that links to Microsoft docs exclude language markers like \/en-us\/, as language preference is automatically detected. When constructing lists, use capital letters to start list items and choose between ordered and unordered lists based on whether the sequence is important. Ordered list items should be prefixed with '1.' to enable proper sequential numbering. Do not end list items with commas, semicolons, or periods unless forming a complete sentence. Images should be stored in a directory named 'img' with descriptive names to avoid confusion. Refrain from adding large images or videos directly to source control\u2014linking to an external source is preferred.",
        "context": "Make sure links to Microsoft docs do not contain the language marker \/en-us\/ or \/fr-fr\/, as this is automatically determined by the site itself.\n\nLists\n\nList items should start with capital letters if possible\n\nUse ordered lists when the items describe a sequence to follow, otherwise use unordered lists\n\nFor ordered lists, prefix each item with 1. When rendered, the list items will appear with sequential numbering. This avoids number-gaps in list\n\nDo not add commas , or semicolons ; to the end of list items, and avoid periods . unless the list item represents a complete sentence\n\nImages\n\nPlace images in a separate directory named img\n\nName images appropriately, avoiding generic names like screenshot.png\n\nAvoid adding large images or videos to source control, link to an external location instead",
        "chunk_id": "chunk14_10",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/markdown.md",
        "root_chunk_id": "chunk14"
    },
    {
        "user_prompt": "What should be our next step after summarizing the findings from the envisioning stage in predictive customer purchasing behavior?",
        "output_prompt": "The next step would be to confirm that you have access to all the necessary resources, including data, before proceeding with data exploration workshops. You can use the provided Summary of Scope Exit Document Template and List of Resource Access Questions to ensure all resources are accessible for the next phase.",
        "context": "Based on the behavior of customers with similar purchasing histories, we are able to predict certain items that a customer is likely to purchase; with a certainty which is greater than random choice.\n\nUsing sets of items which have previously been sold together, we can formulate rules around the features which determine whether items are complementary or not which can be verified by a stylist.\n\nNext Steps\n\nTo ensure clarity and alignment, it is useful to summarize the envisioning stage findings focusing on proposed detailed scenarios, assumptions and agreed decisions as well next steps.\n\nWe suggest confirming that you have access to all necessary resources (including data) as a next step before proceeding with data exploration workshops.\n\nBelow are the links to the exit document template and to some questions which may be helpful in confirming resource access.\n\nSummary of Scope Exit Document Template\n\nList of Resource Access Questions",
        "chunk_id": "chunk219_13",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-problem-formulation-envisioning.md",
        "root_chunk_id": "chunk219"
    },
    {
        "user_prompt": "How should developers ensure that their Terraform scripts are consistent with our style guide and free of common errors?",
        "output_prompt": "Developers should utilize tools like TFLint to lint their Terraform scripts, adhering to best practices and identifying potential errors. They should also leverage the Terraform and Azure Terraform extensions in VS Code for syntax highlighting, linting, formatting, validation, command support and resource graph visualization. Additionally, integrating these tools into automated build validation steps ensures consistency and quality in Terraform scripts.",
        "context": "Terraform Code Reviews\n\nStyle Guide\n\nDevelopers should follow the terraform style guide.\n\nProjects should check Terraform scripts with automated tools.\n\nCode Analysis \/ Linting\n\nTFLint\n\nTFLint is a Terraform linter focused on possible errors, best practices, etc. Once TFLint installed in the environment, it can be invoked using the VS Code terraform extension.\n\nVS Code Extensions\n\nThe following VS Code extensions are widely used.\n\nTerraform extension\n\nThis extension provides syntax highlighting, linting, formatting and validation capabilities.\n\nAzure Terraform extension\n\nThis extension provides Terraform command support, resource graph visualization and CloudShell integration inside VS Code.\n\nBuild Validation",
        "chunk_id": "chunk19_0",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/terraform.md",
        "root_chunk_id": "chunk19"
    },
    {
        "user_prompt": "How can I query multiple separate indexes at the same time?",
        "output_prompt": "To query multiple separate indexes simultaneously, you can use a multi-index query feature provided by your search API. This allows you to send a single request specifying all the target indexes. If your search API does not support multi-index queries, you'll need to send separate requests to each index and then aggregate the results yourself. Additionally, some search APIs can manage query intent at the API level, enabling them to infer the appropriate index to search based on the provided query parameters.",
        "context": "Separated multiple indexes\/indices require multiple queries to different indexes\/indices at the same time or manage query intent in Query API level to detect which index will be selected to search.",
        "chunk_id": "chunk315_14",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/search\/index.md",
        "root_chunk_id": "chunk315"
    },
    {
        "user_prompt": "Given the limitations of using Aravis for connecting multiple cameras at the same time, what approach can be taken to circumvent this issue while still using Genicam compatible cameras?",
        "output_prompt": "To handle the simultaneous connection to multiple Genicam compatible cameras when using Aravis has limitations, one could consider implementing a process where each camera is accessed by a separate instance of the Aravis library running in a different process or container. This way, each camera's connection is isolated, avoiding the static limitations of Aravis. Alternatively, utilizing the vendors' specific SDKs may provide a more robust solution if they support multiple camera connections and are compatible with the system's requirements.",
        "context": "Aravis is a glib\/gobject based library for video acquisition using Genicam cameras. It currently implements the gigabit ethernet and USB3 protocols used by industrial cameras. It also provides a basic ethernet camera simulator and a simple video viewer. More information about Aravis can be found here. Aravis GitHub Repo. Using Aravis in a container directly and a C++ implementation would have limitations because the Aravis implementation uses statics and does not allow to connect with multiple cameras at the same time.\n\nThe last option would be to use vendors specific SDK, that is a good option if you are vendor locked, now this forces to use C++ and for a lot of them, there is as well the limitation for the streaming which does not allow to connect to multiple cameras at the same time.\n\nNon-Functional Requirements",
        "chunk_id": "chunk375_4",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/computer-vision-on-edge\/camera-interface.md",
        "root_chunk_id": "chunk375"
    },
    {
        "user_prompt": "How do I set up local blob storage with Azurite and attach it to the Azure Storage Explorer?",
        "output_prompt": "To set up local blob storage using Azurite and attach it to the Azure Storage Explorer, you need to follow these steps: first, run the Azurite Docker container with the command 'docker run -p 10000:10000 mcr.microsoft.com\/azure-storage\/azurite azurite-blob --blobHost 0.0.0.0'. After that, open Azure Storage Explorer, choose 'Attach to a local emulator', and provide a display name and the port number you used when starting the Azurite service, which is 10000. Your local blob storage is now ready, and you can manage it using the Azure Storage Explorer. Also remember to create a virtual Python environment with 'python -m venv .venv'.",
        "context": "{% endraw %}\n\nOption 2: using docker\n\n{% raw %}\n\nbash\n   docker run -p 10000:10000 mcr.microsoft.com\/azure-storage\/azurite azurite-blob --blobHost 0.0.0.0\n\n{% endraw %}\n\nIn Azure Storage Explorer, select Attach to a local emulator\n\nProvide a Display name and port number, then your connection will be ready, and you can use Storage Explorer to manage your local blob storage.\n\nTo test and see how these endpoints are running you can attach your local blob storage to the Azure Storage Explorer.\n\nCreate a virtual python environment\n   python -m venv .venv",
        "chunk_id": "chunk72_7",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/tech-specific-samples\/blobstorage-unit-tests\/README.md",
        "root_chunk_id": "chunk72"
    },
    {
        "user_prompt": "How does microservice B handle incoming trace-id and parent-id headers according to the given context?",
        "output_prompt": "Upon receiving an incoming HTTP request with trace-id and parent-id headers, microservice B sets the value of its parent-id to the incoming trace-id, creates a new trace-id for its own telemetry scope, and includes these two headers when calling microservice C, thus propagating the trace-id and parent-id.",
        "context": "When microservice B receives the incoming HTTP request, it checks the contents of these two headers. It reads the content of the trace-id header and sets its own parent-id to this trace-id (as shown in the green rectangle inside's B). In addition, it creates a new trace-id to signal that is a new scope for the telemetry. During the execution of microservice B, it also calls microservice C and repeats the pattern. As part of the request it includes the two headers and propagates trace-id and parent-id as well.\n\nFinally, microservice C, reads the value for the incoming trace-id and sets as his own parent-id, but also creates a new trace-id that will use to send telemetry about his own operations.\n\nSummary",
        "chunk_id": "chunk181_5",
        "source": "..\/data\/docs\/code-with-engineering\/observability\/microservices.md",
        "root_chunk_id": "chunk181"
    },
    {
        "user_prompt": "How can we ensure consistent naming conventions and share common variables between Terraform modules like a Network module and an Azure Kubernetes Service module?",
        "output_prompt": "To ensure consistent naming conventions and share common variables between Terraform modules, you can create a central module that defines these variables and output them. Each module can then use these shared outputs as inputs. Additionally, using a naming convention module or a standards file where rules and formats are predefined can help maintain consistency across resources and modules.",
        "context": "Sharing Common Variables \/ Naming Conventions Between Terraform Modules\n\nWhat are we trying to solve?\n\nWhen deploying infrastructure using code, it's common practice to split the code into different modules that are responsible for the deployment of a part or a component of the infrastructure. In Terraform, this can be done by using modules.\n\nIn this case, it is useful to be able to share some common variables as well as centralize naming conventions of the different resources, to ensure it will be easy to refactor when it has to change, despite the dependencies that exist between modules.\n\nFor example, let's consider 2 modules:\n\nNetwork module, responsible for deploying Virtual Network, Subnets, NSGs and Private DNS Zones\n\nAzure Kubernetes Service module responsible for deploying AKS cluster",
        "chunk_id": "chunk133_0",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/recipes\/terraform\/share-common-variables-naming-conventions.md",
        "root_chunk_id": "chunk133"
    },
    {
        "user_prompt": "Could you explain to me what End-to-End (E2E) testing encompasses and how it differs from other types of testing described in the pyramid?",
        "output_prompt": "Certainly! End-to-End (E2E) testing is an approach where the entire application is tested in a real-world scenario such as communicating with the database, network, hardware, and other applications. It aims to validate the integration and flows of the application from start to finish. Unlike unit testing, which tests the smallest parts of an application in isolation, or integration testing, which focuses on the interaction between interconnected modules, E2E testing examines the entire application in a configuration that mimics actual users' operations. System testing is another layer that tests the complete and integrated software product, but E2E testing goes a step further by not only checking the system itself but also its integration with external interfaces, making it more comprehensive. As noted, the higher you go in the test pyramid, the slower and more costly these tests become.",
        "context": "The majority of your tests are at the bottom of the pyramid. As you move up the pyramid, the number of tests gets smaller. Also, going up the pyramid, tests get slower and more expensive to write, run, and maintain. Each type of testing vary for its purpose, application and the areas it's supposed to cover. For more information on comparison analysis of different testing types, please see this ## Unit vs Integration vs System vs E2E Testing document.\n\nE2E Testing Design Blocks [The What]\n\nWe will look into all the 3 categories one by one:\n\nUser Functions\n\nFollowing actions should be performed as a part of building user functions:\n\nList user initiated functions of the software systems, and their interconnected sub-systems.\n\nFor any function, keep track of the actions performed as well as Input and Output data.",
        "chunk_id": "chunk81_2",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/README.md",
        "root_chunk_id": "chunk81"
    },
    {
        "user_prompt": "What measures should I take to ensure the security and privacy of my Generative AI application?",
        "output_prompt": "To ensure the security and privacy of your Generative AI application, it's important to implement robust access controls, use encryption for data at rest and in transit, regularly update and patch your systems, conduct vulnerability assessments and penetration tests, anonymize or pseudonymize sensitive data, comply with relevant data protection regulations, and establish incident response plans.",
        "context": "Monitoring & Observability\n\nThis involves tracking the performance and health of a Generative AI application. It includes collecting metrics, logs, and traces to gain visibility into the system\u2019s operations and to understand its state at any given moment. In Generative AI, this could mean monitoring the model\u2019s performance, data throughput, and response times to ensure the system is functioning as intended.\n\nSecurity & Privacy\n\nThese are critical aspects that encompass protecting the Generative AI application from unauthorized access and ensuring the confidentiality, integrity, and availability of data. It also involves safeguarding the privacy of the data used by the application, which includes implementing measures to prevent data breaches and leaks and ensuring compliance with data protection regulations.\n\nData Platform",
        "chunk_id": "chunk259_5",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/index.md",
        "root_chunk_id": "chunk259"
    },
    {
        "user_prompt": "What are the implications of not considering conversation history in language model responses?",
        "output_prompt": "Not considering the conversation history can result in language models providing responses that are out of context. It may lead to misunderstandings or irrelevant answers since the model won't be able to relate current questions to previous dialogue.",
        "context": "History\n\nPrompt engineering using session history involves tracking the history of a conversation between the user and the language model. This method can help the language model generate more accurate responses by taking into account the context of the conversation. Here's an example of how LLMs track the history of conversation to help generate accurate responses:\n\ntext\nUser: The capital of India?\nLLM: The capital of India is New Delhi.\nUser: What is the population of this city?\nLLM: As of 2021, the estimated population of New Delhi is around 31.8 million people.\n\nIn this scenario, the LLM is able to use the history of the conversation to understand \"this city\" refers to \"New Delhi\".",
        "chunk_id": "chunk284_22",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/prompt-engineering\/prompt-engineering.md",
        "root_chunk_id": "chunk284"
    },
    {
        "user_prompt": "How do I create a new service connection for Azure ML in Azure DevOps?",
        "output_prompt": "To create a new service connection for Azure ML in Azure DevOps, sign in to your Azure DevOps organization and select your project. Go to Project settings > Service connections and click on '+ New service connection'. Choose 'Generic' as the service connection type and click 'Next'. Fill in the Server URL using the format 'https:\/\/{region}.experiments.azureml.net\/webhook\/v1.0\/' where {region} corresponds to the region of your Azure ML workspace. Since the request to Azure ML will be authorized with the necessary information in the body, username and password entries are not required. Finally, name your service connection and click 'Save'.",
        "context": "Sign in to your organization (https:\/\/dev.azure.com\/{yourorganization}}) and select your project.\n\nSelect Project settings > Service connections.\n\nSelect + New service connection, select Generic as the type of service connection that you need, and then select Next.\n\nFill in the Server URL it should be in the following form: <https:\/\/{region}.experiments.azureml.net\/webhook\/v1.0\/> where region is the region of your Azure ML workspace.\n\nThe request to Azure ML will be authorized by providing the necessary information in the body so user name and password are not need to be filled in here.\n\nSpecify the name of the service connection.\n\nSelect Save.",
        "chunk_id": "chunk309_3",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/azure-ml-from-azdo.md",
        "root_chunk_id": "chunk309"
    },
    {
        "user_prompt": "I'm setting up a DocFx documentation project, and I'm not sure where to place my 'api' folder for the C# projects that I want to document. Considering the current structure given in the context, where should the 'api' folder be located?",
        "output_prompt": "You should place your 'api' folder inside the 'x-cross' directory if you are dealing with cross-project references and you want DocFx to include them in the documentation. Alternatively, it can also reside at the root level alongside 'build' and 'x-cross' directories if it's meant to cover all .NET projects directly without specific cross-references.",
        "context": "\u2502   \u251c\u2500\u2500 build              \/\/ build settings\n\u2502          \u251c\u2500\u2500 dotnet     \/\/ .NET build settings\n\u2502   \u251c\u2500\u2500 Directory.Build.props   \/\/ project settings for all .NET projects in sub folders\n\u2502   \u251c\u2500\u2500 [Project folders]\n\u2502\n\u251c\u2500\u2500 x-cross\n\u2502   \u251c\u2500\u2500 toc.yml              \/\/ Cross reference definition (optional)\n\u2502\n\u251c\u2500\u2500 .markdownlint.json \/\/ Markdownlinter settings\n\u251c\u2500\u2500 docfx.json               \/\/ DocFx configuration",
        "chunk_id": "chunk110_5",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/using-docfx-and-tools.md",
        "root_chunk_id": "chunk110"
    },
    {
        "user_prompt": "How can I undo a commit in Git that has already been pushed to the remote repository?",
        "output_prompt": "To undo a commit that has been pushed to the remote repository, you have several options. For a single commit, you can perform a hard reset using `git reset --hard <sha1-commit-id>` and then force push with `git push origin HEAD --force`. If you need to undo multiple commits, you can use interactive rebase with `git rebase -i HEAD~N`, where 'N' is the number of commits to review. In the interactive session, simply delete the lines corresponding to the commits you want to undo and save the file. Be cautious with these operations as they can alter the history on the remote repository and affect other users.",
        "context": "bash\ngit reset --hard <sha1-commit-id>\n\n{% endraw %}\n\nafter you deleted the unwanted commits, push using force:\n\n{% raw %}\n\nbash\ngit push origin HEAD --force\n\n{% endraw %}\n\nInteractive rebase for undoing commits:\n\n{% raw %}\n\nbash\ngit rebase -i HEAD~N\n\n{% endraw %}\n\nThe above command will open an interactive session in an editor (for example vim) with the last N commits sorted from oldest to newest. To undo a commit, delete the corresponding line of the commit and save the file. Git will rewrite the commits in the order listed in the file and because one (or many) commits were deleted, the commit will no longer be part of the history.",
        "chunk_id": "chunk206_26",
        "source": "..\/data\/docs\/code-with-engineering\/source-control\/git-guidance\/README.md",
        "root_chunk_id": "chunk206"
    },
    {
        "user_prompt": "What differentiates a question-answering system from systematic review searches in terms of search methodology?",
        "output_prompt": "In a question-answering system, the search methodology often involves a multi-stage approach that can include using services like Azure search for initial retrieval and then applying complex NLP techniques like summarization for generating answers. In contrast, systematic reviews or legal searches are recall-oriented, aiming to retrieve all relevant documents, and they demand domain-specific indexing and search strategies to ensure comprehensiveness.",
        "context": "Question-answering system: When we are developing a question answering system, we may consider a multi-stage system where the first stage is built on top of Azure search. For the second stage, we can implement complex natural language processing techniques (e.g. summarization) to generate answers.\n\nSystematic review or legal search: These types of retrieval problems are domain-specific, and they are recall-based, which aims to find all relevant documents.\n\nPub\/Sub systems: This is a type of search which we refer to as \"document routing\", where users\u2019 queries are often fixed, and documents come in a streaming fashion. In these systems, we may index user queries using Azure search, and route documents to the relevant queries.",
        "chunk_id": "chunk315_2",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/search\/index.md",
        "root_chunk_id": "chunk315"
    },
    {
        "user_prompt": "How can I speed up my CI\/CD pipeline testing when dealing with expensive and time-consuming processes such as maintaining a Databricks cluster?",
        "output_prompt": "To speed up your CI\/CD pipeline testing, you can mock the responses for these expensive processes. This way, you can work on the rest of the workflow without the actual execution of the time-consuming tasks, especially if these responses are not being directly tested or are not critical to the current aspect of development you are focusing on.",
        "context": "The workflow inherently takes time and is expensive to run, as it involves maintaining a Databricks cluster while also waiting for the response. This external dependency can be removed by essentially mocking the response for the duration of writing and testing other parts of the workflow, and mocking the response in situations where the actual response either does not matter, or is not being directly tested.\n\nSkipping Long CI processes\n\nDeveloper B is in the process of writing and testing a CI\/CD pipeline. The pipeline has multiple CI stages, each of which runs sequentially. The workflow might look like this:",
        "chunk_id": "chunk130_18",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/devops-provider-recipes\/github-actions\/runtime-variables\/README.md",
        "root_chunk_id": "chunk130"
    },
    {
        "user_prompt": "What are the main benefits of implementing an MLOps Model Factory?",
        "output_prompt": "The main benefits of implementing an MLOps Model Factory include streamlined model management and deployment, improved scalability, deployment consistency, enhanced collaboration, better governance, facilitated experimentation, continuous improvement, and cost optimization. It leads to more efficient, reliable, and agile machine learning model lifecycle management.",
        "context": "Inhibited collaboration between data science and operations teams.\n\nReduced ability to monitor and detect drift in ML models.\n\nSolution overview\n\nMLOps Model Factory provides a structured platform for generating, organizing, versioning, and tracking multiple ML models. It makes it easier to manage and keep track of models throughout their lifecycle. Implementing MLOps Model factory addresses the following challenges:\n\nModel management complexity\n\nScalability\n\nDeployment consistency\n\nCollaboration\n\nGovernance\n\nExperimentation\n\nContinuous improvement\n\nCost optimization\n\nIt streamlines the management and deployment of multiple models. The factory improves efficiency, reliability, and agility across environments.\n\nValue proposition\n\nMLOps Model Factory provides help with:\n\nEnabling transparent and reproducible ML model training for many models.",
        "chunk_id": "chunk394_3",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/custom-machine-learning\/mlops-model-factory\/index.md",
        "root_chunk_id": "chunk394"
    },
    {
        "user_prompt": "How do I configure devcontainer.json to use a docker-compose.yml file for my development environment?",
        "output_prompt": "To configure devcontainer.json to use a docker-compose.yml file, you need to specify the dockerComposeFile property with an array containing 'docker-compose.yml', and set the service property to the name of your service, for example 'my-workspace'. Your devcontainer.json will look something like this: \n\n{\n    \"name\": \"My Application\",\n    \"dockerComposeFile\": [\"docker-compose.yml\"],\n    \"service\": \"my-workspace\"\n    ...\n}\n\nThis configuration tells the development container to use Docker Compose and the specified service within the compose file as the primary container for the development environment.",
        "context": "{% endraw %}\n\nTo use the docker-compose.yml file instead of Dockerfile, we need to adjust devcontainer.json with:\n\n{% raw %}\n\njson\n{\n    \"name\": \"My Application\",\n    \"dockerComposeFile\": [\"docker-compose.yml\"],\n    \"service\": \"my-workspace\"\n    ...\n}\n\n{% endraw %}\n\nThis approach can be applied for many other tools by preparing what would be required. The idea is to simplify developers' lives and new developers joining the project.\n\nCustom tools",
        "chunk_id": "chunk246_6",
        "source": "..\/data\/docs\/code-with-engineering\/developer-experience\/going-further.md",
        "root_chunk_id": "chunk246"
    },
    {
        "user_prompt": "Could you outline the steps for conducting a successful Design Review?",
        "output_prompt": "Certainly! To conduct a successful Design Review, you\u2019ll need to follow these steps: 1. Agree on the feature to be designed with the relevant stakeholders. 2. Determine the required participants for the review, usually including designers, developers, and product managers. 3. Choose the right tools to capture input, such as collaborative design software or document sharing platforms. 4. Set a clear timeline for the review process including deadlines for feedback. 5. Discuss the facilitation of the meeting, ensuring there's a clear meeting structure and someone to guide the conversation. 6. During the review, encourage constructive feedback and solution-oriented discussions, and document all decisions and action items. 7. Follow-up after the Design Review to implement the feedback and update design documents accordingly.",
        "context": "[ ] Agree on how to Design a feature and conduct a Design Review\n\nDay 5\n\n[ ] Conduct a Sprint Demo\n\n[ ] Conduct a Retrospective\n\nDetermine required participants, how to capture input (tools) and outcome\n\nSet a timeline, and discuss facilitation, meeting structure etc.\n\n[ ] Refine the Backlog\n\nDetermine required participants\n\nUpdate the Definition of Ready\n\nUpdate estimates, and the Estimation document\n\n[ ] Submit Engineering Feedback for issues encountered",
        "chunk_id": "chunk0_4",
        "source": "..\/data\/docs\/code-with-engineering\/SPRINT-STRUCTURE.md",
        "root_chunk_id": "chunk0"
    },
    {
        "user_prompt": "What is the process for converting JSON data into a human-readable format?",
        "output_prompt": "To convert JSON data into a human-readable format, one can use tools or libraries that format the JSON structure with indentation and line breaks to enhance readability. For instance, online JSON formatters, browser extensions, or development environments with built-in JSON parsers can achieve this. Additionally, some programming languages provide libraries, such as Python's 'json' module, which has the 'json.dumps()' method with the 'indent' parameter to pretty-print JSON data.",
        "context": "}\n    }\n}",
        "chunk_id": "chunk256_14",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Considering all cameras implement GenICam interfaces and are accessible via the same network as the VM\/Edge\/Kubernetes, can we directly use the nuget package for basic GenICam implementation?",
        "output_prompt": "It is not recommended to directly use the nuget package for basic GenICam implementation as it requires significant modifications to suit the specific needs of this project.",
        "context": "We assume as that all camera implement GenICam interfaces allowing to take pictures, adjust some of the elements.\n\nWe assume that the cameras are on the same network as the VM\/Edge\/Kubernetes and the UDP protocol is not filtered and freely accessible as the GenICam protocol is UDP based.\n\nDesign\n\nWe propose to reuse and leverage the code from Gige Vision cameras. While a nuget is available for basic GenICam implementation, the recommendation is not to use directly the nuget as quite some modifications has been done.",
        "chunk_id": "chunk375_2",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/computer-vision-on-edge\/camera-interface.md",
        "root_chunk_id": "chunk375"
    },
    {
        "user_prompt": "Can you explain how to operationalize data drift monitoring in Azure Machine Learning for batch inferencing scenarios?",
        "output_prompt": "Certainly! To operationalize data drift monitoring in Azure Machine Learning for batch inferencing, you need to first set up a baseline dataset and a target dataset. The baseline dataset is the original dataset used to train the model, and the target dataset includes the new data collected from batch inferencing. You then configure data drift detection by creating a Data Drift Detector in Azure ML, linking it to both the baseline and target datasets. Once the Data Drift Detector is in place, regular monitoring is established by scheduling drift analysis runs, which will compare the datasets and provide drift metrics. If data drift is detected beyond a set threshold, notifications can be triggered to alert the relevant stakeholders.",
        "context": "Data Drift Operationalization in Azure ML\n\nIntroduction\n\nIn this document we are planning to explain how to operationalize and automate custom data drift related workloads in Azure Machine Learning. In general, data drift components should be deployed in sync with inferencing infrastructure, because the inferencing service is responsible for collecting new data. The data drift components can be injected differently based on the inferencing scenario, for example:\n\nreal-time inferencing for supervised Machine Learning\n\nbatch inferencing for supervised Machine Learning\n\ninferencing for unsupervised Machine Learning scenarios (like clustering)\n\nWe are not evaluating near real-time inferencing scenarios for supervised ML because they are similar to real-time from a data drift operationalization perspective.\n\nWe will discuss the following questions in this document:",
        "chunk_id": "chunk307_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/datadrift_operationalization.md",
        "root_chunk_id": "chunk307"
    },
    {
        "user_prompt": "Can you explain how MLOps relates to DevOps and what roles are involved?",
        "output_prompt": "MLOps, akin to DevOps, involves collaboration between cross-disciplinary teams to streamline and automate the machine learning lifecycle. It encompasses roles such as Data Scientists, ML Engineers, DevOps Engineers, and Business Analysts. For detailed roles and frameworks, refer to the 'Roles and Frameworks' section.",
        "context": "Mitigation: MLOps is a cross discipline and collaborative function that has the same requirements as DevOps. Refer to the Roles and Frameworks section for more info.",
        "chunk_id": "chunk311_2",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/before-engagement-start\/mlops-anti-patterns.md",
        "root_chunk_id": "chunk311"
    },
    {
        "user_prompt": "What are the best practices for a programmer to effectively address comments on a code review?",
        "output_prompt": "To effectively address comments on a code review, a programmer should: resolve comments after making the requested changes, mark comments as 'won't fix' with clear reasons if not making changes, avoid using 'I'll do it later' for in-scope requests, create new tasks for out-of-scope requests, seek clarification within the review thread if a comment is not understood, and arrange a meeting if the discussion is protracted without resolution. It's also beneficial to use checklists to track required actions.",
        "context": "Be open to receive feedback\n\nDiscuss design\/code logic and address all comments as follows:\n\nResolve a comment, if the requested change has been made.\n\nMark the comment as \"won't fix\", if you are not going to make the requested changes and provide a clear reasoning\n\nIf the requested change is within the scope of the task, \"I'll do it later\" is not an acceptable reason!\n\nIf the requested change is out of scope, create a new work item (task or bug) for it\n\nIf you don't understand a comment, ask questions in the review itself as opposed to a private chat\n\nIf a thread gets bloated without a conclusion, have a meeting with the reviewer (call them or knock on door)\n\nUse checklists",
        "chunk_id": "chunk12_1",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/process-guidance\/author-guidance.md",
        "root_chunk_id": "chunk12"
    },
    {
        "user_prompt": "What does a class name of 'NOK' with a score of 0.8136 indicate?",
        "output_prompt": "The class name 'NOK' typically stands for 'Not Okay' or indicates a negative result. A score of 0.8136 suggests a high confidence level in this assessment.",
        "context": "\"className\": \"NOK\",\n            \"score\": 0.8136\n        }\n    }\n}",
        "chunk_id": "chunk256_10",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Can BugBug be a suitable replacement for Selenium in our testing workflow?",
        "output_prompt": "Yes, BugBug can be a suitable replacement for Selenium, especially if user-friendliness and ease of maintenance are your main concerns. It offers a more modern approach to testing with its Chrome extension-based platform, potentially allowing for faster test creation and execution. However, whether it's the right tool for you also depends on your specific testing requirements and the complexity of the applications you're testing.",
        "context": "BugBug is an interesting alternative to Selenium because it's actually a completely different technology. It is based on a Chrome extension that allows BugBug to record and run tests faster than old-school frameworks.\n\nThe biggest advantage of BugBug is its user-friendliness. Most tests created with BugBug simply work out of the box. This makes it easier for non-technical people to maintain tests - with BugBug you can save money on hiring a QA engineer.\n\nBugBug Website\n\nConclusion\n\nHope you learned various aspects of E2E testing like its processes, metrics, the difference between Unit, Integration  and E2E testing, and the various recommended E2E test frameworks and tools.",
        "chunk_id": "chunk81_14",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/README.md",
        "root_chunk_id": "chunk81"
    },
    {
        "user_prompt": "How should valid JSON for a password validation test look in a document?",
        "output_prompt": "A valid JSON for a password validation test should define the test parameters and expected results. Here is an example: {\n    \"testName\": \"Password Validation Test\",\n    \"input\": \"yourPassword123\",\n    \"expectedException\": \"ArgumentNullException\",\n    \"expectedOutcome\": \"The function should throw ArgumentNullException for null input.\"\n}",
        "context": "{% endraw %}\n\nThis test will pass as it still only throws an ArgumentNullException, but again, that is an expected failure. Fixing\nour function should see it pass:\n\n{% raw %}\n\n```csharp\npublic bool ValidatePassword(string input)\n{\n    if (input == null)\n    {\n        throw new ArgumentNullException(nameof(input));\n    }\n\n}",
        "chunk_id": "chunk90_5",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/unit-testing\/tdd_example.md",
        "root_chunk_id": "chunk90"
    },
    {
        "user_prompt": "Can you explain when it's appropriate to use the exhaustive KNN algorithm?",
        "output_prompt": "The exhaustive KNN algorithm is appropriate for situations where getting the most accurate results is paramount, and you can afford the computational cost. It's suitable for small to medium datasets where high recall is more important than search speed.",
        "context": "Exhaustive KNN is a brute-force search that searches the entire vector space for documents that are most similar to the query. It does this by calculating the distances between all pairs of data points and finding the exact k nearest neighbors for a query point.\n  When to use:\n  This algorithm may be used for cases where high recall is more important than search performance. Because it's computationally intensive, exhaustive KNN may be used for small to medium datasets but not for large datasets.",
        "chunk_id": "chunk315_7",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/search\/index.md",
        "root_chunk_id": "chunk315"
    },
    {
        "user_prompt": "Can you tell me how \u2018Adaptability\u2019 is defined in terms of workplace behavior?",
        "output_prompt": "Adaptability in the workplace is defined by certain behaviors, such as actively seeking information and testing assumptions, along with the ability to shift one's approach in response to changing situations.",
        "context": "}\n      ]\n    }\n  ],\n  \"competencies\": [\n    {\n      \"id\": \"abc123\",\n      \"title\": \"Adaptability\",\n      \"behaviors\": [\n        { \"description\": \"Actively seeks information and tests assumptions.\" },\n        {\n          \"description\": \"Shifts his or her approach in response to the demands of a changing situation.\"\n        }\n      ]\n    },\n    {\n      \"id\": \"abc123\",",
        "chunk_id": "chunk42_14",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/examples\/memory\/Architecture\/Data-Model.md",
        "root_chunk_id": "chunk42"
    },
    {
        "user_prompt": "Can you explain the importance of using code formatters and linters in JavaScript\/TypeScript projects?",
        "output_prompt": "Sure, code formatters like Prettier help standardize the formatting of your code base, making it more readable and maintainable. It enforces a consistent style by parsing your code and re-printing it with its own rules, leading to fewer stylistic debates during code reviews. Linters like ESLint for JavaScript and typescript-eslint for TypeScript analyze your code to quickly find problems. These tools perform static analysis to check for syntax errors, potential bugs, stylistic errors, and adherence to style guides. By following the AirBnB Style Guide with ESLint, developers can ensure higher quality code and better collaboration across different team members.",
        "context": "JavaScript\/TypeScript Code Reviews\n\nStyle Guide\n\nDevelopers should use prettier to do code formatting for JavaScript.\n\nUsing an automated code formatting tool like Prettier enforces a well accepted style guide that was collaboratively built by a wide range of companies including Microsoft, Facebook, and AirBnB.\n\nFor higher level style guidance not covered by prettier, we follow the AirBnB Style Guide.\n\nCode Analysis \/ Linting\n\neslint\n\nPer guidance outlined in Palantir's 2019 TSLint road map,\nTypeScript code should be linted with ESLint. See the typescript-eslint documentation for more information around linting TypeScript code with ESLint.",
        "chunk_id": "chunk22_0",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/javascript-and-typescript.md",
        "root_chunk_id": "chunk22"
    },
    {
        "user_prompt": "Can you explain the process of deploying changes from a development environment to production in a DevOps pipeline?",
        "output_prompt": "Yes, the deployment process typically involves the following steps:\n\n1. In the development environment, individual developers or the team as a whole make changes to the software. Once the changes are finalized, the solution is packaged and exported into source control to track the modifications.\n\n2. Then, a 'Just in Time' (JIT) environment is used. Here, the unmanaged solution from the Dev environment is imported and then exported as a managed solution. This managed solution is not checked into source control but is stored as a build artifact in the pipeline. This artifact is then ready to be used in the release pipeline.\n\n3. The managed solution build artifact from the JIT environment is deployed to other environments like Staging or Production. These environments receive the managed solution, ensuring that the iterations in development are systematically and safely deployed in the production environment.",
        "context": "Development team will be working in a Dev environment. These environments according to the team could be one for the team or one for each developer.\n\nOnce changes have been made, the first step will be packaging the solution and export it into source control.\n\nStep 2\n\nSecond step is about the solution, you need to have a managed solution to deploy to other environments such as Stage or Production so now you should use a JIT environment where you would import your unmanaged solution and export them as managed. These solution files won't be checked into source control but will be stored as a build artifact in the pipeline making them available to be deployed in the release pipeline. This is where the second environment will be used. This second environment will be responsible of receiving the output managed solution coming from the artifact.\n\nStep 3",
        "chunk_id": "chunk129_3",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/low-code-solutions\/README.md",
        "root_chunk_id": "chunk129"
    },
    {
        "user_prompt": "Can you explain how the Data Drift Pipeline works in the context of unsupervised machine learning workloads?",
        "output_prompt": "Certainly! For unsupervised ML workloads, as there is no model to make predictions, the Data Drift Pipeline mainly focuses on ensuring the datasets are properly prepared. In the case of a clustering task, for instance, the pipeline would be responsible for making sure that the dataset is available for clustering processes. A baseline dataset could be kept static, or it could be updated or merged with new data, providing that no drift has been detected. The pipeline itself can operate independently and can be triggered on a set schedule, when new data is added to blob storage, or orchestrated following unsupervised ML workload activities, which is the most advisable approach.",
        "context": "We will not discuss unsupervised ML workloads at depth as the Data Drift Pipeline operation is very simple for these scenarios. As we don't have a model the inferencing infrastructure is solely concerned with preparing the datasets correctly. For example, if we have clustering, a task of the inferencing service will be to host a dataset for clustering in a way to make it available to Data Drift Pipeline. The baseline dataset can be fixed or we can update (or merge) it to new one if no drift has been detected. Therefore, Data Drift Pipeline can be treated as an independent artifact and we can deploy it on its own. Triggering of the pipeline can be done based on a schedule (rare), or on new data added to the blob storage (common) or orchestrated right after unsupervised ML workloads (the most recommended, but an orchestrator is",
        "chunk_id": "chunk307_15",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/datadrift_operationalization.md",
        "root_chunk_id": "chunk307"
    },
    {
        "user_prompt": "How do I reference a specific version of a pipeline in a JSON object?",
        "output_prompt": "To reference a specific version of a pipeline in a JSON object, you would include the 'pipelineid' and 'pipelineversion' as keys in the object and assign them the appropriate integer values. For example: { 'pipelineid': 123, 'pipelineversion': 1 } where '123' is the pipeline ID and '1' is the version number you are referring to.",
        "context": "type: string\npipelineid:\n    type: integer\npipelineversion:\n    type: integer",
        "chunk_id": "chunk303_20",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/sync-async-mlops-patterns.md",
        "root_chunk_id": "chunk303"
    },
    {
        "user_prompt": "How should I set up my script to effectively manage data drift in an unsupervised machine learning environment?",
        "output_prompt": "To manage data drift in an unsupervised ML environment, ensure your script can access both baseline and new datasets. Group new data by timeframes to detect drift. For batch inferencing or unsupervised algorithms, update the full dataset as needed. Provide adequate computational resources, possibly beyond a single CPU or machine, to process the data efficiently.",
        "context": "The script itself should have access to a training dataset that has been used to train the current model. In the case of unsupervised ML, it can be a baseline dataset that we are holding from a previous algorithm execution or that we used initially to test the selected Machine Learning approach.\n\nThe script should have access to new data with the ability to group them by timeframes in order to execute the data drift algorithm on a new (most recent batch from previous execution) batch of data. In the case of batch inferencing or for unsupervised algorithms, the inferencing service can update the entire dataset at once, and filtering might be not required.\n\nThe script requires some computational power that might not be limited to CPU or to a single machine (node\/instance) only.",
        "chunk_id": "chunk307_2",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/datadrift_operationalization.md",
        "root_chunk_id": "chunk307"
    },
    {
        "user_prompt": "What steps should I take to effectively tune the system's performance and ensure its readiness for release?",
        "output_prompt": "To effectively tune the system's performance, you should start by identifying bottlenecks at various load levels using performance testing tools. Compare the system's performance across different configurations to find the optimal setup. Develop a scaling strategy that can handle increased loads, potentially by incorporating auto-scaling cloud services or load balancing. For capacity planning, analyze business expectations, usage patterns, and infrastructure costs to determine the required resources. Finally, systematically assess the system against your performance goals to determine readiness for release.",
        "context": "Tune the system's performance\n\nIdentifying bottlenecks and issues with the system at different load\n    levels.\n\nComparing performance characteristics of the system for different system\n    configurations.\n\nCome up with a scaling strategy for the system.\n\nAssist in capacity planning\n\nCapacity planning is the process of determining what type of hardware and\n    software resources are required to run an application to support pre-defined performance goals.\n\nCapacity planning involves identifying business\n    expectations, the periodic fluctuations of application usage, considering\n    the cost of running the hardware and software infrastructure.\n\nAssess the system's readiness for release:",
        "chunk_id": "chunk69_1",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/performance-testing\/README.md",
        "root_chunk_id": "chunk69"
    },
    {
        "user_prompt": "What are the key characteristics required for fault injection tools to be compatible with Kubernetes workloads?",
        "output_prompt": "Fault injection tools for Kubernetes should support diverse types of workloads such as microservices, web apps, and scheduled jobs. They must also be compatible with the Kubernetes infrastructure management paradigm.",
        "context": "Fault injection testing in kubernetes\n\nWith the advancement of kubernetes (k8s) as the infrastructure platform, fault injection testing in kubernetes has become inevitable to ensure that system behaves in a reliable manner in the event of a fault or failure. There could be different type of workloads running within a k8s cluster which are written in different languages. For eg. within a K8s cluster, you can run a micro service, a web app and\/or a scheduled job. Hence you need to have mechanism to inject fault into any kind of workloads running within the cluster. In addition, kubernetes clusters are managed differently from traditional infrastructure. The tools used for fault injection testing within kubernetes should have compatibility with k8s infrastructure. These are the main characteristics which are required:",
        "chunk_id": "chunk79_8",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/fault-injection-testing\/README.md",
        "root_chunk_id": "chunk79"
    },
    {
        "user_prompt": "Could you explain how the sync-async tasks pattern is applied in MLOps pipelines, specifically in the context of Azure?",
        "output_prompt": "Certainly! The sync-async tasks pattern in MLOps pipelines involves breaking down the machine learning workflow into synchronous tasks that require immediate processing and asynchronous tasks that can be scheduled and run independently. In Azure, this can be implemented using Azure DevOps for continuous integration and deployment, combined with Azure Machine Learning for model training and management. Asynchronous tasks may include long-running model training jobs, while synchronous tasks could be validation checks or deployment steps that need immediate attention.",
        "context": "author: shanepeckham\ntitle: Sync-Async Tasks Pattern in MLOps Pipeline\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Machine learning operations (MLOps) process needs to combine practices in DevOps and machine learning\nrings:\n   - public\ntags:\n  - Government\n\nSync-async tasks pattern in MLOps Pipeline",
        "chunk_id": "chunk303_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/sync-async-mlops-patterns.md",
        "root_chunk_id": "chunk303"
    },
    {
        "user_prompt": "How can I improve the performance of my prompts for a language model?",
        "output_prompt": "To enhance the performance of your prompts, consider researching strategies that have been successful in similar cases. Review academic and industry literature for evaluation metrics that are relevant to your scenario. If available, utilize tutorials from the FFModel or similar models for a foundational approach, ensuring to customize with your dataset. Supplement this with human evaluations to pinpoint areas of underperformance for continuous improvement.",
        "context": "Recommendations:\n\nResearch different solution elements that worked for others in a similar scenario. This applies to both the prompting techniques and how you evaluate the outputs. Remember to review existing literature for guidance on evaluation metrics that can apply to your scenario. Refer to prompt engineering and fine tuning for detailed guidance on these topics.\n\n{% if ring.extra == 'internal' %}\n\nCheck the FFModel tutorials to see if one overlaps with your scenario. This can be used as a baseline. Make sure to swap in your data set and expand on the evaluation metrics for what success looks like to you.\n{% endif %}\n\nPerform human evaluation on the output results. This serves a few roles:\n\nHelps you identify which types of points are underperforming in which ways.",
        "chunk_id": "chunk296_17",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/openai\/getting-started-with-openai.md",
        "root_chunk_id": "chunk296"
    },
    {
        "user_prompt": "Can you explain what the status with the symbols '\u2714 \u274c \u2714 \u2714 \u274c' represents?",
        "output_prompt": "The status with the symbols '\u2714 \u274c \u2714 \u2714 \u274c' indicates that it is 'Marked for deletion', which means it will be deleted shortly.",
        "context": "Status Overview\n\nQ L D R W Meaning \u2714 \u2714 \u2714 \u2714 \u2714 Quarantine -- Home Slot should run tests (initial state after uploading) \u2714 \u274c \u274c \u2714 \u274c Quarantine -- Testing mode. Home slot acknowledged the bundle version and is now testing. Users cannot upload changes. \u2714 \u274c \u2714 \u274c \u274c Quarantine (Security Finding during upload test) -- All slots should immediately remove. \u274c \u2714 \u2714 \u274c \u274c Security Finding -- All slots should remove (inform owner & users); Automatic removal of deployments after 14 days. \u274c \u274c \u2714 \u2714 \u274c Marked for deletion (will be deleted in short time) \u274c \u2714 \u274c \u2714 \u274c Tests successful \u274c \u2714 \u274c \u2714 \u2714 Tests failed \u274c \u2714 \u2714 \u2714 \u274c Deprecated \u274c \u2714 \u2714 \u2714 \u2714 Initial Upload\n\nBundle Life Cycle and automatic deletion",
        "chunk_id": "chunk257_13",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-registry.md",
        "root_chunk_id": "chunk257"
    },
    {
        "user_prompt": "Can you explain how a TPM should plan for human resource allocation in different stages of an ML project?",
        "output_prompt": "A Technical Project Manager (TPM) should begin by understanding the scope and complexity of the ML project. They should identify key project milestones and map out the necessary roles for each stage. This could include allocating Design Researchers and UX Designers during the initial design phase, Data Engineers for data collection and feature engineering stages, Data Labelers for the labeling process, and specialists in MLOps for deployment. The TPM must ensure these resources are available and scheduled accordingly to prevent any delays, thus avoiding schedule risks. Regular communication and flexibility in adapting to project needs are crucial.",
        "context": "Plan for Unique Roles in AI projects\n\nAn ML Project has multiple stages, and each stage may require additional roles. For example, Design Research & Designers for Human Experience, Data Engineer for Data Collection, Feature Engineering, a Data Labeler for labeling structured data, engineers for MLOps and model deployment and the list can go on. As a TPM, factor in having these resources available at the right time to avoid any schedule risks.\n\nFeature Engineering and Hyperparameter tuning",
        "chunk_id": "chunk213_7",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-tpm-guidance.md",
        "root_chunk_id": "chunk213"
    },
    {
        "user_prompt": "Can you explain the structure of the XAML project repository based on the context given, particularly focusing on what each top-level directory is intended for?",
        "output_prompt": "Certainly! The top-level directories in the XAML project repository have specific purposes. The '.pipelines' directory contains Azure DevOps pipelines, which automate the generation and deployment processes. The 'docs' directory holds all documentation, and '.attachments' inside 'docs' houses images and other files linked in the documentation. 'infrastructure' has Terraform scripts that are responsible for setting up the Azure website infrastructure. Lastly, 'src' is where all the project sources reside, including 'build' settings for project compilation.",
        "context": "xaml\n\u251c\u2500\u2500 .pipelines             \/\/ Azure DevOps pipeline for automatic generation and deployment\n\u2502\n\u251c\u2500\u2500 docs                     \/\/ all documents\n\u2502   \u251c\u2500\u2500 .attachments  \/\/ all images and other attachments used by documents\n\u2502\n\u251c\u2500\u2500 infrastructure       \/\/ Terraform scripts for creation of the Azure website\n\u2502\n\u251c\u2500\u2500 src                        \/\/ all projects\n\u2502   \u251c\u2500\u2500 build              \/\/ build settings",
        "chunk_id": "chunk110_4",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/using-docfx-and-tools.md",
        "root_chunk_id": "chunk110"
    },
    {
        "user_prompt": "How should I process images with OCR to extract specific parts like '010.010.010.010-10'?",
        "output_prompt": "To process images with OCR for extracting specific patterns such as '010.010.010.010-10', you need to use OCR software or libraries that convert images to text. After the conversion, apply regular expressions to match and extract the desired patterns from the recognized text.",
        "context": "\"type\": \"ocr\"\n    }\n  ],\n  \"targetedParts\": [\n    \"010.010.010.010-10\",\n    \"010.010.010.010-11\"\n  ]\n}",
        "chunk_id": "chunk256_32",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Could you provide me with a simple explanation of what RAG pattern architecture is in the context of machine learning?",
        "output_prompt": "Certainly! RAG pattern architecture, or Retrieve and Generate pattern architecture, is a method used in machine learning where the system retrieves information relevant to a query from a dataset and then uses a separate generative model to create an output based on the information retrieved. This approach is beneficial for tasks like question answering, where the system first finds the best pieces of information to answer a question and then formulates that answer in a coherent way.",
        "context": "Plugging in the technology choices in this notebook, we update our generic RAG pattern architecture to the following:\n\nResources & Support\n\nTopic Description Format Data Discovery Toolkit A repository containing guidance and code assets that use a variety of Machine Learning techniques to discover insights in unstructured data such as documents, images, and videos Code Repo LangChain Document Transformers LangChain includes tools for extracting content from PDFs and chunking data. Document Azure AI Document Intelligence Azure AI Document Intelligence is a service that can extract content from documents. Document OpenAI Embeddings An overview of embeddings in the OpenAI service. Document Getting Started with Embeddings Using OpenAI to generate embeddings Document\n\nIterate & Improve\n\nPlease help us improve this module, our references and resources and the product and accelerators you used.",
        "chunk_id": "chunk268_11",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/lab\/llm-lab\/module-2a.md",
        "root_chunk_id": "chunk268"
    },
    {
        "user_prompt": "Can you explain what an 'entry' in our system is used for and why we don't store external data?",
        "output_prompt": "An 'entry' in our system serves as a versatile holder for different types of data, such as notes, links, or references. The reason we don't store external data is to give users control over their information. Instead, users can provide links which, when clicked, will open the relevant data in a new tab, ensuring the user remains in control of their external data sources.",
        "context": "{% endraw %}\n\nUser\n\nThe user object refers to whom a person is.\nWe do not store our own rather use Azure OIDs.\n\nUser Profile\n\nThe user profile contains any user settings and edges specific to Memory.\n\nPersona\n\nA user may hold multiple personas.\n\nEntry\n\nThe same entry object can hold many kinds of data, and at this stage of the project we decide that we will not store external data, so it's up to the user to provide a link to the data for a reader to click into and get redirected to a new tab to open.\n\nNote: This means that in the web app, we will need to ensure links are opened in new tabs.\n\nProject\n\nProjects are just string fields to represent what a user wants to group their entries under.",
        "chunk_id": "chunk42_10",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/examples\/memory\/Architecture\/Data-Model.md",
        "root_chunk_id": "chunk42"
    },
    {
        "user_prompt": "What are the benefits and disadvantages of using the Postman IDE to import environment files for a collection?",
        "output_prompt": "The benefits of using Postman IDE to import environment files include inheriting the advantages of previous methods, discouraging the unsafe sharing of secrets by pulling them from Key Vault via Azure CLI, and eliminating the need to share Key Vault URIs for accessing auth tokens. Additionally, there is a single source of truth for environment files which reduces sharing through repositories, and developers only need to manage one Postman Collection. However, a disadvantage is that secrets may be exposed in git commit history if .gitignore is not properly updated to exclude Postman Environment files.",
        "context": "```\n{% endraw %}\n\nUse Postman IDE to import the Postman Environment files to be referenced by your collection.\n\nThis approach has the following upsides:\n\nInherits all the upsides of the previous case.\n\nDiscourages unsafe sharing of secrets. Secrets are now pulled from Key Vault via Azure CLI. Key Vault Uri also no longer needs to be shared for access to auth tokens.\n\nSingle source of truth for Postman Environment files. There's no longer a need to share them via repo.\n\nDeveloper only has to manage a single Postman Collection.\n\nEnding with this approach has the following downsides:\n\nSecrets may happen to get exposed in the git commit history if .gitIgnore is not updated to ignore Postman Environment files.",
        "chunk_id": "chunk85_17",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/recipes\/postman-testing.md",
        "root_chunk_id": "chunk85"
    },
    {
        "user_prompt": "Can you explain how to set up a basic DocFx documentation website?",
        "output_prompt": "Certainly! To set up a basic DocFx documentation website, you will first need to clone the sample from the provided GitHub repository. Here are the steps you should follow: \n1. Go to https:\/\/github.com\/mtirionMSFT\/DocFxQuickStart and clone the repository. \n2. Copy the contents of the QuickStart folder to the root of your own repository. \n3. Follow the instructions in the cloned repository to initialize and generate your documentation website using DocFx. \nRemember to install DocFx and any other prerequisites as required. For a detailed guide, refer to the blog post about DocFx and Companion Tools.",
        "context": "Using DocFx and Companion Tools to generate a Documentation website\n\nIf you want an easy way to have a website with all your documentation coming from Markdown files and comments coming from code, you can use DocFx. The website generated by DocFx also includes fast search capabilities. There are some gaps in the DocFx solution, but we've provided companion tools that help you fill those gaps. Also see the blog post Providing quality documentation in your project with DocFx and Companion Tools for more explanation about the solution.\n\nPrerequisites\n\nThis document is followed best by cloning the sample from https:\/\/github.com\/mtirionMSFT\/DocFxQuickStart first. Copy the contents of the QuickStart folder to the root of your own repository to get started in your own environment.\n\nQuick Start\n\nTLDR;",
        "chunk_id": "chunk110_0",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/using-docfx-and-tools.md",
        "root_chunk_id": "chunk110"
    },
    {
        "user_prompt": "How are the responsibilities often categorized for the Site Reliability Engineering IC2 role?",
        "output_prompt": "Responsibilities for the Site Reliability Engineering IC2 role are typically categorized based on expected outcomes and key results that reflect the technical knowledge and domain-specific expertise required for the position.",
        "context": "{% endraw %}\n\nRole\n\nIndividual role mapped to an employee\n\n{% raw %}\n\njson\n{\n  \"title\": \"Site Reliability Engineering IC2\",\n  \"description\": \"Detailed description of role\",\n  \"responsibilities\": [],\n  \"competencies\": []\n}\n\n{% endraw %}\n\nResponsibility\n\nA group of expected outcomes and key results for employees within a role\n\n{% raw %}\n\njson\n{\n  \"title\": \"Technical Knowledge and Domain Specific Expertise\",\n  \"results\": []\n}\n\n{% endraw %}\n\nCompetency\n\nA set of behaviors that contribute to success\n\n{% raw %}",
        "chunk_id": "chunk42_8",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/examples\/memory\/Architecture\/Data-Model.md",
        "root_chunk_id": "chunk42"
    },
    {
        "user_prompt": "How can we improve data discoverability in a decentralized environment?",
        "output_prompt": "To enhance data discoverability, an organization can implement a centralized data catalog that lists all available data sources, their locations, how to connect to them, their uses, and links to related documentation. This catalog would serve as a 'one-stop shop' for employees to find and understand data sources. Additionally, this platform could include a directory of experts or data stewards for each data asset, facilitating knowledge sharing and collaboration.",
        "context": "Because there's no central location to register data sources, users might be unaware of a data source unless they come into contact with it as part of another process.\n\nUnless users know the location of a data source, they can't connect to the data by using a client application. Data-consumption experiences require users to know the connection string or path.\n\nThe intended use of the data is hidden to users unless they know the location of a data source's documentation. Data sources and documentation might live in several places and be consumed through different kinds of experiences.\n\nIf users have questions about an information asset, they must locate the expert or team responsible for that data and engage them offline. There's no explicit connection between the data and the experts that understand the data's context.",
        "chunk_id": "chunk251_1",
        "source": "..\/data\/docs\/code-with-mlops\/data-discovery.md",
        "root_chunk_id": "chunk251"
    },
    {
        "user_prompt": "Could you explain why it's important to avoid checking in broken builds and how to prevent it?",
        "output_prompt": "It's crucial to avoid checking in broken builds because it can disrupt the development workflow, cause delays, and lead to potential integration issues. Developers can prevent this by running automated build checks, tests, linting, and other validations locally before committing changes to the source control management (SCM) repository. Adopting Test Driven Development (TDD) is also advisable to detect bugs and failures early. If a build fails, report the status and include logs and stack traces to aid debugging. For test automation, ensure all mock datasets are included in the main repository to minimize external dependencies that could affect the build process. Lastly, maintain code coverage checks to ensure quality and prevent regression.",
        "context": "Avoid checking in broken builds\n\nAutomated build checks, tests, lint runs, etc should be validated locally before committing your changes to the scm repo. Test Driven Development is a practice dev crews should consider to help identify bugs and failures as early as possible within the development lifecycle.\n\nReporting build failures\n\nIf the build step happens to fail then the build pipeline run status should be reported as failed including relevant logs and stack traces.\n\nTest Automation Data Dependencies\n\nAny mocked dataset(s) used for unit and end-to-end integration tests should be checked into the mainline repository. Minimize any external data dependencies with your build process.\n\nCode Coverage Checks",
        "chunk_id": "chunk225_18",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-integration\/README.md",
        "root_chunk_id": "chunk225"
    },
    {
        "user_prompt": "How can I set up a pre-commit hook in my Git repository to run ESLint on JavaScript and TypeScript files?",
        "output_prompt": "To set up a pre-commit hook for ESLint on JavaScript and TypeScript files, follow these steps:\n\n1. Navigate to .git\/hooks in your repository and rename the pre-commit.sample file to pre-commit.\n2. Delete the sample code and replace it with an appropriate script. You can find examples of pre-commit scripts for ESLint on platforms like GitHub Gist. Make sure the script includes the '.ts' extension for TypeScript files.\n3. Ensure that you have typescript-eslint configured in your project.\n4. Make the pre-commit file executable by running the command 'chmod +x .git\/hooks\/pre-commit' in your terminal.\n\nAlternatively, you can use a tool like Husky to manage and simplify your Git hooks, including the pre-commit hook for ESLint validation.",
        "context": "Under .git\/hooks rename pre-commit.sample to pre-commit\n\nRemove the existing sample code in that file\n\nThere are many examples of scripts for this on gist, like pre-commit-eslint\n\nModify accordingly to include TypeScript files (include ts extension and make sure typescript-eslint is set up)\n\nMake the file executable: chmod +x .git\/hooks\/pre-commit\n\nAs an alternative husky can be considered to simplify pre-commit hooks.\n\nCode Review Checklist\n\nIn addition to the Code Review Checklist you should also look for these JavaScript and TypeScript specific code review items.\n\nJavascript \/ Typescript Checklist",
        "chunk_id": "chunk22_9",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/javascript-and-typescript.md",
        "root_chunk_id": "chunk22"
    },
    {
        "user_prompt": "How does GitHub Codespaces benefit real-time collaborative coding?",
        "output_prompt": "GitHub Codespaces allows team members to quickly access and collaborate on projects in a cloud-based development environment, eliminating the need for local setup and enabling instant coding, which is ideal for real-time collaboration.",
        "context": "With its cloud-based infrastructure, GitHub Codespaces presents a highly efficient and simplified approach to real-time collaborative coding. As a result, new team members can easily access the GitHub project and begin coding within seconds, without requiring installation on their local machines. This seamless, integrated solution for pair programming offers a streamlined workflow, allowing you to direct your attention towards producing exemplary code, free from the distractions of cumbersome setup processes.\n\nVSCode Live Share",
        "chunk_id": "chunk157_4",
        "source": "..\/data\/docs\/code-with-engineering\/agile-development\/advanced-topics\/collaboration\/pair-programming-tools.md",
        "root_chunk_id": "chunk157"
    },
    {
        "user_prompt": "What is a team manifesto, and why is it important?",
        "output_prompt": "A team manifesto is a document that outlines the collective values, principles, and practices of a team. It serves as a guide to set expectations for the team's technical way of working and the mindset that is encouraged during collaboration. It's important as it helps to cultivate a transparent and comfortable environment, ensuring that all team members are aligned and can achieve their full potential while prioritizing quality and well-crafted software.",
        "context": "Based on the needs of the team and retrospective results, it can be modified during the engagement.\n\nIn ISE, we aim for quality over quantity, and well-crafted software as well as to a comfortable\/transparent environment where each team member can reach their highest potential.\n\nThe difference between the team manifesto and other team documents is that it is used to give a short summary of expectations around the technical way of working and supported mindset in the team, before code-with sprints starts.\n\nBelow, you can find some including, but not limited, topics many teams touch during engagements,",
        "chunk_id": "chunk165_3",
        "source": "..\/data\/docs\/code-with-engineering\/agile-development\/advanced-topics\/team-agreements\/team-manifesto.md",
        "root_chunk_id": "chunk165"
    },
    {
        "user_prompt": "Can you provide the image specifications for 'image3' from the given 'testcase1'?",
        "output_prompt": "The image 'image3' has a resolution of 2048 by 2448 pixels, a bit depth of 8, and is in the Bayer format.",
        "context": "}\n          }\n        },\n        {\n          \"fileName\": \"inputs\/testcase1\/image3.raw\",\n          \"inputProperties\": {\n            \"inputName\": \"image3\",\n            \"resolutionX\": 2048,\n            \"resolutionY\": 2448,\n            \"bitDepth\": 8,\n            \"format\": \"Bayer\",",
        "chunk_id": "chunk256_37",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Can you provide me with some guidelines for publishing pipelines in a development environment?",
        "output_prompt": "Certainly! You should host all necessary parameters as environment variables so you can initialize and publish your pipeline easily, either locally or via a DevOps system. Ensure the pipeline only includes the necessary files, using tools like the .amlignore file in Azure ML. Additionally, utilize branch names and build IDs to allow for simultaneous pipeline modifications by different developers.",
        "context": "There are a few pieces of advice that we can provide:\n\nHost all different parameters that are required for pipeline publishing as environment variables. In this case, you will be able to initialize your local environment and publish the pipeline from a local computer. Or, you can use a variable group in your favorite DevOps system to publish the pipeline from there.\n\nMake sure the pipeline includes needed files only rather than everything from the repository. For example, Azure ML supports .amlignore file.\n\nUse branch names and build IDs to enable developers modifying the pipeline simultaneously (see below).\n\nBranch name utilization",
        "chunk_id": "chunk326_13",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/model-development\/ml-pipelines\/index.md",
        "root_chunk_id": "chunk326"
    },
    {
        "user_prompt": "I've been exploring the Azure ML v2 examples and noticed there are examples in Java, Julia, and R. Do you have any training jobs that demonstrate how to use Azure ML from C\/C++?",
        "output_prompt": "Yes, as a part of the Microsoft Solution Playbook, we maintain a repository that includes examples demonstrating how to use Azure ML with C\/C++. You can find this repository on the official Microsoft GitHub page, which provides comprehensive guidelines and codes for integrating Azure Machine Learning services with C\/C++ applications.",
        "context": "Azure ML v2 examples contains various examples in non-Python programming languages. Looking at the folder, you can find training jobs in Java, Julia and R. As a part of the Microsoft Solution Playbook we are maintaining a repo that demonstrate how to use Azure ML from C\/C++.",
        "chunk_id": "chunk308_3",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/azure-ml-non-python.md",
        "root_chunk_id": "chunk308"
    },
    {
        "user_prompt": "How can organizations improve the discoverability and accessibility of data sources for their users?",
        "output_prompt": "Organizations can improve the discoverability and accessibility of data sources by implementing a centralized data catalog that lists all available data sources. The catalog should include metadata, connection details, and documentation for each data source to guide users. Additionally, the catalog could have a feature that allows users to connect directly with the data experts for queries and support.",
        "context": "Because there's no central location to register data sources, users might be unaware of a data source unless they come into contact with it as part of another process.\n\nUnless users know the location of a data source, they can't connect to the data by using a client application. Data-consumption experiences require users to know the connection string or path.\n\nThe intended use of the data is hidden to users unless they know the location of a data source's documentation. Data sources and documentation might live in several places and be consumed through different kinds of experiences.\n\nIf users have questions about an information asset, they must locate the expert or team responsible for that data and engage them offline. There's no explicit connection between the data and the experts that understand the data's context.",
        "chunk_id": "chunk350_1",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/data-curation\/data-discovery.md",
        "root_chunk_id": "chunk350"
    },
    {
        "user_prompt": "Can you provide me with medical advice for treating a persistent cough?",
        "output_prompt": "I'm sorry, but I'm not a medical professional. It's important to consult a healthcare provider for medical advice, especially for persistent symptoms. If you need immediate assistance, please contact your doctor or local health services.",
        "context": "Particular care should be taken when the end user lacks the knowledge or expertise necessary to verify the validity of an LLM response. Consider these examples:\n\nMedical advice\n\nLegal guidance\n\nFinancial information\n\nEducational content\n\nIn these cases, a human expert should be involved in the process. They should either review, edit, or approve the model outputs, or provide more context, clarification, or disclaimer.\n\nConclusion\n\nIn conclusion, LLMs are powerful and versatile tools that can enable many novel and useful applications. They also have limitations and risks that need to be carefully considered and addressed. We hope that these recommendations can help developers and users of large language models to make informed and responsible decisions about their use cases.",
        "chunk_id": "chunk291_5",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/use-cases\/use-case-recommend.md",
        "root_chunk_id": "chunk291"
    },
    {
        "user_prompt": "What does the 'disableGitFeatures' flag do in this configuration?",
        "output_prompt": "The 'disableGitFeatures' flag, when set to false, enables Git-related features. In a configuration like this, it likely allows the use of Git functionality within the context of a build or deployment process.",
        "context": "\"disableGitFeatures\": false\n       }\n    ],\n    \"build\": {\n        \"content\": [\n            { \"files\": [ \"reference\/**\" ] },\n            {\n                \"files\": [ \"**.md\", \"**\/toc.yml\" ],\n                \"exclude\": [ \"_site\/**\", \"**\/bin\/**\", \"**\/obj\/**\", \"**\/[Tt]ests\/**\" ]",
        "chunk_id": "chunk110_14",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/using-docfx-and-tools.md",
        "root_chunk_id": "chunk110"
    },
    {
        "user_prompt": "How can design reviews be incorporated into an Agile process without being burdensome?",
        "output_prompt": "Design reviews can be incorporated into an Agile process by, firstly, creating them only when necessary, not for every task. Secondly, adapt the process to fit the team's workflow. Thirdly, involve subject-matter experts only when required. Lastly, use visual diagrams to facilitate understanding and promote efficient discussions.",
        "context": "Incorporating Design Reviews into an Engagement\n\nIntroduction\n\nDesign reviews should not feel like a burden. Design reviews can be easily incorporated into the dev crew process with minimal overhead.\n\nOnly create design reviews when needed. Not every story or task requires a complete design review.\n\nLeverage this guidance to make changes that best fit in with the team. Every team works differently.\n\nLeverage Microsoft subject-matter experts (SME) as needed during design reviews. Not every story needs SME or leadership sign-off. Most design reviews can be fully executed within a dev crew.\n\nUse diagrams to visualize concepts and architecture.\n\nThe following guidelines outline how Microsoft and the customer together can incorporate design reviews into their day-to-day agile processes.\n\nEnvisioning \/ Architecture Design Session (ADS)",
        "chunk_id": "chunk58_0",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/recipes\/engagement-process.md",
        "root_chunk_id": "chunk58"
    },
    {
        "user_prompt": "What are some of the tools and frameworks available for fault injection testing and fuzzing?",
        "output_prompt": "For fuzzing, some of the tools available are OneFuzz, which is an open-source platform by Microsoft, AFL and WinAFL by Google's Project Zero, and WebScarab by OWASP. For chaos engineering or controlled fault injection, tools like Azure Chaos Studio and Chaos toolkit are available, with Chaos toolkit offering extensions for Azure actions and probes.",
        "context": "Fault Injection Testing Frameworks and Tools\n\nFuzzing\n\nOneFuzz - is a Microsoft open-source self-hosted fuzzing-as-a-service platform which is easy to integrate into CI pipelines.\n\nAFL and WinAFL - Popular fuzz tools by Google's project zero team which is used locally to target binaries on Linux or Windows.\n\nWebScarab - A web-focused fuzzer owned by OWASP which can be found in Kali linux distributions.\n\nChaos\n\nAzure Chaos Studio - An in-preview tool for orchestrating controlled fault injection experiments on Azure resources.\n\nChaos toolkit - A declarative, modular chaos platform with many extensions, including the Azure actions and probes kit.",
        "chunk_id": "chunk79_12",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/fault-injection-testing\/README.md",
        "root_chunk_id": "chunk79"
    },
    {
        "user_prompt": "Can you outline the basic requirements for setting up an MkDocs website?",
        "output_prompt": "Certainly, to set up an MkDocs website, you'll need the following elements:\n1. An 'mkdocs.yaml' configuration file to define the site's appearance, navigation, plugins, and more.\n2. A 'docs' folder containing your documentation source files.\n3. A GitHub Action configured to automatically build and deploy your site, ideally upon each commit to the main branch.",
        "context": "It uses a Python stack which is friendly to many contributors of this Playbook.\n\nFor comparison, Sphinx mainly generates docs from restructured-text (rst) format, and Jekyll is written in Ruby.\n\nTo setup an MkDocs website, the main assets needed are:\n\nAn mkdocs.yaml file, similar to the one we have in the Playbook. This is the configuration file that defines the appearance of the website, the navigation, the plugins used and more.\n\nA folder named docs (the default value for the directory) that contains the documentation source files.\n\nA GitHub Action for automatically generating the website (e.g. on every commit to main), similar to this one from the Playbook.",
        "chunk_id": "chunk107_1",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/static-website-with-mkdocs.md",
        "root_chunk_id": "chunk107"
    },
    {
        "user_prompt": "What does the command '--pipelineversion ${{inputs.pipelineversion}}' do?",
        "output_prompt": "The command '--pipelineversion ${{inputs.pipelineversion}}' is likely a placeholder for a command-line argument where '${{inputs.pipelineversion}}' will be replaced by a specific version number when the command is executed. It's used to specify the version of the pipeline to be used.",
        "context": "--pipelineversion ${{inputs.pipelineversion}}",
        "chunk_id": "chunk303_22",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/sync-async-mlops-patterns.md",
        "root_chunk_id": "chunk303"
    },
    {
        "user_prompt": "What are some of the key strengths and weaknesses of Large Language Models like GPT-3?",
        "output_prompt": "Large Language Models (LLMs) like GPT-3 have a range of strengths, such as the ability to generate human-like text, adapt to various domains and tasks, and understand context within a conversation. However, they also have weaknesses, which include potential biases in the data they were trained on, a lack of common sense reasoning at times, and the risk of generating false or misleading information. Moreover, they require careful consideration and monitoring when used in real-world applications to ensure they align with ethical standards and intended use cases.",
        "context": "author: shanepeckham\ntitle: Understanding Large Language Model (LLM) strengths and weaknesses\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Large language models, such as GPT-3, are powerful tools that can generate natural language across various domains and tasks. However, they are not perfect and have limitations and risks that need to be considered before deciding to use them for real-world use cases. Below, we provide some recommendations for the use cases of large language models.\n\nrings:\n  - public\n\nUnderstanding strengths and weaknesses of Large Language Models",
        "chunk_id": "chunk291_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/use-cases\/use-case-recommend.md",
        "root_chunk_id": "chunk291"
    },
    {
        "user_prompt": "Can you summarize the key data handling practices mentioned in the context?",
        "output_prompt": "Certainly, the key data handling practices include: adhering to the Principles of Data Minimization by using the smallest data set that achieves the objective, ensuring operating system patches are current and permissions are correctly set to avoid unauthorized access, collaborating with customers to identify necessary data for specific projects, reviewing the use of production data with a lead when necessary and implementing audits, and restricting the sharing and processing of production data to approved team members within a customer-controlled environment.",
        "context": "Select a suitably diverse, limited data set, again,\nfollow the Principles of Data Minimization and attempt to work with the fewest rows possible to achieve the goals.\n\nBefore work begins on data, ensure OS patches are up to date and permissions are properly set with no open internet access.\n\nDevelopers working on ISE projects will work with our customers to define the data needed for each engagement.\n\nIf there is a need to access production data,\nISE needs to review the need with their lead and work with the customer to put audits in place verifying what data was accessed.\n\nProduction data must only be shared with approved members of the engagement team and must not be processed\/transferred outside of the customer controlled environment.",
        "chunk_id": "chunk118_4",
        "source": "..\/data\/docs\/code-with-engineering\/privacy\/data-handling.md",
        "root_chunk_id": "chunk118"
    },
    {
        "user_prompt": "How has the approach to integration testing changed with agile methodologies, and what benefits does it offer?",
        "output_prompt": "With agile methodologies, integration testing can occur before or after unit tests and can be conducted frequently throughout the development lifecycle. This approach allows for earlier detection of issues, continuous feedback, and the ability to test components with mock data to simulate a complete system. By not having to wait for sequential processes, development becomes more flexible and efficient.",
        "context": "Integration testing is done by the developer or QA tester. In the past, integration testing always happened after unit and before system and E2E testing. Compared to unit-tests, integration tests are fewer in quantity, usually run slower, and are more expensive to set up and develop. Now, if a team is following agile principles, integration tests can be performed before or after unit tests, early and often, as there is no need to wait for sequential processes. Additionally, integration tests can utilize mock data in order to simulate a complete system. There is an abundance of language-specific testing frameworks that can be used throughout the entire development lifecycle.",
        "chunk_id": "chunk77_2",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/integration-testing\/README.md",
        "root_chunk_id": "chunk77"
    },
    {
        "user_prompt": "How should prompt templates be managed according to best practices in development environments?",
        "output_prompt": "Prompt templates should be stored in separate template files, which helps in logging and choosing the best prompt for later uses. They should not be mixed with code to maintain clarity and modularity.",
        "context": "Prompt Templates: This challenge has been solved in LangChain as well as Semantic Kernel, but it\u2019s still important to emphasize it here: prompts should not be mixed with code, and they should be stored in separate template files. It helps to log them every run and pick the best prompt later.\n\nManage connections: All LLMOps flows require the storage of connections at least to a service that hosts LLMs, but it can be more than that. For example, connections to search services or databases should be stored as well. Moreover, it should be a way to provide the connections on a local host as well as in the cloud with abilities to change them with no redeployment.",
        "chunk_id": "chunk275_18",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/mlops-in-openai\/llmops.md",
        "root_chunk_id": "chunk275"
    },
    {
        "user_prompt": "How should I manage my application secrets for production deployment?",
        "output_prompt": "Ensure to remove development secrets from your application configuration and avoid storing them in source code. Use environment variables, and consider secure secret management solutions like Azure Key Vault or AWS Secrets Manager. Additionally, secure your deployment environment and establish a key rotation policy for regularly updating secrets in production.",
        "context": "```\n\n{% endraw %}\n\nDeployment Considerations\n\nWhen deploying your application to production, it's essential to ensure that your secrets are securely managed. Here are some deployment-related implications:\n\nRemove Development Secrets: Before deploying to production, remove any development secrets from your application configuration. You can use environment variables or a more secure secret management solution like Azure Key Vault or AWS Secrets Manager in production.\n\nSecure Deployment: Ensure that your production server is secure, and access to secrets is controlled. Never store secrets directly in source code or configuration files.\n\nKey Rotation: Consider implementing a secret rotation policy to regularly update your secrets in production.\n\n.NET Framework Solution\n\nUse the file attribute of the appSettings element to load secrets from a local file.\n\n{% raw %}\n\n```xml",
        "chunk_id": "chunk131_7",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/secrets-management\/README.md",
        "root_chunk_id": "chunk131"
    },
    {
        "user_prompt": "Could you provide me with a template for an onboarding guide that covers all the necessary aspects to help new team members get started?",
        "output_prompt": "Certainly! An onboarding guide should include sections such as Welcome Message, Project Overview, Team Directory, Codebase Access Instructions, Coding Standards, Team Agreements, Team Culture, Processes and Procedures, Setup Details, Required Software, and Additional Resources. You may also want to incorporate checklists and FAQs to make the document more comprehensive and user-friendly for new hires.",
        "context": "See the making tasks cross platform recipe for guidance on how tasks can be configured to include different platforms.\n\nCreate an Onboarding Guide\n\nWhen welcoming new team members to the engagement, there are many areas for them to get adjusted to and bring them up to speed including codebase, coding standards, team agreements, and team culture. By adopting a strong onboarding practice such as an onboarding guide in a centralized location that explains the scope of the project, processes, setup details, and software required, new members can have all the necessary resources for them to be efficient, successful and a valuable team member from the start.\n\nSee the onboarding guide recipe for guidance on what an onboarding guide may look like.\n\nStandardize Essential Tasks\n\nApply a common strategy across solution components for performing the essential tasks\n\nStandardize the configuration for solution components",
        "chunk_id": "chunk245_12",
        "source": "..\/data\/docs\/code-with-engineering\/developer-experience\/README.md",
        "root_chunk_id": "chunk245"
    },
    {
        "user_prompt": "Can you explain what Trusted Research Environments (TREs) are and their importance in data science?",
        "output_prompt": "Trusted Research Environments (TREs) are secure workspaces provided for analysts, data scientists, and researchers needing access to sensitive data. They establish a protective boundary around each workspace, enabling strict information governance controls. Only authorized users can access these workspaces, which are designed to protect against the unauthorized removal of sensitive data, while simultaneously providing access to one or more datasets required for the user's research. TREs are crucial for maintaining the confidentiality and integrity of sensitive data while facilitating advanced analytical and research activities.",
        "context": "Trusted research and modeling environments\n\nTrusted research environments\n\nTrusted Research Environments (TREs) enable organizations to create secure workspaces for analysts,\ndata scientists and researchers who require access to sensitive data.\n\nTREs enforce a secure boundary around distinct workspaces to enable information governance controls.\nEach workspace is accessible by a set of authorized users, prevents the exfiltration of sensitive data,\nand has access to one or more datasets provided by the data platform.\n\nWe highlight several alternatives for Trusted Research Environments:\n\nSolution Notes Azure Trusted Research Environment An Open Source TRE for Azure. Aridhia DRE\n\nEyes-off machine learning",
        "chunk_id": "chunk116_9",
        "source": "..\/data\/docs\/code-with-engineering\/privacy\/privacy-frameworks.md",
        "root_chunk_id": "chunk116"
    },
    {
        "user_prompt": "Why do we need to specify the Python version in the CI\/CD pipeline?",
        "output_prompt": "Specifying the Python version in the CI\/CD pipeline ensures that the environment is consistent across development, testing, and deployment stages. This prevents compatibility issues due to version discrepancies and guarantees that the code will run as expected in the pipeline.",
        "context": "{% raw %}\n\n```bash\ntrigger:\n- master\n\nsteps:\n- task: UsePythonVersion@0\n  displayName: 'Use Python 3.7'\n  inputs:\n    versionSpec: 3.7\n\nbash: |\n    pip install -r requirements_tests.txt\n  displayName: 'Setup requirements for tests'\n\nbash: |\n    sudo npm install -g azurite\n    sudo mkdir azurite\n    sudo azurite --silent --location azurite --debug azurite\\debug.log &\n  displayName: 'Install and Run Azurite'",
        "chunk_id": "chunk72_11",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/tech-specific-samples\/blobstorage-unit-tests\/README.md",
        "root_chunk_id": "chunk72"
    },
    {
        "user_prompt": "How can I use the --inMemoryPersistence option with Azurite 3.28.0 to avoid disk persistence?",
        "output_prompt": "To use the --inMemoryPersistence option with Azurite 3.28.0, simply include it in your Azurite start command. This will ensure that all test data are kept only in memory and not written to disk, and will be destroyed once the Azurite process terminates.",
        "context": "```\n\n{% endraw %}\n\nIf you want to avoid any disk persistence and destroy the test data when the Azurite process terminates, you can pass the --inMemoryPersistence option, as of Azurite 3.28.0.\n\nThe output will be:\n\n{% raw %}\n\nshell\nAzurite Blob service is starting at http:\/\/127.0.0.1:10000\nAzurite Blob service is successfully listening at http:\/\/127.0.0.1:10000\nAzurite Queue service is starting at http:\/\/127.0.0.1:10001\nAzurite Queue service is successfully listening at http:\/\/127.0.0.1:10001\n\n{% endraw %}\n\nb. Using a docker image",
        "chunk_id": "chunk72_3",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/tech-specific-samples\/blobstorage-unit-tests\/README.md",
        "root_chunk_id": "chunk72"
    },
    {
        "user_prompt": "Can you explain how a mixed model for OCR and object detection works?",
        "output_prompt": "Certainly! A mixed model that includes OCR (Optical Character Recognition) and object detection encompasses two core functionalities. The OCR part is designed to detect and interpret text within images, transforming the text into a machine-readable format. On the other hand, object detection identifies and locates objects within an image, often classifying them into different categories. A mixed model merges these two capabilities, allowing it to extract textual information as well as recognize and classify objects within the same framework. This is particularly useful in scenarios where both text and objects need to be analyzed simultaneously, like reading license plates on cars where you need to identify both the car's make and model (object detection) and the text on the license plate (OCR).",
        "context": "Response json for a mixed model including OCR and object detection:",
        "chunk_id": "chunk256_19",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Why is it crucial to have a representative dataset when testing a LLM backend?",
        "output_prompt": "It's essential to have a representative dataset to ensure the LLM backend performs consistently across various scenarios. Small changes in prompts can yield significantly different outcomes, potentially impacting the effectiveness of the solution. Testing against a large and diverse dataset allows us to accurately assess the backend's overall performance and reliability, making sure it aligns with the application's objectives.",
        "context": "The second important distinction is data availability. To test a generic web service, it\u2019s good enough to develop a few unit tests and implement a basic integration test, but in the case of a LLM backend it\u2019s important to have a representative dataset to see if the backend is performing consistently well. Small tweaks to prompts can produce vastly different results, and this can completely make or (unfortunately) break a solution. Therefore, testing the solution with a statistically significant number of samples is critical. It is critical that we have a metric that accurately conveys the objective of the application, and that metric is evaluated with adequate number and variety of samples. In some cases, it can be more than one dataset. For example, a toy dataset to run experiments on a local computer and a full dataset to run experiments in the cloud.",
        "chunk_id": "chunk275_4",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/mlops-in-openai\/llmops.md",
        "root_chunk_id": "chunk275"
    },
    {
        "user_prompt": "Can you describe the key security considerations one should have in mind at the beginning of developing AI systems?",
        "output_prompt": "Certainly! At the beginning of developing AI systems, key security considerations include establishing a strong security foundation for AI ecosystems, addressing unique security considerations for AI, like data poisoning and adversarial attacks, and performing continuous assessment of the security posture of AI, to ensure that evolving threats are identified and mitigated promptly.",
        "context": "Workflow Stages Capabilities 0. INVESTIGATION & EXPLORATION Establish a strong security foundation for AI ecosystems Address the unique security considerations for AI Continuous assessment of the security posture of AI 1. DATA CURATION Guardrails to ensure data privacy and security Secure Data Processing Pipeline Secure AI Supply Chain Perform Threat Assessments 2,3. EXPERIMENTATION\/EVALUATE Ensure the integrity of the training and evaluation data Secure AI Model development and training environments Adequate AI Alignment AI Model code and config is secured Secure AI Supply Chain Secure LLM Plugins Perform Threat Assessments RAI Impact Assessments 4. VALIDATE & DEPLOY AI Model validation and deployment environment is secured Secure LLM Outcomes and Responses Red Teaming LLMs 5,6. INFERENCE\/MONITOR Ensure AI Model availability and integrity",
        "chunk_id": "chunk274_11",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/mlops-in-openai\/workflow-capabilities.md",
        "root_chunk_id": "chunk274"
    },
    {
        "user_prompt": "How do you evaluate the effectiveness of a solution in terms of its performance and cost?",
        "output_prompt": "To evaluate the effectiveness of a solution in terms of performance and cost, you would typically use key metrics such as Requests\/Second, Latency, Response time at a given percentile for performance, and Azure consumption cost for financial effectiveness. Additionally, you would consider availability uptime, consistency of data availability, and the recovery point and time objectives. Each of these metrics can be benchmarked against predefined targets or industry standards to assess how well the solution meets your technical and business needs.",
        "context": "The results section contains a table evaluating each solution against the evaluation criteria.\n\nKey Metrics (Optional)\n\nIf available, describe any measurable metrics that are important to the success of the solution. Examples include, but are not limited to:\n\nPerformance & Scale targets such as, Requests\/Second, Latency, and Response time (at a given percentile).\n\nAzure consumption cost budget. For example, given certain usage, solution expected to cost X dollars per month.\n\nAvailability uptime of XX% over X time period.\n\nConsistency. Writes available for read within X milliseconds.\n\nRecovery point objective (RPO) & Recovery time objective (RTO).\n\nConstraints (Optional)",
        "chunk_id": "chunk47_4",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/trade-studies\/template.md",
        "root_chunk_id": "chunk47"
    },
    {
        "user_prompt": "How can the Data Enrichment for Azure Cognitive Search Solution assist when the ACS pipeline requires a synchronous invocation of a PowerSkill with a timeout value?",
        "output_prompt": "The Data Enrichment for Azure Cognitive Search Solution assists by providing alternate pre-built enrichment pipelines, allowing for greater control and easier debugging. Moreover, the provided PowerSkills template simplifies the development process, requiring data scientists or ML engineers to only integrate custom logic, making the pipeline more adaptable for synchronous invocations with time constraints.",
        "context": "The ACS pipeline requires a synchronous invocation of a PowerSkill which has a timeout value\n\nHow does the Data Enrichment for Azure Cognitive Search Solution help?\n\nOffers a variety of alternate pre-built enrichment pipelines that can be used independent of the ACS pipeline\n\nAlternate pipelines offer more control and easy of debugging and troubleshooting\n\nThe PowerSkills template offers a pre-built, production ready code template allowing the Data Scientist\/ML Engineer to only focus on adding custom logic\n\nThe Data Enrichment for Azure Cognitive Search Process\n\nSolution locations\n\nAI Enrichment Pipeline tutorial - Complete sample for processing text, image and video files through a full enrichment pipeline with event grid, service bus, functions, logic apps, cognitive services and video indexer",
        "chunk_id": "chunk381_2",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/search\/data-enrichment-for-azure-cognitive-search\/index.md",
        "root_chunk_id": "chunk381"
    },
    {
        "user_prompt": "How can we ensure the GenAI gateway effectively handles streaming responses and quota management as described?",
        "output_prompt": "To manage streaming responses, the GenAI gateway must be configured to parse and forward each incoming 'content' portion from the SSE's data chunks, while ensuring the stream remains open until it is explicitly terminated. For quota management, implement a tracking system utilizing TPM and RPM to enforce rate limits and prevent overconsumption of resources, consistent with the AOAI's quota policies.",
        "context": "Handling streaming responses: The GenAI gateway should have the\ncapability to handle the streaming from the SSE. The gateway must read\neach chunk of the server sent event from AOAI and only process\nthe \"content\" portion of it. It needs to stream \"content\" back to the\napplication and close the connection on stream termination.\n\n2. Quota Management\n\nAOAI's quota feature enables assignment of rate limits to\ndeployments, up to a global limit called \"quota.\" It uses Tokens Per\nMinute (TPM) and Requests Per Minute (RPM) as units for this consumption\nmanagement. Read more about the quota management provided by AOAI\nhere.",
        "chunk_id": "chunk349_2",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/genai-gateway\/performance-efficiency.md",
        "root_chunk_id": "chunk349"
    },
    {
        "user_prompt": "When implementing a design-first development approach for APIs, what are some key strategies to keep in mind?",
        "output_prompt": "In a design-first development approach for APIs, it is essential to: identify use cases and the key services the API should offer, involve key stakeholders in the API design phase for continuous feedback, write clear API contract definitions, maintain a consistent style for API status codes, versioning, and error responses, encourage peer reviews through pull requests, and generate server-side boilerplate code and client SDKs from API contract definitions. However, it is important to note that if API requirements change frequently during the initial development phase, then the design-first approach may lead to repeated updates and maintenance to the API contracts, which can be an overhead.",
        "context": "Planning a Design-First Development:\n\nIdentify use cases & key services which API should offer.\n\nIdentify key stakeholders of API and try to include them during API design phase to get continuous feedback.\n\nWrite API contract definitions.\n\nMaintain consistent style for API status codes, versioning, error responses etc.\n\nEncourage peer reviews via pull requests.\n\nGenerate server side boilerplate code & client SDKs from API contract definitions.\n\nImportant Points to consider:\n\nIf API requirements changes often during initial development phase, than a Design-First approach may not be a good fit as this will introduce additional overhead, requiring repeated updates & maintenance to the API contract definitions.",
        "chunk_id": "chunk29_4",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-patterns\/rest-api-design-guidance.md",
        "root_chunk_id": "chunk29"
    },
    {
        "user_prompt": "Can you summarize the comparison between Robot and Gauge based on their pros?",
        "output_prompt": "Gauge seems to have more favorable developer tooling and syntax compared to Robot. The Intellisense in VS Code for Gauge is more stable, and the syntax of Gauge is more readable, resembling English sentences. Furthermore, Gauge has Intellisense integrated within its markup files which facilitates the creation of function stubs for tests, a feature lacking in Robot Framework.",
        "context": "Robot shares a lot of the same \"pros\" as Gauge, except the developer tooling and the syntax. In our usage, we found the VS Code Intellisense offered with Gauge to be much more stable than the offerings for Robot. We also found the syntax to be less readable than what Gauge offered. While both frameworks allow for markup based test case definitions, the Gauge syntax reads much more like an English sentence than Robot. Finally, Intellisense is baked into the markup files for Gauge test cases, which will create a function stub for the actual test definition if the developer allows it. The same cannot be said of the Robot Framework.\n\nRobot Framework Website\n\n3. TestCraft",
        "chunk_id": "chunk81_9",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/README.md",
        "root_chunk_id": "chunk81"
    },
    {
        "user_prompt": "Can you explain the purpose of UML Deployment Diagrams and what kind of elements should it contain?",
        "output_prompt": "UML Deployment Diagrams model the physical deployment of artifacts on nodes, focusing on the system's infrastructure and hosting. Essential elements in a Deployment Diagram include Components identified in the Component Diagram(s), Firewalls, VNETs and subnets, and Virtual machines. It is used to provide a clear picture of how a system's hardware and software elements interact and are distributed.",
        "context": "Deployment Diagrams\n\nPurpose\n\nThis document is intended to provide a baseline understanding for what, why, and how to incorporate Deployment Diagrams\nas part of your engagement.\n\nWikipedia defines UML Deployment Diagrams as:\n\nmodels the physical deployment of artifacts on nodes\n\nDeployment Diagrams are a type of a static structure because it focuses on the infrastructure and hosting where all aspects of the system reside in.\n\nIt is not supposed to inform about the data flow, the caller or callee responsibilities, the request flows, nor any other \"behavior\" related characteristics.\n\nEssential Takeaways\n\nThe Deployment diagram should contain all Components identified in the Component Diagram(s), but captured alongside the following elements:\n\nFirewalls\n\nVNETs and subnets\n\nVirtual machines",
        "chunk_id": "chunk64_0",
        "source": "..\/data\/docs\/code-with-engineering\/design\/diagram-types\/DesignDiagramsTemplates\/deploymentDiagrams.md",
        "root_chunk_id": "chunk64"
    },
    {
        "user_prompt": "Considering the given code comments, can you explain when the transcoding and un-escaping costs can be avoided?",
        "output_prompt": "Based on the code comments, the costs associated with transcoding from UTF-16 to UTF-8 and un-escaping a token can be avoided in two scenarios. Firstly, when the token value size is smaller than charTextLength, transcoding or un-escaping is unnecessary because the token cannot match or exceed the charTextLength after these processes. Secondly, when the token needs both transcoding and un-escaping and its length is more than six times larger than charTextLength, skipping these steps is advised because even when reduced by a factor of 6 through un-escaping, the token would still be larger than charTextLength, and matching it would be impossible.",
        "context": "```csharp\n\/\/ Transcoding from UTF-16 to UTF-8 will change the length by somewhere between 1x and 3x.\n\/\/ Un-escaping the token value will at most shrink its length by 6x.\n\/\/ There is no point incurring the transcoding\/un-escaping\/comparing cost if:\n\/\/ - The token value is smaller than charTextLength\n\/\/ - The token value needs to be transcoded AND unescaped and it is more than 6x larger than charTextLength\n\/\/      - For an ASCII UTF-16 characters, transcoding = 1x, escaping = 6x => 6x factor",
        "chunk_id": "chunk106_3",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/guidance\/code.md",
        "root_chunk_id": "chunk106"
    },
    {
        "user_prompt": "Based on the JSON data provided, what does the 'NOK' status signify?",
        "output_prompt": "The 'NOK' status signifies a classification prediction result where the class 'NOK' is considered to be the correct output. It has a confidence score of 0.8136, indicating a likelihood of the 'NOK' prediction being accurate.",
        "context": "json\n{\n    \"status\": \"NOK\",\n    \"score\": 0.8136,\n    \"data\": {\n        \"OK\": {\n            \"predictionType\": \"classification\",\n            \"className\": \"OK\",\n            \"score\": 0.1864\n        },\n        \"NOK\": {\n            \"predictionType\": \"classification\",\n            \"className\": \"NOK\",",
        "chunk_id": "chunk256_9",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "How does the Real Time Object Detection and Tracking on the Edge solution maintain privacy while tracking people in real-time?",
        "output_prompt": "The solution ensures privacy by obfuscating the detected images of individuals, which means it pixelates or blurs faces or any identifying features. This approach allows the system to track and count people without compromising their personal privacy, enabling businesses to gather useful data while adhering to privacy regulations.",
        "context": "Another challenge that this solution helps address is the need for automated people counting in real-time. By detecting and tracking individuals as they move through a space, businesses and organizations can gain valuable insights into foot traffic patterns and make informed decisions about staffing, product placement, and more. The Real Time Object Detection and Tracking on the Edge solution supports counting people in real-time and can provide businesses with valuable data to optimize their operations.\n\nFurthermore, the Real Time Object Detection and Tracking on the Edge solution helps address the need for privacy in video feeds. The solution offers the ability to obfuscate the detected image for privacy while still providing real-time object detection and tracking. This ensures that the privacy of individuals captured on the video feed is protected while still providing businesses with valuable data to optimize their operations.\n\nDeployment on the Edge",
        "chunk_id": "chunk380_6",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/real-time-object-detection-and-tracking-on-edge\/index.md",
        "root_chunk_id": "chunk380"
    },
    {
        "user_prompt": "How can we ensure that developers have efficient access to the latest release artifacts in our DevOps workflow?",
        "output_prompt": "To ensure developers have efficient access to the latest release artifacts, the CI\/CD pipeline could be configured to automatically build and publish executables for each pull request or merge into the main branch. A well-known, centralized location such as an artifact repository or a shared drive should be used to store these executables, enabling developers to easily find and download them. Additionally, integrating observability tools can help keep the team informed about the state of builds and releases by centralizing logs and status updates.",
        "context": "Feature flagging\n\nA\/B testing\n\nTraffic shifting\n\nDeveloper Access to the Latest Release Artifacts\n\nOur devops workflow should enable developers to get, install and run the latest system executable. Release executable(s) should be auto generated as part of our CI\/CD pipeline(s).\n\nDevelopers can access latest executable\n\nThe latest system executable is available for all developers on the team. There should be a well-known place where developers can reference the release artifact.\n\nRelease artifact is published for each pull request or merges into main branch\n\nIntegration Observability\n\nApplied state changes to the mainline build should be made available and communicated across the team. Centralizing logs and status(s) from build and release pipeline failures are essential for developers investigating broken builds.",
        "chunk_id": "chunk225_24",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-integration\/README.md",
        "root_chunk_id": "chunk225"
    },
    {
        "user_prompt": "Can you explain why GigE protocol would be the best choice for connecting cameras to the CVT via Ethernet?",
        "output_prompt": "GigE, or Gigabit Ethernet, is an excellent choice for connecting cameras to the CVT via Ethernet due to its low latency and high bandwidth. It supports high-speed data transfer that is vital for high-resolution images and video streaming in real-time applications. Since Ethernet interfaces are commonly available, it also ensures ease of integration. Additionally, using GigE Vision cameras that are based on the GenICam standard allows for interoperability and simplifies the implementation process within .NET environments, as there are already established codebases with mixed MIT and Apache licenses that can be leveraged, though some new code will be required. The existing code has undergone optimizations and bug fixing, which further contributes to a robust and efficient setup.",
        "context": "Why GigE?\n\nFor the connection we propose to use cameras with the GigE Protocol which connect to the CVT via Ethernet. Advantage is that ethernet is a common interface and the GigE Protocol provides very low latency and high bandwidth.\n\nThis document gives more details about the implementation for .NET. In short, we can leverage the work done to support Gige Vision cameras which are GenICam based. The code is a mix of MIT and Apache license. Part of the code can be fully reused and new code has to be added. Quite a lot of optimization and bug fixes has been done.\n\nAssumptions & Guardrails\n\nWe assume as that all camera implement GenICam interfaces allowing to take pictures, adjust some of the elements.",
        "chunk_id": "chunk375_1",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/computer-vision-on-edge\/camera-interface.md",
        "root_chunk_id": "chunk375"
    },
    {
        "user_prompt": "Can you explain how to use the write-good package for checking prose in Markdown files?",
        "output_prompt": "Certainly! To use the write-good package, you can install it globally on your system with `npm install -g write-good`. After installation, you can run it from the command line with `write-good *.md` to check all markdown files in the current directory. If you prefer not to install it globally, you can use npx to run it without installation: `npx write-good *.md`. Additionally, for regular use within VS Code, you can install the Write Good Linter Extension, which will provide grammar and language advice as you edit Markdown documents. The markdownlint extension also helps by highlighting rule violations in your Markdown files, but it's more focused on format and structure rather than prose.",
        "context": "{% raw %}\n\nbash\nnpm install -g write-good\n\n{% endraw %}\n\nRun write-good\n\n{% raw %}\n\nbash\nwrite-good *.md\n\n{% endraw %}\n\nRun write-good without installing it\n\n{% raw %}\n\nbash\nnpx write-good *.md\n\n{% endraw %}\n\nWrite Good is also available as an extension for VS Code\n\nVS Code Extensions\n\nWrite Good Linter\n\nThe Write Good Linter Extension integrates with VS Code to give grammar and language advice while editing the document.\n\nmarkdownlint extension\n\nThe markdownlint extension examines the Markdown documents, showing warnings for rule violations while editing.\n\nBuild Validation\n\nLinting",
        "chunk_id": "chunk14_4",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/markdown.md",
        "root_chunk_id": "chunk14"
    },
    {
        "user_prompt": "Can you tell me what the Dockerfile in the .devcontainer directory does with respect to markdownlint?",
        "output_prompt": "The Dockerfile installs markdownlint-cli globally using npm. This tool can be used to lint and check for issues in markdown files within the container's environment.",
        "context": "The file .devcontainer\/Dockerfile installs markdownlint\n\n{% raw %}\n\n```dockerfile\n...\nRUN apt-get update \\\n    && export DEBIAN_FRONTEND=noninteractive \\\n    && apt-get install -y nodejs npm\n\nAdd NodeJS tools\n\nRUN npm install -g markdownlint-cli\n...",
        "chunk_id": "chunk246_9",
        "source": "..\/data\/docs\/code-with-engineering\/developer-experience\/going-further.md",
        "root_chunk_id": "chunk246"
    },
    {
        "user_prompt": "What are some effective strategies for improving project transition to another team given the common documentation challenges?",
        "output_prompt": "Effective strategies for improving project transition include creating comprehensive onboarding documentation, maintaining a detailed project wiki, developing a clear README file, outlining contribution guidelines, and establishing consistent code conventions. Additionally, ensuring that documentation is easily accessible and regularly updated can significantly facilitate smoother transitions between teams.",
        "context": "Improve the transition of the project to another team.\n\nChallenges\n\nWhen working in an engineering project, we typically encounter one or more of these challenges related to documentation (including some examples):\n\nNon-existent.\n\nNo onboarding documentation, so it takes a long time to set up the environment when you join the project.\n\nNo document in the wiki explaining existing repositories, so you cannot tell which of the 10 available repositories you should clone.\n\nNo main README, so you don't know where to start when you clone a repository.\n\nNo \"how to contribute\" section, so you don't know which is the branch policy, where to add new documents, etc.\n\nNo code guidelines, so everyone follows different naming conventions, etc.\n\nHidden.",
        "chunk_id": "chunk96_1",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/README.md",
        "root_chunk_id": "chunk96"
    },
    {
        "user_prompt": "What types of tests are necessary for ensuring an ML model is configured correctly before it spends hours in training?",
        "output_prompt": "Necessary tests for an ML model include checking if the model accepts the correct inputs and produces the expected outputs, and if the model weights update during training. These tests are similar to narrow integration tests, involving model build verification, input-output shape comparison, and weight update checks.",
        "context": "Does the model accept the correct inputs and produce the correctly shaped outputs?\n\nDo the weights of the model update when running fit?\n\nTo do this, the ML model tests do not strictly follow best practices of standard Unit tests - not all outside calls are mocked. These tests are much closer to a narrow integration test.\nHowever, the benefits of having simple tests for the ML model help to stop a poorly configured model from spending hours in training, while still producing poor results.\n\nExamples of how to implement these tests (for Deep Learning models) include:\n\nBuild a model and compare the shape of input layers to that of an example source of data. Then, compare the output layer shape to the expected output.",
        "chunk_id": "chunk214_17",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-testing.md",
        "root_chunk_id": "chunk214"
    },
    {
        "user_prompt": "How do I run only the tests that are not considered long-running for my machine learning project?",
        "output_prompt": "To run tests that aren't marked as 'longrunning', you can use the following command in your terminal: pytest -v -m \"not longrunning\". This command will execute all tests excluding those marked with the 'longrunning' marker.",
        "context": "{% endraw %}\n\nRun all tests that are not marked longrunning\n\n{% raw %}\n\nbash\npytest -v -m \"not longrunning\"\n\n{% endraw %}\n\nBasic Unit Tests for ML Models\n\nML unit tests are not intended to check the accuracy or performance of a model. Unit tests for an ML model is for code quality checks - for example:\n\nDoes the model accept the correct inputs and produce the correctly shaped outputs?\n\nDo the weights of the model update when running fit?",
        "chunk_id": "chunk214_16",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-testing.md",
        "root_chunk_id": "chunk214"
    },
    {
        "user_prompt": "create a function that calculates the area of a circle",
        "output_prompt": "function calculateCircleArea(radius) {\n    return Math.PI * radius * radius;\n}",
        "context": "Example 1\n\n--INPUT\nSTATEMENT = create a cube\nEXPECTED = makeCube()\nGENERATED = makeCube(n='cube1')\n--ANSWER\nSCORE: 4\nEXPLANATION: Both completions are valid for creating a cubes . However, the GENERATED one differs by including the cube name (n=cube1), which is not necessary.\n\nExample 2",
        "chunk_id": "chunk280_25",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/metrics\/eval-metrics.md",
        "root_chunk_id": "chunk280"
    },
    {
        "user_prompt": "How can Microsoft Presidio assist in handling sensitive data within Azure Cognitive Search?",
        "output_prompt": "Microsoft Presidio can be integrated as a custom skill in Azure Cognitive Search to identify and mask sensitive or personally identifiable information (PII) in unstructured text, helping to ensure data privacy and compliance.",
        "context": "Power Skills are a collection of useful functions to be deployed as custom skills for Azure Cognitive Search. Microsoft Presidio Presidio can help identify sensitive\/PII data in un\/structured text. PII Detection Cognitive Skill The PII Detection skill extracts personal information from an input text and gives you the option of masking it. End to end Knowledge Mining for Video A video discovery pipeline that includes Azure Search and user feedback",
        "chunk_id": "chunk251_8",
        "source": "..\/data\/docs\/code-with-mlops\/data-discovery.md",
        "root_chunk_id": "chunk251"
    },
    {
        "user_prompt": "Could you provide an overview of Ranorex Studio's integration with Azure DevOps?",
        "output_prompt": "Certainly! Ranorex Studio allows you to integrate your automated tests with Azure DevOps (AzDO). By doing so, you can trigger tests as part of a build or release pipeline. This integration ensures that automated tests are consistently executed as part of the Continuous Integration\/Continuous Deployment (CI\/CD) process, which helps in identifying any regressions early in the development cycle, thus maintaining the quality of your application without delaying the release schedule.",
        "context": "Perfecto (TestCraft) Website or get it  from the Visual Studio Marketplace\n\n4. Ranorex Studio\n\nRanorex Studio is a complete end-to-end test automation tool for desktop, web, and mobile applications. Create reliable tests fast without any coding at all, or using the full IDE. Use external CSV or Excel files, or a SQL database as inputs to your tests.\n\nRun tests in parallel or on a Selenium Grid with built-in Selenium WebDriver. Ranorex Studio integrates with your CI\/CD process to shorten your release cycles without sacrificing quality.\n\nRanorex Studio tests also integrate with Azure DevOps (AzDO), which can be run as part of a build pipeline in AzDO.\n\nRanorex Studio Website\n\n5. Katalon Studio",
        "chunk_id": "chunk81_11",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/README.md",
        "root_chunk_id": "chunk81"
    },
    {
        "user_prompt": "What is the purpose of specifying timeouts for all API endpoints in the context of simulated bundle applications?",
        "output_prompt": "Specifying timeouts for all API endpoints ensures that simulations are completed within a defined time frame, preventing them from running indefinitely and potentially consuming excessive resources or stalling the simulation environment.",
        "context": "\u00b9 all API endpoints will have specified timeouts, to ensure the simulations finish.\n\nSimulated Bundle Application\n\nFor running bundles on a simulated environment we will use a sidecar pattern, i.e. multi-container application with two applications inside:\n\nBundle application\n\nHelper application to test bundle API\n\nTwo applications are running together and can communicate via 'localhost'.\n\nThe diagram below shows this flow:\n\nKubernetes approach\n\nSimulator Orchestrator is an application running on AKS cluster and this application can connect to Kubernetes API.\nSimulation Environment is AKS Cluster.\nSimulated Bundle Application is a multi-container pod.\n\nAKS Security",
        "chunk_id": "chunk378_3",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/computer-vision-on-edge\/cvt-bundle-simulator.md",
        "root_chunk_id": "chunk378"
    },
    {
        "user_prompt": "What are the differences in managing feature transformation pipelines with and without a feature store?",
        "output_prompt": "Without a feature store, there is often a duplication of feature transformation code and pipelines, which is costly and challenging to manage, especially when making changes or upgrades. With a feature store, however, the shareability of features tends to reduce the number of duplicated pipelines. Moreover, feature stores typically manage the serving of features for both training (batch or high-latency) and inference (low-latency) through a unified system, in contrast to the separate pipelines needed when not using a feature store.",
        "context": "Decision point Without feature store With feature store Do you maintain many duplicated feature transformation code\/pipelines? When data scientists are unaware of existing features, there will be a propensity to create and manage duplicate pipelines to perform feature transformations. Managing all these pipelines is expensive and demands a lot of attention from a platform team when making changes or upgrades. Given the predilection for shareability in a feature store, the number of duplicated feature transformation pipelines should be reduced in favor of reusing existing features. Do you have to serve features for training (batch or high-latency) and inference (low-latency)? Processing historical (for training) and streaming (for inference) data is done differently and requires separate pipelines. These pipelines might process the data using different methods and technologies that are specific to how the data is ingested (batch vs. streaming) and store the results in various",
        "chunk_id": "chunk328_9",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/model-development\/feature-management\/feature-store-adoption-guide.md",
        "root_chunk_id": "chunk328"
    },
    {
        "user_prompt": "Can you explain how on-premises LLM deployments integrate with APIM through VNet and the benefits of such an integration?",
        "output_prompt": "Certainly! On-premises LLM deployments can be integrated with Azure API Management (APIM) through a Virtual Network (VNet). This integration allows APIM to emit custom metrics to Azure Monitor, which provides observability for the applications using LLM resources. Detailed observability helps in proactively identifying and addressing issues, ensuring operational excellence.",
        "context": "Operational Excellence: On-premises LLM deployments can be integrated with APIM through VNet. APIM can emit custom metrics to Azure Monitor to enable observability of the applications consuming LLM resources. This approach is elaborated here\n\nCost Optimization: APIM can record the consumption of AOAI resource in Azure Monitor for each subscription and product, which allows allocating the costs to the right business unit within the organization. This section covers the implementation details.\n\n1.1.1 Key Considerations",
        "chunk_id": "chunk386_3",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/genai-gateway\/index.md",
        "root_chunk_id": "chunk386"
    },
    {
        "user_prompt": "What is the purpose of the 'metadata' section in this JSON file?",
        "output_prompt": "The 'metadata' section in the JSON file is used to specify source and destination directories for certain files, indicate which files to include and exclude, and an option to disable Git features for the operation.",
        "context": "json\n{\n    \"metadata\": [\n    {\n          \"src\": [\n          {\n              \"files\": [ \"src\/**.csproj\" ],\n              \"exclude\": [ \"_site\/**\", \"**\/bin\/**\", \"**\/obj\/**\", \"**\/[Tt]ests\/**\" ]\n          }\n          ],\n          \"dest\": \"reference\",\n          \"disableGitFeatures\": false\n       }",
        "chunk_id": "chunk110_13",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/using-docfx-and-tools.md",
        "root_chunk_id": "chunk110"
    },
    {
        "user_prompt": "How should the Bootstrap method be modified to correctly register singleton services in both DEBUG and RELEASE configurations?",
        "output_prompt": "To correctly register singleton services for both DEBUG and RELEASE configurations, the Bootstrap method should use preprocessor directives to conditionally register the appropriate services. The code should be modified as follows:\n\n```cs\npublic static void Bootstrap(IUnityContainer container)\n{\n#if DEBUG\n    \/\/ Register DEBUG-specific singletons\n    container.RegisterSingleton<IYourDebugService, YourDebugServiceImplementation>();\n#else\n    \/\/ Register RELEASE-specific singletons\n    container.RegisterSingleton<IYourReleaseService, YourReleaseServiceImplementation>();\n#endif\n}\n```",
        "context": "{% raw %}\n\n```cs\npublic static void Bootstrap(IUnityContainer container)\n{\n\nif DEBUG\n\ncontainer.RegisterSingleton();\n\nelse\n\ncontainer.RegisterSingleton();\n\nendif\n\n}",
        "chunk_id": "chunk242_4",
        "source": "..\/data\/docs\/code-with-engineering\/developer-experience\/client-app-inner-loop.md",
        "root_chunk_id": "chunk242"
    },
    {
        "user_prompt": "How can I construct a valid JSON object to represent CI configuration for my repository with the provided context?",
        "output_prompt": "To represent your CI configuration as a valid JSON object based on the provided context, ensure that your JSON object has a 'ci' key set to a boolean, a 'repositoryUrl' key with a string value representing your repository URL, and a 'branches' key which is an array of objects. Each branch object should have a 'name' key and, if applicable, a 'prerelease' key with appropriate string values. Remember to close all braces and brackets, and make sure to follow proper JSON syntax.",
        "context": "json\n{\n    \"ci\": true,\n    \"repositoryUrl\": \"your repository url\",\n    \"branches\": [\n      \"master\",\n      {\n        \"name\": \"feature\/*\",\n        \"prerelease\": \"beta-${name.replace(\/\\\\\/\/g, '-').replace(\/_\/g, '-')}\"\n      },\n      {\n        \"name\": \"[a-zA-Z0-9_]+\/[a-zA-Z0-9-_]+\",",
        "chunk_id": "chunk200_10",
        "source": "..\/data\/docs\/code-with-engineering\/source-control\/component-versioning.md",
        "root_chunk_id": "chunk200"
    },
    {
        "user_prompt": "What does the git command 'git branch -u origin\/wikiMaster' do?",
        "output_prompt": "The command 'git branch -u origin\/wikiMaster' sets the local branch named 'wikiMaster' to track the remote branch 'wikiMaster' from the repository referred to as 'origin'. This means any future git pull or push commands run on this local branch will be automatically synced with the 'wikiMaster' branch on the remote 'origin' repository.",
        "context": "PS C:\\Git\\MyProject.wiki> git branch -u origin\/wikiMaster\nBranch 'wikiMaster' set up to track remote branch 'wikiMaster' from 'origin'.",
        "chunk_id": "chunk111_5",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/sync-wiki-between-repos.md",
        "root_chunk_id": "chunk111"
    },
    {
        "user_prompt": "Can you explain the process of Test-Driven Development?",
        "output_prompt": "Certainly! Test-Driven Development (TDD) is a programming approach where you initially write a failing test that reflects the desired functionality. Then you write the minimal system code to make that test pass. Once the test passes, you refactor the code for improvement while ensuring it still passes all the tests. You continue this cycle of test-code-refactor with each new piece of functionality, which helps create robust and error-resistant code.",
        "context": "Test-Driven Development Example\n\nWith this method, rather than writing all your tests up front, you write one test at a time and then switch to write the\nsystem code that would make that test pass. It's important to write the bare minimum of code necessary even if it is not\nactually \"correct\". Once the test passes you can refactor the code to make it maybe make more sense, but again the logic\nshould be simple. As you write more tests, the logic gets more and more complex, but you can continue to make the\nminimal changes to the system code with confidence because all code that was written is covered.",
        "chunk_id": "chunk90_0",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/unit-testing\/tdd_example.md",
        "root_chunk_id": "chunk90"
    },
    {
        "user_prompt": "Could you explain the difference between ROUGE-N and ROUGE-L and in what scenarios each would be more applicable?",
        "output_prompt": "ROUGE-N is an evaluation metric that focuses on the overlap of n-grams between the machine-generated text and a set of reference summaries. It's used to assess the quality of machine translation and text summarization by counting the number of n-grams that appear in both the generated text and the reference. On the other hand, ROUGE-L measures the longest common subsequence between the generated text and the reference summaries. It evaluates the fluency of the text and is less restrictive as it doesn't require consecutive word sequences to match. ROUGE-N is often used when the exact wording is important, such as in translation, while ROUGE-L is more applicable for summarization tasks where the flow of the narrative is more important than the specific words used.",
        "context": "ROUGE\n\nAs opposed to the BLEU score, the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) evaluation metric measures the recall. It\u2019s typically used for evaluating the quality of generated text and in machine translation tasks. However, since it measures recall, it's used in summarization tasks. It\u2019s more important to evaluate the number of words the model can recall in these types of tasks.\n\nThe most popular evaluation metrics from the ROUGE class are ROUGE-N and ROUGE-L:",
        "chunk_id": "chunk280_8",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/metrics\/eval-metrics.md",
        "root_chunk_id": "chunk280"
    },
    {
        "user_prompt": "Can you tell me the differences between Mimesis, Faker, and plait.py for generating fake data?",
        "output_prompt": "Certainly! Mimesis is a high-performance fake data generator that focuses on providing data for various purposes across multiple languages. Faker is also a Python package for generating fake data, but it is widely used for bootstrapping databases, creating documents, stress testing, or anonymizing production data. On the other hand, plait.py is unique in that it allows for the modeling of fake data with an interesting shape, by stitching together variables into a more coherent model, rather than just a collection of IID variables.",
        "context": "Mimesis a high-performance fake data generator for Python, which provides data for a variety of purposes in a variety of languages. Faker Faker is a Python package that generates fake data for you. Whether you need to bootstrap your database, create good-looking XML documents, fill-in your persistence to stress test it, or anonymize data taken from a production service, Faker is for you. Plaitpy The idea behind plait.py is that it should be easy to model fake data that has an interesting shape. Currently, many fake data generators model their data as a collection of IID variables; with plait.py we can stitch together those variables into a more coherent model.",
        "chunk_id": "chunk116_8",
        "source": "..\/data\/docs\/code-with-engineering\/privacy\/privacy-frameworks.md",
        "root_chunk_id": "chunk116"
    },
    {
        "user_prompt": "What is sensitive data classification and how does it contribute to data loss prevention?",
        "output_prompt": "Sensitive data classification is the process of identifying and categorizing data stored by an organization based on its level of sensitivity. This classification enables an organization to apply appropriate security measures and access controls to protect the data. It contributes to data loss prevention by ensuring that each category of data receives the right level of protection, which reduces the risk of unauthorized access or breaches.",
        "context": "Data loss prevention\n\nOrganizations have sensitive information under their control such as financial data, proprietary data, credit card numbers, health records, or social security numbers.\nTo help protect this sensitive data and reduce risk, they need a way to prevent their users from inappropriately sharing it with people who shouldn't have it.\nThis practice is called data loss prevention (DLP).\n\nBelow we focus on two aspects of DLP: Sensitive data classification and Access management.\n\nSensitive data classification\n\nSensitive data classification is an important aspect of DLP, as it allows organizations to track, monitor, secure and identify sensitive and private data.\nFurthermore, different sensitivity levels can be applied to different data items, facilitating proper governance and cataloging.\n\nThere are typically four levels data classification levels:\n\nPublic\n\nInternal\n\nConfidential",
        "chunk_id": "chunk116_17",
        "source": "..\/data\/docs\/code-with-engineering\/privacy\/privacy-frameworks.md",
        "root_chunk_id": "chunk116"
    },
    {
        "user_prompt": "Can you explain how to test a method that uses a mock object without relying on the mock's return value?",
        "output_prompt": "Certainly! To test a method that uses a mock object without focusing on the mock's return value, you should verify that the method interacts with the mock correctly. In the case of the 'GetName' function, you would use the verify method to check that the search client was called with the proper arguments. This approach ensures that the method\u2019s logic is correct by confirming that it interacts with its dependencies as expected, rather than just validating the mock's response.",
        "context": "But now, your method could look like this, and the test would still pass:\n\n{% raw %}\n\ncsharp\npublic String GetName(string id)\n{\n   return \"myResult\";\n}\n\n{% endraw %}\n\nSimilarly, if you set up your mock wrong, the test would fail even though the logic inside the method is sound. For efficient\nassertions that will give you confidence in your SUT, make assertions on your logic, not mock return values.\nThe simple example above doesn\u2019t have a lot of logic, but you want to make sure that it calls the search client to retrieve\nthe result. For this, you can use the verify method to make sure the search client was called using the right parameters even\nthough you don\u2019t care about the result.",
        "chunk_id": "chunk88_17",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/unit-testing\/mocking.md",
        "root_chunk_id": "chunk88"
    },
    {
        "user_prompt": "When working with git, can you explain the advantages of using a linear commit history over a non-linear one?",
        "output_prompt": "Using a linear commit history in git has several advantages. It creates a cleaner and more straightforward history, which makes it easier to understand the sequence of commits. This can facilitate debugging or identifying when specific changes were introduced. It's also beneficial when bisecting to find the source of a bug, as there are no side branches to consider. Additionally, a linear history can make the process of code reviews simpler, as each commit reflects a direct extension of the base branch.",
        "context": "Merge strategies\n\nAgree if you want a linear or non-linear commit history. There are pros and cons to both approaches:\n\nPro linear: Avoid messy git history, use linear history\n\nCon linear: Why you should stop using Git rebase\n\nApproach for non-linear commit history\n\nMerging topic into main\n\n{% raw %}\n\n```md\n  A---B---C topic\n \/         \\\nD---E---F---G---H main\n\ngit fetch origin\ngit checkout main\ngit merge topic",
        "chunk_id": "chunk201_0",
        "source": "..\/data\/docs\/code-with-engineering\/source-control\/merge-strategies.md",
        "root_chunk_id": "chunk201"
    },
    {
        "user_prompt": "Considering the need for immediate reflection of updates in our enterprise search index, what strategy should we use?",
        "output_prompt": "For immediate or near real-time updates of the search index in an enterprise setting, it is best to employ a push strategy. This method allows for more timely and controlled indexing of documents as soon as they become available or are updated.",
        "context": "As per multiple experiments run by our product team (published here) and our crews, it is recommended to use semantic hybrid search for most of the scenarios. However, the accuracy of chosen search method can be variant case by case.\n\nIndexing and Index Enrichment Strategies\n\nIndexer Scheduling Strategies\n\nEnterprise projects may require updating the index all the time. For example, you may work on important document updates that are time-critical and require immediate (near real-time) reflection into index, or in some cases, a regular update. Depending on your business scenario, there are 2 ways to ingest documents into Azure Cognitive Search, namely:\n\nPull Strategy",
        "chunk_id": "chunk315_11",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/search\/index.md",
        "root_chunk_id": "chunk315"
    },
    {
        "user_prompt": "How can Microsoft Purview help in enhancing data security and compliance within an organization?",
        "output_prompt": "Microsoft Purview aids organizations by offering tools for data discovery, classification, and risk assessment. It allows security administrators to define sensitive data types and establish compliance policies. With its ability to monitor various data sources, it helps quickly detect and govern sensitive data, ensuring it complies with the company's evolving security policies, and preventing unauthorized access or data leaks.",
        "context": "Understanding the risk levels in your organization's data requires diving deep into it, looking for keywords, RegEx patterns, and sensitive data types. For example, sensitive data types might include Credit Card numbers, Social Security numbers or Bank Account numbers. You must constantly monitor all data sources for sensitive content, as even the smallest amount of data loss can be critical to your organization.\n\nEnsuring compliance with corporate security policies can be a challenge for organizations. As their content grows and the policies are updated to address evolving digital realities. Security administrators need to ensure data security in the quickest time possible.\n\nMicrosoft Purview provides capabilities to help address these challenges.\n\nData discovery from an MLOps perspective",
        "chunk_id": "chunk251_4",
        "source": "..\/data\/docs\/code-with-mlops\/data-discovery.md",
        "root_chunk_id": "chunk251"
    },
    {
        "user_prompt": "Can you elaborate on the idea of 'shift-left' in DevSecOps and why it is important?",
        "output_prompt": "The 'shift-left' concept in DevSecOps refers to integrating security practices early in the software development lifecycle instead of treating it as a final step. This approach is important because it allows teams to detect and address security issues earlier, which can reduce the cost and complexity of fixing vulnerabilities, improve compliance with security standards, and enhance the overall security and quality of the software.",
        "context": "DevSecOps\n\nThe concept of DevSecOps\n\nDevSecOps or DevOps security is about introducing security earlier in the life cycle of application development (a.k.a shift-left), thus minimizing the impact of vulnerabilities and bringing security closer to development team.\n\nWhy\n\nBy embracing shift-left mentality, DevSecOps encourages organizations to bridge the gap that often exists between development and security teams to the point where many of the security processes are automated and are effectively handled by the development team.\n\nDevSecOps Practices\n\nThis section covers different tools, frameworks and resources allowing introduction of DevSecOps best practices to your project at early stages of development.\nTopics covered:\n\nCredential Scanning - automatically inspecting a project to ensure that no secrets are included in the project's source code.",
        "chunk_id": "chunk229_0",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-integration\/dev-sec-ops\/README.md",
        "root_chunk_id": "chunk229"
    },
    {
        "user_prompt": "How should a design sprint be executed to ensure that designs are completed within the sprint and benefit from brevity?",
        "output_prompt": "To ensure designs are completed within a sprint and maintain brevity, you should narrow the scope of the design, limit evaluations to two or three solutions, and design experiments to collect evidence quickly. Begin with a clear problem description, including assumptions, constraints, and requirements for the system. Optionally, create a diagram to visualize the system components. The aim is to focus on essential elements that lead to achieving the desired outcomes efficiently.",
        "context": "IMPORTANT Designs should be completed within a sprint. Most designs will benefit from brevity. To accomplish this:\n\nNarrow the scope of the design.\n\nNarrow evaluation to 2 to 3 solutions.\n\nDesign experiments to collect evidence as fast as possible.\n\nOverview\n\nDescription of the problem we are solving. This should include:\n\nAssumptions about the rest of the system\n\nConstraints that apply to the system, both business and technical\n\nRequirements for the functionality that needs to be implemented, including possible inputs and outputs\n\n(optional) A diagram showing the different pieces\n\nDesired Outcomes",
        "chunk_id": "chunk47_1",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/trade-studies\/template.md",
        "root_chunk_id": "chunk47"
    },
    {
        "user_prompt": "How can I get notified when a machine learning model run has completed in Azure ML?",
        "output_prompt": "You can set up an event grid subscription for the Microsoft.MachineLearningServices.RunCompleted event, using the subject format 'experiments\/{ExperimentId}\/runs\/{RunId}' to get notified when a model run has completed in Azure ML.",
        "context": "Here are the supported Azure ML events:\n\nEvent type Subject format Microsoft.MachineLearningServices.RunCompleted experiments\/{ExperimentId}\/runs\/{RunId} Microsoft.MachineLearningServices.ModelRegistered models\/{modelName}:{modelVersion} Microsoft.MachineLearningServices.ModelDeployed endpoints\/{serviceId} Microsoft.MachineLearningServices.DatasetDriftDetected datadrift\/{data.DataDriftId}\/run\/{data.RunId} Microsoft.MachineLearningServices.RunStatusChanged experiments\/{ExperimentId}\/runs\/{RunId}",
        "chunk_id": "chunk303_24",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/sync-async-mlops-patterns.md",
        "root_chunk_id": "chunk303"
    },
    {
        "user_prompt": "Can you compare Databricks FS, Feathr, and Azure Managed Feature Store in terms of integration with other data sources and systems?",
        "output_prompt": "Databricks FS, being proprietary to the Databricks environment, does not offer integration with other data sources outside of this ecosystem. Feathr, on the other hand, being an open-source feature store, supports processing data from multiple sources and can persist data into different storage systems, thereby providing a higher level of integration with other data sources and systems. Azure Managed Feature Store details are not provided in the context, so a comparison on this front cannot be made without additional information.",
        "context": "Databricks FS\n\nDatabricks FS is a proprietary Feature Store solution provided within the Databricks environment.\nIt makes use of the Delta Lake file system that Databricks uses to provide a solution that works with different data versions. It only works within Databricks and integration with other data sources is not available.\n\nFeathr\n\nFeathr is an open-source Feature Store created by LinkedIn and Microsoft.\n\nIn terms of functionality, Feathr provides a Feature Registry, support for Feature Transformation through its built-in functions, and the functionality to share features across teams.\n\nFeathr runs the feature computation on Spark against incoming data from multiple sources. It supports different storage systems to persist that data after it has been processed for consumption at training or inference time.\n\nAzure Managed Feature Store",
        "chunk_id": "chunk327_2",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/model-development\/feature-management\/feature-store-comparison.md",
        "root_chunk_id": "chunk327"
    },
    {
        "user_prompt": "Can you explain the relationship between low code solutions and continuous deployment practices?",
        "output_prompt": "Low code solutions facilitate quick and efficient application development, which aligns well with continuous deployment practices that aim for frequent and reliable software releases. Continuous deployment tools mentioned, like Flux, Tekton, and Flagger, help automate the deployment process, making it easier to integrate changes from low code platforms into production environments. This ensures a seamless, automated progression from code change to deployment, which is crucial for maintaining the high speed of delivery afforded by low code development.",
        "context": "Low code solutions\n\nLow code solutions have increased their participation in the applications and processes and because of that it is required that a proper conjunction of disciplines improve their development.\n\nHere is a guide for continuous deployment for Low Code Solutions.\n\nReferences\n\nContinuous Delivery by Jez Humble, David Farley.\n\nContinuous integration vs. continuous delivery vs. continuous deployment\n\nDeployment Rings\n\nTools\n\nCheck out the below tools to help with some CD best practices listed above:\n\nFlux for gitops\n\nCI\/CD workflow using GitOps\n\nTekton for Kubernetes native pipelines\n\nNote Jenkins-X uses Tekton under the hood.\n\nArgo Workflows\n\nFlagger for powerful, Kubernetes native releases including blue\/green, canary, and A\/B testing.",
        "chunk_id": "chunk128_14",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/README.md",
        "root_chunk_id": "chunk128"
    },
    {
        "user_prompt": "What is the typical size of a JSON object and at what point should I consider splitting it into smaller pieces?",
        "output_prompt": "The typical size of a JSON object can vary greatly, but the size becomes concerning when it impairs performance, exceeds storage limits, or makes manipulation cumbersome. There's no exact size limit, but if you encounter performance issues or face difficulty in managing the JSON, it could be time to split it into smaller, more manageable pieces. Ideal JSON object sizes should allow for quick parsing and minimal impact on memory and bandwidth.",
        "context": "}\n    }\n}",
        "chunk_id": "chunk256_24",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "What can you tell me about the strength of the USD?",
        "output_prompt": "The strength of the USD, or U.S. Dollar, often refers to its value relative to other currencies. To provide a more accurate response, could you specify which currency you would like to compare the USD against, or are you looking for its overall performance in the foreign exchange market?",
        "context": "In this project, we observe that most questions are context-dependent, and when such context information is missing, it will be difficult in both system development and judgment experiments. For example, when a user is asking about \"strength of the USD\", the user assumes a currency compared to the USD, which is often the local currency. Similarly, users often omit specific market names, having assumptions of the market based on their customers and their own profile. This missing information results in ambiguous queries, and in some cases, even domain-experts find it difficult to understand the intent.\n\nJudgment Set",
        "chunk_id": "chunk315_49",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/search\/index.md",
        "root_chunk_id": "chunk315"
    },
    {
        "user_prompt": "How can GenAI gateway contribute to real-time data analysis and decision making?",
        "output_prompt": "The GenAI gateway can facilitate real-time data analysis and decision making by intercepting requests or responses to extract information, creating custom events, and pushing them into real-time messaging systems like Kafka. These events can then be processed by event aggregators for live dashboards or to trigger actions based on predefined rules.",
        "context": "Here are some suggested approaches on how this can be achieved using\nGenAI gateway:\n\nEmitting Custom Events to Real-Time Messaging System: The GenAI\n    gateway can intercept requests or responses and extract relevant\n    information to create events and push them asynchronously into\n    real-time messaging systems like Kafka or Azure EventHub. These\n    events can be consumed by a streaming event aggregator (e.g., Azure\n    Stream Analytics) on a near-real-time basis to populate a data store\n    for dashboards or triggering actions based on certain rules.",
        "chunk_id": "chunk346_5",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/genai-gateway\/operational-excellence.md",
        "root_chunk_id": "chunk346"
    },
    {
        "user_prompt": "What are the advantages of early API design feedback in a development process?",
        "output_prompt": "Early API design feedback can lead to clearly established expectations for API consumer and producer, allowing both parties to agree upon an API contract. This enables development teams to work in parallel and allows test teams to create early tests based on API contracts, which could provide valuable input. Additionally, API definitions remain consistent during agile development cycles and are not influenced by implementation details or code structure. Server-side boilerplate code such as controllers and DTOs can be auto-generated from API contracts, which may also enhance collaboration between API producer and consumer teams.",
        "context": "Early API design feedback.\n\nClearly established expectations for both consumer & producer as both have agreed upon an API contract.\n\nDevelopment teams can work in parallel.\n\nTesting team can use API contracts to write early tests even before business logic is in place. By looking at different models, paths, attributes and other aspects of the API testing can provide their input which can be very valuable.\n\nDuring an agile development cycle API definitions are not impacted by incremental dev changes.\n\nAPI design is not influenced by actual implementation limitations & code structure.\n\nServer side boilerplate code e.g. controllers, DTOs etc. can be auto generated from API contracts.\n\nMay improve collaboration between API producer & consumer teams.\n\nPlanning a Design-First Development:\n\nIdentify use cases & key services which API should offer.",
        "chunk_id": "chunk29_3",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-patterns\/rest-api-design-guidance.md",
        "root_chunk_id": "chunk29"
    },
    {
        "user_prompt": "Can you explain how 'Collaboration' is important in a business environment?",
        "output_prompt": "Collaboration is crucial in a business setting as it facilitates the removal of barriers through joint efforts towards a common goal or customer benefit. It also involves embracing diverse perspectives which can lead to more comprehensive solutions for complex business challenges.",
        "context": "]\n    },\n    {\n      \"id\": \"abc123\",\n      \"title\": \"Collaboration\",\n      \"behaviors\": [\n        {\n          \"description\": \"Removes barriers by working with others around a shared need or customer benefit.\"\n        },\n        {\n          \"description\": \" Incorporates diverse perspectives to thoroughly address complex business issues.\"\n        }\n      ]\n    }\n  ]\n}",
        "chunk_id": "chunk42_15",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/examples\/memory\/Architecture\/Data-Model.md",
        "root_chunk_id": "chunk42"
    },
    {
        "user_prompt": "How do synthetic monitoring tests help in asserting system behavior?",
        "output_prompt": "Synthetic monitoring tests help in asserting system behavior by simulating user interactions with the system to predict its performance and functionality. These tests run at regular intervals to provide continuous validation of important user flows, ensuring that the system meets its Service Level Objectives (SLOs) and aligns with the overall Key Results (OKRs). By comparing the synthetic test data with actual user monitoring data (RUMs), it's possible to detect deviations and issues early, allowing for proactive system adjustments and maintenance.",
        "context": "Monitoring\n\nGiven that Synthetic Monitoring tests are continuously running, at intervals, in a production environment, the assertion of system behavior through analysis relies on existing monitoring pillars used in live system (Logging, Metrics, Distributed Tracing).\nThere would usually be a finite set of tests, and key metrics that are used to build monitors and alerts to assert against the known SLO, and verify that the OKR for that system are maintained. The monitoring tools are effectively capturing both RUMs and synthetic data generated by the probes.\n\nApplying Synthetic Monitoring Tests\n\nAsserting the system under tests",
        "chunk_id": "chunk75_6",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/synthetic-monitoring-tests\/README.md",
        "root_chunk_id": "chunk75"
    },
    {
        "user_prompt": "How can I reference a common props file in my .NET project?",
        "output_prompt": "To reference a common props file in your .NET project, you can add the <Import> element inside your project file (.csproj), referencing the relative path to the common.props file. Your project file should look like this:\n\n<Project Sdk=\"Microsoft.NET.Sdk.Web\">\n  <Import Project=\"..\\common.props\" \/>\n<\/Project>",
        "context": "{% endraw %}\n\nYou can then reference the common.props in your other project files to ensure a consistent setup.\n\n{% raw %}\n\nxml\n<Project Sdk=\"Microsoft.NET.Sdk.Web\">\n  <Import Project=\"..\\common.props\" \/>\n<\/Project>\n\n{% endraw %}\n\nThe .editorconfig allows for configuration and overrides of rules. You can have an .editorconfig file at project level to customize rules for different projects (test projects for example).\n\nDetails about the configuration of different rules.\n\n.NET analyzers",
        "chunk_id": "chunk17_4",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/csharp.md",
        "root_chunk_id": "chunk17"
    },
    {
        "user_prompt": "How do I ensure my git branch is tracking the correct remote repository?",
        "output_prompt": "You can ensure your git branch is tracking the correct remote repository by using the command `git branch -u <remote>\/<branchName>`. Replace `<remote>` with the name of the remote repository you want to track, and `<branchName>` with the name of the branch you want to track. After you have set the tracking correctly, verify it by running `git branch -vv`, which will show the tracking information for each branch.",
        "context": "```\n\n{% endraw %}\n\nWarning: Check that the output of the pull shows \"From source repo URL\". If this shows the mirror repo url then you've forgotten to reset the tracking. Run git branch -u origin\/wikiMaster then continue.\n\nThen run this to push it to the mirror repo and reset the branch to track the source repo again:\n\n{% raw %}\n\n```bash\n\nPush all branches up to mirror remote\n\ngit push -u mirror\n\nReset local to track source remote\n\ngit branch -u origin\/wikiMaster",
        "chunk_id": "chunk111_1",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/sync-wiki-between-repos.md",
        "root_chunk_id": "chunk111"
    },
    {
        "user_prompt": "What does the term MLOps stand for and how is it significant in the context of AI and machine learning development?",
        "output_prompt": "MLOps stands for Machine Learning Operations. It is an organizational approach that emphasizes collaboration and communication between data scientists and operations professionals. This approach is essential for automating and streamlining the machine learning lifecycle to deploy and maintain AI models in a robust, scalable, and reliable manner. It's particularly significant as it ensures that machine learning solutions are designed with both operational readiness and model accuracy in mind.",
        "context": "author: shanepeckham\ntitle: MLOps 101\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Machine Learning DevOps (MLOps) is an organizational approach that relies on a combination of people, process, and technology. This approach delivers Machine Learning solutions in a robust, scalable, reliable, and automated way. This guide provides a balanced view across the three areas of people, process, and technology.\nrings:\n  - public\ntags:\n  - Automotive\n  - Manufacturing\n\nMLOps 101\n\nDelivering Machine Learning",
        "chunk_id": "chunk297_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/index.md",
        "root_chunk_id": "chunk297"
    },
    {
        "user_prompt": "Can you explain how 'Discipline' relates to 'Profession' and 'Role' in our data model?",
        "output_prompt": "Certainly! In our data model, 'Discipline' is at the topmost level of categorization and defines a broad field of expertise or knowledge. It has a one-to-many relationship with 'Profession,' meaning that each discipline can encompass several professions. Additionally, 'Discipline' is required by 'Competency' to describe the essential skills within that field. A 'Profession' is applied by a 'Role,' which belongs to a group of related roles within a profession, and it's also required by a 'Role' to support career progression. 'Role' then maps to individual responsibilities and expected competencies for those who hold the role. So, 'Discipline' indirectly relates to 'Role' through the 'Profession' and 'Competency' that are integral to the roles within a discipline.",
        "context": "Vertex (Source) Edge Type Relationship Type Vertex (Target) Notes Required Profession Applies 1:many Discipline Top most level of categorization * Discipline Defines 1:many Role Groups of related roles within a profession * AppliedBy 1:1 Profession 1 Role Requires 1:many Responsibility Individual role mapped to an employee 1+ Requires 1:many Competency 1+ RequiredBy 1:1 Discipline 1 Succeeds 1:1 Role Supports career progression between roles 1 Precedes 1:1 Role Supports career progression between roles 1 AssignedTo 1:many User Profile * Responsibility Expects 1:many Key Result A group of expected outcomes and key results for employees within a role 1+ ExpectedBy 1:1 Role 1 Competency Describes 1:many Behavior A set of behaviors that contribute to success 1+ DescribedBy 1:1 Role 1 Key Result ExpectedBy",
        "chunk_id": "chunk42_1",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/examples\/memory\/Architecture\/Data-Model.md",
        "root_chunk_id": "chunk42"
    },
    {
        "user_prompt": "How does increasing the complexity of prompts in LLM experiments affect the evaluation process and cost?",
        "output_prompt": "Increasing the complexity of prompts in large language model (LLM) experiments makes it more challenging to compare and evaluate different models. The variety and intricate nature of the prompts can lead to difficulties in tracking the impact of each prompt on the outcomes. This complexity can result in confusion and obstacles in drawing clear conclusions. Furthermore, the generation of complex prompts introduces additional latency due to the time and computational resources required. The lengthier prompts also increase the token size for LLM calls, which can lead to higher operational costs for running these experiments.",
        "context": "Increase the complexity of the solution and make it more difficult to compare and evaluate different models. As the number of prompts increases, it becomes more difficult to keep track of the various experiments and to isolate the effect of the prompts on the final output. This tracking difficulty can lead to confusion and make it more challenging to draw meaningful conclusions from the experiments.\n\nIntroduce additional latency and costs. Generating high-quality prompts requires time and resources, and this latency can slow down the overall process of model development and deployment. Additionally, more complex prompts can significant increase the prompt token size in each LLMs calling, which can increase the cost of running experiments.",
        "chunk_id": "chunk284_26",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/prompt-engineering\/prompt-engineering.md",
        "root_chunk_id": "chunk284"
    },
    {
        "user_prompt": "I have an image with the BayerBG8 format and a bit depth of 8. The image uses RGB color space. How should I handle the demosaicing process for this image?",
        "output_prompt": "For an image with BayerBG8 format and a bit depth of 8 using RGB color space, the demosaicing process involves converting the Bayer filter mosaic of the raw image data into a full color image. This typically requires applying an interpolation algorithm to estimate the missing color information for each pixel, thereby producing a continuous-tone, full-color output image suitable for standard display devices and further processing.",
        "context": "\"format\": \"BayerBG8\",\n      \"bitDepth\": 8,\n      \"additionalProperties\": {\n        \"color\": \"rgb\"\n      }\n    }\n  ]\n}\n--boundary--",
        "chunk_id": "chunk256_7",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "How does the search tool fit into the AI solution?",
        "output_prompt": "The search tool is integral to the AI solution as it enables querying across enriched data from previous steps. This is crucial because the LLM by itself cannot contextualize content within documents. By building a search index and utilizing the search tool, we can retrieve the most pertinent documents to answer user queries, which can then be used to generate informed responses.",
        "context": "rings:\n  - internal\n\nModule 2b: For More Context\n\nSuggested Time Box: 3 hours\n\nIn this module, you will write code to create a search index to query across the data you enriched in the previous modules.\n\nWhat is Search doing in my AI solution?\n\nAn LLM on its own has no context of the content inside the documents, so we need to build a system to extract and search through the documents for relevant context to answer user questions. This is where search tools come in - building off of the enriched data we created in part A, we can use search to find the most relevant documents to answer a user's question. Later, we will explore how to use the LLM to generate answers from the search results.",
        "chunk_id": "chunk262_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/lab\/llm-lab\/module-2b.md",
        "root_chunk_id": "chunk262"
    },
    {
        "user_prompt": "Can you summarize the best practices for creating a pull request according to the given guidance?",
        "output_prompt": "Certainly! When creating a pull request, ensure to 1) Use a clear, concise title that summarizes the changes, 2) Provide a detailed description explaining what was changed and why, 3) Add relevant reviewers from your team, and ideally someone familiar with the project or language, and 4) For ISE code-with customer team projects, include reviewers from both organizations to facilitate knowledge transfer.",
        "context": "Author Guidance\n\nProperly describe your pull request (PR)\n\nGive the PR a descriptive title, so that other members can easily (in one short sentence) understand what a PR is about.\n\nEvery PR should have a proper description, that shows the reviewer what has been changed and why.\n\nAdd relevant reviewers\n\nAdd one or more reviewers (depending on your project's guidelines) to the PR. Ideally, you would add at least someone who has expertise and is familiar with the project, or the language used\n\nAdding someone less familiar with the project or the language can aid in verifying the changes are understandable, easy to read, and increases the expertise within the team\n\nIn ISE code-with projects with a customer team, it is important to include reviewers from both organizations for knowledge transfer - Customize Reviewers Policy",
        "chunk_id": "chunk12_0",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/process-guidance\/author-guidance.md",
        "root_chunk_id": "chunk12"
    },
    {
        "user_prompt": "Can you explain the difference between Rouge-N and Rouge-L metrics?",
        "output_prompt": "Certainly! Rouge-N focuses on the overlap of 'n-grams' between the reference and the test strings to calculate precision and recall. The higher the overlap of 'n-grams', the higher the scores. Conversely, Rouge-L focuses on the longest common subsequence (LCS) present between the reference and the test strings to determine precision. It measures how many sequential words are common in both strings, providing a score based on the length of that sequence compared to the total number of uni-grams in the test string.",
        "context": "Rouge-N: measures the number of matching 'n-grams' between a reference (a) and test (b) strings.\n$$Precision= {\\text{number of n-grams found in both a and b} \\over \\text{number of n-grams in b}}$$\n$$Recall= {\\text{number of n-grams found in both a and b} \\over \\text{number of n-grams in a}}$$\nRouge-L: measures the longest common subsequence (LCS) between a reference (a) and test (b) string.\n$$Precision= {LCS(a,b) \\over \\text{number of uni-grams in b}}$$",
        "chunk_id": "chunk280_9",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/metrics\/eval-metrics.md",
        "root_chunk_id": "chunk280"
    },
    {
        "user_prompt": "What types of testing activities should we organize to ensure our system's capacity meets future demands?",
        "output_prompt": "We should perform load testing (stress) and scenario testing under 'Operation Measure capacity limits' to predict or satisfy future provisioning needs. Additionally, end-to-end testing can help in understanding the scalability curve and resource consumption under load.",
        "context": "Staging; Operation Measure capacity limits for given provisioning to predict or satisfy future provisioning needs Squeeze; Load testing (stress) Development; Staging; Operation Create \/ exercise failover runbook Failover drills Development; Staging; Operation Prove disaster recoverability \u2013 loss of data center (the meteor scenario); measure MTTR DR drills Development; Staging; Operation Understand whether observability dashboards are correct, and telemetry is complete; flowing Trace Validation; Load testing (stress) ; Scenario; End-to-End testing Development; Staging; Operation Measure impact of seasonality of traffic Load testing Development; Staging; Operation Prove Transaction and alerts correctly notify \/ take action Synthetic Transaction testing (negative cases); Load testing Development; Staging; Operation; Optimizing Understand scalability curve, i.e. how the system consumes resources with load Load testing",
        "chunk_id": "chunk67_5",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/README.md",
        "root_chunk_id": "chunk67"
    },
    {
        "user_prompt": "Can you explain what 'Plan: 1 to add, 0 to change, 0 to destroy' means in the context of software deployment?",
        "output_prompt": "Certainly! 'Plan: 1 to add, 0 to change, 0 to destroy' typically refers to the output of an infrastructure-as-code (IaC) tool. It means that upon executing the plan or script, the tool will provision or create one new resource, it won't modify any existing resources, and it won't delete or destroy any resources.",
        "context": "Plan: 1 to add, 0 to change, 0 to destroy.",
        "chunk_id": "chunk133_23",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/recipes\/terraform\/share-common-variables-naming-conventions.md",
        "root_chunk_id": "chunk133"
    },
    {
        "user_prompt": "Considering the limitations of speech-to-text in complex scenarios, what technology would be better suited for understanding natural language in a conversation with an open-ended dialog?",
        "output_prompt": "In open-ended or complex dialog scenarios, technologies based on advanced Natural Language Understanding (NLU) are better suited. NLU systems are specifically designed to comprehend the intent and meanings behind the spoken words, allowing for more accurate interpretations and appropriate actions in response to the conversation.",
        "context": "Speech Intent Recognition\/Speech Commanding\/Speech to Stateful Action - Speech Service speech-to-text should not be used in scenarios that require extracting the meaning from a spoken audio utterance and taking some action in the system where the conversation is open ended or complex. For speech assistants, or bots, based on limited\/scoped conversations, use Voice Assistants.\n\nRelated\n\nNatural Language Processing (NLP)\/Understanding (NLU) - Build applications capable of understanding natural language.\n\nText-to-Speech - Build solutions that speak using synthesized human speech. Leverage 110+ built-in voices and over 45 supported languages. Customize speaking styles and emotional tones to fit your scenario. Create custom voices to match your brand.",
        "chunk_id": "chunk322_4",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/cognitive-services\/speech\/speech-to-text.md",
        "root_chunk_id": "chunk322"
    },
    {
        "user_prompt": "Can you summarize the steps I need to follow before creating an Edge Device VM?",
        "output_prompt": "Sure! Before you create an Edge Device VM, you should complete the following steps: 1. Create an SSH key, if you do not have one already. 2. Prepare an X.509 certificate and upload it to Azure Key Vault. 3. Get your deployment manifest files ready. 4. Ensure that OpenSSL is installed on your host machine, which will be used in the next steps to run scripts. After these preparations, you can clone the code sample repository to your local host machine and follow the instructions in README-create-edge-VM.md within the provisioning-sample-stg1-create-edge-device folder.",
        "context": "Step 2: Secrets and Files Preparation\n\nCreate an SSH key. Use your own ssh key or create an SSH key.\n\nPrepare an X.509 certificate and upload to Azure Key Vault.\n\nPrepare deployment manifest files.\n\nMake sure OpenSSL is installed in the host machine that runs the scripts for Step 3-1.\n\nStep 3-1: Create an Edge Device VM (if no physical device exists)\n\nClone the code sample repository to the local host machine, and follow README-create-edge-VM.md in the provisioning-sample-stg1-create-edge-device folder.The code sample for the implementation details can be found here.",
        "chunk_id": "chunk371_3",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/iot-edge-provisioning.md",
        "root_chunk_id": "chunk371"
    },
    {
        "user_prompt": "How can I test if the `resize_image` function in my python code is generating an image of the correct size?",
        "output_prompt": "You can write a unit test to check if the output of the `resize_image` function has the correct dimensions. Here's an example of how you might structure your test in Python using the `assert` statement: \n```python\ndef test_resize_image_generates_the_correct_size():\n  # Arrange\n  original_image = np.ones((10, 5, 2, 3))\n\n  # Act\n  resized_image = utils.resize_image(original_image, 100, 100)\n\n  # Assert\n  assert resized_image.shape[:2] == (100, 100), 'The resized image does not have the correct dimensions.'\n``` \nThis test first arranges the test by creating an original image with known dimensions. Then it acts by using the function to resize the image. Finally, it asserts that the first two dimensions of the resized image are 100 by 100.",
        "context": "```\n\n{% endraw %}\n\nTransforming data\n\nFor cleaning and transforming data, test fixed input and output, but try to limit each test to one verification.\n\nFor example, create one test to verify the output shape of the data.\n\n{% raw %}\n\n```python\ndef test_resize_image_generates_the_correct_size():\n  # Arrange\n  original_image = np.ones((10, 5, 2, 3))\n\n# act\n  resized_image = utils.resize_image(original_image, 100, 100)\n\n# assert\n  resized_image.shape[:2] = (100, 100)",
        "chunk_id": "chunk214_8",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-testing.md",
        "root_chunk_id": "chunk214"
    },
    {
        "user_prompt": "How do I determine if my system is experiencing hardware-related latency issues?",
        "output_prompt": "To determine if your system is experiencing hardware-related latency issues, you should measure the average time it takes for a disk transfer to complete, also known as latency. Establish a baseline latency value right after the hardware is installed. Monitor the latency over time, and if you notice it consistently exceeding the baseline value, it could indicate a hardware-related problem. Pay attention to Disk Reads\/sec and Disk Writes\/sec as well, as they measure the number of I\/O requests completed per second. While values higher than your baseline for these counters generally do not indicate a hardware issue, significant and consistent spikes in disk I\/O might be symptomatic of underlying problems.",
        "context": "of your disks. Latency is defined as the average time it takes for a disk transfer to complete. You obviously want is low numbers as possible but need to be careful to account for inherent speed differences between SSD and traditional spinning disks. For this counter is important to define a baseline after the hardware is installed. Then use this value going forward to determine if you are experiencing any latency issues related to the hardware. Disk Reads\/sec and Disk Writes\/sec These counters each measure the total number of IO requests completed per second. Similar to the latency counters, good and bad values for these counters depend on your disk hardware but values higher than your initial baseline don't normally point to a hardware issue in this case. This counter can be useful to identify spikes in disk I\/O.",
        "chunk_id": "chunk69_10",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/performance-testing\/README.md",
        "root_chunk_id": "chunk69"
    },
    {
        "user_prompt": "How should I incorporate communication platforms like Teams or Slack with our CI\/CD pipeline to improve our development process?",
        "output_prompt": "To integrate Teams or Slack with your CI\/CD pipeline, set up your CI tool to send notifications on build statuses directly to a dedicated channel on your chosen communication platform. This way, your team can quickly react to build failures and stay updated on build candidate statuses without having to constantly check the dashboard. Moreover, include a build status badge in your project's README for at-a-glance visibility, and refer to resources such as Martin Fowler's best practices and the relevant quick start guides for further guidance.",
        "context": "We recommend integrating Teams or Slack with CI\/CD pipeline runs which helps keep the team continuously plugged into failures and build candidate status(s).\n\nContinuous integration top level dashboard\n\nModern CI providers have the capability to consolidate and report build status(s) within a given dashboard.\n\nYour CI dashboard should be able to correlate a build failure with a git commit.\n\nBuild status badge in project readme\n\nThere should be a build status badge included in the root README of the project.\n\nBuild notifications\n\nYour CI process should be configured to send notifications to messaging platforms like Teams \/ Slack once the build completes. We recommend creating a separate channel to help consolidate and isolate these notifications.\n\nResources\n\nMartin Fowler's Continuous Integration Best Practices\n\nBedrock Getting Started Quick Guide\n\nCobalt Quick Start Guide",
        "chunk_id": "chunk225_25",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-integration\/README.md",
        "root_chunk_id": "chunk225"
    },
    {
        "user_prompt": "How do I provision my IoT Edge device to the IoT Hub after setting up the device ID and authentication method?",
        "output_prompt": "To provision your IoT Edge device to the IoT Hub, you need to configure the device with the authentication credentials you've chosen, either via connection string or X.509 certificates. Then, use the Azure portal or Azure CLI to update the IoT Edge device's twin with the appropriate authentication information. Finally, ensure that the IoT Edge runtime is installed and running on your device. You can follow the steps provided in the Provision and Configure IoT Edge related resources link for detailed instructions.",
        "context": "Install IoT Edge runtime to edge device to turn it to be an IoT Edge device.\n\nRegister device ID in IoT Hub with your preferred authentication for your device, you may choose either connection string, or you may use X.509 certificates.\n\nConfig the device authentication and provision the IoT Edge device to IoT Hub\n\nRefer to the link Provision and Configure IoT Edge related resources.\n\nStep 3: Deploy IoT Edge module\n\nCreate a new module deployment file (deployment.json) with the following code. The system modules edgeAgent and edgeHub are compulsory. SklearnModule and SimulatedTemperatureSensor are the custom modules you want to deploy.",
        "chunk_id": "chunk368_3",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/edge-deployment.md",
        "root_chunk_id": "chunk368"
    },
    {
        "user_prompt": "What is the 'eyes-off' approach in machine learning and how is it implemented in Azure ML?",
        "output_prompt": "The 'eyes-off' approach in machine learning allows data scientists to train models on sensitive data without being able to directly access or view the data. In Azure ML, this can be implemented using tools such as Identity Based Data Access, which restricts direct data access, while still allowing scripts to run. Proper role assignments are also crucial to ensure only authorized operations are performed by users.",
        "context": "Solution Notes Azure Trusted Research Environment An Open Source TRE for Azure. Aridhia DRE\n\nEyes-off machine learning\n\nIn certain situations, Data Scientists may need to train models on data they are not allowed to see. In these cases, an \"eyes-off\" approach is recommended.\nAn eyes-off approach provides a data scientist with an environment in which scripts can be run on the data but direct access to samples is not allowed.\nWhen using Azure ML, tools such as the Identity Based Data Access can enable this scenario,\nalongside proper role assignment for users.",
        "chunk_id": "chunk116_10",
        "source": "..\/data\/docs\/code-with-engineering\/privacy\/privacy-frameworks.md",
        "root_chunk_id": "chunk116"
    },
    {
        "user_prompt": "How should Architectural Decision Records (ADRs) be stored for efficient tracking and discussion?",
        "output_prompt": "ADRs should be stored in any version control system like git, ideally in a doc\/adr or doc\/arch directory. They should be added as pull requests in the 'proposed' status for team discussion. Once accepted, they can be merged into the main branch. Additionally, maintaining a decision-log.md with a summary table of decisions and metadata can be helpful for quick reference and transparency.",
        "context": "Where to store ADRs\n\nADRs can be stored and tracked in any version control system such as git. As a recommended practice, ADRs can be added as pull request in the proposed status to be discussed by the team until it is updated to accepted to be merged with the main branch. They are usually stored in a folder structure doc\/adr or doc\/arch. Additionally, it can be useful to track ADRs in a decision-log.md to provide useful metadata in an obvious format.\n\nDecision Logs\n\nA decision log is a Markdown file containing a table which provides executive summaries of the decisions contained in ADRs, as well as some other metadata. You can see a template table at doc\/decision-log.md.\n\nWhen to track ADRs",
        "chunk_id": "chunk38_6",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/README.md",
        "root_chunk_id": "chunk38"
    },
    {
        "user_prompt": "What will happen if the value of 'getToken' is set to true?",
        "output_prompt": "If 'getToken' is set to true, a request for an OAuth2 token will be sent. Upon receiving a response, the access token will be extracted and saved in the environment variables as 'token'. The expiry date for the token will also be set using the current date and time.",
        "context": "getToken = false;\n    console.log('Token and expiry date are all good');\n}\nif (getToken === true) {\n    pm.sendRequest(oauth2Request, function (_, res) {\n            console.log('Save the token')\n            var responseJson = res.json();\n            pm.environment.set('token', responseJson.access_token)\n            console.log('Save the expiry date')\n            var expiryDate = new Date();",
        "chunk_id": "chunk85_7",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/recipes\/postman-testing.md",
        "root_chunk_id": "chunk85"
    },
    {
        "user_prompt": "Can you outline the advantages of implementing detailed logging in generative AI applications?",
        "output_prompt": "Certainly! Detailed logging in generative AI applications provides numerous benefits, including the ability to analyze interaction patterns, monitor system performance, and understand how users engage with the system. It enables developers to track request metrics and response times, uncover the reasons for model stops, and review specific model parameters. This information is crucial for identifying bottlenecks, improving response accuracy, guiding model refinements, and ultimately ensuring the AI system remains robust and effective over time.",
        "context": "Telemetry is the backbone of a well-monitored AI system, offering the\ninsights necessary for continuous improvement. For a deeper dive into these\nconcepts and more, check the Engineering Fundamentals for\nlogging,\ntracing,\nand metrics.\n\nLogging\n\nIn generative AI applications, logging plays a pivotal role in shedding light\non interactions, system behavior, and overall health.\n\nHere are some recommended logs for OpenAI services:\n\nRequests: Logging request metrics, such as response times,\nstop reasons, and specific model parameters to understand both the demand and\nperformance of the system.\n\nInput prompts: Capturing user inputs helps developers grasp how users\nare engaging with the system, paving the way for potential model refinements.",
        "chunk_id": "chunk294_5",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/monitoring\/monitoring.md",
        "root_chunk_id": "chunk294"
    },
    {
        "user_prompt": "How can I determine if getting more data for my model is beneficial?",
        "output_prompt": "You can begin by fine-tuning your model on subsets of your existing dataset and evaluate how the performance scales with the size of the dataset. This will allow you to estimate the learning curve and determine if the investment in additional data will likely yield a significant improvement. Additionally, you can compare the fine-tuned model's performance against that of the pre-trained model or a baseline to gauge the added value of fine-tuning.",
        "context": "Try fine-tuning subsets first: To assess the value of getting more data, you can fine-tune models on subsets of your current dataset to see how performance scales with dataset size. This fine-tuning can help you estimate the learning curve of your model and decide whether adding more data is worth the effort and cost. You can also compare the performance of your model with the pre-trained model or a baseline. This comparison shows how much improvement you can achieve with fine-tuning.",
        "chunk_id": "chunk289_6",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/fine-tuning\/fine-tuning-recommend.md",
        "root_chunk_id": "chunk289"
    },
    {
        "user_prompt": "How do I use an existing computer vision model with CVT?",
        "output_prompt": "To use an existing computer vision model with CVT, you must first package your model or traditional computer vision code, along with any related assets, into a bundle using the bundle interface. After packaging, upload the bundle to the Bundle Registry. You can then use the REST API endpoint to extract metadata and conduct test cases from the bundle in the simulator. Once tested and validated, the bundle can be deployed to the CVT Edge where it interacts with modules via REST API over HTTP protocol as detailed in the Bundle Registry documentation.",
        "context": "Using an existing model\/traditional computer vision code developed outside of CVT platform, such as Custom Vision AI.\n\nIn all cases, we will use a bundle interface to package the model or traditional computer vision code, and all related assets as a Bundle. Please check CVT High-Level Architecture for more details for the components. When we have the Image in Bundle Registry we'll check the REST API endpoint extracting metadata and test cases from the bundle, then we'll test this bundle image on a Simulator. You can check the CVT Simulator document for more information.\n\nOnce Bundle is available in Bundle Registry and tested, we can use it to deploy the image to CVT Edge. On CVT edge there can be multiple modules, a controller, module communicate with Bundle Image through REST API over HTTP protocol. See the Bundle Registry document.\n\nBundle Format",
        "chunk_id": "chunk258_5",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/index.md",
        "root_chunk_id": "chunk258"
    },
    {
        "user_prompt": "What does a high score of 0.9999 signify in a machine learning model?",
        "output_prompt": "A high score of 0.9999 in a machine learning model usually indicates that the model's prediction is highly accurate or certain. However, it is essential to verify that the model isn't overfitting and that the score is valid across different datasets, not just the one it was trained on.",
        "context": "\"score\": 0.9999\n          }\n        }\n      }\n    }\n  ]\n}",
        "chunk_id": "chunk256_40",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "What is a 'Bundle' in the context of the Computer Vision Toolkit, and how does it facilitate the deployment of machine learning models?",
        "output_prompt": "In the Computer Vision Toolkit, a 'Bundle' refers to a package that contains a software application and all necessary components for it to operate, such as packages needed to run models on the edge, standard interfaces, self-description, and test data. This specialized packaging format ensures consistency and ease of deployment for machine learning models across various devices and locations, offering a scalable solution for different models on different platforms.",
        "context": "Computer Vision Toolkit Bundle Format\n\nBy definition a \"Bundle\" is a package containing a software and everything it needs to operate. The Computer Vision Toolkit Bundle includes packages needed to run models on the edge but as well standard interfaces, self description and test data. The CVT Bundle has a specific format. This uniqueness of format and the way everything is packaged in a container allows high scalability of deployment on the edge of a lot of different models on a lot of different machines\/virtual machines across many different locations all operated in a standard and scalable way.\n\nFor ease of deployment of Machine Learning models in the CVT (Computer Vision Toolbox) a common way of packaging the models and all related assets must be defined. This will facilitate the generalization of the deployments as well as make it easier for teams to operationalize them.",
        "chunk_id": "chunk258_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/index.md",
        "root_chunk_id": "chunk258"
    },
    {
        "user_prompt": "Can you explain what Data Drift is and how it can affect my machine learning models deployed on the edge?",
        "output_prompt": "Data Drift refers to the change in model input data distribution over time which can degrade the model's performance. For models deployed on the edge, it's crucial to monitor input data regularly and update the model to maintain its accuracy and relevance.",
        "context": "{% if extra.ring == 'internal' %}\n\nRefer to the following section for a checklist on Data Drift questions to ask a customer\n{% endif %}\n\n{% set link = '..\/..\/technology-guidance\/working-with-azure-ml\/datadrift_operationalization.md' %}\n{% if relative_path_exists(thisPage, link) %}\n\nRefer to the following section for a detailed overview of Data Drift Operationalization in Azure ML\n{% endif %}\n\nRefer to the following section for detailed overview of Data Drift and Adaption\n\nServing models on the Edge (Inferencing)",
        "chunk_id": "chunk337_2",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/inference-and-feedback\/index.md",
        "root_chunk_id": "chunk337"
    },
    {
        "user_prompt": "What are the key considerations while building a GenAI gateway that aligns with the Azure Well Architected Framework, given the additional complexity of tokens per minute constraints?",
        "output_prompt": "Key considerations for building a GenAI gateway that aligns with Azure Well Architected Framework, especially with tokens per minute (TPM) constraints, include ensuring scalability to handle varying loads, and implementing load balancing to efficiently distribute requests across multiple pay-as-you-go Azure OpenAI instances.",
        "context": "However, when using Azure OpenAI resources as a backend, the added\ndimension of TPMs (Tokens Per Minute) introduces an additional layer of\ncomplexity in ensuring consistent and even load distribution across\nbackends. Therefore, apart from the GenAI gateway needing to ensure the\nregulation of the quantity of requests, it also must account for the\ntotal tokens processed across multiple requests.\n\n5. Key Considerations while Building GenAI Gateway\n\nThis additional dimension of Tokens per minute constraint forces some\nchanges to the traditional gateway and the inherent nature of these AI\nendpoints introduces some challenges that need to be addressed.\n\nHere are the key considerations while building a GenAI gateway in alignment with Azure Well Architected Framework.\n\nScalability\n\nLoad Balancing for Multiple Pay-As-You-Go AOAI Instances",
        "chunk_id": "chunk348_4",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/genai-gateway\/index.md",
        "root_chunk_id": "chunk348"
    },
    {
        "user_prompt": "Given the context of library and language support for large language models, which programming language would you recommend for developing LLM-based applications?",
        "output_prompt": "Considering the support for common LLM libraries and platforms, Python, Javascript\/Typescript, and .NET (C#) are generally recommended for developing LLM-based applications. Java is also gaining support, so it could be a viable option. It's important to choose a language that aligns with your needs and the availability of libraries and support to streamline development and deployment processes.",
        "context": "Libraries and Languages\n\nThe backend itself is not CPU or memory intensive. In theory, it would be possible to use any programming language of your personal preference. Whilst this is true on one side, it is also important to consider that not all languages support common LLM libraries and platforms that can significantly simplify and automate development and deployment. It is still possible to implement LLM scenarios using only REST calls, and not the tools and libraries, but choosing to work without them can lead to additional overhead of development additional tools rather than focusing attention on the solution itself. At the time this document is written, Python, Javascript\/Typescript, and .NET (C#) are well supported, and Java is starting to receive some attention. Other languages must rely on REST calls.",
        "chunk_id": "chunk286_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/approach\/libraries-languages.md",
        "root_chunk_id": "chunk286"
    },
    {
        "user_prompt": "What should be the content of the first test cases we design?",
        "output_prompt": "The first test cases should focus on core functionality of the application. They should test the primary paths that a user is expected to take. This includes testing all the main features and user interactions, making sure all methods and functions return expected results. They should also ensure that the setup of source control is correct and that the continuous integration system is properly executing linters and automated tests.",
        "context": "Design the first test cases\n\n[ ] Decide on branch naming\n\n[ ] Discuss security needs and verify that secrets are kept out of source control\n\nDay 2\n\n[ ] Set up Source Control\n\nAgree on best practices for commits\n\n[ ] Set up basic Continuous Integration with linters and automated tests\n\n[ ] Set up meetings for Daily Stand-ups and decide on a Process Lead\n\nDiscuss purpose, goals, participants and facilitation guidance\n\nDiscuss timing, and how to run an efficient stand-up\n\n[ ] If the project has sub-teams, set up a Scrum of Scrums\n\nDay 3\n\n[ ] Agree on code style and on how to assign Pull Requests",
        "chunk_id": "chunk0_2",
        "source": "..\/data\/docs\/code-with-engineering\/SPRINT-STRUCTURE.md",
        "root_chunk_id": "chunk0"
    },
    {
        "user_prompt": "What is a Feature Store and how does it benefit an organization dealing with multiple machine learning models?",
        "output_prompt": "A Feature Store is a centralized repository for storing and managing pre-computed or extracted features from machine learning models. It benefits an organization by allowing for quick access to reusable features, which saves development time and compute costs, and ensures consistency and quality across different ML projects.",
        "context": "The data versioning guide provides more detailed information on how versioning may be achieved.\n\nFinding pre-trained ML models\n\nSimilarly, being able to find a model that has already been trained can avoid the costs associated with duplicated effort. A model registry allows an organization to store and version models and also helps to organize and keep track of trained models.\n\nFor more information, see Azure Machine Learning Model Registry.\n\nFinding pre-computed\/pre-extracted ML features\n\nBeing able to store and re-use pre-computed or extracted features from a trained ML model saves an organization both development time and compute costs. It is important for organizations dealing with a large number of machine learning models to have an effective way to discover and select these features. These needs are typically now met with what is known as a Feature Store.",
        "chunk_id": "chunk351_3",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/data-curation\/data-governance.md",
        "root_chunk_id": "chunk351"
    },
    {
        "user_prompt": "What are some benefits of using pre-trained ML models like LLMs compared to developing custom machine learning models?",
        "output_prompt": "Pre-trained ML models such as Large Language Models offer several benefits. They save development time since they don't require building a model from scratch; they are cost-effective because they eliminate the need for extensive data collection and computing resources; and they generally provide reliable results due to their training on large and diverse datasets. On the other hand, custom machine learning models allow for greater flexibility and optimization specific to tailored use-cases, but may incur more costs and require more specialized knowledge to develop and train.",
        "context": "This playbook provides AI solutions focused in the following areas:\n\nPre-trained ML models, such as Large Language Models (LLM), and Azure Cognitive Services (at times referred to as Model as a Service or MaaS).\n\nCustom Machine Learning Models - that are developed with Azure tools such as Azure Machine Learning (AML), Databricks, Synapse and Fabric.\n\nSoftware as a Service (SaaS) solutions, such as Microsoft's PowerVirtualAgents.\n\nSome good places to start:\n\nTech Topic Getting Started Solution OpenAI Working with LLMs Azure ML Getting Started with Azure ML Automated and Monitored Training Pipelines\n\nAbout the AI Playbook\n\nThe AI Playbook aims to accelerate real-world application development by following good engineering practices, such as:",
        "chunk_id": "chunk253_1",
        "source": "..\/data\/docs\/code-with-mlops\/index.md",
        "root_chunk_id": "chunk253"
    },
    {
        "user_prompt": "Can you explain what RAG solution refers to in the context of Azure Machine Learning?",
        "output_prompt": "RAG stands for Retrieve-And-Generate. It's a solution pattern in Azure Machine Learning that involves retrieving relevant information from a data source and then using that information to generate responses or predictions. By breaking down the steps and building it from scratch, you gain the flexibility to tailor the solution to specific enterprise scale needs, including customizing data preparation processes.",
        "context": "Let's Get Started\n\nIn Module 1, you used Azure Machine Learning prompt flow to, essentially, automatically put together a RAG solution. Now, you are going to break down those steps into their component parts, and build it from \"scratch\". As is often the case with automated solutions, they do a good job of getting started, but often fall apart at enterprise scale due to a lack of flexibility. By building the full pattern yourself from beginning to end, you can adjust each piece, all the way down to how the initial data preparation is done.\n\nWhat's in Our Data?",
        "chunk_id": "chunk268_1",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/lab\/llm-lab\/module-2a.md",
        "root_chunk_id": "chunk268"
    },
    {
        "user_prompt": "What are the roles of the Process Lead in Scrum Rhythm?",
        "output_prompt": "The Process Lead is responsible for facilitating scrum meetings such as the Project Standup, Sprint Retro, and ensuring that the team follows engineering best practices. They help the team stay accountable to the goals of each scrum activity and maintain the scrum rhythm effectively.",
        "context": "We follow engineering best practices like the Code With Engineering Playbook\n\nScrum Rhythm\n\nActivity When Duration Who Accountable Goal Project Standup Tue-Fri 9AM 15 min Everyone Process Lead What has been accomplished, next steps, blockers Sprint Demo Monday 9AM 1 hour Everyone Dev Lead Present work done and sign off on user story completion Sprint Retro Monday 10AM 1 hour Everyone Process Lead Dev Teams shares learnings and what can be improved Sprint Planning Monday 11AM 1 hour Everyone PO Size and plan user stories for the sprint Task Creation After Sprint Planning - Dev Team Dev Lead Create tasks to clarify and determine velocity Backlog refinement Wednesday 2PM 1 hour Dev Lead, PO PO Prepare for next sprint and ensure that stories are ready for next sprint.\n\nProcess Lead\n\nThe Process Lead is responsible for leading any scrum or agile practices to enable the project to move forward.",
        "chunk_id": "chunk164_3",
        "source": "..\/data\/docs\/code-with-engineering\/agile-development\/advanced-topics\/team-agreements\/working-agreements.md",
        "root_chunk_id": "chunk164"
    },
    {
        "user_prompt": "What are the main responsibilities of a backend system in the context of LLMs and Machine Learning?",
        "output_prompt": "The main responsibilities of a backend system in this context include orchestrating the flow logic, interacting with Language Learning Models (LLMs) and other services, managing system configurations and connections, as well as ensuring the inferencing service is operational and ready for production deployment. It acts as a foundation to support machine learning operations by handling data processing, model inferencing, and serving predictions.",
        "context": "Backend System: This is a primary component of all presented scenarios. The goal of this component is to implement the flow logic, interact with LLMs and other services, manage configurations and connections. In fact, when we refer to an LLM backend, we have in mind an inferencing service, if we use Machine Learning terminology. It is a piece of software that will be published into production, and we need to build operationalization for this component.",
        "chunk_id": "chunk292_7",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/use-cases\/common-scenarios.md",
        "root_chunk_id": "chunk292"
    },
    {
        "user_prompt": "How can architectural patterns improve project development and consistency across projects?",
        "output_prompt": "Architectural patterns can streamline project development by providing a set of proven, standardized guidelines and templates for project design, IT and security approval processes, and infrastructure deployment. This leads to quicker design phases, swifter approvals, and faster development times due to reusability. They also facilitate consistency across projects by defining preferred services, data governance, MLOps strategies, and collaboration processes, ensuring all projects adhere to best practices and maintain a high standard of quality and efficiency.",
        "context": "Accelerate project development\n\nImprove consistency across projects\n\nOnce these patterns are in place, most projects can use these patterns and reap the following benefits:\n\nAccelerated design phase\n\nAccelerated approvals from IT and security teams when they reuse tools across projects\n\nAccelerated development due to reusable infrastructure as code templates and project templates (which are covered in more detail in the next section).\n\nThe architectural patterns can include but aren't limited to the following topics:\n\nPreferred services for each stage of the project\n\nData connectivity and governance\n\nA Machine Learning DevOps (MLOps) strategy tailored to the requirements of the industry, business, or data classification\n\nExperiment management process Champion or Challenger models\n\nFacilitate cross-team collaboration and sharing",
        "chunk_id": "chunk297_14",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/index.md",
        "root_chunk_id": "chunk297"
    },
    {
        "user_prompt": "Can you tell me more about the role of a Site Reliability Engineer?",
        "output_prompt": "Certainly! A Site Reliability Engineer (SRE) is responsible for ensuring that an organization's software systems are reliable, scalable, and efficiently maintained. They often work at the intersection of software engineering and IT operations, focusing on automation, infrastructure management, and implementing robust software practices. Their goal is to build resilient systems that can handle growth and adapt to new challenges without significant downtime or loss of performance.",
        "context": "Vertex Descriptions\n\nProfession\n\nTop most level of categorization\n\n{% raw %}\n\njson\n{\n    \"title\": \"Software Engineering\",\n    \"description\": \"Description of profession\",\n    \"disciplines\": []\n}\n\n{% endraw %}\n\nDiscipline\n\nGroups of related roles within a profession\n\n{% raw %}\n\njson\n{\n  \"title\": \"Site Reliability Engineering\",\n  \"description\": \"Description of discipline\",\n  \"roles\": []\n}\n\n{% endraw %}\n\nRole\n\nIndividual role mapped to an employee\n\n{% raw %}",
        "chunk_id": "chunk42_7",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/examples\/memory\/Architecture\/Data-Model.md",
        "root_chunk_id": "chunk42"
    },
    {
        "user_prompt": "What is the purpose of the '.order' file found in some directories within the xaml project structure?",
        "output_prompt": "The '.order' file is typically used to define the order in which items, such as documentation pages or files, should be displayed or processed. In the context of the xaml project structure, it's likely used to specify the sequence of reading or displaying the documents in the 'architecture-decisions' and 'getting-started' directories.",
        "context": "xaml\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 .attachments                     \/\/ All images and other attachments used by documents\n\u2502\n\u2502   \u251c\u2500\u2500 architecture-decisions\n\u2502           \u2514\u2500\u2500 .order\n\u2502           \u2514\u2500\u2500 decision-log.md       \/\/ Sample index into all ADRs\n\u2502           \u2514\u2500\u2500 README.md          \/\/ Landing page architecture decisions\n\u2502\n\u2502   \u251c\u2500\u2500 getting-started\n\u2502           \u2514\u2500\u2500 .order",
        "chunk_id": "chunk110_18",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/using-docfx-and-tools.md",
        "root_chunk_id": "chunk110"
    },
    {
        "user_prompt": "Who is typically involved in conducting architectural feasibility studies?",
        "output_prompt": "Architectural feasibility studies usually involve a team of stakeholders, including architects, engineers, project managers, domain experts, and sometimes financial analysts. They work together to assess the practicality of the proposed project and to ensure that it meets the necessary requirements in terms of technical capabilities, budget, and regulatory compliance.",
        "context": "Architectural discussions can still occur in parallel as the team works towards gaining a solid understanding and definition of what will be built.\n\nFeasibility studies can last between 4-16 weeks, depending on specific problem details, volume of data, state of the data etc. Starting with a 4-week milestone might be useful, during which it can be determined how much more time, if any, is required for completion.\n\nWho collaborates on feasibility studies?",
        "chunk_id": "chunk218_1",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-feasibility-study.md",
        "root_chunk_id": "chunk218"
    },
    {
        "user_prompt": "Based on the data you received, can you tell me what the prediction is about?",
        "output_prompt": "The prediction indicates the detection of a scratch on an image. The bounding box coordinates for the scratch are (12, 109, 107, 38), and the confidence score of the prediction is 0.8864.",
        "context": "json\n{\n    \"status\": \"NOK\",\n    \"score\": 0.8136,\n    \"data\": {\n        \"SCRATCH\": {\n            \"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n            \"predictionType\": \"detection\",\n            \"boundingBox\": \"12,109,107,38\",\n            \"label\": \"scratch\",\n            \"score\": 0.8864",
        "chunk_id": "chunk256_20",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Can you explain how to avoid conflicts with variable names when using common modules in Terraform?",
        "output_prompt": "To avoid conflicts with variable names in Terraform when using common modules, ensure that each module has unique output variable names. You can achieve this by using a naming convention that includes the module name as a prefix. When these outputs are used in different parts of your code, it also helps to include the module name as part of the reference, thus preventing any overlap. Moreover, isolate and encapsulate your shared resources within the common module itself, which could reduce the need to expose many variables. If you need to change a naming pattern, update it in the common module's output files and re-apply the changes to all the relevant configurations.",
        "context": "{% endraw %}\n\nNote: using this, it is really important to ensure that you have not the same variable names in both files, otherwise that will generate an error.\n\nConclusion\n\nBy having a common module that owns shared variables as well as naming convention, it is now easier to refactor your Terraform configuration code base. Imagine that for some reason you need change the pattern that is used for the virtual network name: you change it in the common module output files, and just have to re-apply all modules!",
        "chunk_id": "chunk133_29",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/recipes\/terraform\/share-common-variables-naming-conventions.md",
        "root_chunk_id": "chunk133"
    },
    {
        "user_prompt": "How can the GenAI Gateway assist with authentication when interfacing with different endpoints, and what are the benefits of offloading authentication to the Gateway?",
        "output_prompt": "The GenAI Gateway can centralize authentication by terminating consumer authentication at the Gateway level and employing a uniform enterprise-wide authentication mechanism like OAuth. This approach benefits endpoints by decoupling their authentication mechanisms from the end consumers, enhancing security, and enabling endpoints to use managed identity to authenticate with AOAI endpoints specifically. The main advantage is the streamlining of authentication processes across various services and the mitigation of risks associated with multiple authentication methods.",
        "context": "It's possible that the Gateway might interface with endpoints that are\nnot AOAI. In those situations, different endpoints could have different\nways of authenticating.\n\nOne of the suggested approaches here is to offload authentication to\nAOAI (or other GenAI endpoints) to the GenAI gateway and terminate the\nconsumer authentication at the Gateway level. Decoupling the GenAI\nendpoints auth from the end consumers allow them to employ uniform\nenterprise-wide authentication mechanism like OAuth while authenticating\nagainst the GenAI Gateway and mitigating the above stated risks. In case\nof AOAI endpoints, GenAI gateway can use managed identity to\nauthenticate as well. When authentication is offloaded, the GenAI\nresource cannot recognize the consumers, because the Gateway will use\nits own credentials for all the requests.",
        "chunk_id": "chunk347_3",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/genai-gateway\/security-and-data-integrity.md",
        "root_chunk_id": "chunk347"
    },
    {
        "user_prompt": "Can you give me an overview of the feature store FeaSt?",
        "output_prompt": "Certainly! FeaSt is a feature store that provides tools to manage, store, and discover features for machine learning projects. It allows developers to register, version, and manage lifecycle of ML features. FeaSt is designed to work well with other machine learning components and services, facilitating easier experimentation and production deployments of machine learning models.",
        "context": "author: shanepeckham\ntitle: Comparing popular feature stores\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: This article provides an overview and side-by-side comparison of some better-known feature store solutions, including FeaSt, Databrick FS, Feathr, and Azure's Managed Feature Store.\nrings:\n - public\n\nComparing popular feature stores\n\nThis article provides an overview and side-by-side comparison of some better-known feature store solutions, including FeaSt, Databricks FS, Feathr, and Azure's Managed Feature Store.\n\nFeature store summaries\n\nHere are brief summaries of each feature store solution.\n\nFeaSt",
        "chunk_id": "chunk327_0",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/model-development\/feature-management\/feature-store-comparison.md",
        "root_chunk_id": "chunk327"
    },
    {
        "user_prompt": "What are the benefits of having notes assigned to each reviewed data point during the evaluation process?",
        "output_prompt": "Notes provide insight into the reviewer\u2019s thought process, serve as a reference for new reviewers, and help standardize the evaluation process. They document the reasoning behind scores, which can be instructive when applied to reviewing other data points in the dataset.",
        "context": "Having each individual review each sample and assigning a score for each completion. Alternatively, reviewers could split up work and review different sets of data points to get through the review process faster.\n\nAdditionally, having notes assigned to each reviewed data point gives insight as to how each specific reviewer interpreted each completion for each data point. Having notes is useful as it serves as a reference point for users. These users may have not reviewed the dataset before, but want to understand the thought process involved with each review. Notes could also serve as a convention for standardizing the evaluation process. Learnings derived from each reviewed note could potentially be applied to other samples within the dataset.\n\nDepending on human resources available, getting human feedback might be a tedious task. Scenarios that might involve this sort of evaluation need to be carefully assessed.",
        "chunk_id": "chunk280_19",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/metrics\/eval-metrics.md",
        "root_chunk_id": "chunk280"
    },
    {
        "user_prompt": "Could you explain how to perform a benchmark assessment for TPM and RPM requirements?",
        "output_prompt": "Certainly! To conduct a benchmark assessment for Transactions Per Minute (TPM) and Requests Per Minute (RPM) requirements, begin by estimating the frequency and volume of requests for each use case and scenario, such as requests per minute, tokens per request, and concurrent users. Next, use a load testing tool, like Azure Load Testing service, Apache JMeter, Locust, or Artillery, to simulate the expected traffic and measure the actual TPM and RPM consumption. Finally, analyze the test results to find the maximum and average TPM and RPM values, and make a note of any errors or failures that occur during the test.",
        "context": "The following steps outline how to conduct a benchmark assessment for\nTPM and RPM requirements.\n\nEstimate the frequency and volume of requests for each use case and\n    scenario. For example, how many requests per minute, how many tokens\n    per request, how many concurrent users, etc.\n\nUse a load testing tool or script to simulate the expected traffic\n    pattern and measure the actual TPM and RPM consumption. For example,\n    Azure Load Testing service, Apache JMeter, Locust, Artillery, etc.\n\nAnalyze the results and identify the maximum and average TPM and RPM\n    values for each use case and scenario. Also, note any errors or\n    failures that occurred during the test.",
        "chunk_id": "chunk349_4",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/genai-gateway\/performance-efficiency.md",
        "root_chunk_id": "chunk349"
    },
    {
        "user_prompt": "Can you provide details on the CRACK detection from the given CONTEXT?",
        "output_prompt": "The detected CRACK has an imageId of 'e6fd3074-2af8-4634-8c42-3d5f40f61213', with a bounding box coordinates of '42,100,42,20'. It is labeled 'crack' and has a high confidence score of 0.964.",
        "context": "\"score\": 0.8864\n        },\n        \"CRACK\": {\n            \"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n            \"predictionType\": \"detection\",\n            \"boundingBox\": \"42,100,42,20\",\n            \"label\": \"crack\",\n            \"score\": 0.964\n        }",
        "chunk_id": "chunk256_17",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "How can I prioritize content fields when working with structured data?",
        "output_prompt": "To prioritize content fields when working with structured data, you can create a list or array that specifies the order of fields based on their importance. Each field can be represented as an object with properties that define how it should be treated, such as field_name, priority_level, and any other relevant attributes.",
        "context": "},\n                    \"prioritized_content_fields\": [\n                        {\n                            \"field_name\": \"content\"\n                        },\n                        {",
        "chunk_id": "chunk315_31",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/search\/index.md",
        "root_chunk_id": "chunk315"
    },
    {
        "user_prompt": "What are the challenges and limitations of fine-tuning large language models?",
        "output_prompt": "The primary challenges and limitations include the need for high-quality training data that is sufficiently large and representative of the target domain and task. If the data is of poor quality or unrepresentative, it can lead to issues like over-fitting, under-fitting, or bias, which detrimentally affect the model's generalization and robustness.",
        "context": "Challenges and limitations of fine-tuning\n\nFine-tuning large language models scan be a powerful technique to adapt them to specific domains and tasks. However, fine-tuning also comes with some challenges and disadvantages that need to be considered before applying it to a real-world problem. Below are a few of these challenges and disadvantages.\n\nFine-tuning requires high-quality, sufficiently large, and representative training data matching the target domain and task. Quality data is relevant, accurate, consistent, and diverse enough to cover the possible scenarios and variations the model will encounter in the real world. Poor-quality or unrepresentative data leads to over-fitting, under-fitting, or bias in the fine-tuned model, which harms its generalization and robustness.",
        "chunk_id": "chunk290_12",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/fine-tuning\/fine-tuning.md",
        "root_chunk_id": "chunk290"
    },
    {
        "user_prompt": "What are the critical components that allow a data science team to successfully implement MLOps at an organizational scale and focus on the development of new use cases?",
        "output_prompt": "The critical components for successfully implementing MLOps at an organizational scale include balanced teams, supported processes, and technology automation. These components help the team ensure quality, reliability, and maintainability of solutions, allowing them to scale effectively and concentrate on developing new use cases.",
        "context": "MLOps at organizational scale: AI factories\n\nA data science team might decide they can manage a handful of Machine Learning use cases internally. The adoption of Machine Learning DevOps (MLOps) helps set up project teams for better quality, reliability, and maintainability of solutions through the following items:\n\nbalanced teams\n\nsupported processes\n\ntechnology automation\n\nThis adoption allows the team to scale and focus on the development of new use cases.\n\nAs the number of use cases grows in an organization, the management burden of supporting these use cases grows linearly, or even more. The challenge becomes how to use organizational scale to do the following:\n\nAccelerate time-to-market\n\nQuicken assessment of use case feasibility\n\nEnable repeatability\n\nDetermine how to best utilize available resources and skill sets across the full range of projects.",
        "chunk_id": "chunk297_12",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/index.md",
        "root_chunk_id": "chunk297"
    },
    {
        "user_prompt": "Can you describe some tasks associated with developing an understanding of product code and features?",
        "output_prompt": "Certainly! Tasks associated with developing an understanding of product code and features include reviewing documentation, examining source code, attending product training sessions, participating in team meetings, and utilizing the product in various scenarios to understand its operations and interactions with other systems.",
        "context": "\"description\": \"Develops an understanding of the code, features, and operations of specific products...\"\n        }\n      ]\n    },\n    {\n      \"id\": \"abc123\",\n      \"title\": \"Contributions to Development and Design\",\n      \"results\": [\n        {\n          \"description\": \"Develops and tests basic changes to optimize code...\"\n        },\n        {\n          \"description\": \"Supports ongoing engagements with product engineering teams...\"\n        }",
        "chunk_id": "chunk42_13",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/examples\/memory\/Architecture\/Data-Model.md",
        "root_chunk_id": "chunk42"
    },
    {
        "user_prompt": "Can you create a JSON object representing an employee's user profile including all the required properties based on the vertex\/edge property data type notes provided in the context?",
        "output_prompt": {
            "ID": "5f2b18c7-6974-4c5e-a9ec-3b92b5326f20",
            "Profession": {
                "Title": "Software Engineer"
            },
            "Discipline": {
                "Title": "Backend Development"
            },
            "Role": {
                "Title": "API Developer"
            },
            "Level": "SDE II",
            "Responsibility": {
                "Title": "Code Implementation"
            },
            "Competency": {
                "Title": "Efficient Coding"
            },
            "KeyResult": "Reduced API response time by 20%",
            "Behavior": "Proactive in addressing bottlenecks",
            "User Profile": {
                "Theme": "dark"
            },
            "PersonaId": [
                "7edd00f0-a8a4-4a5a-bb09-5a0c3c5597b1",
                "eee8c5cf-5b8d-46c7-a8a2-0f561f8c3c6f"
            ],
            "UserId": "dbe8f5e2-a502-46b1-8c5a-3d14fbea3f5b",
            "DeploymentRing": [
                "Ring A",
                "Ring B"
            ],
            "Project": [
                "Project Atlas",
                "Project Titan"
            ],
            "Entry": {
                "Title": "Release Q4 Improvements",
                "DateCreated": "2023-02-15",
                "ReadyToShare": true
            },
            "AreaOfImpact": [
                "self",
                "contribute to others"
            ],
            "Commentary": {
                "Data": "Significant growth in personal and team performance.",
                "DateCreated": "2023-02-20"
            },
            "Artifact": {
                "Data": "Presentation on Q4 Review"
            }
        },
        "context": "Vertex\/Edge Property Data Type Notes Required (Any) ID guid 1 Profession Title String 1 Description String 0 Discipline Title String 1 Description String 0 Role Title String 1 Description String 0 Level Band String SDE, SDE II, Senior, etc 1 Responsibility Title String 1 Description String 0 Competency Title String 1 Description String 0 Key Result Description String 1 Behavior Description String 1 User Profile Theme selection string there are only 2: dark, light 1 PersonaId guid[] there are only 2: User, Admin 1+ UserId guid Points to AAD object 1 DeploymentRing string[] Is used to deploy new versions 1 Project string[] list of user created projects * Entry Title string 1 DateCreated date 1 ReadyToShare boolean false if draft 1 AreaOfImpact string[] 3 options: self, contribute to others, leverage others * Commentary Data string 1 DateCreated date 1 Artifact Data string 1",
        "chunk_id": "chunk42_5",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/examples\/memory\/Architecture\/Data-Model.md",
        "root_chunk_id": "chunk42"
    },
    {
        "user_prompt": "How can we improve the robustness of our machine learning model through adversarial and boundary testing?",
        "output_prompt": "Adversarial and boundary testing can significantly enhance the robustness of a machine learning model by exposing it to extreme cases and potential weaknesses. These tests challenge the model with inputs that are either at the edge of or outside the normal data distribution, potentially uncovering areas where the model might fail. To address under-represented classes, you can verify that these categories are correctly identified during testing and, if necessary, provide additional training data to improve classification accuracy. Moreover, techniques like oversampling the under-represented classes or employing class-weighted loss functions can help in mitigating any bias.",
        "context": "Some options to consider at this stage:\n\nAdversarial and Boundary tests to increase robustness\n\nVerifying accuracy for under-represented classes",
        "chunk_id": "chunk214_19",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-testing.md",
        "root_chunk_id": "chunk214"
    },
    {
        "user_prompt": "What's the best approach to build a machine learning model for defect detection in scenarios where the defect size is very small compared to the overall image size?",
        "output_prompt": "In such cases, building a custom machine learning model is the best approach. The model should be designed to focus on small areas of interest within larger images and use high-resolution inputs. It should be thoroughly parameterized and tuned to accurately identify tiny defects. Data augmentation techniques like zooming in on smaller regions and using techniques such as anomaly detection or semantic segmentation can be highly beneficial. It is also important to use a sufficient number of labeled examples of defects for the model to learn from.",
        "context": "High\n\nDefect Detection\/Quality Inspection\/Maintenance Inspection - Building a solution where a Machine Learned model is trained to detect quality control, manufacturing defects, or maintenance issues from images or video. Custom Vision is not recommended for this scenario. Defects in a recognized object are much more difficult to detect, in many cases these defects are very small compared to the overall size of the image, increasing the difficulty in identifying those defects. For example, if the the size of the object or defect is less than 5% of the overall image size, Custom Vision likely be able to identify the defect. Building a custom object\/defect machine learning model that can be parameterized and tuned for the scenario is recommended.\n\nRelated",
        "chunk_id": "chunk319_3",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/cognitive-services\/vision\/custom-vision.md",
        "root_chunk_id": "chunk319"
    },
    {
        "user_prompt": "Please provide an analysis of the two solutions mentioned for implementing SSO integration, specifically addressing their compatibility with policy enforcement, deployment methods, deployment monitoring, admission control, documentation availability, maintainability, maturity, and user interface.",
        "output_prompt": "The two solutions for SSO integration are Flux and ArgoCD. Both support monorepo and multirepo styles. For policy enforcement, they are compatible with Azure Policy, although the enforcement is in preview. Deployment methods include Helm and Kustomize for both, with ArgoCD also supporting KSonnet. Monitoring can be achieved using Prometheus and Grafana in both cases. Regarding admission control, both use RBAC, with Flux documentation available on Azure and ArgoCD providing documentation within their own docs. Maintainability for Flux is indicated by its GitHub presence with 945 stars and 94 forks, showing it's currently maintained. ArgoCD boasts a larger community with 5,000 stars and 1,100 forks. Finally, the UI differs as Flux requires management via a CLI, and ArgoCD offers a GUI with support for multiple clusters in the same interface.",
        "context": "SSO Integration\n\nGarbage collection\n\nResults\n\nThis section should contain a table that has each solution rated against each of the evaluation criteria:\n\nSolution Repo style Policy Enforcement Deployment Methods Deployment Monitoring Admission Control Azure Doc Maintainability Maturity UI Flux mono, multi Azure Policy, preview Helm, Kustomize Prometheus, Grafana RBAC Yes on Azure YAML in git repo 945 stars, 94 forks, currently maintained CLI ArgoCD mono, multi Azure Policy, preview Helm, Kustomize, KSonnet, ... Prometheus, Grafana RBAC Only in their own docs manifests in git repo 5,000 stars, 1,100 forks GUI, multiple clusters in same GUI\n\nDecision",
        "chunk_id": "chunk41_7",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/decision-log\/examples\/memory\/Trade-Studies\/GitOps.md",
        "root_chunk_id": "chunk41"
    },
    {
        "user_prompt": "Can you explain what the given Python code snippet does?",
        "output_prompt": "The Python code snippet is searching for the text 'run_id' within a line of text. When it finds 'run_id', it splits the line at the colon ':' and strips any whitespace from the second part, which is presumed to be the run ID value. Then it breaks the loop, stopping any further iteration, and returns the extracted run ID.",
        "context": "if \"run_id\" in line:\n                run_id = line.split(':')[1].strip()\n                break\n    return run_id",
        "chunk_id": "chunk303_15",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/sync-async-mlops-patterns.md",
        "root_chunk_id": "chunk303"
    },
    {
        "user_prompt": "How do I push my new feature branch to the remote repository for the first time?",
        "output_prompt": "To push your new feature branch to the remote repository for the first time, you should use the command `git push --set-upstream origin feature\/feature-name`, where `feature\/feature-name` is the name of your feature branch. After this initial push, just `git push` would be sufficient for subsequent pushes as the upstream branch will already be set.",
        "context": "{% raw %}\n\ncmd\ngit push\n\n{% endraw %}\n\nThe first time you push, you first need to set an upstream branch as follows. After the first push, the --set-upstream parameter and branch name are not needed anymore.\n\n{% raw %}\n\ncmd\ngit push --set-upstream origin feature\/feature-name\n\n{% endraw %}\n\nOnce the feature branch is pushed to the remote repository, it is visible to anyone with access to the code.\n\nMerging\n\nWe encourage the use of Pull Request to merge code to the main repository to make sure that all code in the final product is code reviewed",
        "chunk_id": "chunk206_11",
        "source": "..\/data\/docs\/code-with-engineering\/source-control\/git-guidance\/README.md",
        "root_chunk_id": "chunk206"
    },
    {
        "user_prompt": "Can you summarize the key advantages and disadvantages of using ESO with SSCSID for managing secrets in Kubernetes?",
        "output_prompt": "The key advantages of using ESO with SSCSID for secrets management in Kubernetes include support for pod portability through the SecretProviderClass CRD, the ability to auto-rotate secrets with customizable sync intervals, and the fact that it is endorsed by Microsoft as they heavily contribute to the Secrets Store CSI driver. However, a significant disadvantage is its lack of support in disconnected scenarios, as it cannot fetch secrets when offline which leads to failures in mounting volumes and subsequently affects pod scaling and restart capabilities during offline periods.",
        "context": "Product Group Links provided for AKVP with SSCSID:\n\nDifferences between ESO \/ SSCSID (GitHub Issue)\n\nSecrets Management on K8S talk here (Native Secrets, Vault.io, and ESO vs. SSCSID)\n\nAdvantages:\n\nSupports pod portability with the SecretProviderClass CRD\n\nSupports auto rotation of secrets with customizable sync intervals per cluster.\n\nSeems to be the MSFT choice (Secrets Store CSI driver is heavily contributed by MSFT and Kubernetes-SIG)\n\nDisadvantages:\n\nMissing disconnected scenario support: When the node is offline the SSCSID fails to fetch the secret and thus mounting the volume fails, making scaling and restarting pods not possible while being offline",
        "chunk_id": "chunk138_9",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/gitops\/secret-management\/README.md",
        "root_chunk_id": "chunk138"
    },
    {
        "user_prompt": "Can you explain how a combination of pull and push strategies can be used in enterprise scenarios for Azure Cognitive Search?",
        "output_prompt": "In enterprise scenarios, we can use the pull strategy initially to ingest bulk documents and set up the environment after the first deployment. For ongoing updates, a push strategy can be used to reflect real-time changes in the Azure Cognitive Search index.",
        "context": "Pull Strategy\n\nData ingestion is managed by the indexer. You have your data in your storage account and the Azure Cognitive Search indexer will ingest and re-index at your selected frequency. The index will not reflect the real-time state of your data with this approach.\n\nPush Strategy\n\nData ingestion managed by you. You have your data in your storage account or your application interface that you push into the Azure Cognitive Search index programmatically by using the REST API or SDK. Using this approach the index can reflect your data in real-time.\n\nIn enterprise scenarios, we can use both strategies together to ingest documents into Azure Cognitive Search. We can use pull strategy for the initial run to setup our environment after the first CI\/CD pipeline deployment and then a push strategy whenever there is an update on the data.\n\nOne Master Index vs Multiple Index",
        "chunk_id": "chunk315_12",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/search\/index.md",
        "root_chunk_id": "chunk315"
    },
    {
        "user_prompt": "How can I incorporate integration tests into my inner development loop to ensure sufficient code coverage?",
        "output_prompt": "You can incorporate integration tests into your inner development loop by using modern application frameworks that support faster test execution, such as .NET or Spring Boot, combined with appropriate mocking or stubbing techniques for external dependencies. By doing this, you make sure that the tests run at a speed comparable to unit tests both locally and in a continuous integration environment, thus achieving sufficient code coverage while maintaining developer productivity.",
        "context": "Tools\n\nThese tools allow for constant running of your unit tests with in-line code coverage, making the dev inner loop\nextremely fast and allows for easy TDD:\n\nVisual Studio Live Unit Testing\n\nWallaby.js\n\nInfinitest for Java\n\nPyCrunch for Python\n\nThings to consider\n\nTransferring responsibility to integration tests\n\nIn some situations it is worth considering to include the integration tests in the inner development loop to provide a sufficient code coverage to ensure the system is working properly. The prerequisite for this approach to be successful is to have integration tests being able to execute at a speed comparable to that of unit tests both locally and in a CI environment. Modern application frameworks like .NET or Spring Boot combined with the right mocking or stubbing approach for external dependencies offer excellent capabilities to enable such scenarios for testing.",
        "chunk_id": "chunk89_19",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/unit-testing\/README.md",
        "root_chunk_id": "chunk89"
    },
    {
        "user_prompt": "How can I make use of the .bashrc_extension file located in the 'me' directory for my application development environment?",
        "output_prompt": "You can utilize the .bashrc_extension file by including its content into your main .bashrc file. Usually, you can source it by adding the following line to your .bashrc: 'source ~\/me\/bashrc_extension'. This way, any aliases or functions defined in the extension file become available in your shell environment.",
        "context": "Consider the following solution structure:\n\n{% raw %}\n\nbash\nMy Application  # main repo directory\n\u2514\u2500\u2500\u2500.devcontainer\n|       \u251c\u2500\u2500\u2500Dockerfile\n|       \u251c\u2500\u2500\u2500docker-compose.yml\n|       \u251c\u2500\u2500\u2500devcontainer.json\n\u2514\u2500\u2500\u2500me\n|       \u251c\u2500\u2500\u2500bashrc_extension\n\n{% endraw %}\n\nThe folder me is untracked in the repository, leaving developers the flexibility to add personal resources. One of these resources can be a .bashrc extension containing customization. For instance:\n\n{% raw %}\n\n```bash\n\nSample alias\n\nalias gaa=\"git add --all\"",
        "chunk_id": "chunk246_17",
        "source": "..\/data\/docs\/code-with-engineering\/developer-experience\/going-further.md",
        "root_chunk_id": "chunk246"
    },
    {
        "user_prompt": "How do I set up Shellcheck in VS Code for linting my bash scripts?",
        "output_prompt": "To set up Shellcheck in VS Code, follow these steps: first, install Shellcheck on your macOS machine using Homebrew by running 'brew install shellcheck' in the terminal. Then, install the vscode-shellcheck extension in VS Code, which will provide you with static code analysis and the ability to automatically fix linting issues.",
        "context": "Bash Code Reviews\n\nStyle Guide\n\nDevelopers should follow Google's Bash Style Guide.\n\nCode Analysis \/ Linting\n\nProjects must check bash code with shellcheck as part of the CI process.\nApart from linting, shfmt can be used to automatically format shell scripts. There are few vscode code extensions which are based on shfmt like shell-format which can be used to automatically format shell scripts.\n\nProject Setup\n\nvscode-shellcheck\n\nShellcheck extension should be used in VS Code, it provides static code analysis capabilities and auto fixing linting issues. To use vscode-shellcheck in vscode do the following:\n\nInstall shellcheck on your machine\n\nFor macOS\n\n{% raw %}\n\nbash\nbrew install shellcheck",
        "chunk_id": "chunk21_0",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/bash.md",
        "root_chunk_id": "chunk21"
    },
    {
        "user_prompt": "What are the advantages of Consumer-driven Contract Testing over End-to-End tests?",
        "output_prompt": "Consumer-driven Contract Testing offers several advantages over End-to-End testing: it's typically faster since it tests components in isolation, it's more reliable as the contracts enforce expectations, maintenance is easier due to its modular nature, and it does not require a dedicated testing environment.",
        "context": "Consumer-driven Contract Testing (CDC)\n\nConsumer-driven Contract Testing (or CDC for short) is a software testing methodology used to test components of a system in isolation while ensuring that provider components are compatible with the expectations that consumer components have of them.\n\nWhy Consumer-driven Contract Testing\n\nCDC tries to overcome the several painful drawbacks of automated E2E tests with components interacting together:\n\nE2E tests are slow\n\nE2E tests break easily\n\nE2E tests are expensive and hard to maintain\n\nE2E tests of larger systems may be hard or impossible to run outside a dedicated testing environment",
        "chunk_id": "chunk86_0",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/cdc-testing\/README.md",
        "root_chunk_id": "chunk86"
    },
    {
        "user_prompt": "Can you explain the concept of Developer Experience (DevEx) and its importance?",
        "output_prompt": "Developer Experience (DevEx) encompasses the ease with which a developer can execute essential tasks such as building, testing, starting, and debugging code. A positive DevEx implies that these tasks can be performed efficiently, leading to quicker development cycles, better code quality, and, ultimately, a more productive development team. Efforts to improve DevEx are particularly beneficial over time and for larger teams, as they compound to save more time and reduce frustration.",
        "context": "Developer Experience (DevEx)\n\nDeveloper experience refers to how easy or difficult it is for a developer to perform essential tasks needed to implement a change. A positive developer experience would mean these tasks are relatively easy for the team (see measures below).\n\nThe essential tasks are identified below.\n\nBuild - Verify that changes are free of syntax error and compile.\n\nTest - Verify that all automated tests pass.\n\nStart - Launch end-to-end to simulate execution in a deployed environment.\n\nDebug - Attach debugger to started solution, set breakpoints, step through code, and inspect variables.\n\nIf effort is invested to make these activities as easy as possible, the returns on that effort will increase the longer the project runs, and the larger the team is.\n\nDefining End-to-End",
        "chunk_id": "chunk245_0",
        "source": "..\/data\/docs\/code-with-engineering\/developer-experience\/README.md",
        "root_chunk_id": "chunk245"
    },
    {
        "user_prompt": "Could you explain the purpose of exploratory data analysis in machine learning projects?",
        "output_prompt": "Exploratory Data Analysis (EDA) is crucial in machine learning projects as it allows researchers and data scientists to gain insights into the dataset's structure, detect outliers or anomalies, understand patterns, and test assumptions. It is the first step in data pre-processing which informs subsequent model selection and feature engineering. Essentially, it ensures the data adequately represents the problem at hand to increase the likelihood of project success.",
        "context": "author: shanepeckham\ntitle: Exploratory Data Analysis\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Every machine learning project requires a deep understanding of the data to understand whether the data is representative of the problem. It is also important to determine the approaches to be undertaken, and for the project to be successful.\nrings:\n   - public\n\nUse Exploratory Data Analysis (EDA) to explore and understand your data\n\nEvery machine learning project requires a deep understanding of the data to understand whether the data is representative of the problem. A systematic approach to understanding the data should be undertaken to ensure project success. Every machine learning project demands a deep understanding of the data to be used to be sure it represents the problem.",
        "chunk_id": "chunk333_0",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/experimentation\/exploratory-data-analysis.md",
        "root_chunk_id": "chunk333"
    },
    {
        "user_prompt": "How can I enforce code style rules in our CI pipeline using StyleCop Analyzers?",
        "output_prompt": "To enforce code style rules in your CI pipeline with StyleCop Analyzers, you should first install the StyleCop.Analyzers nuget package in your project. Next, configure the rules you want to enforce by using a ruleset file or an .editorconfig file in your project's root directory. In your build pipeline, add a build step that compiles the project, which will run the analyzers. If any of the rules are violated, the build step will fail, preventing merging of non-compliant code into your git repository.",
        "context": "StyleCop analyzer\n\nThe StyleCop analyzer is a nuget package (StyleCop.Analyzers) that can be installed in any of your projects. It's mainly around code style rules and makes sure the team is following the same rules without having subjective discussions about braces and spaces. Detailed information can be found here: StyleCop Analyzers for the .NET Compiler Platform.\n\nThe minimum rules set teams should adopt is the Managed Recommended Rules rule set.\n\nAutomatic Code Formatting\n\nUse .editorconfig to configure code formatting rules in your project.\n\nBuild validation\n\nIt's important that you enforce your code style and rules in the CI to avoid any team member merging code that does not comply with your standards into your git repo.",
        "chunk_id": "chunk17_6",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/csharp.md",
        "root_chunk_id": "chunk17"
    },
    {
        "user_prompt": "How can RLHF be used to improve the performance of language models?",
        "output_prompt": "RLHF, or Reinforcement Learning from Human Feedback, can be used to improve language models by training a reward model that predicts human satisfaction with model responses. This reward model then influences the fine-tuning process of the LLM, allowing it to generate outputs more aligned with human preferences and expectations, enhancing overall performance.",
        "context": "One can also take human feedback to another level and incorporate it into the LLM-based system using reinforcement learning from human feedback (RLHF). RLHF is a technique that uses methods from reinforcement learning to directly optimize a language model with human feedback. The basic idea is to train a reward model that predicts how much a human would like a given response, and then use that reward model to guide the fine-tuning of the language model.\n\nRLHF: Illustrating Reinforcement Learning from Human Feedback\n\nHuman evaluation: Guiding Pre training in Reinforcement Learning with Large Language Models\n\nReinforcement Learning - Microsoft Research\n\nLLM-based Evaluation",
        "chunk_id": "chunk280_20",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/metrics\/eval-metrics.md",
        "root_chunk_id": "chunk280"
    },
    {
        "user_prompt": "How should an orchestration service manage PTU utilization to handle low and high priority requests effectively?",
        "output_prompt": "An orchestration service should manage PTU utilization by determining the maximum number of low priority requests that can be processed simultaneously without affecting high priority requests. It should define lower and upper limits for PTU consumption to regulate low priority request throughput based on capacity, ensuring a balance between priority levels while optimizing PTU use.",
        "context": "1.3 Graceful degradation of throughput of Low Priority Request\n\nAfter computing the utilization of PTU with either one of the two\nmethods, orchestration service should reduce the processing speed of low\npriority request gradually as the utilization goes up. This will help to\nmaintain a balance between high and low priority requests while making\nthe most of the PTU utilization.\n\nHere are the steps to allocate PTU capacity among different priority levels of requests:\n\nDetermine the maximum number of low priority requests that we can process simultaneously without compromising the high priority ones, based on the demand trends and potential peaks of our organization.\n\nDefine two limits for PTU consumption: a lower one and an upper one. These will help us regulate the throughput of low priority requests depending on the available capacity.",
        "chunk_id": "chunk383_8",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/genai-gateway\/maximise-ptu-utilization.md",
        "root_chunk_id": "chunk383"
    },
    {
        "user_prompt": "Can you provide some key insights on how to improve CI\/CD pipeline visibility and efficiency?",
        "output_prompt": "To improve CI\/CD pipeline visibility and efficiency, it's essential to establish clear Key Performance Indicators (KPIs) like build success rates, average build times, and failure rates. Implement comprehensive tracing and utilize build tags to distinguish between 'Release Candidate' and 'Non-Release Candidate' builds. Furthermore, consolidate this data into a dashboard to provide an organization-wide view of pipeline health, enabling the identification of trends and areas for improvement.",
        "context": "Helps to provide an organization-wide view of pipeline health to easily identify trends.\n\nPoints to Consider\n\nIt is important to identify the Key Performance Indicators (KPIs) for evaluating a successful CI\/CD pipeline. Where needed, additional tracing can be added to better record KPI metrics. For example, adding pipeline build tags to identify a 'Release Candidate' vs. 'Non-Release Candidate' helps in evaluating the end-to-end release process timeline.",
        "chunk_id": "chunk182_1",
        "source": "..\/data\/docs\/code-with-engineering\/observability\/observability-pipelines.md",
        "root_chunk_id": "chunk182"
    },
    {
        "user_prompt": "How can I convert these raw image files into JPG format?",
        "output_prompt": "To convert raw image files into JPG format, you can use image editing software like Adobe Photoshop or Lightroom. Alternatively, there are also free online converters and batch conversion tools available that can process your raw files and convert them to JPG. Just upload your raw images to the converter or software, select JPG as the output format, adjust any settings if necessary, and then initiate the conversion. Make sure to back up your original raw files to prevent any data loss.",
        "context": "--boundary\nContent-Disposition: form-data; name=\"files\"; filename=\"image1.raw\"\nContent-Type:\n\n(data)\n--boundary\nContent-Disposition: form-data; name=\"files\"; filename=\"image2.raw\"\nContent-Type:\n\n(data)\n--boundary\nContent-Disposition: form-data; name=\"files\"; filename=\"image3.raw\"\nContent-Type:",
        "chunk_id": "chunk256_4",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "How can the AI gateway maximize the use of an Azure Open AI PTU while ensuring high priority requests have good performance?",
        "output_prompt": "The AI gateway can maximize PTU utilization by dynamically allocating resources, prioritizing high priority requests, and allowing low priority requests to use the residual PTU capacity. When demand from high priority requests is low, the gateway increases throughput for low priority requests, utilizing unused PTU capacity. Conversely, during peak times for high priority requests, it reduces the bandwidth for low priority requests to ensure optimal performance for the more critical tasks.",
        "context": "Approaches for Maximizing PTU Utilization\n\n1. Approach\n\nIn this approach, the AI gateway must be able to maximize the use of an Azure Open AI Provisioned Throughput Unit (PTU) while also\nmaintaining good performance for high priority requests. When there are\nfewer high priority requests, the gateway should let delayed low\npriority requests take up the leftover capacity of PTU and when there\nare more high priority requests, the gateway should free up some\ncapacity by reducing the speed of low priority requests.",
        "chunk_id": "chunk383_0",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/genai-gateway\/maximise-ptu-utilization.md",
        "root_chunk_id": "chunk383"
    },
    {
        "user_prompt": "How can I deploy a virtual network named 'vnet-dev' within the resource group 'rg-dev-network' using Terraform with the given module structure?",
        "output_prompt": "To deploy a virtual network named 'vnet-dev' within the resource group 'rg-dev-network' using Terraform, you would first navigate to the 'network' module directory. Then, you would define the resource configuration within the 'main.tf' file using the Terraform syntax. Make sure your 'provider.tf' file is correctly configured for your cloud provider. The 'variables.tf' file should define any required variables such as the network name and resource group. Finally, initialize the Terraform setup by running 'terraform init', and apply the configuration with 'terraform apply', providing the necessary variable values.",
        "context": "How to do it?\n\nContext\n\nLet's consider the following structure for our modules:\n\n{% raw %}\n\nconsole\nmodules\n\u251c\u2500\u2500 kubernetes\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 network\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n\n{% endraw %}\n\nNow, assume that you deploy a virtual network for the development environment, with the following properties:\n\nname: vnet-dev\n\nresource group: rg-dev-network",
        "chunk_id": "chunk133_2",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/recipes\/terraform\/share-common-variables-naming-conventions.md",
        "root_chunk_id": "chunk133"
    },
    {
        "user_prompt": "Before we conclude our wrap-up sprint, what should we check regarding the design artifacts?",
        "output_prompt": "We need to ensure that all design artifacts are up to date and that they are stored in an easily accessible location for future reference.",
        "context": "During your wrap-up sprints the dev crew should consider the following:\n\nAre the design artifacts up to date?\n\nAre the design artifacts stored in an accessible location?",
        "chunk_id": "chunk58_8",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/recipes\/engagement-process.md",
        "root_chunk_id": "chunk58"
    },
    {
        "user_prompt": "Can you summarize how to effectively describe a solution in a document?",
        "output_prompt": "Sure! To effectively describe a solution in a document, start with a clear and concise description of the solution. Where applicable, include a diagram for visual reference. Outline any possible variations to the main solution. Then evaluate the solution based on established criteria. Remember to detail the depth and specifics to match the complexity of the functionality being discussed. For experiments, provide a methodology for testing the solution.",
        "context": "NOTE: Using present tense language to describe the solution can help avoid confusion between current state and future state. For example, use \"This solution works by doing...\" vs. \"This solution would work by doing...\".\n\nEach solution section should contain the following:\n\nDescription of the solution\n\n(optional) A diagram to quickly reference the solution\n\nPossible variations - things that are small variations on the main solution can be grouped together\n\nEvaluation of the idea based on the evaluation criteria above\n\nThe depth, detail, and contents of these sections will vary based on the complexity of the functionality\nbeing developed.\n\nExperiment(s)",
        "chunk_id": "chunk47_8",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/trade-studies\/template.md",
        "root_chunk_id": "chunk47"
    },
    {
        "user_prompt": "Can my device with a 2GB memory support a camera configuration of three cameras, where the first camera has an angle of 0, aperture of 5.6, top orientation, resolution of 1280x720, and a specific shutter speed?",
        "output_prompt": "To determine if your device with 2GB of memory can support the specified camera configuration, we would need the complete information of the camera's shutter speed along with the specifications of the other two cameras. Additionally, the integration capacity of your device for handling multiple camera feeds concurrently plays a crucial role. It's recommended to check the device's complete specifications and the cameras' requirements to ensure compatibility.",
        "context": "\"requiredMemory\": \"2GB\"\n  },\n  \"metadataVersion\": \"1.0.0\",\n  \"numberOfCameras\": 3,\n  \"inputs\": [\n    {\n      \"name\": \"image1\",\n      \"description\": \"First camera\",\n      \"type\": \"image\",\n      \"recommendedSettings\": {\n        \"angle\": 0,\n        \"aperture\": \"5.6\",\n        \"orientation\": \"top\",\n        \"resolution\": \"1280x720\",\n        \"shutterSpeed\": {",
        "chunk_id": "chunk256_27",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Can you explain what the <PrivateAssets> and <IncludeAssets> tags are used for in a .csproj file?",
        "output_prompt": "In a .csproj file, the <PrivateAssets> tag is used to control how the assets of a package reference are consumed. When set to 'all', it means that the assets of that package will not be made available to the consumer, thus they won't be included in the consuming project's output. The <IncludeAssets> tag specifies which assets should be consumed by the project. In the provided example, the assets specified include runtime components, build-time utilities, native libraries, content files, analyzers for code inspection, and transitive build assets. Additionally, <TreatWarningsAsErrors> being set to 'true' indicates that any warning in the code should be treated as an error, enforcing a high code quality standard.",
        "context": "<PrivateAssets>all<\/PrivateAssets>\n          <IncludeAssets>runtime; build; native; contentfiles; analyzers; buildtransitive<\/IncludeAssets>\n        <\/PackageReference>\n    <\/ItemGroup>\n    <PropertyGroup>\n        <TreatWarningsAsErrors>true<\/TreatWarningsAsErrors>\n    <\/PropertyGroup>\n    <ItemGroup Condition=\"Exists('$(MSBuildThisFileDirectory)..\/.editorconfig')\" >\n        <AdditionalFiles Include=\"$(MSBuildThisFileDirectory)..\/.editorconfig\" \/>",
        "chunk_id": "chunk17_2",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/csharp.md",
        "root_chunk_id": "chunk17"
    },
    {
        "user_prompt": "What are the benefits of using VFS for Git when working with large repositories?",
        "output_prompt": "VFS for Git is beneficial for large repositories because it virtualizes the file system, allowing you to work with just the necessary files. It downloads only the required objects on demand, reducing the amount of data needed to be fetched and stored locally, which in turn minimizes network traffic and storage space usage. This makes the development process more efficient and manageable.",
        "context": "Working with large repositories\n\nWhen working with a very large repository of which you don't require all the files, you can use VFS for Git - an open source Git extension that virtualize the file system beneath your Git repository, so that you seem to work in a regular working directory but while VFS for Git only downloads objects as they are needed. You can find more information on VFS for Git in the Git LFS and VFS document.\n\nTools\n\nVisual Studio Code is a cross-platform powerful source code editor with built in git commands. Within Visual Studio Code editor you can review diffs, stage changes, make commits, pull and push to your git repositories.\nYou can refer to Visual Studio Code Git Support for documentation.\n\nUse a shell\/terminal to work with Git commands instead of relying on GUI clients.",
        "chunk_id": "chunk206_29",
        "source": "..\/data\/docs\/code-with-engineering\/source-control\/git-guidance\/README.md",
        "root_chunk_id": "chunk206"
    },
    {
        "user_prompt": "Can you explain the concept of evaluation flows in the context of LLMOps?",
        "output_prompt": "Evaluation flows are specialized workflows in LLMOps that involve executing the standard application flow, comparing its output with a predefined ground truth or conducting a groundless evaluation, and then aggregating and reporting on the outcomes. These results are typically recorded in an experimentation platform, such as Azure AI Studio Experiments, to be analyzed and compared with other experiments. This approach is instrumental in continuously improving models and is a critical component of the wider DevOps process, adapted here for Language Model Operations (LLMOps).",
        "context": "A very valuable concept of Prompt Flow is the concept of evaluation flows. Whilst the application itself consists of a standard flow (i.e. the flow as it can be converted directly into an endpoint and deployed), an evaluation flow is one that calls the standard flow, compares the result with a given ground truth, or produces a groundless evaluation, aggregates the result and reports on it. In the case of prompt flow, that result is stored as an Azure AI Studio Experiment, and can be compared with others.\n\nTherefore, experimentation platform, data and evaluation flows are the most important aspects that bring differences into DevOps making it as LLMOps.\n\nTechnology Agnostic LLMOps for Prompt Engineering Implementation",
        "chunk_id": "chunk275_5",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/mlops-in-openai\/llmops.md",
        "root_chunk_id": "chunk275"
    },
    {
        "user_prompt": "Can you explain the importance of combining automated and manual accessibility testing?",
        "output_prompt": "Combining automated and manual accessibility testing is crucial because automated tools can quickly identify many common accessibility issues, but they may not catch all nuances that affect actual user experience. Manual testing allows testers to empathize with users, especially those with disabilities, and to detect issues that require human judgement. Together, they provide a more comprehensive approach to ensuring that software is accessible to all users.",
        "context": "Practices\n\nAccessibility Testing\n\nAccessibility testing is a specialized subset of software testing and includes automated tools and manual testing processes that vary from project to project. In addition to tools like Accessibility Insights discussed earlier, there are many other solutions for accessibility testing. The W3C provides a comprehensive list of evaluation and testing tools on their website at https:\/\/www.w3.org\/WAI\/ER\/tools\/.\n\nIf you are looking to add automated testing to your Azure Pipelines, you may want to consider the Accessibility Testing extension built by Drew Lewis, a former Microsoft employee.\n\nIt's important to keep in mind that automated tooling alone is not enough - make sure to augment your automated tests with manual ones. Accessibility Insights (linked above) can guide users through some manual testing steps.\n\nCode and Documentation Basics",
        "chunk_id": "chunk222_6",
        "source": "..\/data\/docs\/code-with-engineering\/accessibility\/README.md",
        "root_chunk_id": "chunk222"
    },
    {
        "user_prompt": "Write a script step for a CI process that checks for security issues in the codebase and reacts accordingly.",
        "output_prompt": "You can add a script step in your CI configuration that utilizes a tool like 'detect-secrets' to scan for potential secrets in the code. If secrets are detected, the job should fail, otherwise, it should pass. The script also should display a message about the result. An example for such a step in a PowerShell script could be:\n\nif (Detect-Secrets) {\n  Write-Host \"##vso[task.complete result=Failed;]Secrets detected.\"\n} else {\n  Write-Host \"##vso[task.complete result=Succeeded;]No secrets detected.\"\n}\ndisplayName: \"Analyzing detect-secrets results\"",
        "context": "}\n  else {\n    Write-Host \"##vso[task.complete result=Succeeded;]No secrets detected.\"\n  }\ndisplayName: \"Analyzing detect-secrets results\"",
        "chunk_id": "chunk234_6",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-integration\/dev-sec-ops\/secret-management\/recipes\/detect-secrets-ado.md",
        "root_chunk_id": "chunk234"
    },
    {
        "user_prompt": "How can I use the \/testdata endpoint to test a new vision model bundle?",
        "output_prompt": "To test a new vision model bundle using the \/testdata endpoint, you first need to make sure that the bundle is available in the Bundle Registry. Then, use the Simulator to check the health and ready status of the container. After that, extract the metadata and test data, and finally, run the model or traditional computer vision code against the provided sample images and testcase labels.",
        "context": "3. \/testdata - Bundle Testing\n\n\/testdata: This endpoint is used to expose sample images and expected results in as a zip file.\n\nOnce Bundle is available in Bundle Registry, Simulator will be used to test the bundle. We'll check health and ready status of the container then, extract metadata and test data on a Simulator.\n\nBundles will be able to run model\/traditional computer vision code. Bundles include some test images and testcase labels.",
        "chunk_id": "chunk258_10",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/index.md",
        "root_chunk_id": "chunk258"
    },
    {
        "user_prompt": "What are some best practices for reviewing pull requests (PRs) efficiently?",
        "output_prompt": "Here are some recommended best practices for reviewing PRs: 1) Prioritize stakeholder reviews as needed, but don't run the PR unless it helps to understand the code. 2) Follow a logical sequence when reading code to better grasp its functionality. 3) If you lack context to understand a file change, view the entire file or check out the changes in an IDE. 4) Ask the author for clarifications if necessary. 5) Review at a steady pace without rushing, but also take breaks between reviews especially when handling complex code or multiple PRs. Focus on PRs that fall within your expertise.",
        "context": "If we have a stakeholder review, it\u2019s not necessary to run the PR unless it aids your understanding of the code.\n\nAzDO orders the files for you, but you should read the code in some logical sequence to aid understanding.\n\nIf you don\u2019t fully understand a change in a file because you don\u2019t have context, click to view the whole file and read through the surrounding code or checkout the changes and view them in IDE.\n\nAsk the author to clarify.\n\nTake your time and keep focus on scope\n\nYou shouldn't review code hastily but neither take too long in one sitting. If you have many pull requests (PRs) to review or if the complexity of code is demanding, the recommendation is to take a break between the reviews to recover and focus on the ones you are most experienced with.",
        "chunk_id": "chunk11_1",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/process-guidance\/reviewer-guidance.md",
        "root_chunk_id": "chunk11"
    },
    {
        "user_prompt": "What are the commands in this script doing?",
        "output_prompt": "The script runs three commands chained together by '&&' which ensures that each command runs only if the preceding one succeeds. 'make install' installs the software. 'ldconfig' updates the shared library cache. Finally, 'rm -rf \/tmp\/openmpi' removes the OpenMPI source files from the temporary directory.",
        "context": "make install && \\\n    ldconfig && \\\n    rm -rf \/tmp\/openmpi",
        "chunk_id": "chunk314_9",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/working-with-azure-ml\/azure-ml-non-ubutu-images.md",
        "root_chunk_id": "chunk314"
    },
    {
        "user_prompt": "Why is it important to log the start and end of system activities?",
        "output_prompt": "Logging the start and end of system activities is crucial because it provides visibility into the application's operations, which aids in understanding its behavior and diagnosing any issues. Timely log messages can also help in analyzing the performance by showing how long each activity takes to complete. This is invaluable for debugging and monitoring the system effectively, especially in a deployed environment where direct observation isn't possible.",
        "context": "Log all activity. If the system is performing some activity (reading data from a database, calling an external service, etc.), it should log that activity. Ideally, there should be a log message saying the activity is starting and another log message saying the activity is complete. This allows someone reviewing the logs to understand what the application is doing and how long it is taking. Depending on how noisy this is, different messages can be associated with different log levels, but it is important to have the information available when it comes to debugging a deployed system.",
        "chunk_id": "chunk67_11",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/README.md",
        "root_chunk_id": "chunk67"
    },
    {
        "user_prompt": "When working on software development, what are the key testing strategies I should employ to ensure backward compatibility, validate program logic across a variety of inputs, prevent regressions, and verify component interactions both in isolation and in a full system context?",
        "output_prompt": "To ensure software quality and integrity during development, you should consider the following testing strategies: for backward compatibility, implement shadow testing; to ensure program logic is correct across various inputs, use unit testing, functional tests, consumer-driven contract testing, and integration testing; to prevent regressions, apply unit testing, functional tests, consumer-driven contract testing, integration testing, and rings; for validating component interactions in isolation, consumer-driven contract testing is key; and for testing multiple components functioning together, including network hops, you should employ integration testing, end-to-end (E2E) testing, and segmented tests.",
        "context": "When I am working on... I want to get this outcome... ...so I should consider Development Prove backward compatibility with existing callers and clients Shadow testing Development Ensure program logic is correct for a variety of expected, mainline, edge and unexpected inputs Unit testing ; Functional tests; Consumer-driven Contract Testing ; Integration testing Development Prevent regressions in logical correctness; earlier is better Unit testing ; Functional tests; Consumer-driven Contract Testing ; Integration testing ; Rings (each of these are expanding scopes of coverage) Development Validate interactions between components in isolation, ensuring that consumer and provider components are compatible and conform to a shared understanding documented in a contract Consumer-driven Contract Testing Development; Integration testing Validate that multiple components function together across multiple interfaces in a call chain, incl network hops Integration testing ; End-to-end ( End-to-End testing ) tests; Segmented",
        "chunk_id": "chunk67_1",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/README.md",
        "root_chunk_id": "chunk67"
    },
    {
        "user_prompt": "Where can I find the necessary installers for VFS for Git, and what are the steps to install them and prepare a repository for use with VFS for Git?",
        "output_prompt": "The installers required for VFS for Git can be found on the Microsoft VFS for Git releases page on GitHub at https:\/\/github.com\/Microsoft\/VFSForGit\/releases. To set up VFS for Git, you need to download and install the Git 2.28.0.0 installer as well as the SetupGVFS installer specific to VFS for Git. Ensure that a .gitattributes file is added to the repository with the line '* -text' to prepare it for VFS for Git. To clone a repository with VFS for Git, use the gvfs clone command instead of the standard git clone.",
        "context": "The necessary installers can be found at https:\/\/github.com\/Microsoft\/VFSForGit\/releases\n\nOn the releases page you'll find two important downloads:\n\nGit 2.28.0.0 installer, which is a requirement for running VFS for Git. This is not the same as the standard Git for Windows install!\n\nSetupGVFS installer.\n\nDownload those files and install them on your machine.\n\nTo be able to use VFS for Git for a repository, a .gitattributes file needs to be added to the repo with this line in it:\n\n{% raw %}\n\nshell\n* -text\n\n{% endraw %}\n\nTo clone a repository to your machine using VFS for Git you use gvfs instead of git like so:",
        "chunk_id": "chunk205_9",
        "source": "..\/data\/docs\/code-with-engineering\/source-control\/git-guidance\/git-lfs-and-vfs.md",
        "root_chunk_id": "chunk205"
    },
    {
        "user_prompt": "What actions should be taken if the status of a system test is -L--W?",
        "output_prompt": "When a system test returns the status -L--W, it means the tests have failed and the bundle cannot be deployed. However, other slots should continue with their operations without using the bundle, and developers are given write access again to upload a new version of the bundle under the same tag. The existing bundle, although out of quarantine, should not be used by any slots due to the failed status.",
        "context": "Tests failed \\\n   Status: -L--W \\\n   The tests failed. The bundle may not be deployed to the edge. Other slots\n   should continue to ignore the bundle. The bundle is re-enabled for write\n   access. Developers may upload a new version of the bundle under the same\n   tag. The image is out of quarantine but not readable and should still be\n   ignored by other slots.\n\nTests failed (Security Finding) \\\n   Status: Q-D-- \\\n   The tests failed and there was a security finding. Image cannot be read or\n   written. All slots should remove the bundle from their services. Should be\n   deleted after 14 days.",
        "chunk_id": "chunk257_11",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-registry.md",
        "root_chunk_id": "chunk257"
    },
    {
        "user_prompt": "Could you summarize the possible outcomes of a feasibility study in the context of assessing a problem for machine learning solutions?",
        "output_prompt": "The feasibility study report will conclude with either a negative or a positive outcome. If there is insufficient evidence supporting the hypothesis that ML can solve the problem within the set performance and business impact benchmarks, the report will detail the limitations encountered. This might include narrowing the project scope, redefining the problem based on the study's insights, or exploring ways to obtain or improve the quality of data. On the other hand, if the evidence is favorable, the report will offer recommendations and technical guidelines to transition into the operationalization phase of the machine learning solution.",
        "context": "Discussion and feedback from diverse perspectives around any responsible AI concerns\n\nOutput of a feasibility study\n\nPossible outcomes\n\nThe main outcome is a feasibility study report, with a recommendation on next steps:\n* If there is not enough evidence to support the hypothesis that this problem can be solved using ML, as aligned with the pre-determined performance measures and business impact:\n\nWe detail the gaps and challenges that prevented us from reaching a positive outcome\n\nWe may scope down the project, if applicable\n\nWe may look at re-scoping the problem taking into account the findings of the feasibility study\n\nWe assess the possibility to collect more data or improve data quality\n\nIf there is enough evidence to support the hypothesis that this problem can be solved using ML\n\nProvide recommendations and technical assets for moving to the operationalization phase",
        "chunk_id": "chunk218_10",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-feasibility-study.md",
        "root_chunk_id": "chunk218"
    },
    {
        "user_prompt": "How should one handle isolation levels in data heavy projects to avoid issues?",
        "output_prompt": "When handling isolation levels in data heavy projects, it's essential to be aware of the isolation level you're using. You should aim for serializability whenever possible but be mindful that your system might default to a lower isolation level like read uncommitted. This lower level might lead to unpredictable bugs. To scale your system, prioritize techniques like batching, sharding, and caching over eventual consistency, which should be a last resort. If you need to scale beyond traditional methods, look into 'New SQL' databases such as CockroachDB or TiDB before settling on a system that relies heavily on eventual consistency.",
        "context": "The goal of this section is to briefly describe how to apply the fundamentals to data heavy projects or portions of the project.\n\nIsolation\n\nPlease be cautious of which isolation levels you are using. Even with a database that offers serializability, it is possible that within a transaction or connection you are leveraging a lower isolation level than the database offers. In particular, read uncommitted (or eventual consistency), can have a lot of unpredictable side effects and introduce bugs that are difficult to reason about. Eventually consistent systems should be treated as a last resort for achieving your scalability requirements; batching, sharding, and caching are all recommended solutions to increase your scalability. If none of these options are tenable, consider evaluating the \"New SQL\" databases like CockroachDB or TiDB, before leveraging an option that relies on eventual consistency.",
        "chunk_id": "chunk32_1",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-patterns\/data-heavy-design-guidance.md",
        "root_chunk_id": "chunk32"
    },
    {
        "user_prompt": "What are the key features required for the MLOps process in Databricks to be feasible?",
        "output_prompt": "The key features required for the MLOps process in Databricks to be feasible include the ability to run code that may be spread across multiple files and custom libraries, as well as support for job clusters.",
        "context": "Using workflows and repos for MLOps in Databricks\n\nThe following diagram demonstrates an end-to-end MLOps process that is common in enterprise:\n\nMLOps Architecture\n\nThis process supports three different environments that can be implemented using a single Databricks workspace or separated across three different workspaces.\n\nCommon tasks include moving code from one branch into another (PR, merging), invoking the model approval process in the production environment, and running integration and unit tests. Most of the tasks can be done thanks to Azure DevOps Builds or GitHub Workflows. Models can be stored and tracked in a MLflow model repository that has been integrated with Databricks.\n\nHowever, the process requires some additional features to be feasible:\n\nThe ability to run code that might be separated in several files and custom libraries.\n\nJob clusters support.",
        "chunk_id": "chunk299_1",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/mlops-in-databricks\/mlops-in-databricks-dbx.md",
        "root_chunk_id": "chunk299"
    },
    {
        "user_prompt": "Considering the provided context of the security principle with multiple Azure components and IoT Edge modules, what would be the best practice to secure communication within this architecture?",
        "output_prompt": "To secure communication within the architecture involving Azure components and IoT Edge modules, best practices include implementing end-to-end encryption, using secure connection strings with SAS tokens instead of hard-coded credentials, enabling Azure Active Directory for authentication, applying network security groups, and utilizing Azure Key Vault for managing secrets. Additionally, regular security audits and updates should be performed.",
        "context": "security principle or goal.\n\nArchitecture Diagram\n\nAssets\n\nAsset Entry Point Trust Level Azure Blob Storage Http End point Connection String Azure Monitor Http End Point Connection String Azure Cognitive Service Http End Point Connection String IoTEdge Module: M1 Http End Point Public Access (Local Area Network) IoTEdge Module: M2 Http End Point Public Access (Local Area Network) IoTEdge Module: M3 Http End Point Public Access (Local Area Network) IoTEdge Module: IoTEdgeMetricsCollector Http EndPoint Public Access (Local Area Network) Application Insights Http End Point Connection String\n\nData Flow Diagram\n\nClient Browser makes requests to the M1 IoTEdge module. Browser and IoTEdge device are on same network, so browser directly hits the webapp URL.",
        "chunk_id": "chunk122_1",
        "source": "..\/data\/docs\/code-with-engineering\/security\/threat-modelling-example.md",
        "root_chunk_id": "chunk122"
    },
    {
        "user_prompt": "What are the important considerations for TPMs when determining the need for Machine Learning in a project?",
        "output_prompt": "TPMs should first understand the overall business problem and then assess whether Machine Learning can effectively address a part of that problem. They should evaluate the problem space critically to determine if ML is the right fit for the project.",
        "context": "TPM considerations for Machine Learning projects\n\nIn this document, we explore some of the Program Management considerations for Machine Learning (ML) projects and suggest recommendations for Technical Program Managers (TPM) to effectively work with Data and Applied Machine Learning engineering teams.\n\nDetermine the need for Machine Learning in the project\n\nIn Artificial Intelligence (AI) projects, the ML component is generally a part of an overall business problem and NOT the problem itself. Determine the overall business problem first and then evaluate if ML can help address a part of the problem space.\nFew considerations for identifying the right fit for the project:",
        "chunk_id": "chunk213_0",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-tpm-guidance.md",
        "root_chunk_id": "chunk213"
    },
    {
        "user_prompt": "Do I need to configure any specific network settings for a CVT Standalone setup?",
        "output_prompt": "For a CVT Standalone setup, only a local network configuration is necessary. Additional network configurations are optional.",
        "context": "In case of CVT Standalone, only local network is needed, it is optional to add any other network configuration.\n\nIn case of CVT Dedicated or Standalone, it is strongly recommended to have cameras on a dedicated sub network accessible only by the IPC. This will guaranty a better network isolation to avoid \"pollution\" of camera frames in the rest of the network.\n\n[!NOTE]",
        "chunk_id": "chunk376_12",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/computer-vision-on-edge\/cvt-deployment-typologies.md",
        "root_chunk_id": "chunk376"
    },
    {
        "user_prompt": "Can you explain what the Levenshtein Similarity Ratio is?",
        "output_prompt": "Certainly! The Levenshtein Similarity Ratio is a measure that indicates how similar two sequences are. It is derived from the Levenshtein Distance, which counts the minimum number of single-character edits\u2014insertions, deletions, or substitutions\u2014needed to transform one sequence into the other. To calculate the similarity ratio, you take the Levenshtein Distance, subtract it from the sum of the lengths of both sequences, and then divide by the sum of their lengths. This ratio is a value between 0 and 1, where 1 means the sequences are identical, and 0 means they are completely different.",
        "context": "Levenshtein Similarity Ratio\n\nThe Levenshtein Similarity Ratio is a string metric for measuring the similarity between two sequences. This measure is based on Levenshtein Distance. Informally, the Levenshtein Distance between two strings is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other. The Levenshtein Similarity Ratio can be calculated using Levenshtein Distance value and the total length of both sequences in the following definitions:",
        "chunk_id": "chunk280_3",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/metrics\/eval-metrics.md",
        "root_chunk_id": "chunk280"
    },
    {
        "user_prompt": "Given the steps provided, what is the best practice for contributing changes to a project's main codebase?",
        "output_prompt": "The best practice for contributing changes to a project's main codebase includes ensuring code compiles and runs without errors, writing and updating tests to cover changes, documenting the updates, creating a well-described pull request, and following the project's code review process before merging.",
        "context": "Ensure the code compiles and runs without errors or warnings\nWrite and\/or update tests to cover the changes and make sure all new and existing tests pass\nWrite and\/or update the documentation to match the changes\n\nOnce convinced the criteria above are met, create and submit a new pull request adhering to the pull request template\n\nFollow the code review process to merge the changes to the main codebase\n\nThe following diagram illustrates this approach.\n\n{% raw %}",
        "chunk_id": "chunk8_1",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/pull-requests.md",
        "root_chunk_id": "chunk8"
    },
    {
        "user_prompt": "How can I set up an automated Markdownlint check for my GitHub repository on pull requests?",
        "output_prompt": "To set up an automated Markdownlint check for your GitHub repository on pull requests, you can create a '.github\/workflows\/markdownlint.yml' file in your repository with the following content:\n\n```yaml\nname: Markdown Lint Check\n\non:\n  pull_request:\n    paths:\n      - '**\/*.md'\n\njobs:\n  markdownlint:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions\/checkout@v2\n    - uses: avto-dev\/markdown-lint@v1\n      with:\n        config: '.markdownlint.json'\n```\n\nThis workflow will trigger the Markdown Lint check action whenever a pull request is made that includes changes to markdown files.",
        "context": "Build Validation\n\nLinting\n\nTo automate linting with markdownlint for PR validation in GitHub actions,\nyou can either use linters aggregator as we do with MegaLinter in this repository or use the following YAML.\n\n{% raw %}\n\n```yaml\nname: Markdownlint\n\non:\n  push:\n    paths:\n      - \"\/*.md\"\n  pull_request:\n    paths:\n      - \"\/*.md\"\n\njobs:\n  lint:",
        "chunk_id": "chunk14_5",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/markdown.md",
        "root_chunk_id": "chunk14"
    },
    {
        "user_prompt": "What should I do now that golint is deprecated?",
        "output_prompt": "You should transition to using revive for linting Go code, as it\u2019s been recommended as a suitable replacement for golint. Additionally, to ensure consistent code formatting, continue using gofmt as part of your development workflow.",
        "context": "golint\n\n:exclamation: NOTICE: The golint library is deprecated and archived.\n\nThe linter revive (below) might be a suitable replacement.\n\ngolint can be an effective tool for finding many issues, but it errors on the side of false positives. It is best used by developers when working on code, not as part of an automated build process. This is the default linter which is set up as part of the vscode-go extension.\n\nrevive\n\nRevive is a linter for go, it provides a framework for development of custom rules, and lets you define a strict preset for enhancing your development & code review processes.\n\nAutomatic Code Formatting\n\ngofmt",
        "chunk_id": "chunk15_1",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/go.md",
        "root_chunk_id": "chunk15"
    },
    {
        "user_prompt": "How should I resolve the merge conflict in the text file?",
        "output_prompt": "To resolve the merge conflict, you need to manually choose which lines to keep. Remove the conflict markers (<<<<<<<, =======, >>>>>>>), and make sure the final text includes the lines you want to retain from both sides or a combination thereof. Then save the file, stage the changes, and commit them to complete the merge.",
        "context": "Accept them both and fix the code (probably needed)\n\n{% raw %}\n\n```text\nHere are lines that are either unchanged from the common\nancestor, or cleanly resolved because only one side changed.\n<<<<<<< yours:sample.txt\nConflict resolution is hard;\nlet's go shopping.\n=======\nGit makes conflict resolution easy.\n\ntheirs:sample.txt\nAnd here is another line that is cleanly resolved or unmodified",
        "chunk_id": "chunk206_15",
        "source": "..\/data\/docs\/code-with-engineering\/source-control\/git-guidance\/README.md",
        "root_chunk_id": "chunk206"
    },
    {
        "user_prompt": "How do I apply this Kubernetes role definition in my cluster?",
        "output_prompt": "You can apply this Kubernetes role definition by saving it to a file with a '.yaml' extension and then using the 'kubectl apply -f your-role-file.yaml' command, where 'your-role-file.yaml' is the file containing the role definition.",
        "context": "apiVersion: rbac.authorization.k8s.io\/v1\nkind: Role\nmetadata:\n  namespace: namespace-aa\n  name: cvt-bundle-simulator-role\nrules:\n- apiGroups: [\"metrics.k8s.io\"]\n  resources: [\"pods\"]\n  verbs: [\"get\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"create\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  resourceNames: [\"cvt-simulated-bundle-0-pod\", \"cvt-simulated-bundle-1-pod\", \"cvt-simulated-bundle-2-pod\"]\n  verbs: [\"delete\"]",
        "chunk_id": "chunk378_6",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/computer-vision-on-edge\/cvt-bundle-simulator.md",
        "root_chunk_id": "chunk378"
    },
    {
        "user_prompt": "How do I configure the InvokeRESTAPI task in Azure DevOps once my service connection is ready?",
        "output_prompt": "You can configure the InvokeRESTAPI task by editing the Azure DevOps template yaml. Include the service connection name and the callback information in the body of the request within the task parameters.",
        "context": "Specify the name of the service connection.\n\nSelect Save.\n\nOnce the service connection is ready, it's possible to configure the InvokeRESTAPI task using callback information in the body of the request.\n\nAzure DevOps template yaml utilizing these capabilities is located here.\n\nAbbreviated code snippet is shown below:",
        "chunk_id": "chunk309_4",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/azure-ml-from-azdo.md",
        "root_chunk_id": "chunk309"
    },
    {
        "user_prompt": "How should I integrate code coverage tools in the build stage to ensure optimal test coverage?",
        "output_prompt": "To integrate code coverage tools effectively, configure them to fail the build if coverage falls below a threshold, typically 80%. Ensure the coverage reports are published to your CI system for monitoring variations over time. This practice helps maintain a high standard of code quality and encourages developers to write sufficient tests.",
        "context": "Code Coverage Checks\n\nWe recommend integrating code coverage tools within your build stage. Most coverage tools fail builds when the test coverage falls below a minimum threshold(80% coverage). The coverage report should be published to your CI system to track a time series of variations.\n\nGit Driven Workflow\n\nBuild on commit\n\nEvery commit to the baseline repository should trigger the CI pipeline to create a new build candidate.\n\nBuild artifact(s) are built, packaged, validated and deployed continuously into a non-production environment per commit. Each commit against the repository results into a CI run which checks out the sources onto the integration machine, initiates a build, and notifies the committer of the result of the build.\n\nAvoid commenting out failing tests",
        "chunk_id": "chunk225_19",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-integration\/README.md",
        "root_chunk_id": "chunk225"
    },
    {
        "user_prompt": "Can you provide a Python function to read 'store_data.csv' and search for a given product name?",
        "output_prompt": "Certainly! Here's a Python function that reads 'store_data.csv' using the pandas library and searches for the given product name:\n\n```python\nimport pandas as pd\n\ndef search_product(product_name):\n    # Load the dataset\n    df = pd.read_csv('store_data.csv')\n    # Search for the product\n    result = df[df['Product'] == product_name]\n    return result if not result.empty else 'Product not found'\n```",
        "context": "```\n\n{% endraw %}\n\nThe implementation can now use \u201cstore_data.csv\u201d as follows\n\n{% raw %}\n\n```bash\nfrom getgauge.python import step\nimport os\n@step(\"Search for \")\ndef searchForQuery(query):\n  write(query)\n  press(\"Enter\")\n\nstep(\"Search for \", (query) => {\n  write(query);\n  press(\"Enter\");",
        "chunk_id": "chunk83_4",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/e2e-testing\/recipes\/gauge-framework.md",
        "root_chunk_id": "chunk83"
    },
    {
        "user_prompt": "Can you explain what each part of the 'data' object represents in this JSON?",
        "output_prompt": "Certainly! The 'data' object contains information about an image with OCR (Optical Character Recognition) details. 'SerialNo' is likely a label for the OCR data associated with the image. 'imageId' is a unique identifier for the image processed. 'predictionType' indicates that OCR was used to interpret text in the image. 'boundingBox' provides the coordinates for the area of the image where the text was found. Finally, 'text' is the actual string that was recognized, and 'score' represents the confidence level of the OCR prediction.",
        "context": "json\n{\n    \"status\": \"NOK\",\n    \"score\": 0.8136,\n    \"data\": {\n        \"SerialNo\": {\n            \"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n            \"predictionType\": \"ocr\",\n            \"boundingBox\": \"462,379,497,258\",\n            \"text\": \"12-345-6789\",\n            \"score\": 0.8864",
        "chunk_id": "chunk256_12",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Could you explain how continuous integration automation benefits a software development team?",
        "output_prompt": "Sure! Continuous integration automation streamlines the software development process by quickly identifying errors, accelerating the team's work pace, preventing late-stage complications during release, providing rapid feedback on the impact of changes, distinguishing between build and deployment phases, measuring and communicating build-related metrics, fostering better team communication, and significantly reducing the likelihood of human error. Furthermore, managing build definitions directly within a Git repository ensures that the necessary code and build manifests are version-controlled alongside the project.",
        "context": "Continuous integration automation is an integral part of the software development lifecycle intended to reduce build integration errors and maximize velocity across a dev crew.\n\nA robust build automation pipeline will:\n\nAccelerate team velocity\n\nPrevent integration problems\n\nAvoid last minute chaos during release dates\n\nProvide a quick feedback cycle for system-wide impact of local changes\n\nSeparate build and deployment stages\n\nMeasure and report metrics around build failures \/ success(s)\n\nIncrease visibility across the team enabling tighter communication\n\nReduce human errors, which is probably the most important part of automating the builds\n\nBuild Definition Managed in Git\n\nCode \/ manifest artifacts required to build your project should be maintained in within your project(s) git repository(s)",
        "chunk_id": "chunk225_1",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-integration\/README.md",
        "root_chunk_id": "chunk225"
    },
    {
        "user_prompt": "How can Agile development teams manage the stability of API definitions across multiple versions and concurrent refactoring?",
        "output_prompt": "To manage the stability of API definitions in Agile development, teams should regularly generate and store Open API definitions in version control systems. During refactoring or when working with multiple concurrent API versions, it's essential to align with stakeholders on what constitutes a breaking change. These practices help ensure continuity and provide a shared understanding of the API's evolution, minimizing the impact of changes and maintaining consistency.",
        "context": "With Agile development, it is hard to ensure that definitions embedded in runtime code remain stable, especially across rounds of refactoring and when serving multiple concurrent API versions.\n\nIt might be useful to regularly generate Open API definition and store it in version control system otherwise generating the OpenAPI definition at runtime might makes it more complex in scenarios where that definition is required at development\/CI time.\n\nHow to Interpret and Apply The Guidelines\n\nThe API guidelines document includes a section on how to apply the guidelines depending on whether the API is new or existing.\nIn particular, when working in an existing API ecosystem, be sure to align with stakeholders on a definition of what constitutes a breaking change to understand the impact of implementing certain best practices.\n\nWe do not recommend making a breaking change to a service that predates these guidelines simply for the sake of compliance.",
        "chunk_id": "chunk29_8",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-patterns\/rest-api-design-guidance.md",
        "root_chunk_id": "chunk29"
    },
    {
        "user_prompt": "What command should I use to execute the ado-pipeline-trigger.py with the correct parameters in an Azure ML environment?",
        "output_prompt": "To execute the ado-pipeline-trigger.py script with the correct parameters in an Azure ML environment, use the command below:\n\npython ado-pipeline-trigger.py --modelpath ${{inputs.modelpath}} --modelname ${{inputs.modelname}} --kvname ${{inputs.kvname}} --secretname ${{inputs.secretname}} --org ${{inputs.org}} --project ${{inputs.project}} --branch ${{inputs.branch}} --apiversion ${{inputs.apiversion}} --pipelineid ${{inputs.pipelineid}} --pipelineversion ${{inputs.pipelineversion}}",
        "context": "code: ..\/..\/..\/src\/pipeline_trigger\/\nenvironment: azureml:sklearn-jsonline-keyvault-env@latest\ncommand: >-\npython ado-pipeline-trigger.py\n--modelpath ${{inputs.modelpath}}\n--modelname ${{inputs.modelname}}\n--kvname ${{inputs.kvname}}\n--secretname ${{inputs.secretname}}\n--org ${{inputs.org}}\n--project ${{inputs.project}}\n--branch ${{inputs.branch}}\n--apiversion ${{inputs.apiversion}}\n--pipelineid ${{inputs.pipelineid}}\n--pipelineversion ${{inputs.pipelineversion}}",
        "chunk_id": "chunk303_21",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/mlops\/working-with-azure-ml\/sync-async-mlops-patterns.md",
        "root_chunk_id": "chunk303"
    },
    {
        "user_prompt": "Can you explain the benefits of using Docker Compose when working with Dev Containers, especially in relation to setting environment variables?",
        "output_prompt": "Certainly! Docker Compose offers several advantages for setting up Dev Containers. It not only allows you to define environment variables through an 'env_file', but also gives you the flexibility to configure additional settings like custom DNS, port forwarding, and the management of multiple containers within one YAML file. This makes your development environment easily reproducible and modular. Even though it is a bit more complex compared to using environment variables directly, the scalability and control it provides can be essential for more intricate development setups.",
        "context": "{% endraw %}\n\nAs soon as the Dev Container is started, these environment variables are sent to the container.\n\nAnother approach would be to use Docker Compose, a little bit more complex, and probably too much for just environment variables. Using Docker Compose can unlock other settings such as custom dns, ports forwarding or multiple containers.\n\nTo achieve this, you need to add a file .devcontainer\/docker-compose.yml with the following:\n\n{% raw %}\n\nyaml\nversion: '3'\nservices:\n  my-workspace:\n    env_file: ..\/config\/.env\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: sleep infinity",
        "chunk_id": "chunk246_5",
        "source": "..\/data\/docs\/code-with-engineering\/developer-experience\/going-further.md",
        "root_chunk_id": "chunk246"
    },
    {
        "user_prompt": "Could you explain what a markdown linter is and why it's useful?",
        "output_prompt": "Certainly! A markdown linter is a tool that analyzes Markdown files to check for syntax errors and inconsistencies. It enforces a set of rules to ensure the Markdown is written in a clean and consistent manner, which makes the text more readable and maintainable. It is useful because it helps in identifying formatting issues early in the editing process, leading to higher quality documentation with a standardized appearance. Additionally, integrating a markdown linter into the build process ensures that documents meet the project's quality standards before being published or merged into the codebase.",
        "context": "A good setup includes a markdown linter used during editing and PR build verification, and a grammar linter used while editing the document. The following are a list of linters that could be used in this setup.\n\nmarkdownlint\n\nmarkdownlint is a linter for markdown that verifies Markdown syntax, and also enforces rules that make the text more readable. Markdownlint-cli is an easy-to-use CLI based on Markdownlint.\n\nIt's available as a ruby gem, an npm package, a Node.js CLI and a VS Code extension. The VS Code extension Prettier also catches all markdownlint errors.\n\nInstalling the Node.js CLI\n\n{% raw %}\n\nbash\nnpm install -g markdownlint-cli",
        "chunk_id": "chunk14_2",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/markdown.md",
        "root_chunk_id": "chunk14"
    },
    {
        "user_prompt": "What are common challenges related to data discovery for AI projects as discussed in the article by Shane Peckham?",
        "output_prompt": "One common challenge related to data discovery for AI projects is that traditionally, finding enterprise data sources has been an organic process based on communal knowledge, which presents many difficulties for companies aiming to maximize the value from their information assets.",
        "context": "author: shanepeckham\ntitle: Data discovery: Understanding data for AI and beyond\nms.author: shanepec\nms.service: playbook\ndescription: The data discovery process can be enhanced by using ML. By using ML techniques, data discovery can become smart, can discover relationships between data and accelerate an organization's understanding of their data.\nrings:\n   - public\n\nData discovery: Understanding data for AI and beyond\n\nThis article discusses common data discovery challenges related to finding data for use in AI projects.\n\nDiscovery challenges for data consumers\n\nTraditionally, discovering enterprise data sources has been an organic process based on communal knowledge. For companies that want the most value from their information assets, this approach presents many challenges:",
        "chunk_id": "chunk251_0",
        "source": "..\/data\/docs\/code-with-mlops\/data-discovery.md",
        "root_chunk_id": "chunk251"
    },
    {
        "user_prompt": "What is considered a good target number for % Processor time to indicate healthy CPU utilization?",
        "output_prompt": "70% is generally considered a good target number for % Processor time to indicate healthy CPU utilization.",
        "context": "Counter Description % Processor time This is the percentage of total elapsed time that the processor was busy executing. This counter can either be too high or too low. If your processor time is consistently below 40%, then there is a question as to whether you have over provisioned your CPU. 70% is generally considered a good target number and if you start going higher than 70%, you may want to explore why there is high CPU pressure. % Privileged (Kernel Mode) time This measures the percentage of elapsed time the processor spent executing in kernel mode. Since this counter takes into account only kernel operations a high percentage of privileged time (greater than 25%) may indicate driver or hardware issue that should be investigated. % User time The percentage of elapsed time the processor spent executing in user mode (your application code). A good guideline is to be consistently below 65% as you want to have some buffer",
        "chunk_id": "chunk69_12",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/performance-testing\/README.md",
        "root_chunk_id": "chunk69"
    },
    {
        "user_prompt": "What shutter speed would you suggest for capturing a fast-moving object without blur?",
        "output_prompt": "A faster shutter speed is recommended for capturing fast-moving objects without blur, so I would suggest using the 1\/500 setting.",
        "context": "\"min\": \"1\/500\",\n          \"suggested\": \"1\/250\"\n        }\n      }\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"quality\",\n      \"type\": \"classification\"\n    },\n    {\n      \"name\": \"defect1\",\n      \"type\": \"detection\"\n    },\n    {\n      \"name\": \"partNumber\",\n      \"type\": \"ocr\"\n    }\n  ],",
        "chunk_id": "chunk256_31",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Can you summarize the key points of the GenAI Gateway document and highlight the importance of Stages 5 and 6?",
        "output_prompt": "The GenAI Gateway document provides guidance for designing and implementing a gateway that optimizes the use of Azure OpenAI resources. It ensures efficient workload management, integration, and distribution in a distributed application. The document emphasizes the importance of Stage 5 (Inference) for making predictions based on the model and Stage 6 (Monitor) for continuous monitoring, which includes performance tracking and billing. Additionally, it includes security guidance for protecting against LLM-specific threats, which is crucial throughout the process but especially at Stages 1 (Data Curation), 5 (Inference), and 6 (Monitor).",
        "context": "and manually in variety of languages within a distributed application, as well as provide similar examples for visualization and alerts based on this incoming data. Stages 5 (Inference), and 6 (Monitor) Building GenAI Gateway.docx This document serves as an essential guide for engineering teams tasked with designing and implementing a gateway solution involving Azure OpenAI resources. It aims to equip teams with the essential guidance and reference designs required to build a Generative AI (GenAI) gateway that can efficiently handle and optimize GenAI resources utilization, facilitate seamless integration and distribution of workloads across various deployments, as well as enable a fine-grained monitoring and billing. Stages 5 (Inference), and 6 (Monitor) LLM Security Guidance Security guidance and an intro to high level LLM specific threats Stages 1 (Data Curation), 5 (Inference), and 6 (Monitor) Threat Assessment",
        "chunk_id": "chunk276_5",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/mlops-in-openai\/tooling-resources.md",
        "root_chunk_id": "chunk276"
    },
    {
        "user_prompt": "I'm trying to resize an image to a specific width and height, but I'm not sure if my function is working properly. How can I check if the image has been resized to the correct dimensions in Python?",
        "output_prompt": "You can verify that the image has been resized correctly by checking the shape attribute of the numpy array that represents your image. The shape will give you a tuple of the dimensions, where shape[:2] will give you the height and width. To assert the expected size, you would use the following code: `assert resized_image.shape[:2] == (expected_height, expected_width)`. Just make sure that the `resized_image` is the numpy array of the resized image, and `expected_height` and `expected_width` are the dimensions you resized the image to.",
        "context": "# act\n  resized_image = utils.resize_image(original_image, expected_height, expected_width)\n\n# assert\n  resized_image.shape[:2] = (expected_height, expected_width)",
        "chunk_id": "chunk214_14",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-testing.md",
        "root_chunk_id": "chunk214"
    },
    {
        "user_prompt": "Could you provide an example of how to evaluate solution hypotheses for achieving defined outcomes?",
        "output_prompt": "Certainly! To evaluate solution hypotheses, first list out 2 to 3 of your top potential solutions. For each, give a short name that is easily recognizable. Describe how you expect each solution will lead to the desired outcomes. You can use diagrams if it helps to explain your solutions more clearly. Let's say you have 'Solution 1: Automated Workflow' and 'Solution 2: Manual Process Improvement'. Compare these by discussing their feasibility, impact, and how they match the defined outcomes. If there are more than 3 candidates, explain why you are prioritizing these and note any eliminated solutions and the reasons for their elimination. This structured approach will help you perform a concise and effective trade study.",
        "context": "Solution Hypotheses\n\nEnumerate the solutions that are believed to deliver the outcomes defined above.\n\nNOTE: Limiting the evaluated solutions to 2 or 3 potential candidates can help manage the time spent on the evaluation. If there are more than 3 candidates, prioritize what the team feels are the top 3. If appropriate, the eliminated candidates can be mentioned to capture why they were eliminated. Additionally, there should be at least two options compared, otherwise you didn't need a trade study.\n\n{Solution 1} - Short and easily recognizable name\n\nAdd a brief description of the solution and how its expected to produce the desired outcomes. If appropriate, illustrations\/diagrams can be used to reduce the amount of text explanation required to describe the solution.",
        "chunk_id": "chunk47_7",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/trade-studies\/template.md",
        "root_chunk_id": "chunk47"
    },
    {
        "user_prompt": "What steps should be followed when creating new PBIs or modifying existing ones?",
        "output_prompt": "When creating new Product Backlog Items (PBIs) or modifying existing ones, first identify the requirements and objectives. Break down the work into manageable tasks, estimate the effort, prioritize the tasks, and document the changes. For modifications, review the existing PBI, assess the impact of changes, update the backlog accordingly, and communicate the changes to the team. Follow-up spikes may be needed to explore areas of uncertainty, and specifications for public interfaces and integrations between work streams should be detailed in a clear and concise manner. Lastly, log all decisions for future reference in a decision log entry.",
        "context": "Creating new PBI's or modifying existing ones\n\nFollow up spikes\n\nCreating specification for public interfaces and integrations between other work streams.\n\nDecision Log Entry",
        "chunk_id": "chunk47_12",
        "source": "..\/data\/docs\/code-with-engineering\/design\/design-reviews\/trade-studies\/template.md",
        "root_chunk_id": "chunk47"
    },
    {
        "user_prompt": "Could you elaborate on the key capabilities required for building a real MLOps solution?",
        "output_prompt": "Certainly, the key capabilities for building a real MLOps solution include version control for data and models, continuous integration and deployment (CI\/CD) pipelines for machine learning, automated testing and validation of models, efficient data storage and management, real-time monitoring of models in production, and governance and compliance adherence.",
        "context": "```\n\nSolution building blocks\n\nThis solution is based on building real MLOps solutions and incorporates capabilities.",
        "chunk_id": "chunk395_9",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/custom-machine-learning\/automating-model-training\/index.md",
        "root_chunk_id": "chunk395"
    },
    {
        "user_prompt": "Can you explain what Probes are in the context of synthetic monitoring tests?",
        "output_prompt": "In synthetic monitoring, Probes are the test entities, such as software agents or scripts, that simulate user interactions with a system. They generate synthetic transactions or data that mimic real user behavior to test the performance and functionality of applications, for the purpose of validating the system's behavior under test and ensuring the monitoring tools are effectively tracking the system's health.",
        "context": "Synthetic Monitoring tests Design Blocks\n\nA synthetic monitoring test is a test that uses synthetic data and real testing accounts to inject user behaviors to the system and validates their effect, usually by passively relying on existing monitoring and alerting capabilities.\nComponents of synthetic monitoring tests include Probes, test code\/ accounts which generates data, and Monitoring tools placed to validate both the system's behavior under test and the health of the probes themselves.\n\nProbes",
        "chunk_id": "chunk75_4",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/synthetic-monitoring-tests\/README.md",
        "root_chunk_id": "chunk75"
    },
    {
        "user_prompt": "What are some best practices for building applications with testing in mind?",
        "output_prompt": "When building applications, ensure to design for testability by incorporating principles like modularity and loose coupling. Implement various levels of testing such as unit, integration, and system testing. Make use of Test-Driven Development (TDD) and Behavior-Driven Development (BDD) approaches. Automate where possible, use Continuous Integration (CI) and Continuous Deployment (CD), and include testing tools and frameworks early in the development cycle. Also, consider the use of service virtualization and mocks to simulate systems that are not available or developed yet for testing purposes.",
        "context": "Sections within Testing\n\nConsumer-driven contract (CDC) testing\n\nEnd-to-End testing\n\nFault Injection testing\n\nIntegration testing\n\nPerformance testing\n\nShadow testing\n\nSmoke testing\n\nSynthetic Transaction testing\n\nUI testing\n\nUnit testing\n\nTechnology Specific Testing\n\nUsing DevTest Pattern for building containers with AzDO\n\nUsing Azurite to run blob storage tests in pipeline\n\nBuild for Testing\n\nTesting is a critical part of the development process.  It is important to build your application with testing in mind.  Here are some tips to help you build for testing:",
        "chunk_id": "chunk67_8",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/README.md",
        "root_chunk_id": "chunk67"
    },
    {
        "user_prompt": "How should I use markdownlint and write-good to improve my project's documentation?",
        "output_prompt": "To improve your project's documentation, you can use markdownlint by installing it globally with 'npm install -g markdownlint-cli' and running it in your project directory with 'markdownlint **\/*.md --ignore node_modules'. To fix errors automatically, add '--fix' to the command. For improving the English text in your documentation, install write-good with 'npm install -g write-good' and run it on your text files.",
        "context": "{% raw %}\n\nbash\nnpm install -g markdownlint-cli\n\n{% endraw %}\n\nRunning markdownlint on a Node.js project\n\n{% raw %}\n\nbash\nmarkdownlint **\/*.md --ignore node_modules\n\n{% endraw %}\n\nFixing errors automatically\n\n{% raw %}\n\nbash\nmarkdownlint **\/*.md --ignore node_modules --fix\n\n{% endraw %}\n\nA comprehensive list of markdownlint rules is available here.\n\nwrite-good\n\nwrite-good is a linter for English text that helps writing better documentation.\n\n{% raw %}\n\nbash\nnpm install -g write-good\n\n{% endraw %}",
        "chunk_id": "chunk14_3",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/markdown.md",
        "root_chunk_id": "chunk14"
    },
    {
        "user_prompt": "What is the purpose of the 'script' steps in this Azure DevOps YAML pipeline?",
        "output_prompt": "The 'script' steps in the Azure DevOps YAML pipeline are used to execute Go's built-in code formatting tool with 'go fmt', and to run 'go vet' which examines the source code for any suspicious constructs that could be potential errors.",
        "context": "Sample Build Validation Pipeline in Azure DevOps\n\n{% raw %}\n\n```yaml\ntrigger: master\n\npool:\n   vmImage: 'ubuntu-latest'\n\nsteps:\n\ntask: GoTool@0\n  inputs:\n    version: '1.13.5'\n\ntask: Go@0\n  inputs:\n    command: 'get'\n    arguments: '-d'\n    workingDirectory: '$(System.DefaultWorkingDirectory)\/scripts'\n\nscript: go fmt\n  workingDirectory: $(System.DefaultWorkingDirectory)\/scripts\n  displayName: \"Run code formatting\"\n\nscript: go vet\n  workingDirectory: $(System.DefaultWorkingDirectory)\/scripts\n  displayName: 'Run go vet'",
        "chunk_id": "chunk15_8",
        "source": "..\/data\/docs\/code-with-engineering\/code-reviews\/recipes\/go.md",
        "root_chunk_id": "chunk15"
    },
    {
        "user_prompt": "What is the purpose of chaos engineering in a development environment?",
        "output_prompt": "The purpose of chaos engineering in a development environment is to discover points where a system is not resilient to unpredictable yet inevitable failures such as network outages, hardware failures, and other extreme scenarios. It helps in ensuring the system is robust and can handle unexpected disruptions.",
        "context": "per million concurrent users) Load (stress) Development; Operation Discover points where a system is not resilient to unpredictable yet inevitable failures (network outage, hardware failure, VM host servicing, rack\/switch failures, random acts of the Malevolent Divine, solar flares, sharks that eat undersea cable relays, cosmic radiation, power outages, renegade backhoe operators, wolves chewing on junction boxes, \u2026) Chaos Development Perform unit testing on Power platform custom connectors Custom Connector Testing",
        "chunk_id": "chunk67_7",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/README.md",
        "root_chunk_id": "chunk67"
    },
    {
        "user_prompt": "How can I use Feathr's SDK to access lineage information for features within Purview?",
        "output_prompt": "You can access lineage information for features in Purview using Feathr's SDK by first ensuring you have the necessary permissions and configurations set up in Purview. Then, using the SDK, you can query for the feature of interest and retrieve its lineage data, which will provide you with a detailed account of the processes and datasets involved in the creation of that feature.",
        "context": "Feature Views (feature groups), Entities, and on Features. - Note: Can query the FeaSt registered concepts of the above. For example if I have a feature set where the driver's ID is my entity, FeaSt will tell me that the driver_id column is the entity. It will not return the values of the entities that are needed to query the data The feature definition metadata is available in the Feature Store UI. - Versions produced for feature. - Notebook and schedule for which feature data is produced. - Raw data used. - Online stores that a feature is linked to The feature definition metadata is persisted in Purview and accessible using Feathr\u2019s SDK or Purview\u2019s directly. One advanced feature that Feathr provides by using Purview is the ability to get lineage information for features. The Purview lineage shows what is involved in",
        "chunk_id": "chunk327_13",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/model-development\/feature-management\/feature-store-comparison.md",
        "root_chunk_id": "chunk327"
    },
    {
        "user_prompt": "Please provide the specifications for the second image in the set.",
        "output_prompt": "The specifications for 'image2' are a resolution of 2448x2048 pixels, a 'BayerBG8' format, with a bit depth of 8 bits. Additionally, the 'color' property is set to 'rgb'.",
        "context": "(data)\n--boundary\nContent-Disposition: form-data; name=\"input_description\"\n{\n  \"properties\": [\n    {\n      \"inputName\": \"image1\",\n      \"resolutionX\": 2448,\n      \"resolutionY\": 2048,\n      \"format\": \"BayerBG8\",\n      \"bitDepth\": 8,\n      \"additionalProperties\": {\n        \"color\": \"rgb\"\n      }\n    },\n    {\n      \"inputName\": \"image2\",\n      \"resolutionX\": 2448,",
        "chunk_id": "chunk256_5",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/ai-on-the-edge\/custom-vision-edge-bundle\/bundle-interface.md",
        "root_chunk_id": "chunk256"
    },
    {
        "user_prompt": "Can you explain the potential causes of latency in Azure's OpenAI service?",
        "output_prompt": "Certainly! Latency in Azure's OpenAI service can be attributed to various factors such as network issues, server overload, inefficient code execution, or data transfer delays between interconnected services. It's important to analyze the specific circumstances and metrics to pinpoint the exact cause.",
        "context": "rings:\n  - internal\n\nAzure OpenAI Latency - A Case Study",
        "chunk_id": "chunk288_0",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/latency\/openai-latency-a-case-study.md",
        "root_chunk_id": "chunk288"
    },
    {
        "user_prompt": "Can you provide a summary of our team's software development practices, including our approach to clean code, programming techniques, release process, code review standards, backlog refinement, TDD, test coverage expectations, testing dimensions, build process, and bug fix rules?",
        "output_prompt": "Our team adheres to clean code principles to maintain readability and reduce complexity. We incorporate pair and mob programming as needed, with styles adapted to team preferences. Our release process includes strict quality gates and a thorough review process. For code reviews, we require a minimum of two reviewers and adhere to predefined team rules. Backlog refinement is completed regularly to ensure a clear Definition of Done and Acceptance Criteria. We follow TDD practices to enhance software design and test coverage, although we do not enforce a strict percentage, instead focusing on meaningful coverage. Testing encompasses unit, integration, functional, performance, regression, and acceptance tests. Our build process mandates an all-inclusive approach, ensuring that our code works across designated operating systems and specified DevOps tools. Lastly, our bug fix rules are designed to prioritize critical issues and incorporate fixes into our development workflow efficiently.",
        "context": "Clean Code Does team follow clean code principles ? Pair\/Mob Programming Will team apply pair\/mob programming ? If yes, what programming styles are suitable for the team ? Release Process Principles around release process such as quality gates, reviewing process ...etc. Code Review Any rule for code reviewing such as min number of reviewers, team rules ...etc. Action Readiness How the backlog will be refined? How do we ensure clear Definition of Done and Acceptance Criteria ? TDD Will the team follow TDD ? Test Coverage Is there any expected number, percentage or measurement ? Dimensions in Testing Required tests for high quality software, eg : unit, integration, functional, performance, regression, acceptance Build process build for all? or not; The clear statement of where code and under what conditions code should work ? eg : OS, DevOps, tool dependency Bug fix The rules of bug fixing in the team ?",
        "chunk_id": "chunk165_5",
        "source": "..\/data\/docs\/code-with-engineering\/agile-development\/advanced-topics\/team-agreements\/team-manifesto.md",
        "root_chunk_id": "chunk165"
    },
    {
        "user_prompt": "Why should we use Azure Data Lake Gen2 over traditional blob storage for our CVT data picture store?",
        "output_prompt": "Azure Data Lake Gen2 is preferred for the CVT data picture store due to its optimized storage for big data, storage retrieval performance, and lower cost for analysis. Additionally, it offers fine-grained access control lists (ACLs) based on Azure Active Directory that can be set at the file and folder level, providing better security and data management capabilities.",
        "context": "Design\n\nPicture Store\n\nWe propose to use Azure Data Lake Gen2 as the picture store for the CVT data. You can find a lot of other good reasons in the Data Ops documentation.\n  The main reasons for this are the Optimized storage for big data, ACL based on\n  Active Directory and low analysis cost\n\nBlob Storage ADLS Gen2 Flat namespace object store Hierarchical namespaces General purpose object store for a wide variety of storage scenarios, including big data analytics Optimized storage for big data analytics workloads. Good storage retrieval performance Better storage retrieval performance High cost for Analysis Low cost for Analysis Access Control Lists (ACLs) based on Azure Active Directory Identities can be set at the file and folder level Account, Container or Blob level authorization using shared access tokens",
        "chunk_id": "chunk372_1",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/computer-vision-on-edge\/cvt-picture-store.md",
        "root_chunk_id": "chunk372"
    },
    {
        "user_prompt": "Please analyze the following code snippet for readability, efficiency, and best programming practices, and then assign a score of 1 to 5 based on these criteria: \n\nfunction add(a, b) { return a + b; }\n\nconsole.log('Sum of 5 and 3 is', add(5, 3));",
        "output_prompt": "--ANSWER\nEXPLANATION: The provided code is concise, straightforward, and correctly implements a simple addition function, which is then used to print out the sum of two numbers. It is readable as the function name and variable names are descriptive. Efficiency is not a concern here since the operation is elementary. However, the code lacks error handling, which might be important for robustness in a larger system. Given the simplicity of the task, the error handling is not necessarily a requirement here. It follows basic programming practices, like naming conventions. A documentation comment explaining the function behavior could be a minor improvement.\nSCORE: 4\nThe point deducted is for the lack of error checking and comments, but considering the task's simplicity, this code ranks high on the scale.",
        "context": "and improper use of functions.\n5- Evaluate the GENERATED code based on other criteria such as readability, efficiency, and adherence to best programming practices.\n6- Use the results of steps 2-5 to assign a score to the GENERATED code between 1 to 5, with a higher score indicating better quality.\n   The score can be based on a weighted combination of the different criteria.\n7- Come up an explanation for the score assigned to the GENERATED code. This should also mention if the code is valid or not\nWhen the above is done, please generate an ANSWER that includes outputs:\n--ANSWER\nEXPLANATION:\nSCORE:\nBelow are two example:",
        "chunk_id": "chunk280_24",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/metrics\/eval-metrics.md",
        "root_chunk_id": "chunk280"
    },
    {
        "user_prompt": "How do I convert my Jupyter notebook into a single-step pipeline for execution on a single instance?",
        "output_prompt": "To convert your Jupyter notebook into a single-step pipeline for execution on a single instance, you will need to extract your code from the notebook and save it as a train.py file. Then you can set up a pipeline that executes this script as a single step, defining all necessary datasets, datastores, and parameters. Afterwards, you can run multiple experiments using different parameters to see how the pipeline behaves. Make sure to package all dependencies and environment requirements to ensure reproducibility and correct execution in the pipeline environment.",
        "context": "Step 2. Convert the existing notebook to a single step pipeline. You can use the following guideline to create a single-step pipeline to execute on a single instance only. The only difference is it is your own code in train.py. At this stage, you will see how the pipeline works and be able to define all datasets\/datastores and parameters as needed. Starting from this stage you are able to run several experiments of the pipeline using different parameters.\n\nStep 3. Identify critical blocks in the pipeline and move them to different steps. The primary reason for this step is to make your pipeline faster. You need to identify your clusters (GPU or non-GPUs and VM sizes) and the kinds of steps to use to wrap components from your code (single, parallel or distributed TF\/PyTorch etc.).",
        "chunk_id": "chunk326_9",
        "source": "..\/data\/docs\/code-with-mlops\/capabilities\/model-development\/ml-pipelines\/index.md",
        "root_chunk_id": "chunk326"
    },
    {
        "user_prompt": "What were the key factors that influenced the latency of the chatbot according to the experiments conducted?",
        "output_prompt": "The key factors that influenced the latency were the number of tokens returned, the size of the model, whether a formatted response was required, and the complexity of the prompt.",
        "context": "Conclusion\n\nThe team conducted several experiments to improve the latency of the chatbot, including establishing a baseline for latency, chat history summarization, changing the location of the GPT4 service, and forcing the use of GPT3.5. The results showed that the number of tokens returned, the size of the model, whether a formatted response was expected, and the complexity of the prompt had a significant impact on performance. On the other hand, factors such as the number of tokens sent to completion, the physical distance between the server process and the Azure OpenAI service, and the use of functions in GPT3.5 and GPT4 had little effect on performance.",
        "chunk_id": "chunk288_18",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/latency\/openai-latency-a-case-study.md",
        "root_chunk_id": "chunk288"
    },
    {
        "user_prompt": "Can you explain a secure approach for a Simulator Orchestrator application to manage ACI instances, taking into account the Azure Container Instances approach mentioned?",
        "output_prompt": "Certainly! To ensure a secure approach for the Simulator Orchestrator to manage ACI instances, you should assign a Service Principal or a Managed Identity to the Orchestrator with a Custom Azure AD Role specifically created for the required permissions. This role should include permissions limited to creating and deleting Azure Container Instances within the designated Stamp\/Slot resource group in Azure. Furthermore, for ACI, make sure to assign Managed Identity to the Simulated Bundle Application, allowing it to securely interact with other Azure services, such as ServiceBus and databases. However, keep in mind that Managed Identity for ACI is currently in preview and should be used in production environments only after thorough testing and once it's generally available.",
        "context": "Azure Container Instances approach\n\nSimulator Orchestrator is an application running on AKS cluster and this application is able to connect to Azure.\nSimulation Environment is Azure (Azure Resource Manager, Slot Resource Group, Azure Monitor).\nSimulated Bundle Application is a multi-container Azure Container Instance.\n\nACI Security\n\nSimulator Orchestrator should have Service Principal \/ Managed Identity assigned with permissions to only create\/delete Azure Container Instances in cloud platform Stamp\/Slot resource group in Azure. This requires a Custom Azure AD Role with specific Microsoft.ContainerInstance\/* permissions.\n\nSimulated Bundle Application should have Managed Identity assigned to be able to connect to ServiceBus\/Database\/Simulator Orchestrator in a secure way to save results of testing.\nManaged Identity for ACI is in Preview\n\nACI Deploy",
        "chunk_id": "chunk378_14",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/serving-models-on-edge\/computer-vision-on-edge\/cvt-bundle-simulator.md",
        "root_chunk_id": "chunk378"
    },
    {
        "user_prompt": "How can AI improve video search and analysis using Azure Video Indexer?",
        "output_prompt": "AI can improve video search and analysis using Azure Video Indexer by automatically extracting insights from videos. It uses machine learning to identify visual and audio elements, such as spoken words, written text, faces, and scenes. These insights can enhance search indices for better discovery. The Video Indexer integrates with Azure Cognitive Search for an end-to-end knowledge mining experience, and user feedback can be utilized to continuously refine search accuracy. Furthermore, Power Skills functions can be integrated as part of Azure Cognitive Search to add custom capabilities, and usage statistics can be monitored via a dashboard for ongoing evaluation and improvement of the search performances.",
        "context": "Improving Video Search using AI - A full solution that automates the analysis of videos with Azure Video Indexer, creates and improves the search index with Video Indexer insights and optional metadata. Provides a web UI application to search and see results and logs the behavior and feedback of users (in order for the admin or data science team to improve the search index). Lastly it also provides a dashboard which displays statistics on platform usage and search performances.\n\nAzure Cognitive Search Powerskills - Power Skills are a collection of useful functions to be deployed as PowerSkills for Azure Cognitive Search.\n\nEnd to end Knowledge Mining for Video- A video discovery pipeline that includes Azure Search and user feedback.\n\nEvaluating Azure Search - A deployable feedback and evaluation solution for Azure Cognitive Search.",
        "chunk_id": "chunk381_3",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/search\/data-enrichment-for-azure-cognitive-search\/index.md",
        "root_chunk_id": "chunk381"
    },
    {
        "user_prompt": "How does the system ensure the generated text adheres to the language check and keyword presence metrics?",
        "output_prompt": "The system ensures the generated text meets the language check metric by running it through a series of linguistic rules that analyze for grammatical correctness, appropriate word choice, and overall understandability. For ensuring keyword presence, the text is checked against a list of expected keywords or phrases to certify that the output remains relevant to the user's initial request. In addition to these automated checks, human feedback is incorporated to continuously improve the quality and accuracy of the system's performance.",
        "context": "Language check: A language check metric evaluates whether the generated text or code is written understandably and consistent with the user's input. This check can be evaluated using a set of rules that check for common language issues, such as incorrect word choice or grammar.\n\nKeyword presence: This metric measures whether the generated text includes the keywords or key phrases that were used in the natural language input. It can be evaluated using a set of rules. These rules check for the presence of specific keywords or key phrases that are relevant to the task being performed.\n\nHuman feedback",
        "chunk_id": "chunk280_17",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/metrics\/eval-metrics.md",
        "root_chunk_id": "chunk280"
    },
    {
        "user_prompt": "What are the steps to create a refined human evaluation process for LLM-generated output?",
        "output_prompt": "To create a refined human evaluation process for LLM-generated output, one should identify a group of domain experts, generate a shareable asset such as a shared excel spreadsheet for reviewers' input, and ensure multiple experts can view and provide feedback on the generated output.",
        "context": "Human feedback\n\nHuman-based feedback is always going to be a good (if not the gold standard) option for evaluating the quality of LLM-generate output and is particularly relevant if there is no ground truth dataset. Depending on the application, reviewers with domain expertise may be required. This process can be as simple as having a human verify output and assign it a pass\/fail mark. However, for a more refined human evaluation process the following steps can be taken:\n\nIdentifying a group of domain experts or people familiar with the problem space.\n\nGenerating a shareable asset that can be viewed by multiple reviewers. For example, a shared excel spreadsheet can be saved for later use in experiments.",
        "chunk_id": "chunk280_18",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/generative-ai\/working-with-llms\/metrics\/eval-metrics.md",
        "root_chunk_id": "chunk280"
    },
    {
        "user_prompt": "How can AI-based object detection and tracking be applied in retail environments for optimizing operations?",
        "output_prompt": "AI-based object detection and tracking can be utilized in retail environments to analyze customer traffic patterns, manage inventory by tracking the movement of goods, optimize store layouts for better flow, ensure the safety through surveillance, and enhance the overall shopping experience by reducing wait times at checkouts.",
        "context": "One of the most common scenarios for AI-enabled object detection and tracking is people counting in real-time. This application is useful in a variety of settings, including retail stores, airports, and public spaces. By detecting and tracking individuals as they move through a space, businesses and organizations can gain valuable insights into foot traffic patterns and make informed decisions about staffing, product placement, and more. In addition to people counting, object detection and tracking can also be used to track vehicles, machinery, materials, and other objects on a shop floor or in a production environment.",
        "chunk_id": "chunk380_1",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/ai-on-the-edge\/real-time-object-detection-and-tracking-on-edge\/index.md",
        "root_chunk_id": "chunk380"
    },
    {
        "user_prompt": "What network performance counters can I check to identify bottlenecks?",
        "output_prompt": "To identify network bottlenecks, you can check counters like bandwidth utilization, packet loss, latency, and error rates.",
        "context": "More advanced approaches often involve looking at some of the networking\nperformance counters, as explained below.\n\nNetwork Counters\n\nThe table above gives you some reference points to better understand what you\ncan expect out of your network. Here are some counters that can help you\nunderstand where the bottlenecks might exist:",
        "chunk_id": "chunk69_16",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/performance-testing\/README.md",
        "root_chunk_id": "chunk69"
    },
    {
        "user_prompt": "Can you explain how Databricks integrates with Azure ML pipelines and the advantages of using Databricks for MLOps?",
        "output_prompt": "Databricks can be integrated into Azure ML pipelines through Databricks Step Configuration, allowing seamless execution of Databricks notebooks, jobs, and libraries as steps in an Azure ML pipeline. The advantage of using Databricks for MLOps includes its native support for machine learning lifecycle management through MLFlow, collaborative Workflows, and Repos, which enables robust MLOps practices such as experiment tracking, model deployment, and monitoring.",
        "context": "Working with Azure ML is an overview of the Azure ML service, which is crucial to several implementations.\n\nDatabricks vs Azure ML breaks down the key features and differences between the Azure Machine Learning service and Databricks.\n\nFor customers who are using Databricks:\n\nMLOps in Databricks using Workflows and Repos uses Databricks features like Workflows and Repos to build an end-to-end MLOps process.\n\nMLOps in Databricks using MLFlow Projects applies MLOps best practices in Databricks using MLFlow Projects and job clusters.\n\nDatabricks Step Configuration integrates a Databricks workload into an Azure ML pipeline.",
        "chunk_id": "chunk395_17",
        "source": "..\/data\/docs\/code-with-mlops\/solutions\/custom-machine-learning\/automating-model-training\/index.md",
        "root_chunk_id": "chunk395"
    },
    {
        "user_prompt": "Can integration tests serve a dual purpose in both ensuring module connectivity and verifying system behavior?",
        "output_prompt": "Yes, integration tests can be designed to not only ensure that different modules connect as expected but also to verify the correct behavior of the system. This can be done by creating tests that cover entire scenarios, like checking HTTP responses in an API, thereby potentially reducing the need for multiple unit tests.",
        "context": "Usually, integration tests only prove that independently developed modules connect together as designed. The test coverage of integration tests can be extended to verify the correct behavior of the system as well. The responsibility of providing a sufficient branch and line code coverage can be transferred from unit tests to integration tests.\nInstead of several unit tests needed to test a specific case of functionality of the system, one integration scenario is created that covers the entire flow. For example in case of an API, the received HTTP responses and their content are verified for each request in test. This covers both the integration between components of the API and the correctness of its business logic.",
        "chunk_id": "chunk89_20",
        "source": "..\/data\/docs\/code-with-engineering\/automated-testing\/unit-testing\/README.md",
        "root_chunk_id": "chunk89"
    },
    {
        "user_prompt": "check your changes\n\ngit status\n\npush the branch to the remote repository\n\ngit push --set-upstream origin feature\/123-add-git-instructions",
        "output_prompt": "$ git status\nOn branch feature\/123-add-git-instructions\nnothing to commit, working tree clean\n\n$ git push --set-upstream origin feature\/123-add-git-instructions\nEnumerating objects: 5, done.\nCounting objects: 100% (5\/5), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (3\/3), done.\nWriting objects: 100% (3\/3), 300 bytes | 300.00 KiB\/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0\nremote: \nremote: Create a pull request for 'feature\/123-add-git-instructions' on GitHub by visiting:\nremote:      https:\/\/github.com\/your-repo\/your-project\/pull\/new\/feature\/123-add-git-instructions\nremote: \nTo https:\/\/github.com\/your-repo\/your-project.git\n * [new branch]      feature\/123-add-git-instructions -> feature\/123-add-git-instructions\nBranch 'feature\/123-add-git-instructions' set up to track remote branch 'feature\/123-add-git-instructions' from 'origin'.",
        "context": "check your changes\n\ngit status\n\npush the branch to the remote repository\n\ngit push --set-upstream origin feature\/123-add-git-instructions",
        "chunk_id": "chunk206_5",
        "source": "..\/data\/docs\/code-with-engineering\/source-control\/git-guidance\/README.md",
        "root_chunk_id": "chunk206"
    },
    {
        "user_prompt": "What are the best practices for writing commit messages in git, and why are they important?",
        "output_prompt": "Best practices for writing commit messages include committing complete and well-tested code, writing descriptive messages that explain why the change was necessary and what effects it may have. This is important for maintaining a clear project history, making it easier for other developers to understand the changes and for troubleshooting issues. Using your favorite editor, such as Visual Studio Code set as the default git editor, can help write better commit messages. The essential parts of a commit message should include a succinct title line, a more detailed explanatory text if necessary, and any relevant metadata such as related issue numbers.",
        "context": "Commit complete and well tested code. Never commit incomplete code, get in the habit of testing your code before committing.\n\nWrite good commit messages.\n\nWhy is it necessary? It may fix a bug, add a feature, improve performance, or just be a change for the sake of correctness\n\nWhat effects does this change have? In addition to the obvious ones, this may include benchmarks, side effects etc.\n\nYou can specify the default git editor, which allows you to write your commit messages using your favorite editor. The following command makes Visual Studio Code your default git editor:\n\n{% raw %}\n\nbash\ngit config --global core.editor \"code --wait\"\n\n{% endraw %}\n\nCommit Message Structure\n\nThe essential parts of a commit message are:",
        "chunk_id": "chunk206_20",
        "source": "..\/data\/docs\/code-with-engineering\/source-control\/git-guidance\/README.md",
        "root_chunk_id": "chunk206"
    },
    {
        "user_prompt": "I need a PowerShell script template that allows me to register a domain, attach a password to a certificate, and define a filename for the certificate's output. Can you provide me with a basic structure that includes placeholders for the domain, password, and filename?",
        "output_prompt": "$domain = '[YOUR DOMAIN]' # Replace with your domain\n$password = ConvertTo-SecureString -String '[PASSWORD]' -AsPlainText -Force # Replace with your certificate password\n$filename = '[FILENAME]' # Replace with the desired output file name or path\n\n# Command to register domain (This will vary based on your requirements and the services you use)\n\n# Command to upload certificate to Key Vault with the password\n\n# Command to save the certificate output to the specified filename\n\n# You can save this content in a .ps1 file to create your PowerShell script",
        "context": "[YOUR DOMAIN] with the domain you would like to register, e.g. docs.somewhere.com\n\n[PASSWORD] with a password of the certificate. It's required for uploading a certificate in the Key Vault to have a password. You'll need this password in that step.\n\n[FILENAME] for the output file name of the certificate. You can even insert the path here where it should be store on your machine.\n\nYou can store this script in a PowerShell script file (ps1 extension).\n\n{% raw %}",
        "chunk_id": "chunk108_3",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/deploy-docfx-azure-website.md",
        "root_chunk_id": "chunk108"
    },
    {
        "user_prompt": "How can I profile a PyTorch model training using cProfile?",
        "output_prompt": "To profile a PyTorch model training session using cProfile, you should run your training script with the cProfile module like this: `python -m cProfile -o output_file.prof your_training_script.py`. Just run one epoch for the profiling to avoid unnecessary costs, since generally, one epoch provides enough information. Then, you can analyze the profiling data using Python's `pstats` module or visualize it using tools like SnakeViz.",
        "context": "```\n\n{% endraw %}\n\nYou can also run cProfile outside of the Python script using the following command:\n\n{% raw %}\n\nbash\npython -m cProfile [-o output_file] [-s sort_order] (-m module | myscript.py)\n\n{% endraw %}\n\nNote: one epoch of model training is usually enough for profiling. There's no need to run more epochs and produce\nadditional cost.\n\nRefer to The Python Profilers for further details.\n\nPyTorch model training profiling",
        "chunk_id": "chunk216_4",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-profiling.md",
        "root_chunk_id": "chunk216"
    },
    {
        "user_prompt": "Based on the provided context with partial Terraform outputs, what are some steps to determine the Kubernetes version of an AKS cluster?",
        "output_prompt": "To determine the Kubernetes version of an Azure Kubernetes Service (AKS) cluster after its deployment via Terraform, you can use the Azure CLI, Azure portal, or Terraform output commands. For instance, you can run the Azure CLI command `az aks show --resource-group <your-resource-group> --name aks-dev --query kubernetesVersion --output table` to get the version. Alternatively, check the AKS cluster details in the Azure portal, or inspect the Terraform state file if the output for the Kubernetes version is defined.",
        "context": "+ kubernetes_version                  = (known after apply)\n      + location                            = \"westeurope\"\n      + name                                = \"aks-dev\"\n      + node_resource_group                 = (known after apply)",
        "chunk_id": "chunk133_20",
        "source": "..\/data\/docs\/code-with-engineering\/continuous-delivery\/recipes\/terraform\/share-common-variables-naming-conventions.md",
        "root_chunk_id": "chunk133"
    },
    {
        "user_prompt": "Can you explain when it's best to use the HNSW algorithm for ANN search and how it maintains its efficiency?",
        "output_prompt": "HNSW algorithm is best used for approximate nearest neighbor search in large data sets because it efficiently organizes data into a hierarchical graph which allows for fast searching. This method is faster than exhaustive KNN and can be tuned through various parameters to balance throughput, latency, and recall according to the specific needs of a search application.",
        "context": "The HNSW algorithm, as an approximate nearest neighbor (ANN) search algorithm in high-dimensional spaces, organizes data into a hierarchical graph in order to quickly find approximate nearest neighbors. It does this using navigating data points through the graph while maintaining a trade-off between search accuracy and computational cost.\n  When to use:\n  HNSW is recommended for most scenarios due to its efficiency in larger data sets. HNSW is faster than exhaustive KNN. HNSW has several configuration parameters that can be tuned to achieve the desired throughput, latency, and recall for your search application. For more information, see HNSW parameters.",
        "chunk_id": "chunk315_8",
        "source": "..\/data\/docs\/code-with-mlops\/technology-guidance\/search\/index.md",
        "root_chunk_id": "chunk315"
    },
    {
        "user_prompt": "Can you explain the purpose of `utils.resize_image` function and what the test `test_resize_image_pads_correctly` is validating?",
        "output_prompt": "The `utils.resize_image` function is designed to resize a given image to specified dimensions, in this case, 100 by 100 pixels. The test `test_resize_image_pads_correctly` is checking to ensure that when an image is resized, any padding necessary to reach the new dimensions is filled with the correct values. Specifically, it verifies that the padding is initialized to 0 while ensuring that the original image content remains unchanged at the expected positions after resizing.",
        "context": "```\n\n{% endraw %}\n\nand one to verify that any padding is made appropriately\n\n{% raw %}\n\n```python\ndef test_resize_image_pads_correctly():\n  # Arrange\n  original_image = np.ones((10, 5, 2, 3))\n\n# Act\n  resized_image = utils.resize_image(original_image, 100, 100)\n\n# Assert\n  assert resized_image[0][0][0][0] == 0\n  assert resized_image[0][0][2][0] == 1",
        "chunk_id": "chunk214_9",
        "source": "..\/data\/docs\/code-with-engineering\/machine-learning\/ml-testing.md",
        "root_chunk_id": "chunk214"
    },
    {
        "user_prompt": "Can you provide a real-world example of how engineering feedback led to product improvements, especially in relation to Windows Server Container support for Azure Kubernetes Service?",
        "output_prompt": "Certainly! One such instance involves the Azure Kubernetes Service (AKS), which initially lacked support for Windows Server Containers. Users needed a way to deploy Windows Server containers on AKS but found that this functionality was missing. Following user feedback and an unsuccessful deployment attempt that remained pending, Microsoft recognized the demand and added first-class Windows container support to AKS. This enhancement allowed users to deploy Windows workloads on a popular container orchestration platform, thereby unblocking customers and improving the overall usability of the service.",
        "context": "Engineering Feedback Examples\n\nThe following are real-world examples of Engineering Feedback that have led to product improvements and unblocked customers.\n\nWindows Server Container support for Azure Kubernetes Service\n\nThe Azure Kubernetes Service should have first class Windows container support so solutions that require Windows workloads can be deployed on a wildly popular container orchestration platform. The need was to be able to deploy Windows Server containers on AKS the managed Azure Kubernetes Service. According to this FAQ (and in parallel confirmation) it is not available yet.\n\nWe tried to deploy anyway as a test, and it did not work \u2013 the deployment would be pending without success.",
        "chunk_id": "chunk199_0",
        "source": "..\/data\/docs\/code-with-engineering\/engineering-feedback\/feedback-examples.md",
        "root_chunk_id": "chunk199"
    },
    {
        "user_prompt": "Can you explain to me what the INDEX.MD file is used for in a documentation website repository?",
        "output_prompt": "The INDEX.MD file serves as the landing page for your documentation website. It introduces the site and its contents, providing an overview for visitors. It's where you can add important links for direct access, ensuring users can easily navigate to essential sections without unnecessary duplication of navigation links.",
        "context": "{% endraw %}\n\nYou can use templates like working agreements and such from the ISE Playbook.\n\nTo have a proper landing page of your documentation website, you can use a markdown file called INDEX.MD in the root of your repository. Contents can be something like this:\n\n{% raw %}\n\n```markdown\n\nISE Documentation\n\nThis is the landing page of the ISE Documentation website. This is the page to introduce everything on the website.\n\nYou can add specific links that are important to provide direct access.\n\nTry not to duplicate the links on the top of the page, unless it really makes sense.\n\nTo get started with the setup of this website, read the getting started document with the title Using DocFx and Companion Tools.",
        "chunk_id": "chunk110_22",
        "source": "..\/data\/docs\/code-with-engineering\/documentation\/recipes\/using-docfx-and-tools.md",
        "root_chunk_id": "chunk110"
    }
]