{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QA Pairs based on the first 200 docs in Solution Ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import tqdm\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "aoi_deployment_name = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "aoi_api_key = os.environ[\"aoi_api_key\"]\n",
    "azure_aoai_endpoint = os.environ[\"azure_aoai_endpoint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_generation_prompt_from_chunk_context(chunk_text: str) -> list[dict]:\n",
    "    system_prompt = \"\"\"you are a prompt creator and have ability to generate new JSON prompts based on the given CONTEXT.\n",
    "Generate 1 most relevant new prompt in valid json format according to \"RESPONSE SCHEMA EXAMPLE\" completely from scratch.\n",
    "\"RESPONSE SCHEMA EXAMPLE\":\n",
    "[\n",
    "    {\n",
    "        \"role: \"user\",\n",
    "        \"content\": \"This is the generated prompt text\",\n",
    "    },\n",
    "    {\n",
    "        \"role: \"assistant\",\n",
    "        \"content\": \"the expected, rightful and correct answer for the question\"\n",
    "    },\n",
    "]\n",
    "\"\"\"\n",
    "    user_prompt = \"\"\"The response must be valid JSON array containing two objects. The first object must contain the keys \"role\" and \"content\". The second object must also contain the keys \"role\" and \"content\".\n",
    "    The response must follow the \"RESPONSE SCHEMA EXAMPLE\".\n",
    "    The most important thing is that the response must be valid JSON ARRAY. DO NOT include anything other than valid schema.\n",
    "\n",
    "    CONTEXT:\n",
    "\n",
    "    \"\"\"\n",
    "    final_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt + chunk_text + \"\\nEND OF CONTEXT\"\n",
    "        }\n",
    "    ]\n",
    "    return final_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks():\n",
    "    with open(\"./output/chunks-solution-ops-200.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        chunks = json.load(file)\n",
    "    print(chunks)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def generate_qa_from_chunk_text(chunk_text:str) -> dict:\n",
    "    final_prompt = create_qa_generation_prompt_from_chunk_context(chunk_text)\n",
    "    return final_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\n",
    "    \"AZURE_OPENAI_ENDPOINT\"\n",
    ")  # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-07-01-preview\"  # this might change in the future\n",
    "\n",
    "\n",
    "def call_aoai_gpt4(messages: list[dict]):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=aoi_deployment_name, messages=messages  # engine = \"deployment_name\".\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTRAGEOUSLY LONG: make it async later\n",
    "def generate_qa_pairs() -> list:\n",
    "    chunks = read_chunks()\n",
    "\n",
    "    column_names = [\"user_prompt\", \"output_prompt\", \"context\"]\n",
    "    final_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    i = 0\n",
    "    for chunk_dict in chunks: \n",
    "        # chunk_id, chunk_dict in tqdm.tqdm(chunks_dict.items()):\n",
    "        chunk_id = chunk_dict[\"chunkId\"]\n",
    "        if i <10:\n",
    "            i += 1\n",
    "            chunk_text = chunk_dict[\"chunkContent\"]\n",
    "            prompt_from_chunk_context = generate_qa_from_chunk_text(chunk_text)\n",
    "            response = call_aoai_gpt4(prompt_from_chunk_context)\n",
    "            \n",
    "            try:\n",
    "                response_dict = json.loads(response)\n",
    "                for item in response_dict:\n",
    "                    if item[\"role\"] == \"user\":\n",
    "                        user_prompt = item[\"content\"]\n",
    "                    if item[\"role\"] == \"assistant\":\n",
    "                        output_prompt = item[\"content\"]\n",
    "\n",
    "                data = {\n",
    "                    \"user_prompt\": user_prompt,\n",
    "                    \"output_prompt\": output_prompt,\n",
    "                    \"context\": chunk_text,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"source\": chunk_dict[\"source\"]\n",
    "                }\n",
    "\n",
    "                final_df = final_df._append(data, ignore_index=True) #logic needs to be rewritten here to write better code to create DF (lambd/map)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        else:\n",
    "\n",
    "            break\n",
    "    final_df.to_json(\"./output/qa/solution-ops-200-qa.json\", orient=\"records\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'chunkId': 'chunk0_0', 'chunkContent': 'layout: forward\\ntarget: https://aka.ms/solutionops\\nrings:\\n  - public\\n\\nforward', 'source': '..\\\\data\\\\docs\\\\index.md'}, {'chunkId': 'chunk1_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Playbook\\n\\nThe Data Playbook provides enterprise software engineers with solutions, capabilities, and code developed to solve real-world problems. Everything in the playbook is developed with, and validated by, some of Microsoft's largest and most influential customers and partners.\\n\\n{% if extra.ring == 'internal' %}\\nYou are invited to share your enterprise-grade production solutions as well. Refer to Contributing to the Solutions Playbook.\\n\\n{% endif %}\\n\\nData Solutions\\n\\nModern Data Warehouse solution\\n{% if extra.ring == 'internal' %}\\n\\nData Mesh solution\\n\\nAnalytics and ML for enterprise business applications solution\\n\\nEnterprise Data Sharing solution\\n{% endif %}\\n\\n{% if extra.ring == 'internal' %}\\n\\n{% else %}\\n\\n{% endif %}\\n\\nAbout the Data Playbook\\n\\nThese Playbook solutions employ good engineering practices to accelerate real-world application development. Common themes include:\\n\\nImproving application design and developer productivity by sharing code and knowledge developed by experts for Microsoft customers.\\n\\nUsing automation to make repetitive tasks faster, more reliable, and auditable\\n\\nMaking application deployments and operations secure and observable.\\n\\nSecuring applications by following security best practices at all stages of the engineering lifecycle.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\index.md'}, {'chunkId': 'chunk1_1', 'chunkContent': \"Making application deployments and operations secure and observable.\\n\\nSecuring applications by following security best practices at all stages of the engineering lifecycle.\\n\\nWriting portable solutions that run in multiple locations, including edge devices, on-premises data centers, the Microsoft cloud, and competitor's clouds.\\n\\nIntegrated solutions\\n\\nPlaybook solutions span multiple Microsoft products and services and focus on creating integrated end-to-end solutions often using a range of open-source software libraries.\\n\\nProven with real customers\\n\\nAll code linked from playbook solutions and capabilities was created working with our customers to develop production solutions. This documentation and code is generalized to remove confidential details.\\n\\nReferences\\n\\nEngineering Fundamentals\\n\\nAzure Architecture Center: Data Checklist\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\index.md'}, {'chunkId': 'chunk2_0', 'chunkContent': \"rings:\\n  - public\\n\\nCapabilities\\n\\nCapabilities define the fundamental conceptual building blocks within a technical area. This section not only describes the core concept behind each capability, but also provides specific implementations using various Azure services, programming languages and usage.\\n\\nUnderstanding the Structure\\n\\nThe playbook uses Capability Maps to organize different capabilities.\\n\\nCapability Pillars\\n\\nThe capability pillars are the highest level grouping with DataOps section of playbook. Currently, there are five such pillars:\\n\\nDevOps for Data\\n\\nTransactional Systems\\n\\nAnalytical Systems\\n\\nData Governance and Protection\\n\\nData Platform Infrastructure\\n\\n{% if extra.ring == 'internal' %}\\n\\n{% endif %}\\n{% if extra.ring == 'public' %}\\n\\n{% endif %}\\n\\nEach capability pillar also has a capability map that provides a depiction of how different capabilities are grouped together and interact with each other. The different sections of the capability maps are clickable and takes the reader directly to the specific capability or group.\\n\\nCapability Groups\\n\\nThe capability groups are the next level grouping within each capability pillar. These groups logically put together related capabilities. And all the capability groups together cover the overall capability pillar.\\n\\nIndividual Capabilities\\n\\nCapabilities are the most granular items in the hierarchy that describe a generalized functionality and it's where bulk of the content resides.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\index.md'}, {'chunkId': 'chunk2_1', 'chunkContent': 'Capabilities are the most granular items in the hierarchy that describe a generalized functionality and it\\'s where bulk of the content resides.\\n\\nIn summary, this hierarchy can be represented as below:\\n\\ntext\\nCapability Pillars (has Capability Maps)\\n  - Capability Groups\\n    - Capabilities\\n\\nCapability Pillars Description\\n\\nDevOps for Data\\n\\n\"DevOps for Data\" is the core functional area that sets the foundation for implementing DevOps capabilities for data solutions. This pillar is fundamental and supports all other pillars and capabilities.\\n\\nFor detailed information, see DataOps: DevOps for Data.\\n\\nAnalytical Systems\\n\\nThe \"Analytical Systems\" pillar contains guidance in relation to analytical systems. The primary use case is to support data-driven business decisions in the form of data products, reports, interactive dashboards, etc. Common technical patterns include traditional ETL and Business Intelligence, Modern Data Warehouse (ingest, process, and serve) and Lambda / Kappa architectures for real-time data systems. Most of the data sharing scenarios will sit under this pillar.\\n\\nFor detailed information, see DataOps: Analytical Systems.\\n\\nTransactional Systems', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\index.md'}, {'chunkId': 'chunk2_2', 'chunkContent': 'For detailed information, see DataOps: Analytical Systems.\\n\\nTransactional Systems\\n\\nThe \"Transactional Systems\" pillar contains guidance in relation to systems that underpins operational business transactions. Traditionally, transactional systems primarily encompassed data systems used for real commercial transactions like sales, online banking, or stock trading. However, the scope has been expanded to include any data system that requires low latency read and writes.\\n\\nIn most cases, OLTP systems provide ACID guarantees; however, certain systems may prioritize lower latency and intentionally relax one of the ACID properties.\\n\\n{% if extra.ring == \\'internal\\' %}For detailed information, see DataOps: Transactional Systems.{% endif %}\\n{% if extra.ring == \\'public\\' %}This capability pillar is not currently covered in the playbook. It will be added in the future.{% endif %}\\n\\nData Governance and Protection\\n\\nThe \"Data Governance and Protection\" pillar contains all related guidance to manage, govern and protect the entire data estate including both OLTP and OLAP data systems. If there is specific technical guidance, it would be added to individual capabilities within other pillars. The content would then be linked back to this pillar to refer to common governance practices.\\n\\nFor detailed information, see DataOps: Data Governance and Protection.\\n\\nData Platform Infrastructure', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\index.md'}, {'chunkId': 'chunk2_3', 'chunkContent': 'For detailed information, see DataOps: Data Governance and Protection.\\n\\nData Platform Infrastructure\\n\\nThe \"Data Platform Infrastructure\" pillar contains guidance as it relates to designing and building the platform / infrastructure layer of a data estate on Azure. This pillar will draw heavily on the existing guidance of the Cloud Adoption Framework (CAF) specifically the CAF: Cloud-Scale Analytics scenario. It includes guidance on how to organize Azure services into a logical architecture and implement central security and governance.\\n\\n{% if extra.ring == \\'internal\\' %}For detailed information, see DataOps: Data Platform Infrastructure.{% endif %}\\n{% if extra.ring == \\'public\\' %}This capability pillar is not currently covered in the playbook. It will be added in the future.{% endif %}', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\index.md'}, {'chunkId': 'chunk3_0', 'chunkContent': 'rings:\\n  - public\\n\\nAnalytical Systems\\n\\nThis pillar contains guidance in relation to Analytical systems where the primary use case is to support data-driven business decisions. Some of the common technical patterns in OLAP (Online Analytical Processing) include:\\n\\nTraditional ETL (Extract, Transform, Load) and Business Intelligence.\\n\\nModern Data Warehouse, which involves ingesting, processing, and serving data.\\n\\nLambda/Kappa architectures for real-time data systems.\\n\\nMost of the data sharing scenarios also sit under this pillar.\\n\\nCapability Map\\n\\nHere is a high-level capability map for Analytical Systems:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\index.md'}, {'chunkId': 'chunk4_0', 'chunkContent': 'rings:\\n  - public\\n\\nAdvanced Analytics\\n\\nModern machine learning algorithms and frameworks make it increasingly easy to develop models that can make accurate predictions. MLOps, or Machine Learning Operations, is a paradigm that aims to deploy and maintain machine learning models in production reliably and efficiently.\\n\\nAI: MLOps Experimentation\\n\\nAI: MLOps Model Development\\n\\nAI: MLOps Model Deployment\\n\\nAI: MLOps Inference & Feedback\\n\\nFor more information, see AI section.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\advanced-analytics\\\\index.md'}, {'chunkId': 'chunk5_0', 'chunkContent': 'rings:\\n  - public\\n\\nCommon Capabilities\\n\\nStorage\\n\\nThe storage system is the persistence layer for the data at rest. Choosing the right storage is important for the success of data projects as it affects the whole data process, from collecting to sharing.\\n\\nWith the rise of cloud platforms, cloud-native object storage is replacing disk-based storage technologies for building data lakes and data warehouses. Object storage is decoupled from the compute and provides a low-cost option for storing a large amount of data.\\n\\nData access patterns for analytical systems are different from transactional systems. A common example is, accessing a particular attribute or column for large number of records. This made columnar data formats a popular choice for fast analytics workloads.\\n\\nFor detailed information, see DataOps: Storage.\\n\\nData Pipeline Operability\\n\\nData Pipeline Operability refers to a set of best practices that span the entire lifecycle of a data pipeline. All data pipelines read data from a source and write it to a target location with or without transformation. Due to this similarity in the nature of data pipelines, they share a common set of best practices.\\n\\nFor detailed information, see DataOps: Data Pipeline Operability.\\n\\nConfig Driven Data Pipelines', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\index.md'}, {'chunkId': 'chunk5_1', 'chunkContent': 'For detailed information, see DataOps: Data Pipeline Operability.\\n\\nConfig Driven Data Pipelines\\n\\nConfig-driven data pipelines are an essential tool for efficiently managing and processing large amounts of data. As the name suggests, these pipelines rely on a configuration to define data sources, transformation logic, and destinations. Config-driven pipelines offer many benefits, including increased flexibility and reusability, as changes can be made to the pipeline configuration without modifying the underlying code. Config-driven pipelines can also be automated to reduce manual intervention that speeds up data processing and improves accurate.\\n\\nFor detailed information, see DataOps: Config Driven Data Pipelines.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\index.md'}, {'chunkId': 'chunk6_0', 'chunkContent': \"rings:\\n  - public\\n\\nConfig Driven Data Pipelines\\n\\nProblem Statement\\n\\nHow to set up an extensible and configurable data platform consisting of multiple pipelines?\\n\\nThe pipelines need to ingest, transform, and curate data from a large array of source systems and producers. Additionally, the data can be in different formats with varying business logic that will need to be applied during the transformation and curation of the datasets. Building and maintaining individual data pipelines by hand becomes infeasible in the long term.\\n\\nHere are some examples:\\n\\nA food-manufacturing company wants to ingest food recipes from the on-premises servers of its multiple factories and provide a curated view.\\n\\nA solution provider wants to build a configurable and reusable common data platform. Such a platform should reduce development efforts during ingestion, transformation, and curation from different data sources and, in turn, reduce time to insights.\\n\\nThe solution should be able to:\\n\\nEnable designing data pipelines through configuration files that can be converted into Databricks jobs at run-time.\\n\\nProvide a low-code/no-code data app solution for the customer's business or operation teams.\\n\\nSolution Overview\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\config-driven-data-pipelines\\\\index.md'}, {'chunkId': 'chunk6_1', 'chunkContent': 'Provide a low-code/no-code data app solution for the customer\\'s business or operation teams.\\n\\nSolution Overview\\n\\nThis solution is following the Medallion Architecture introduced by Databricks. The above diagram shows a data pipeline that includes three layers/zones: Bronze, Silver, and Gold. In data platform projects, these zones can also be named as Staging, Standard and Serving.\\nA data pipeline handles the flow of data from an initial source to a designated endpoint. Usually, a data system consists of multiple data pipelines for different type of batch/streaming data.\\n\\nZone Description Bronze/Staging The data from external systems is ingested and stored in the staging zone. The data structures in this zone correspond to the source system\\'s table structures \"as-is\". This structure also includes any metadata columns that capture the load date-time, process ID etc. Silver/Standard The data from the staging zone is cleansed and transformed and then stored in the standard zone. It provides enriched datasets for further business analysis. The master data could be versioned with slowly changed dimension (SCD) patterns and the transaction data is deduplicated and contextualized with master data. Gold/Serving The data from the standard zone is aggregated and then stored in the serving zone. The data is organized in consumption-ready \"project-specific\" databases, such as Azure SQL.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\config-driven-data-pipelines\\\\index.md'}, {'chunkId': 'chunk6_2', 'chunkContent': 'The above picture shows a typical way to implement a data platform consisting of multiple data pipelines based on Databricks.\\n\\nAzure Data Factory can be used to load external data and store it to Azure Data Lake Storage.\\n\\nAzure Data Lake Storage (ADLS) is applied as the storage layer of the staging, standard, and serving zone.\\n\\nAzure Databricks is the calculation engine for data transformation, and most of the transformation logic is implemented with PySpark or SparkSQL.\\n\\nAzure Synapse Analytics or Azure Data Explorer is the solution of serving zone.\\n\\nThe medallion architecture and Azure big data services provide the infrastructure of an enterprise data platform. Then, data engineers can build transformation and aggregation logic with programming languages such as Scala, Python, SQL etc. Furthermore, applying DevOps principles such as CI/CD and monitoring is crucial for efficiently operationalizing the solution.\\n\\nTo make the solution extensible and configurable, the system needs to be designed through a configuration that consists of entities, attributes, relationships, rules etc. Within this system, the data pipelines can be built based on a framework and the configuration (metadata) defined by the users.\\n\\nArchitecture\\n\\nThe diagram below is the architecture of an end-to-end sample solution that illustrates a configurable and reusable framework to build data pipelines.\\n\\nThe sample solution processes two kinds of data:\\n\\nMaster data', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\config-driven-data-pipelines\\\\index.md'}, {'chunkId': 'chunk6_3', 'chunkContent': 'The sample solution processes two kinds of data:\\n\\nMaster data\\n\\nData about the business entities that provide context for business transactions\\n\\nData processing type: batch processing\\n\\nEvent/transactional data\\n\\nData captured in transactions and will be used in reporting and analytics\\n\\nData processing type: stream processing\\n\\nBoth master and event data will go through staging, standard and serving zones. Finally, the dataset will be ready for the down stream and business consumption.\\n\\nThe solution includes two parts:\\n\\nFramework: It loads the configuration files and converts them into Databricks Jobs. It encapsulates complex Spark clusters and job run-times and provides a simplified interface to users, who can focus on the business logic. The framework is based on PySpark and Delta Lake and managed by developers. The framework supports the following types of pipelines:\\n\\nMaster data pipelines: batch processing for the master data.\\n\\nEvent data pipelines: stream processing for the event data.\\n\\nConfiguration: The configuration or metadata file defines:\\n\\nThe pipeline zones.\\n\\nData source information.\\n\\nTransformation and aggregation logic implemented in SparkSQL.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\config-driven-data-pipelines\\\\index.md'}, {'chunkId': 'chunk6_4', 'chunkContent': 'The pipeline zones.\\n\\nData source information.\\n\\nTransformation and aggregation logic implemented in SparkSQL.\\n\\nThe configuration can be added and updated by the developers by using IDEs and Azure Pipelines.\\n  In future versions, shown as the diagram below, the configuration can also be managed by a set of APIs. The technical operation team is able to manage the pipeline configuration via a Web UI on top of the API layer.\\n\\nTechnical Stack\\n\\nIn order to set up a solution of config/metadata driven data pipelines, the following components are needed in the technical stack. The relevant Azure services are also listed below.\\n\\nComponent Azure Service Apache Spark Azure Databricks Data lake storage Azure Data Lake Storage Gen 2 Delta Lake Delta Lake managed by Azure Databricks A workflow system (Optional) Azure Databricks jobs and tasks, Azure Data Factory A data warehouse (Optional) Azure Synapse Analytics A data visualization tool (Optional) Power BI\\n\\nSample Code\\n\\nFor the detailed instruction and the source code of the end-to-end sample solutions, refer to Config-driven Data Pipeline.\\n\\nFurther Reading\\n\\nDatabricks: Medallion Architecture\\n\\nDataOps: Data Pipeline Operability\\n\\nDataOps: Batch Processing\\n\\nDataOps: Modern Data Warehouse', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\config-driven-data-pipelines\\\\index.md'}, {'chunkId': 'chunk7_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Pipeline Operability\\n\\nData Pipeline Design and Operability best practices\\n\\nThis article is meant to summarize key learnings when designing and building analytical systems. It covers cross-cutting concerns and best practices for designing data pipelines. These practices are top considerations that are applicable when designing and building any analytical data workloads.\\n\\nData Storage\\n\\nHow do I store and structure data in the data lake?\\n\\nPractice data zoning in your data lake\\n\\nMost common approach is to divide your data lake into multiple logical zones with increasing levels of data quality. These zones typically map directly to different data ingestion, transformation and serving outputs of data pipeline activities. Typical recommendation is to have three zones (Bronze/Silver/Gold or Raw/Enriched/Curated) although it is not uncommon to have more:\\n\\nBronze: Raw, unprocessed data.\\n\\nSilver: Cleansed, augmented data. May be composed of multiple zones, depending on the use case.\\n\\nGold: Optimized for downstream consumers.\\n\\nMore information:\\n\\nCAF: Data lake zones\\n\\nDatabricks: Medallion Architecture\\n\\nDataOps: Storage\\n\\nDesigning and building Data Pipelines\\n\\nWhat are general best practices for designing and building data pipelines?\\n\\nValidate early in your pipeline', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\index.md'}, {'chunkId': 'chunk7_1', 'chunkContent': 'What are general best practices for designing and building data pipelines?\\n\\nValidate early in your pipeline\\n\\nData validation should be done between the Bronze and Silver zones of the data pipeline. Validating early in the data pipelines ensures that all succeeding datasets conform to a specific schema and known data invariants. This process can potentially prevent data pipeline failures in cases of unexpected changes to the input data.\\n\\nIt is not recommended to add validation prior to the Bronze zone of the data lake. The Bronze datasets ensure that there is a similar copy of the source system data. This dataset can then be used for pipeline reruns to test the validation logic and for data recovery (for example, when data corruption occurs due to a bug in the transformation logic).\\n\\nMore information:\\n\\nDataOps: Data Quality\\n\\nDataOps: Data Quality Monitoring\\n\\nDataOps: Continuous Testing (Tests for Data)\\n\\nStrive for reproducibility (idempotency)\\n\\nSilver and Gold datasets can get corrupted due to many reasons such as unintended bugs, unexpected changes in the inputs, and more. When data pipelines are reproducible and idempotent, one can recover through deployment of code fix and replaying the pipelines. Idempotency also ensures data-duplication is mitigated when replaying the data pipelines.\\n\\nEnsure data transformation code is testable', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\index.md'}, {'chunkId': 'chunk7_2', 'chunkContent': 'Ensure data transformation code is testable\\n\\nData transformation code logic needs to be abstracted from the code that accesses source datasets for effective unit-testing purposes. Having a clear de-coupling between the two allows moving data transformation code from notebooks into packages. While it is possible to run tests against notebooks, shifting tests increase developer productivity by increasing the speed of the feedback cycle.\\n\\nMore information:\\n\\nDataOps: Continuous Testing (Unit Testing)\\n\\nConsider metadata-based data pipelines\\n\\nWith increasing number of datasets, it becomes harder to build and maintain individual data pipelines tailored to every permutation of data sources and transformation requirements. Instead, consider a configuration-based design (metadata) that can dynamically specify data ingestion and transformation requirements.\\n\\nMore information:\\n\\nDataOps: Config-driven Data Pipelines\\n\\nADF Metadata-driven Copy Data Tool\\n\\nOptimize serving layers for specific consumers (reverse ETL)\\n\\nServing layers are meant to provide a mechanism for downstream consumers to access the data. Different customers will have different data consumption requirements ranging from interactive reports, to API and even availability in an RDBMS. Instead of providing a single consumption mechanism, consider creating multiple serving points that are tailored to the needs of the consumers.\\n\\nUse proper orchestrator for data pipeline', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\index.md'}, {'chunkId': 'chunk7_3', 'chunkContent': 'Use proper orchestrator for data pipeline\\n\\nAn orchestrator is a tool or system that coordinates the execution of tasks and manages the flow of data through the pipeline. It is responsible for scheduling the different stages of the pipeline, such as data extraction, processing, and loading. An orchestrator can also manage error handling and recovery, and provide monitoring and reporting capabilities.\\n\\nSome examples of orchestrators include Azure Data Factory, Apache Airflow, Argo Workflows, SQL Server Integration Services (SSIS) and Apache Nifi. These tools provide a way to define and schedule pipelines, monitor and manage pipeline execution via a web-based user interface.\\n\\nHere is a list of some key features to consider while choosing a particular tool for this purpose:\\n\\nScheduling: The ability to schedule and trigger pipeline tasks on a regular or event-based schedule.\\n\\nWorkflow management: The ability to define and manage complex pipeline workflows, including branching and conditional logic.\\n\\nError handling: The ability to detect and handle errors that occur during pipeline execution, and provide recovery options.\\n\\nMonitoring and reporting: The ability to monitor the status of pipeline tasks and provide detailed reporting on pipeline performance and errors.\\n\\nData lineage: The ability to track the flow of data through the pipeline and provide information on where data originated and how it has been transformed.\\n\\nMore information:\\n\\nChoose a data pipeline orchestration technology in Azure', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\index.md'}, {'chunkId': 'chunk7_4', 'chunkContent': 'More information:\\n\\nChoose a data pipeline orchestration technology in Azure\\n\\nEmpowering Users to build Data Products\\n\\nHow do I empower project teams to build Data Products?\\n\\nProvide a Data Exploratory Environment\\n\\nAside from the usual data zones, individual workspaces for data users and engineers should be setup using a section within the larger logical data lake. Furthermore, appropriate access to tools and environments for experimentation should be provided to them.\\n\\nMore information:\\n\\nMLOps: Experimentation\\n\\nFacilitate data discovery and governance through a data catalog\\n\\nIn order for users to start building data products, they need a way to discover and explore existing datasets available to them along with relevant business metadata, lineage and governance controls. Having a data catalog is imperative to have your data products easily discoverable and properly governed.\\n\\nMore information:\\n\\nDataOps: Data Catalog\\n\\nOperationalizing the Data Platform\\n\\nHow do I efficiently operate my data platform?\\n\\nAutomate deployments and Testing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\index.md'}, {'chunkId': 'chunk7_5', 'chunkContent': 'Operationalizing the Data Platform\\n\\nHow do I efficiently operate my data platform?\\n\\nAutomate deployments and Testing\\n\\nTo automate deployments and testing, all artifacts required to build the analytical system should be added to a source control system such as git. These artifacts include infrastructure-as-code, database objects (schema definitions, functions, stored procedures, etc.), application data, pipeline definitions, data validation and transformation logic. There should be a safe and repeatable process to deploy changes with automated tests through dev, test and production stages. This process is mainly to ensure correctness and to identify bugs early.\\n\\nMore information:\\n\\nDataOps: Code-First Development\\n\\nDataOps: Infrastructure-as-Code\\n\\nDataOps: Continuous Integration\\n\\nDataOps: Continuous Delivery\\n\\nDataOps: Continuous Testing\\n\\nCentralize configuration with Azure Key Vault\\n\\nMaintain a central, secure location for sensitive configuration such as database connection strings that can be accessed by the appropriate services within the specific environment. An example would be to secure secrets in Azure Key Vault per environment, then have the relevant services query Key Vault for the configuration.\\n\\nMore information:\\n\\nAzure Key Vault\\n\\nEngineering Fundamentals: Secrets Management\\n\\nMonitor infrastructure, pipelines and data', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\index.md'}, {'chunkId': 'chunk7_6', 'chunkContent': 'More information:\\n\\nAzure Key Vault\\n\\nEngineering Fundamentals: Secrets Management\\n\\nMonitor infrastructure, pipelines and data\\n\\nA proper monitoring solution should be in-place to ensure failures are identified, diagnosed, and addressed in a timely manner. Aside from the base infrastructure, compute, and pipeline runs, data quality should also be continuously monitored post deployment to production.\\n\\nMore information:\\n\\nDataOps: Data Quality Monitoring\\n\\nDataOps: Monitoring and Logging\\n\\nFurther Reading\\n\\nDataOps: Modern Data Warehouse\\n\\nDataOps: Batch Processing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\index.md'}, {'chunkId': 'chunk8_0', 'chunkContent': 'rings:\\n  - public\\n\\nSynapse Serverless and Synapse Pipelines Operability Best Practices\\n\\nServerless technology addresses important customer requirements. Serverless provides a flexible and cost-efficient way to drive value from data stored in data lakes using simple and common T-SQL statements. With Serverless, there is no need for an intermediate storage as the data is stored and queried on top of the lake.\\n\\nServerless SQL pool is also the only SQL pool that supports the delta lake format in Azure Synapse Analytics. Furthermore, serverless is a great candidate to drive sustainable choices on data engineering and analytics.\\n\\nProblem Statements\\n\\nDespite all the advantages that Serverless offers, challenges have been identified at the enterprise level:\\n\\nHow to successfully automate the deployment and dynamic configurations for Synapse Serverless pools?\\n\\nServerless pools are not supported by SQL Data Tools as happens for Dedicated pools. That fact gives Serverless less flexibility when implementing the CI/CD process and is seen as a disadvantage.\\n\\nHow to successfully automate and work in combination with Delta format?\\n\\nSynapse Serverless and the Delta Lake format are a popular combination. Automation is key to succeed and achieve a robust end-2-end enterprise implementation.\\n\\nHow to automate and successfully apply ACLs to the data lake based on specific configurations?', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\serverless-best-practices.md'}, {'chunkId': 'chunk8_1', 'chunkContent': 'How to automate and successfully apply ACLs to the data lake based on specific configurations?\\n\\nApplying granular level permissions to the data lake hierarchy is an important step. These permissions authorize the right people to access to the right data following the least access privilege principle.\\n\\nHow to successfully implement CLS (Column Level Security) on top of the Data Lake files?\\n\\nThere are no out-of-the-box mechanisms to address CLS in ADLS Gen2. Third-party tools exist that offer this feature among others, however it might add complexity to the solution.\\n\\nHow to successfully and continuously incorporate Data retention requirements along the data lifecycle?\\n\\nEvery organization has compliance and legal obligations when it comes to data retention. It is important to delete the relevant partitions or records when the obligations have been fulfilled. Deleting unnecessary records is necessary for optimizing performance and reducing costs.\\n\\nPipeline Deployment Operability\\n\\nImplementing CI/CD of a Data pipeline within a Data Service (in this case a Synapse Serverless pipeline) requires a different approach when compared to other standard code CI/CD implementations. A practical approach is to:\\n\\nDeploy the needed infrastructure, which is a solved problem for Synapse.\\n\\nDeploy the database and database objects, where one problem needs to be addressed as there is no Azure CLI support to deploy databases and databases objects.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\serverless-best-practices.md'}, {'chunkId': 'chunk8_2', 'chunkContent': 'Deploy the database and database objects, where one problem needs to be addressed as there is no Azure CLI support to deploy databases and databases objects.\\n\\nDeploy the artifacts dynamically, which is also a solved problem and when dynamically configured it contributes to address the previous challenge.\\n\\nImplementing custom code or a PowerShell script to address the second point would require extra work and maintenance. It can also be addressed with a more elegant and simplified solution. The solution is to use a Synapse pipeline combined with Linked Services configurations based on the infrastructure deployment information.\\n\\nWorking with Delta Files\\n\\nDelta format is becoming the preferred choice among organizations due to the advantages it offers when compared with Parquet. When working with Delta, especially in the analytics space, the number of Delta tables can be high. As a result, automation is the key for efficiency.\\n\\nApplying ACLs to the Data Lake Hierarchy\\n\\nApplying ACLs to the Data Lake is a process that needs to be coupled with the data ingestion.\\nDynamically applying ACLs within a data pipeline is possible by using a Spark pool and the right storage libraries to access the portions of the lake that need to be secured. However, it can also be a separate process, decoupled from the pipeline that is triggered after the ingestion is done.\\n\\nApplying Column Level Security', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\serverless-best-practices.md'}, {'chunkId': 'chunk8_3', 'chunkContent': 'Applying Column Level Security\\n\\nSecurity is a top of mind concern that supports Data privacy and Data access implementations in the Enterprise. At the core of this requirement is often CLS (Column Level Security) when dealing with sensitive and personal data.\\n\\nThere is no native way to provide CLS on top of files in the data lake backed by ADLS Gen2. One way to address it is by allowing different levels of access on a range of containers by excluding sensitive columns. However, it adds complexity to the solution and allows for data duplication. Another possibility is to use third-party tools that allow to implement CLS at the file level in the data lake. Using another tool means more integration and upskilling efforts for the data engineering teams.\\n\\nCLS is possible in Serverless by using partitioned views, where the access can be granted at the column level of the view, providing the desired level of granularity. It can be also combined with Azure AD groups mapping to the database users to improve auditing and security.\\n\\nIncorporating Data Retention on the Data Lifecycle\\n\\nDepending on which fields the data retention requirements are defined, approaches for the tactical implementation might differ. They can be summarized as below:\\n\\nIf the field driving the data retention requirement is a field used to partition the datasets, then the deletion of the data might be done at the partition level in alignment with the retention period.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\serverless-best-practices.md'}, {'chunkId': 'chunk8_4', 'chunkContent': 'If the field targeted by the data retention requirement is a column timestamp in the dataset, then the deletion of the records needs to happen as a query. The query should cover the entire data lake using a predicate with the following form c_timestamp < current_retention_date.\\n\\nA combination of the previous two cases.\\n\\nSample Code\\n\\nFor more details, explore the Synapse Serverless best practices sample.\\n\\nThis content provides guidance on a possible way to address the challenges mentioned on the Problem Statement section preferably using Serverless pools. However, for some challenges, a Spark pool is used to drive some of the functionality.\\n\\nSustainability\\n\\nAs mentioned earlier, Serverless technology is considered to be one great vehicle to address sustainable choices.\\n\\nBy addressing the existing automation and security challenges, the aim is to increase the adoption of Serverless as a reliable and easy to deploy and maintain option.\\n\\nFinally, the usage of the current or any other sample can dramatically reduce the development, testing and compute time used when compared to an implementation made from the ground up.\\n\\nFurther reading\\n\\nSynapse Analytics Serverless\\n\\nPartitioned Views\\n\\nEngineering Fundamentals: Sustainability\\n\\nGreen Software Foundation\\n\\nPrinciples of Green Software Engineering\\n\\nMicrosoft Cloud for Sustainability', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\data-pipeline-operability\\\\serverless-best-practices.md'}, {'chunkId': 'chunk9_0', 'chunkContent': \"rings:\\n  - public\\n\\nStorage\\n\\nStorage is a central entity for all data-related solutions in cloud. Often it's overlooked as cloud storage tends to be inexpensive, however there are some areas to evaluate when considering storage for cloud analytics.\\n\\nFile Type/Extension\\n\\nFile type/extension may seem inconsequential and are often overlooked. They're important for analytical solutions that use a data lake and can affect on areas such as performance and flexibility.\\n\\nAvro\\n\\nApache Avro is a highly reliable open-source file format for big data systems that have streaming data stored with the intention for row-level retrieval/processing. The schema for the data is stored along with the data itself. This self-describing nature of the data allows it to have faster data serialization. Avro provides native compression and thus a better option for larger data sets over flat files.\\n\\nJSON\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_1', 'chunkContent': \"JSON\\n\\nJSON is an open standard definition for data stored in a highly structured key/value pair schematic. This format although structured, does allow for schema on read types of patterns as more attributes can be freely added to JSON files. Downstream systems should account for any addition of new attributes or deletion of existing attributes. JSON allows parameters to be stored to describe bits of data in a well-defined way. So, it's heavily used in IaC (infrastructure as code) implementations,  data pipeline automation and many other systems. JSON files are also the staple of many NoSQL and non-relational database systems, especially document-based ones. JSON has become the preferred to format to capture and transmit semi-structured data originating from sources like IoT devices, mobiles and the web.\\n\\nFlat Files, CSVs, Delimited Files\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_2', 'chunkContent': 'Flat Files, CSVs, Delimited Files\\n\\nThese file formats have been time tested in the data space for decades and tend to be portable across various operating systems and storage mediums. They consist of data being stored in a uniform fashion to be read into analytical systems. Data processes usually determine the structure of the files using delimitation. However, due to their simplicity, there are no error handling mechanisms within the file; for example, there is no structure or data type constraints. Error handling and cleansing rely completely on the developed solution to handle these files. These file types are also natively uncompressed and thus can create storage size and performance issues with larger datasets. Performance issues come mainly from the concept that data workloads are mostly reliant on RAM for processing.\\n\\nParquet\\n\\nApache Parquet is another popular open-source file format targeted for columnar storage. This file format is aligned with OLAP/analytical workloads that depend mostly on column-based aggregations on large data volumes. Parquet format features native compression and embedded metadata.\\n\\nDelta Format/Lake', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_3', 'chunkContent': \"Delta Format/Lake\\n\\nThe Delta format is an open-source implementation of transactional consistency applied onto the Parquet file format. Using Delta format ACID principles can be applied on a data lake to help with solutions that are a combination of both streaming and batch data. Metadata stored with the Parquet files is one of the drivers for how the Delta format allows for transactional consistency. Delta format helps combine the best of both worlds in the data, operational/transactional consistent data and batch/analytical data loads.\\n\\nSchema evolution is a strong selling point for using the Delta Lake approach for an analytics solution. With schema evolution, data schema changes can be integrated with existing data pipelines. Delta format has schema enforcement option to facilitate error handling where schema changes aren't to be accepted. These programmatic features allow for the data lake to be more dynamic and error tolerant at the storage layer.\\n\\nDelta Lake FAQ and Delta Lake Schema Evolution provide more information on Delta format.\\n\\nConfiguration Implications\\n\\nSoft Delete and Versioning\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_4', 'chunkContent': 'Delta Lake FAQ and Delta Lake Schema Evolution provide more information on Delta format.\\n\\nConfiguration Implications\\n\\nSoft Delete and Versioning\\n\\nFile versioning  in a data lake is often overlooked resulting unexpectedly large storage bills. In such scenarios, IT departments often scramble to figure out the increased data storage costs without a change in the processed data volumes. With storage accounts in Azure, there are options for storing different versions of the data at different tiers (hot/cool/archive) and for soft delete. With soft delete enabled, a backup copy of the data is stored for a specific retention period. There is a charge for back-up versions of data although it appears as deleted. Versioning also has similar concepts. If versioning is turned on, older versions of the data are stored and thus charged to the storage account. Proper data lifecycle and access policies ensure that data is: retained for correct duration, has correct protections, stored at lower costs.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_5', 'chunkContent': \"Soft delete and versioning of data can be done in a way that helps maximize the benefits of using a data lake. Consider a typical data warehouse/analytics scenario where data is loaded nightly. Here data can be reloaded from the source easily if it doesn't change often. Hence, there may not be a need to have versioning or soft delete turned on. If a data load process reloads the data for example, with these options turned on, one can end up with several copies of the data: a previous version and as a soft delete protected version.\\n\\nIn other scenarios, these features are a welcomed addition for data protection. Consider an IoT system where data comes off of sensors there may not be another copy at the source. Some of these IoT devices send data and don't have a local store of it, or the local storage is frequently overwritten. Having protection such as soft delete or versioning turned on can help in accidental data deletion/manipulation.\\n\\nThere are options to fine-tune the retention period and amount of data that's protected that can help maximize the benefit for specific scenarios.\\n\\nMore information can be found here: Blob versioning and soft delete.\\n\\nStorage Replication\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_6', 'chunkContent': 'More information can be found here: Blob versioning and soft delete.\\n\\nStorage Replication\\n\\nThere are many options when selecting the type of storage account redundancy. In terms of the analytics space, there are a few things to keep in mind. The first is obviously cost, as mentioned in the previous section, the type of storage redundancy selected can affect cost. Another important aspect is the SLA coverage for each type of storage account and ingress/egress performance. Azure regions may have different SLAs for both redundancy and performance metrics (refer to documentation below for details).', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_7', 'chunkContent': 'Finally, the last point to consider is availability for different redundancy options. Replication is done synchronously within a data center for LRS ensuring at least three replicas. ZRS replication also happens synchronously across data centers with in a region ensuring at least three replicas. GRS/GZRS expands on ZRS by adding three more replicas in a secondary region using asynchronous replication. The secondary region is only available for reading/writing under certain conditions. The first condition is that the secondary region is made a primary; in a disaster recovery scenario. Granting read-only access on the account (RA-GRS or RA-GZRS) in the secondary region will enable applications/services read-only access to the data in that region. The caveat here is that since the replication is asynchronous there is a latency in data availability. Applications/services designed to use the readable secondary need to take into account this latency. In conjunction to providing higher data protection, RA-GRS/RA-GZRS can also provide scalability for read-only workloads.\\n\\nStorage redundancy and scalability targets for standard accounts provide more information on store replication and scalability.\\n\\nData/Storage Lifecycle Management', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_8', 'chunkContent': \"Storage redundancy and scalability targets for standard accounts provide more information on store replication and scalability.\\n\\nData/Storage Lifecycle Management\\n\\nData sets possess distinct lifecycle, with early stages characterized by frequent access to certain data. However, as the data ages, the need for access decreases dramatically. Some data remains unused in the cloud, with infrequent access once stored, while others may expire days or months after creation. Some data sets are actively read and modified throughout their lifecycle. Data can be transitioned to different access tiers or expired using Azure Storage Lifecycle Management's rule-based policies.\\n\\nAzure storage offers different access tiers namely hot, cool and archive. By default when data is created, it's stored under hot tier. The data then can be moved or deleted based on Storage Lifecycle Management rules, including an option to move it to cool or archive tiers.\\n\\nAn important consideration when using storage tiers is the rehydration process from the archive tier. Reading data from archive tier requires data rehydration first. The rehydration process may be time-consuming and expensive. For more information, see overview of blob rehydration from the archive tier.\\n\\nA sample implementation can be found in MDW repo: Storage (Single). These Terraform based IaC templates demonstrate Azure storage accounts/containers creation and data lifecycle rule configuration at scale.\\n\\nSee Lifecycle management overview for more information.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_9', 'chunkContent': \"See Lifecycle management overview for more information.\\n\\nStorage on Microsoft Fabric\\n\\nMicrosoft Fabric is an end-to-end analytics solution with full-service capabilities that include - data movement, data lakes, data engineering, data integration, data science, real-time analytics, and business intelligence. Microsoft Fabric is backed by a shared platform providing robust data security, governance, and compliance.\\n\\nOneLake\\n\\nOneLake is a single, unified, logical data lake for the whole organization that acts as the foundation of Microsoft Fabric's lake-centric architecture.\\n\\nHere are some key features of OneLake:\\n\\nWith Microsoft Fabric, every customer tenant has exactly one OneLake that's provisioned automatically.\\n\\nShortcuts in OneLake allows the unification of data across domains, clouds and accounts by creating a single virtualized data lake for the entire enterprise.\\n\\nOneLake is built on top of Azure Data Lake Storage Gen2 and can support any type of file, structured or unstructured.\\n\\nMicrosoft OneLake provides open access to the Fabric items through existing ADLS Gen2 APIs and SDKs. The data in OneLake can be accessed through any tool compatible with ADLS Gen2 just by using the OneLake URI.\\n\\nOneLake has seamless integration with Azure services such as Azure Synapse Analytics, Azure Storage Explorer, Azure Databricks, and Azure HDInsight.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_10', 'chunkContent': \"All the compute engines in Microsoft Fabric automatically store their data in OneLake. Data stored in OneLake is directly accessible by all compute engines without requiring to be moved/copied. For tabular data, the analytical engines in Fabric write data in delta parquet format.\\n\\nMicrosoft OneLake has detailed information on this subject.\\n\\nLakehouse\\n\\nLakehouse is one of the fundamental data building blocks in Microsoft Fabric - built on top of the OneLake scalable storage layer and uses Apache Spark and SQL compute engines for big data processing. A Lakehouse is a unified platform that combines the SQL-based analytical capabilities of a relational data warehouse and the flexibility and scalability of a data lake.\\n\\nSome key features of Lakehouse are:\\n\\nAbility to create shortcuts to data in external sources, such as Azure Data Lake Store Gen2 or a Microsoft OneLake location outside of the Lakehouse's own storage. Shortcuts can also be created for connecting to Amazon S3 buckets.\\n\\nA read-only serving layer by auto generating an SQL endpoint and a default dataset during creation that provides a SQL-based experience for Lakehouse Delta tables.\\n\\nSupports Direct Lake mode, which is a new dataset capability for analyzing large data volumes in Power BI.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk9_11', 'chunkContent': \"Supports Direct Lake mode, which is a new dataset capability for analyzing large data volumes in Power BI.\\n\\nAutomatic table discovery and registration. Automatic validation of supported structured formats(currently Delta format only) is performed on the files dropped in the managed area of the Lakehouse. File is then registered into the metastore with the metadata (Example: column names, formats, compression). Registered files can be referenced as a table and queried using SparkSQL.\\n\\n{% if extra.ring == 'internal' %}\\nGetting Started with Microsoft Fabric page collates various internal and external resources to delve deeper into the subject.\\n\\n{% endif %}\\n\\nFurther Reading\\n\\nIntroduction to Azure Blob Storage\\n\\nAzure Storage Data Services\\n\\nIntroduction to Azure Data Lake Storage Gen2\\n\\nDataOps: Modern Data Warehouse\\n\\nMicrosoft Fabric\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\index.md'}, {'chunkId': 'chunk10_0', 'chunkContent': \"rings:\\n  - public\\n\\nPartitioning Strategy\\n\\nSelecting a partitioning strategy\\n\\nData partitioning is a method of dividing data into separate storage. Using this method, data will be physically partitioned into segments that are logical. This method is useful and crucial when there is a large amount of data involved. Partitioning strategy has a direct impact on the query performance as it dictates on how easily the data can be accessed.\\n\\nThe choice of partition strategy heavily depends on the use case's query pattern. For that reason, the columns that are used as the filtered key should be inspected first (i.e., predicates that are included in the WHERE clause). The columns that cover the highest percentage of the query use cases are an excellent candidate to be chosen as the partitioning keys.\\n\\nFor instance, in a use case where climate data is used, most of the queries will be filtered on the longitude and latitude of a point/area. The query pattern will heavily rely on the latitude and longitude value.\\n\\nsql\\nSELECT XLAT, XLONG, WINDSPEED\\nWHERE XLAT BETWEEN 38.560 AND 38.563\\nAND XLONG BETWEEN -96.999 AND -96.995\\n\\nThus, it is logical to choose a partitioning strategy that uses latitude and longitude as partition keys.\\n\\nmd\\nxlat/\\n    xlong/\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\partitioning.md'}, {'chunkId': 'chunk10_1', 'chunkContent': 'md\\nxlat/\\n    xlong/\\n\\nAfter choosing the main partition key, consider the cardinality of the partition and the individual file size in these partitions. The aim is to make sure that the engine can handle the data loading efficiently. Every engine has a different strategy when dealing with optimal data size for each partition. For example in Databricks Delta, data should be stored in a few large files between 128 MB to 1 GB. This strategy allows the driver and workers to operate efficiently. When the data is broken into many small files, it tends to make reading the Delta store slower and overwhelms the driver memory. It is because the driver needs to load metadata for each file into memory.\\n\\nApplying this strategy to the climate data use case, consider how data can be organized to get the most optimal size. One strategy can use monthly climate data to be included on the same partition that results in the partition as below:\\n\\nmd\\n2011/\\n    01/\\n      xlat/\\n        xlong/\\n    02/\\n      xlat/\\n        xlong/\\n2012/\\n    01/\\n      xlat/\\n        xlong/', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\partitioning.md'}, {'chunkId': 'chunk10_2', 'chunkContent': 'Other strategies can consider expanding or reducing the size of each partition. For instance, by making the partition 10 times xlat and 10 times xlong it results in the partition as below:\\n\\nmd\\n10_x_xlat/\\n  10_x_xlong/\\n\\nEven though, partitioning is a fundamental step when designing a data system, not all cases need partitioning. When a dataset is small enough to be handled by a single server, partitioning often adds unnecessary complexity. However, having a small dataset does not always mean that partitioning is not relevant; one needs to consider long-term data needs and the use cases as well. For example, partitioning will help in a small datastore that might be heavily accessed by hundreds of concurrent clients.\\n\\nPartitioning scheme\\n\\nTable partitioning is a common optimization approach that enables partition discovery. Storing data in partitioned tables allows partitioning column values to be encoded in the path of each partition directory. A Spark job can efficiently read only the data for a given range of partition values, using partition pruning. For example using customer and day as partition keys, the sample scheme is shown below:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\partitioning.md'}, {'chunkId': 'chunk10_3', 'chunkContent': 'text\\n\\\\- baseDirectory\\n   \\\\- formatVersion=1\\n      \\\\- customer=Contoso\\n         \\\\- year=2023\\n            \\\\- month=2023-04\\n               \\\\- day=2023-04-04\\n                  \\\\- file1.parquet.snappy\\n                  \\\\- file2.parquet.snappy\\n\\nIt can be prudent to plan for incompatible evolutions of the file layout (represented here with the formatVersion path component). Since Spark requires the partitioning layout to be consistent within a dataset, changing the scheme over time (e.g. adding an hour partition level) requires a new dataset to be created, and read separately.\\n\\nQuerying spark.read.parquet(\"baseDirectory/formatVersion=1\") produces a data frame with the columns customer, year, month and day taken from the partition directory names, and the columns from the Parquet files.\\n\\nSuch a partitioning scheme can also be used with other file formats such as Avro, CSV or JSON.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\partitioning.md'}, {'chunkId': 'chunk10_4', 'chunkContent': 'Such a partitioning scheme can also be used with other file formats such as Avro, CSV or JSON.\\n\\nFile names are unimportant for Spark (besides the fact that the file extension must match the compression scheme, if any). If files originate from an external source, additional information may be coded in the filename, and retrieved in the read dataframe.\\n\\nValidating partitioning strategy\\n\\nThe easiest way to validate if the data is optimally stored for a given query is to analyze the query execution plan. One should look for the first task that hits the Spark_catalog.\\n\\nFor instance, in our use case of climate data, the query\\n\\nsql\\nSELECT XLAT, XLONG, WINDSPEED\\nWHERE XLAT BETWEEN 38.560 AND 38.563\\nAND XLONG BETWEEN -96.999 AND -96.995\\n\\nwill produce a Scan Parquet spark_catalogue detail as below:\\n\\nSome important metrics to be looked into are:\\n\\npartitions read\\n\\nfiles read\\n\\nfiles pruned\\n\\nsize of files pruned\\n\\nsize of file read\\n\\nA good data partitioning strategy will maximize the files pruned and minimize the files and partitions read. It is important to benchmark different partitioning strategies with comparable metrics to choose the best strategy based on comparison.\\n\\nFurther reading\\n\\nData partitioning best practices\\nData partitioning strategies', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\common-capabilities\\\\storage\\\\partitioning.md'}, {'chunkId': 'chunk11_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Distribution through REST APIs\\n\\nData can be distributed through different channels or tools.\\n\\nA common requirement is to deliver query functionality through a REST API, so that such calls can be integrated in custom applications.\\n\\nAdditionally, data tools that are used to execute the queries don't offer the ability to deliver results though a REST API. This layer needs development and integration with such data tools and with an end-user front end.\\n\\nSuch approach has advantages and challenges.\\n\\nThe benefits of using an API Layer to deliver dataset results from a set of queries are:\\n\\nREST APIs are a common standard for developers could be integrated into existing applications.\\n\\nQueries can be parametrized with the end-user input.\\n\\nBy integrating with custom applications, the complexity of using specific Data tools is hidden from the end users.\\n\\nEnd users do not need to acquire specific knowledge on Data Engineering to query results on their specific domain of expertise.\\n\\nThe challenges can be summarized as follows:\\n\\nDepending on the volume of the queried data and complexity, the results might take long time to be made available to end users.\\n\\nAn extra layer needs to be built between the data tool or query engine service that executes the query and the end user.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-distribution-through-rest-api.md'}, {'chunkId': 'chunk11_1', 'chunkContent': \"An extra layer needs to be built between the data tool or query engine service that executes the query and the end user.\\n\\nGiven the benefits and challenges mentioned above, there are three scenarios that might be implemented depending on the specific synchronization requirements to get the results: synchronous, asynchronous or a combination of both synchronous and asynchronous.\\n\\nThe illustrated logical architecture presented below, summarizes the components that might be required to implement such requirements:\\n\\nSynchronous distribution of query results\\n\\nThere are two main use cases that can be served with the synchronous data distribution use case:\\n\\nQuerying for fresh data: the caching layer will only serve requests for the last N time periods. The time period's granularity may vary depending on what can reasonably fit the cache. In some cases, it could be weeks; in some other cases, it could be minutes.\\n\\nQuerying for cached data: when the data has been previously requested and is available in the cache, it can be retrieved directly from the cache.\\n\\nFor both use cases, the generic implementation building blocks are as explained below:\\n\\nREST API as access point.\\n\\nCaching Layer as abstraction layer for data.\\n\\nSpark backend\\n\\nThe illustrated logical architecture presented below, summarizes the flow between the different layers:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-distribution-through-rest-api.md'}, {'chunkId': 'chunk11_2', 'chunkContent': 'Spark backend\\n\\nThe illustrated logical architecture presented below, summarizes the flow between the different layers:\\n\\nIn this scenario, the REST API is connected to a Caching layer. This Caching layer is designed to serve, whenever possible, a pre-computed set of data or a previous query result without the need to reprocess all the input data from the backend.\\nThis architecture enables scenarios like multi users concurrency queries, reporting and data sharing. All of these operations can be resource-consuming or might not be adhering to responsiveness requirements if run directly on the Spark backend.\\n\\nSome examples of services that can be used as a Caching layer for a Spark backend are:\\n\\nRDBMS (like Azure Sql Server).\\n\\nNoSQL Database (like Azure Cosmos DB).\\n\\nCaching Service (like Azure Redis).\\n\\nOther Data Stores (Azure Data Lake, Blob, etc.).\\n\\nThe choice of the correct service for the caching layer fully depends on the data type to store, the use case, and the performance needs. As an example, if the desired outputs are JSON documents and the Spark backend is Azure Synapse Analytics, the best choice can be a Cosmos DB instance as cache. That implementation allows to push the result of analytical workload from the backend to Cosmos DB. Furthermore, it assures that the Caching layer can serve the data without unpredictably consuming resources on the backend.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-distribution-through-rest-api.md'}, {'chunkId': 'chunk11_3', 'chunkContent': 'Asynchronous distribution of query results\\n\\nFor asynchronous distribution of results scenarios, the provided functionality might include:\\n\\nQuery catalog\\n\\nQuery parameter validation\\n\\nQuery execution\\n\\nQuery monitoring\\n\\nQuery cancellation\\n\\nQuery result download\\n\\nFor this scenario, the fundamental requisite is the ability to download the query results as it becomes impossible to deliver them in the default interface in a performant and timely manner.\\n\\nOther fundamental aspects to consider due to the large data volumes and complexity involved are:\\n\\nAbility to monitor the query execution from the user perspective. Be able to answer the following questions:\\n\\nIs the query still running?\\n\\nIs the query elapsed time within the expected duration threshold for that type of query?\\n\\nIs the execution query engine up and running and processing query requests as expected?\\n\\nIs the query execution halted due to lack of resources?\\n\\nIs the query execution halted due to high concurrency in the query execution engine?\\n\\nAbility to cancel a query. Be able to answer the following questions:\\n\\nWas the query canceled successfully? If not, why?\\n\\nIs the elapsed time for cancellation within normal values?\\n\\nAn implementation example is shared in the next section. In this specific case, a Query scheduler service is included in addition to the REST API service. A Query scheduler is a service responsible to manage and schedule all query requests along with the integration with the query execution engine backend.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-distribution-through-rest-api.md'}, {'chunkId': 'chunk11_4', 'chunkContent': 'Tech Specific Samples\\n\\nThe following samples are example of different implementations for both scenarios described above.\\n\\nSynchronous Sample and Link (coming soon)\\n\\nAsynchronous Sample', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-distribution-through-rest-api.md'}, {'chunkId': 'chunk12_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Distribution\\n\\nData distribution covers the tools, techniques and patterns to distribute the data and analytics with a broader audience via an appropriate distribution channel (SQL table, report/dashboard, API, etc.).\\n\\nThe value of data lies in its practical application. Data has to be easily searchable and accessible for people to use it. The data should be organized in a data model that well represents the semantics of the business.\\n\\nIn many scenarios, a visual representation of the data will add more value than sharing the data itself. It's worth investing the time to build reports and dashboards to make sure critical information is communicated with a broader audience properly.\\n\\nDataOps: Data Modeling\\n\\nDataOps: Data Virtualization\\n\\nDataOps: Data Sharing\\n\\nData Visualization\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\index.md'}, {'chunkId': 'chunk13_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Modeling\\n\\nOverview\\n\\nData modeling is a process of reflecting the data landscape of an organization. It involves defining and analyzing data requirements that are critical to support the business needs and processes. Data models are at the core of data projects. They determine the design of the application and describe:\\n\\nThe types of data.\\n\\nThe structure of the data.\\n\\nThe relationship between data.\\n\\nHow data flows at different phases across organization.\\n\\nKey terms\\n\\nEntity - Something, which can be distinctly identified [1]. Ex: a person, a product\\n\\nRelationship/Relation - An association among entities [1]. Ex: \"father-son\" is a relationship between two \"person\" type entities\\n\\nWhy Data Modeling is Important?', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_1', 'chunkContent': \"Why Data Modeling is Important?\\n\\nData Modeling is important especially if the organization has a large data estate mainly because:\\n    - It helps create a simplified, logical database that eliminates redundancy, reduces storage requirements, and enables efficient retrieval.\\n    - Data modeling and data analytics go hand in hand. A quality data model is required to get the most impactful analytics for business intelligence that informs decision making.\\n    - Using a model-first approach to design solutions is a way to make each business unit look at how they contribute to holistic business goals.\\n    - Solid data model means optimized analytics performance, no matter how large and complex the data estate is—or becomes.\\n\\nCommon Design Considerations\\n\\nThere are many aspects to consider when data modeling an analytics solution:\\n\\nMaterialization\\nOne of the most important tools for building a top-notch data model is materialization. Materialization refers to whether or not a given relation is created as a table or as a view. If the relation is represented as a table, any required calculations are pre-computed resulting in faster query response times. If the relation is left as a view, the users will get more up-to-date data when they query, but response times will be slower.\\nIt's important to weigh-in the different tradeoffs based on the underlying storage/data technology choice.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_2', 'chunkContent': \"Permissions and Governance\\nIn the data model design, be mindful of where personally identifying customer information is stored. It's good practice to keep potentially personal identifying information separate from the rest of the relations.  However, the access to the sensitive information can also be controlled and implemented by using Column Level Security or Row Level Security.\\n The data models should be compatible with the data access policies that are in place.\\n\\nStorage usage/cost\\nDesign inefficiencies can lead to excessive storage usage, which can increase cost and degrades performance. Proper modeling for analytical systems can help reduce both the amount of data stored and its monetary and performance costs.\\n\\nPerformance\\nPoorly modeled data will lead to poor performance. Data modeling helps define and track target performance indicators to meet business objectives. Indexing of data is highly correlated with data modeling, and thus it should be a consideration for systems with optimal performance requirements.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_3', 'chunkContent': 'Capability matching with business processes\\nCapability matching with business processes is essential from two different view points, the first being usability of the data. The central theme around any information system should be usability, as the lack of it creates not only frustration from users but a system that is improperly used or not used at all. Beyond waste, consequences can also lead to inaccuracy when using data for important decision reporting (financial, tax, operational, etc.). An unusable or inaccurate system as a result of misalignment to business processes could lead to an overall failure for a data project.\\n\\nPortability\\nGood designs can be reimplemented in other systems in the future as poor designs are often complex and create unnecessary dependencies, which reduce the capability for a system to be upgraded, migrated, or implemented in another part of the business. Good designing practices in terms of modeling allow for an Analytical system to be efficient in many parts of an organization. It allows portability of the solution design and can play a role during migrations or even consolidations of systems. A couple of examples: consolidation of systems during mergers and acquisitions, bringing in third-party data for analysis.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_4', 'chunkContent': 'Expandability\\nIn modern data estates, there is no lack of growth. Modeling analytical systems for expandability is essential to any organization that wishes to remain modern and relevant in terms of the analytics space. Expandability is not just limited to logical design, but also physical design including selection of data file types to use for storing data. Some file types are more compatible with specific modeling strategies; for example, Parquet format is more useful for columnar data stored for analytical systems.\\n\\nData modeling life cycle\\n\\n2-4]. Next step is to identify the relationships among these entities followed by the data model development that reflects the business usage of data. Several of the common data modeling techniques are listed\\n\\nbelow. In this step, keys are assigned. The degree of\\n\\nnormalization that balances redundancy and performance requirements is also determined [\\n\\n4]. After implementing the data model, the process is to re-iterated to refine the model while monitoring for operational performance and changing business requirements. Usually, once determined - subject area level changes are rare compared to other steps of the data modeling process.\\n\\nAs seen in above data modeling process diagram, data modeling is an iterative process. The level of detail in the resulting data model increases with each iteration. Refer to the diagram below [3-5].', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_5', 'chunkContent': 'The process starts with the requirement gathering phase resulting in a conceptual model (also known as domain model). It is meant to capture the high-level details of a data system. It typically includes business entities and their relationships and relevant security and data integrity requirements. A logical data model builds on the conceptual model with specific data attributes within each entity. It also includes specific relationships between those attributes. The result is the technical model of the rules and data structures as defined by data architects and business analysts. Finally, a physical model is the specific implementation of the logical model and is developed by database admins/developers. A conceptual model is highly abstract in nature while a physical model is the least abstract of all [3,4].\\n\\nTypes of data modeling\\n\\nConsider a banking application, with the following requirements:\\n\\nStore transactional and operational data in a relational model (OLTP).\\n\\nUse an analytical system (OLAP) for analytical and reporting purposes.\\n\\nAs seen in the above example, most applications are built by layering one data model on top of another. It is common to see multiple types of databases and multiple modeling techniques that are employed by a single modern big data system. The diagram - Types of Data Modeling shows different data modeling techniques and how they are employed in a data system [4,6].\\n\\nHierarchical data modeling\\n\\n4]. This model is great at representing one-to-many relationships and providing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_6', 'chunkContent': \"Hierarchical data modeling\\n\\n4]. This model is great at representing one-to-many relationships and providing\\n\\nrelational modeling were proposed. A\\n\\n6].\\n\\nRelational data modeling\\n\\nThe relational data modeling presents information as collection of records (tables) and relationships between those collections. Most of the data systems currently in use employ relational data models to store their transactional/operational data (OLTP use case). These systems employ SQL for data management, and are great for storing many-to-many relationships. They also maintain data integrity and minimize data redundancy [4,6].\\n\\n{% if extra.ring == 'internal' %}\\nRefer to DataOps: RDBMS page for more details on this topic.\\n\\n{% endif %}\\n\\nEntity-Relationship data modeling\\n\\nThe entity-relationship[ER] modeling adopts the more natural view that the real world consists of entities and relationships. This model can be used to provide a unified view of data and to convey database design objectives [1,2]. An ER diagram can be used to refer any type of data model i.e., it can be a conceptual or a logical or a physical data model.\\n\\nObject-oriented data modeling\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_7', 'chunkContent': 'Object-oriented data modeling\\n\\nObject-oriented data modeling became popular with the rise of  object-oriented programming and can be thought of as a combination of  object-oriented programming and  relational database model. The objects involved are abstractions of real-world entities. Objects are grouped in class hierarchies, and have associated features. Object-oriented databases can incorporate tables, but can also support more complex data relationships. This approach is commonly employed in multimedia and hypertext databases [4].\\n\\nDimensional data modeling\\n\\nDimensional models aimed at analytical and reporting purposes - OLAP use case-, and are designed to optimize for data retrieval speeds in a data warehouse environment. Unlike relational and ER models, which emphasize efficient storage, dimension models increase redundancy(denormalization) in favor of query/data retrieval performance [4]. Popular dimensional data models are: Star schema and Snowflake schema.\\n\\nRefer to slowly changing dimensions for different types of SCDs with examples. The following table summarizes the common use case for each of the [7]:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_8', 'chunkContent': 'SCD type Use case scenarios SCD type 0 Durable data like constants, date dimensions SCD type 1 Only current version of truth available, no need for historical data SCD type 2 Need historical versions of data and the periods during which they were current SCD type 3 Need for current data and the previous last value ( alternate reality ) SCD type 4 Used when a group of attributes in a dimension rapidly changes and is split off to a mini–dimension ( rapidly changing monster dimension. ) SCD type 5 Rarely used - to accurately preserve historical attribute values, plus report historical facts according to current attribute values; SCD 5 is equivalent to SCD 1 + SCD 4 SCD type 6 Rarely used - Unpredictable Changes with Single-Version Overlay; SCD 6 is equivalent to SCD 1 + SCD 2 + SCD 3 SCD type 7 Rarely used - Hybrid technique that supports both as-was and as-is reporting\\n\\nGraph data modeling\\n\\n6].\\n\\nThe section on DataOps: Graph Database Best Practices has information on common considerations before choosing a graph db.\\n\\nDesigning for Online Analytical Processing (OLAP)', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_9', 'chunkContent': 'Designing for Online Analytical Processing (OLAP)\\n\\nDesigning for Online Analytical Processing (OLAP) systems for data storage and usage is different than designing for OLTP systems. The approach with analytical systems is to analyze and process data in a columnar fashion. Data is retrieved and processed at mass as opposed to at the transactional level. Data here is processed in bulk for decisions to be made across specific \"view points\" of the data, which is where dimensional modeling comes into play. Dimensional modeling is built on denormalized modeling approaches. In short, data duplication and relationships are reduced between the number of dimension tables that relates with the main table. These \"views points\" or dimensions of the data are how end users will view or analyze the data. For example, data analyzed from the perspective (dimension) of geographic or date and time.  This data is processed, retrieved and analyzed longitudinally over the mass of the data. In the other end, OLTP systems use a fine-grained approach.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_10', 'chunkContent': 'In the simplified OLAP design above, most of the relationships are between dimension and fact tables.  When dimensions are related directly to the fact tables, it is in a star schema design. If a dimension is related to a fact via another dimension, then the design starts to follow an approach known as snowflake design. In this example, the results of the data are aggregated in a columnar format. In many analytical systems, data is stored in a columnar format in order to enable high levels of performance over millions, billions, or even trillions of rows.\\n\\nDataOps: Slowly Changing Dimension\\n\\nReporting considerations\\n\\nOne of the important aspects of designing reporting for OLAP is to consider the needs and preferences of the end users. OLAP reports should provide relevant, accurate and timely information that can support decision making and analysis. Some of the factors that should be taken into account when designing OLAP reports are:\\n\\nThe level of detail and aggregation that the users require. Some users want to see high-level summaries and trends, while others need to drill down to the underlying data and details.\\n\\nThe type and format of the visualizations that the users prefer. Some users may like to see charts, graphs, maps and dashboards, while others may prefer tables, matrices and lists.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_11', 'chunkContent': \"The frequency and mode of delivery of the reports. Some users want to access the reports on demand, while others need scheduled or automated delivery via email, web or mobile devices.\\n\\nThe performance and scalability of the reporting system. The reports should be able to handle large volumes of data and complex queries without compromising on speed and quality.\\n\\nBy considering these factors, the reporting system can be designed to meet the expectations and requirements of the end users and provide them with valuable insights from the OLAP data.\\n\\nDesigning for a data lake\\n\\nA data lake is a centralized repository that stores raw and structured data from various sources. Such sources may include databases, applications, sensors, and social media. A data lake allows users to access and analyze data without requiring a predefined schema or structure. Therefore, it enables users to explore and discover new insights from diverse and complex data sets.\\n\\nData modeling for data lake is different from traditional data modeling for relational databases or data warehouses. Data modeling for data lake does not require a predefined schema or structure for the data. Instead, it allows users to apply schemas on read or on write, depending on their needs and preferences.\\n\\nDespite of the difference, it doesn't mean that data modeling for data lake is irrelevant or optional. On the contrary, data modeling for data lake is crucial for achieving the benefits of data lake, such as scalability, performance, cost reduction, and flexibility.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_12', 'chunkContent': 'Some of the benefits of implementing a data lake are:\\n\\nScalability: A data lake can scale horizontally to store and process large volumes of data. A data lake can also use cloud-based\\nservices and technologies to provide elasticity and cost-efficiency.\\n\\nFlexibility: A data lake can accommodate any type of data, whether it is structured, semi-structured, or unstructured. A data lake can also support multiple formats, such as JSON, XML, CSV, Parquet, or Avro.\\n\\nAgility: A data lake can enable faster and easier data ingestion and integration. A data lake can also support various analytics tools and frameworks, such as SQL, Spark, Hadoop, or Machine Learning.\\n\\nInnovation: A data lake can foster innovation and experimentation by allowing users to explore and discover new insights from diverse and complex datasets. A data lake can also enable advanced analytics, such as predictive modeling, sentiment analysis, or anomaly detection.\\n\\nDesigning a data lake involves several challenges in three main categories: governance, performance, and usability.\\n\\nGovernance ensures the quality, security, privacy, compliance and lineage of the data in the data lake. Bear in mind, that without proper governance, a data lake can suffer from rapid growth of data proliferation, data swamp and lack of metadata. These problems can compromise its value and integrity. Therefore, governance is a crucial challenge in designing a data lake.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_13', 'chunkContent': 'Performance optimizes storage and processing costs, availability, reliability, latency and complexity of the data lake. Without proper performance, a data lake can face scalability, concurrency, workload spikes, and resource utilization issues. These issues can affect its functionality and scalability. Therefore, performance is another important challenge in designing a data lake.\\n\\nUsability provides:\\n\\nUser-friendly interfaces and tools.\\n\\nEffective search.\\n\\nDiscovery capabilities.\\n\\nCollaboration and communication among users.\\n\\nFurthermore, without proper usability, a data lake can create user interface, search and discovery, collaboration and communication, and skills and expertise difficulties. These difficulties can affect its accessibility and utility.\\n\\nSome of the best practices for designing a data lake are:\\n\\nDefine clear objectives and use cases for your data lake. Identify the sources and types of data you need to store and analyze. Determine the users and stakeholders who will access and use your data lake. Establish the metrics and outcomes you want to achieve with your data lake.\\n\\nDesign a logical architecture for your data lake. Organize your data into zones or layers based on its purpose and characteristics. For example, you can use:\\n\\nA landing zone for raw and unprocessed data.\\n\\nA curated zone for cleansed and standardized data.\\n\\nAn enriched zone for transformed and enriched data.\\n\\nAn aggregated zone for summarized and aggregated data.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_14', 'chunkContent': 'An enriched zone for transformed and enriched data.\\n\\nAn aggregated zone for summarized and aggregated data.\\n\\nAn analytical zone for analytical models and outputs\\n\\nAn archive zone for historical or infrequently accessed data.\\n\\nTake in consideration how to optimize the ingestion and query of the data in each zone. For ingestion, one should use appropriate tools and methods to load the data efficiently and reliably. For query, appropriate formats and partitioning schemes should be used to enable fast and flexible access to the data.\\n\\nImplement a governance framework for the data lake. Define the policies and procedures for managing data quality, security, privacy, compliance, and lineage. Implement the tools and processes for monitoring and auditing the data lake operations and performance. Use metadata and cataloging to document and describe the data assets and schemas.\\n\\nReporting considerations\\n\\nOne of the important aspects of designing reporting for data lake is to ensure optimal data performance.\\nDirect querying of the data lake can be inefficient and costly, especially for complex or frequent queries that involve large amounts of data. For that reason, it is advisable to use pre-aggregate results instead of direct querying the lake.\\nPre-aggregate results are summary statistics or intermediate calculations that are stored  in the data lake or another storage system. They can reduce the amount of scanned and processed data. Pre-aggregate results can improve the query performance and reduce the resource consumption.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk13_15', 'chunkContent': 'Another possible solution is to use a caching system or a different service like DBMS or NoSQL to store the aggregated results. A caching system is a temporary storage layer that keeps frequently accessed or recently computed data in memory or disk for faster retrieval. A caching system can improve the reporting performance by avoiding redundant computations and reducing the latency of accessing the data lake. A different service like RDBMS or NoSQL is a permanent storage layer that can store structured or semi-structured data in a more efficient and scalable way than the data lake. A different service can improve the reporting performance by providing:\\n\\nFaster query execution.\\n\\nBetter concurrency control.\\n\\nMore advanced features like indexing, partitioning, and replication.\\n\\nData modeling tools\\n\\nWikipedia has a comparison of some of data modeling tools.\\n\\nReferences\\n\\nER model - Research paper by Peter Chen\\n\\nER model - Wikipedia\\n\\nData modeling - Microsoft\\n\\nData modeling - IBM\\n\\nData modeling - Wikipedia\\n\\nData models and query languages - Chapter 2 - Designing Data-Intensive Applications by Martin Kleppmann\\n\\nDimensional-modeling-techniques\\n\\nFurther reading\\n\\nBest-practices-for-dimensional-model-using-dataflows\\n\\nGuide-to-uml-diagramming-and-database-modeling\\n\\nData-modeling-on-Power-Platforms\\n\\nSynapse Database Templates', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\index.md'}, {'chunkId': 'chunk14_0', 'chunkContent': \"rings:\\n- public\\n\\nOverview\\n\\nIn data warehousing scenarios, it's common to create a consolidated presentation layer table from multiple source tables. When dealing with slowly changing dimensions (SCD), especially SCD Type 2, it becomes essential to maintain historical changes accurately.\\n\\nThis article starts by introducing the concept of driving and referential tables and then proposes a solution to handle updates for such entities. It also explores the exceptional cases where updates occur in both driving and referential tables for a certain natural key.\\n\\nBusiness Scenario\\n\\nIn this scenario, there are three delta tables that acts as the source:\\n\\nemployee\\n\\nemployee_address\\n\\nemployee_information\\n\\nThe aim is to create a single unified target table called target_employee that can provide a consolidated view of these tables. This presentation layer table also holds the versioning for all the three source tables.\\n\\nThe SCD Type 2 logic is implemented in a pyspark notebook. The following diagram provides the high-level architecture:\\n\\nApplied Concepts and Capabilities\\n\\nHere are the key concepts and delta table capabilities that are used for this use case.\\n\\nDriving and Referential Tables\\n\\nThe driving table represents the primary or key source table from which most of the final entity's information is derived. In this case, employee table is the driving table.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_1', 'chunkContent': 'The referential tables act as additional sources, providing supplementary details to enrich the target entity. In this case, employee_address and employee_information tables are the referential tables.\\n\\nChange Data Feed\\n\\nChange data feed allows Azure Databricks to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records change events for all the data written into the table.\\n\\nFor this use case, the change data feed has been enabled for all the three source tables.\\n\\nDelta table Properties\\n\\nHere are some other characteristics of Delta tables that are used in this use case:\\n\\nEach operation that modifies a Delta Lake table creates a new table version.\\n\\nDelta tables offer \"time travel\" capabilities, allowing users to query table\\'s previous state based on timestamp or table version.\\n\\nThe historic information can be used to audit operations, rollback a table, or query a table at a specific point in time using time travel.\\n\\nSample Data\\n\\nThe use case is explained based on the following sample data.\\n\\nEmployee\\n\\n```txt\\n employee_id  first_name  last_name  email                         create_update_date', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_2', 'chunkContent': '100          Steven      King       steven.king@contoso.com       2023-01-27\\n 101          Neena       Kochhar    neena.kocchar@contoso.com     2023-02-09\\n 102          Lex         De Haan    lex.de-haan@contoso.com       2023-04-07\\n 103          Alexander   Hunold     alexander.hunold@contoso.com  2023-02-24\\n 104          Bruce       Ernst      bruce.ernst@contoso.com       2023-04-24', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_3', 'chunkContent': '```\\n\\nEmployee Address\\n\\n```txt\\nemployee_id  city      region         street_address      country         create_update_date', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_4', 'chunkContent': '100          New York  NY             123 Main St         United States   2023-01-27\\n101          London    England        456 Park Ave        United Kingdom  2023-02-09\\n102          Paris     ële-de-France  789 Rue de la Paix  France          2023-04-07\\n103          Tokyo     Tokyo          1-2-3 Shibuya       Japan           2023-02-24\\n104          Sydney    NSW            456 George St       Australia       2023-04-24', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_5', 'chunkContent': '```\\n\\nEmployee Information\\n\\n```txt\\nemployee_id  salary  is_fte  is_remote  employment_date  create_update_date\\n\\n100          5000    True    False      2020-01-15       2023-01-27\\n102          5500    True    False      2021-03-22       2023-04-07\\n101          6000    True    True       2019-05-10       2023-02-09\\n103          5200    True    True       2018-11-30       2023-02-24\\n104          5800    True    False      2017-09-12       2023-04-24', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_6', 'chunkContent': '```\\n\\nTarget Employee\\n\\n```txt\\nid                               employee_id first_name last_name     email                    city     region        street_address     country        salary is_fte is_remote employment_date employee_create_update_date address_create_update_date info_create_update_date address_commit_version info_commit_version employee_commit_version valid_flag valid_from valid_to', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_7', 'chunkContent': 'd4d0569aa28e28d83a3d9a8888a30a27 101         Neena      Kochhar   neena.kocchar@contoso.com    London   England       456 Park Ave       United Kingdom 6000   True   True      2019-05-10      2023-02-09                  2023-02-09                 2023-02-09              1                      1                   1                       1          2023-02-09 9999-12-31', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_8', 'chunkContent': 'be613e2a8166f377f65967bd9073188d 102         Lex        De Haan   lex.de-haan@contoso.com      Paris    ële-de-France 789 Rue de la Paix France         5500   True   False     2021-03-22      2023-04-07                  2023-04-07                 2023-04-07              1                      1                   1                       1          2023-04-07 9999-12-31', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_9', 'chunkContent': '5a60257eabea5111d4e8aa0cf56fb109 103         Alexander  Hunold    alexander.hunold@contoso.com Tokyo    Tokyo         1-2-3 Shibuya      Japan          5200   True   True      2018-11-30      2023-02-24                  2023-02-24                 2023-02-24              1                      1                   1                       1          2023-02-24 9999-12-31', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_10', 'chunkContent': '014830e89472a8ac527a5c6b2c1e5fe5 100         Steven     King      steven.king@contoso.com      New York NY            123 Main St        United States  5000   True   False     2020-01-15      2023-01-27                  2023-01-27                 2023-01-27              1                      1                   1                       1          2023-01-27 9999-12-31', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_11', 'chunkContent': '3b2c89840dc66c48dedbbb8ecac15ada 104         Bruce      Ernst     bruce.ernst@contoso.com      Sydney   NSW           456 George St      Australia      5800   True   False     2017-09-12      2023-04-24                  2023-04-24                 2023-04-24              1                      1                   1                       1          2023-04-24 9999-12-31', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_12', 'chunkContent': '```\\n\\nThe target target_employee table is created from driving employee table and the referential employee_address and employee_information tables. There are following additional columns for tracking changes from each of these source tables:\\n\\nemployee_create_update_date: Tracks the created/updated date of the record for the employee table.\\n\\naddress_create_update_date: Tracks the created/updated date of the record for the address table.\\n\\ninfo_create_update_date: Tracks the created/updated date of the record for the information table.\\n\\nemployee_commit_version: Tracks the commit version of the record from the employee table.\\n\\naddress_commit_version: Tracks the commit version of the record from the address table.\\n\\ninfo_commit_version: Tracks the commit version of the record from the information table.\\n\\nvalid_flag: Tracks if the record is the latest for that natural key or a stale record.\\n\\nvalid_from: Tracks from when the record is active.\\n\\nvalid_to: Tracks to when the record is active.\\n\\nHandled Scenarios\\n\\nThere are two scenarios that are handled in this use case:\\n\\nScenario 1: Multiple Updates\\n\\nIn this scenario, data for a particular natural key is updated in both the driving and referential tables.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_13', 'chunkContent': \"In this scenario, data for a particular natural key is updated in both the driving and referential tables.\\n\\nTo implement SCD Type 2 for this scenario, a two-step process is followed.\\n\\nStep 1: Separate Capture of Updates\\n\\nWhen an update occurs in the driving table, it is captured separately and marked with an appropriate version number, signifying the change. Similarly, when a change happens in any of the referential tables, it is also captured independently, retaining its own version number.\\n\\nStep 2: Union of Updates\\n\\nTo obtain a comprehensive view of all the updates made to a particular record, a union of the updates from both the driving and referential tables is performed. This union ensures that all relevant changes are accounted for, and the historical chain of updates remains intact.\\n\\nScenario 2: Simultaneous Updates\\n\\nIn certain exceptional cases, both the driving and referential tables might receive updates for the same natural key simultaneously. To maintain the integrity of historical data, the proposal is to create a historical chain of updates.\\n\\nIn this scenario, the driving table's update is treated as the primary update, and the relevant referential table updates are linked to it in a chronological order. This approach preserves the historical context and provides a comprehensive view of all changes made to the record.\\n\\nConsider a scenario with following updates:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_14', 'chunkContent': 'Consider a scenario with following updates:\\n\\n```sql\\n-- Apply some updates to \"employee\" table.\\nupdate employee set first_name = \\'Steven2\\',  dt = \\'2023-07-13\\' where employee_id = \\'100\\';\\nupdate employee set first_name = \\'Steven3\\' , dt = \\'2023-07-17\\' where employee_id = \\'100\\';\\n\\n-- Apply some updates to \"employee_address\" table.\\nupdate employee_address set city = \\'Lille\\',     dt = \\'2023-07-13\\'                where employee_id = \\'102\\';\\nupdate employee_address set city = \\'NewCastle\\', dt = \\'2023-07-14\\'                where employee_id = \\'104\\';\\nupdate employee_address set street_address = \\'456 Baker Street\\', dt =\\'2023-07-16\\' where employee_id = \\'100\\';', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_15', 'chunkContent': '-- Apply some updates to \"employee_information\" table.\\nupdate employee_information set salary = 7000,  dt = \\'2023-07-17\\' where employee_id = \\'108\\';\\nupdate employee_information set salary = 8000,  dt = \\'2023-07-16\\' where employee_id = \\'102\\';\\nupdate employee_information set is_fte = False, dt = \\'2023-07-15\\' where employee_id = \\'104\\';', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_16', 'chunkContent': '```\\n\\nImplementation Logic\\n\\nHere is the high-level implementation logic for this use case:\\n\\nGather the changes (updates and inserts) that have arrived in all three tables.\\n\\nRead the commit versions stored in target_employee table for employee, employee_address and employee_information tables and get the greatest value.\\n\\nGet the changes that have a commit version greater than the calculated value in the previous step. Do it for all the three tables.\\n\\nCreate a change dataframe for each source table.\\n\\nCreate a dataframe capturing referential changes from employee_address and employee_information tables.\\n\\nread the entire driving table employee to get all natural keys.\\n\\nPerform a join with change dataframe that captures updates for records of employee_address and employee_information tables\\n\\nCreate a new column changed_commit which will hold a value, if an update has occurred for that natural key in either address or information table\\n\\nFilter out the records with natural key for which an update has happened using changed_commit column.\\n\\nCreate a dataframe capturing driving table changes\\n\\nread the updates of the driving table and join it with changes from address and information table\\n\\nUnion the driving and referential table changes to get all the changes\\n\\nApply the merge logic\\n\\nMethods to Track SCD Type 2\\n\\nThe solution describes two methods of tracking SCD type 2 for the target entity.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_17', 'chunkContent': 'Methods to Track SCD Type 2\\n\\nThe solution describes two methods of tracking SCD type 2 for the target entity.\\n\\nMethod 1: Capture latest updates only\\n\\nIn this method, if multiple updates are coming from sources for each record, only the latest updates are captured. To get all the updates, following logic is applied:\\n\\npython\\nspark.sql(\"\"\"\\n  select *\\n    from (select *,\\n                row_number() over (partition by employee_id\\n                                       order by changed_commit desc) as rank\\n            from changes_employee_target)\\n   order by\\n         employee_id,\\n         changed_commit desc\\n\"\"\")\\n\\nAnd then, the latest updates are captured using the following query:\\n\\npython\\nspark.sql(\"\"\"\\n  select *\\n    from employee_all_updates\\n   where rank=1\\n\"\"\")', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_18', 'chunkContent': 'Consider the scenario, where for a certain natural key, an update has happened in the employee_address table but not in other tables. While capturing the changes, the columns corresponding to other tables will show as null that is not a right representation of the update. This is how the record will look like:\\n\\n```txt\\nemployee_id first_name last_name email                   city     region street_address   country       salary is_fte is_remote employment_date employee_create_update_date address_create_update_date info_create_update_date address_commit_version info_commit_version employee_commit_version valid_flag valid_from                   valid_to                     changed_commit rank', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_19', 'chunkContent': '100         Steven3    King      steven.king@contoso.com New York NY     456 Baker Street United States null   null   null      null            2023-07-17                  2023-07-13                 null                    4                      null                4                       1          2023-07-17 9999-12-31 4              1', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_20', 'chunkContent': '```\\n\\nTo solve this, the coalesce function is used to capture values of columns in the target_employee table that have not changes. This approach represents the more accurate view of the record.\\nNote that the above approach does not cover the edge case where the actual value being updated for a column is Null.\\n\\nAfter coalescing, the record looks like:\\n\\n```txt\\nemployee_id first_name last_name email                     city     region street_address   country       salary   is_fte   is_remote employment_date employee_create_update_date address_create_update_date info_create_update_date address_commit_version info_commit_version employee_commit_version valid_flag valid_from                   valid_to                     _commit_version', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_21', 'chunkContent': '100         Steven3    King      steven.king@contoso.com New York NY     456 Baker Street United States 5000 True False 2020-01-15      2023-07-17                  2023-07-13                 2023-01-27              4                      1                   4                       1          2023-07-17 9999-12-31 4', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_22', 'chunkContent': '```\\n\\nThe values for columns is_fte, is_remote, and salary are retained as these column values have not change.\\n\\nBelow, only the latest update has been considered. The date of job run (\"2023-08-03\") is used to derive valid_from and valid_to columns.\\n\\n```txt\\nemployee_id first_name last_name email                     city     region street_address   country       salary is_fte is_remote employment_date employee_create_update_date address_create_update_date info_create_update_date address_commit_version info_commit_version employee_commit_version valid_flag valid_from valid_to', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_23', 'chunkContent': '100         Steven     King      steven.king@contoso.com New York NY     123 Main St      United States 5000   True   False     2020-01-15      2023-01-27                  2023-01-27                 2023-01-27              1                      1                   1                       0          2023-01-27 2023-08-11', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_24', 'chunkContent': '100         Steven3    King      steven.king@contoso.com New York NY     456 Baker Street United States 5000   True   False     2020-01-15      2023-07-17                  2023-07-13                 2023-01-27              4                      1                   4                       1          2023-08-11 9999-12-31', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_25', 'chunkContent': '```\\n\\nMethod 2: Historical chaining of records\\n\\nIn this method, all the updates that happen for a certain id are captured and a chain of history is maintained for those updates.\\n\\nThe lead function takes the next start date for the same natural key, ordered by date (dt). The row_number function ranks the record in the order of modification in the source table.\\n\\nsql\\n  lead(greatest(employee_create_update_date, address_create_update_date, info_create_update_date))\\n        over (partition by employee_id\\n                  order by employee_create_update_date) as valid_to,\\n   ---take the greatest value of dates\\n  row_number()\\n        over (partition by employee_id\\n                  order by employee_create_update_date) rn\\n\\nStore all the changed records in a table called as changes_history\\n\\nUpdate valid_flag=1 and valid_to= <infinite_end_date> for highest rank record\\n\\nProceed to close the current record in target table using update sql operation', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_26', 'chunkContent': 'Proceed to close the current record in target table using update sql operation\\n\\nsql\\nupdate target_employee as t\\n   set valid_to = (\\n         select first(valid_to) --- rn=1 stored in changes_history corresponds to original record in target table . Use the calculated valid_to to hence update original record\\n           from changes_history as c\\n          where c.employee_id = t.employee_id\\n            and c.rn = 1),\\n       valid_flag = 0\\n where employee_id IN (\\n          select employee_id\\n            from changes_history\\n           where rn = 1)\\n---latest records now become stale records\\n   and valid_flag = 1;\\n\\nInsert the records into target table having rank > 1', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_27', 'chunkContent': 'Insert the records into target table having rank > 1\\n\\nBelow is an example of how the historical chain of updates is maintained for a natural key.\\nHere, all the prior updates are considered to maintain a chain of history. The valid _from and valid_to columns consider the dt column in the source that indicates when the row was added or updated (in this case 2023-07-13, 2023-07-17).\\n\\n```txt\\nid                               employee_id first_name last_name email                   city     region street_address   country       salary is_fte is_remote employment_date employee_create_update_date address_create_update_date info_create_update_date address_commit_version info_commit_version employee_commit_version valid_flag valid_from valid_to', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_28', 'chunkContent': '5e64f88f256b598f6c8a801b8d560285 100         Steven2    King      steven.king@contoso.com New York NY     456 Baker Street United States 5000   True   False     2020-01-15      2023-07-13                  2023-07-13                 2023-01-27              4                      1                   3                       0          2023-07-13 2023-07-17', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_29', 'chunkContent': '08f663dec02e58679ba6e6af5cf67882 100         Steven3    King      steven.king@contoso.com New York NY     456 Baker Street United States 5000   True   False     2020-01-15      2023-07-17                  2023-07-13                 2023-01-27              4                      1                   4                       1          2023-07-17 9999-12-31', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_30', 'chunkContent': '014830e89472a8ac527a5c6b2c1e5fe5 100         Steven     King      steven.king@contoso.com New York NY     123 Main St      United States 5000   True   False     2020-01-15      2023-01-27                  2023-01-27                 2023-01-27              1                      1                   1                       0          2023-01-27 2023-07-13', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk14_31', 'chunkContent': '```\\n\\nCode\\n\\nReference Notebook\\n\\nConclusion\\n\\nIn this article, an advanced SCD type 2 implementation has been introduced for capturing changes originating from multiple source tables. The concepts of driving and referential tables, along with the discussion of change data feeds, were presented. Building upon these concepts, various techniques for capturing the most recent changes and establishing a historical chain for the presentation layer have been explored.\\n\\nReference\\n\\nFor the implementation of SCD with Databricks, see dbxscd project.\\n\\nFurther Reading\\n\\nDataOps: Slowly Changing Dimension\\n\\nDataOps: Data Modeling', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\scd-type2-multiple-tables.md'}, {'chunkId': 'chunk15_0', 'chunkContent': \"rings:\\n  - public\\n\\nSlowly Changing Dimension\\n\\nOverview\\n\\nWhen modeling modern data application, a Slowly Changing Dimension (SCD) can be introduced that stores and manages both current and historical data over time. Basically, for different business scenarios, there are different types of SCD. The most common types are Type 1, Type 2, and Type 3. The following list shows the differences between these types.\\n\\nSlowly Changing Dimensions Type1 (SCD1): No history preservation. It overwrites old data with new one, and therefore does not track historical data.\\n\\nSlowly Changing Dimensions Type2 (SCD2): Unlimited history is preserved for each insert. It tracks historical data by creating multiple records for a given natural key in the dimension tables.\\n\\nSlowly Changing Dimensions Type3 (SCD3): Limited history preservation, it tracks changes using separate columns and preserves limited history.\\n\\nLet's take a look at the following example to understand the differences between these types. The following table shows a sample data of fruits and their prices.\\n\\nShortName Fruit Color Price GG Green Grape Green 2.0 FA Fiji Apple Red 3.5 BN Banana Yellow 1.0\\n\\nAnd then the price of Fiji Apple is changed into 4.0 and a new fruit Red Grape is added into the table.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_1', 'chunkContent': 'And then the price of Fiji Apple is changed into 4.0 and a new fruit Red Grape is added into the table.\\n\\nShortName Fruit Color Price GG Green Grape Green 2.0 FA Fiji Apple Red 4.0 BN Banana Yellow 1.0 RG Red Grape Red 2.5\\n\\nSCD1\\n\\nTable Structure of SCD1\\n\\nTo implement SCD1, the following extra columns are added to the original data:\\n\\nkey_hash: the hash value of the key column. In this example, the key columns are ShortName and Fruit, and the value is the hash value of the concatenation of ShortName and Fruit.\\n\\ndata_hash: the hash value of the data column. In this example, the data columns are Color and Price, and the value is the hash value of concatenation of Color and Price.\\n\\ndw_insert_ts: the timestamp of the insert operation.\\n\\ndw_update_ts: the timestamp of the update operation.\\n\\nThe key_hash is used to identify a record, and the data_hash is used to identify if the data has changed.\\n\\nOriginal records of SCD1', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_2', 'chunkContent': 'Original records of SCD1\\n\\nShortName Fruit Color Price key_hash data_hash dw_insert_ts dw_update_ts GG Green Grape Green 2 4bb08b043be3da18c3e9c01fa484a30c 2725b5d2846ec3d6f79c254132719b4f 2022-09-30T06:16:29.000+0000 2022-09-30T06:16:29.000+0000 FA Fiji Apple Red 3.5 18fa2df6b8fadb816dbb608ec0731274 82ac239d4ccf6603d373fe5a92df73d9 2022-09-30T06:16:29.000+0000 2022-09-30T06:16:29.000+0000 BN Banana Yellow 1 3a9e0d6d4c67616c6ec27c67cf3e561d df5c5b76a127715bd35a5bb93b482247 2022-09-30T06:16:29.000+0000 2022-09-30T06:16:29.000+0000\\n\\nUpdated records of SCD1', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_3', 'chunkContent': 'Updated records of SCD1\\n\\nSCD1 stands for no history preservation, the price of Fiji Apple is changed into 4 and the data_hash and dw_update_ts are updated. And the Red Grape is added into the table.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_4', 'chunkContent': 'ShortName Fruit Color Price key_hash data_hash dw_insert_ts dw_update_ts GG Green Grape Green 2 4bb08b043be3da18c3e9c01fa484a30c 2725b5d2846ec3d6f79c254132719b4f 2022-09-30T07:01:07.000+0000 2022-09-30T07:01:07.000+0000 FA Fiji Apple Red 4 18fa2df6b8fadb816dbb608ec0731274 8a7df8a63a114cf96577fb4173d1b716 2022-09-30T07:01:07.000+0000 2022-09-30T07:01:45.000+0000 BN Banana Yellow 1 3a9e0d6d4c67616c6ec27c67cf3e561d df5c5b76a127715bd35a5bb93b482247 2022-09-30T07:01:07.000+0000 2022-09-30T07:01:07.000+0000 RG Red Grape Red 2 d28a4e648dd37db4bee9531ceae8cdbf 65e44fd552ae20dad64a8cf50b92067b', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_5', 'chunkContent': '65e44fd552ae20dad64a8cf50b92067b 2022-09-30T07:01:45.000+0000 2022-09-30T07:01:45.000+0000', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_6', 'chunkContent': \"SCD2\\n\\nTable Structure  of SCD2\\n\\nBesides the columns in SCD1, the following columns are added.\\n\\nrow_latest_ind: the indicator of the latest record, it is set to Y for the latest record, and N for the historical records.\\n\\nrow_valid_from_ts: the timestamp of the record is valid from.\\n\\nrow_valid_to_ts: the timestamp of the record is valid to. '9999-12-31T00:00:00.000+0000' refers to no end date.\\n\\nOriginal records of SCD2\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_7', 'chunkContent': 'ShortName Fruit Color Price key_hash data_hash dw_insert_ts dw_update_ts row_latest_ind row_valid_from_ts row_valid_to_ts GG Green Grape Green 2 4bb08b043be3da18c3e9c01fa484a30c 2725b5d2846ec3d6f79c254132719b4f 2022-09-30T09:58:25.000+0000 2022-09-30T09:58:25.000+0000 Y 2022-09-30T09:58:25.000+0000 9999-12-31T00:00:00.000+0000 FA Fiji Apple Red 3.5 18fa2df6b8fadb816dbb608ec0731274 82ac239d4ccf6603d373fe5a92df73d9 2022-09-30T09:58:25.000+0000 2022-09-30T09:58:25.000+0000 Y 2022-09-30T09:58:25.000+0000 9999-12-31T00:00:00.000+0000 BN Banana Yellow 1 3a9e0d6d4c67616c6ec27c67cf3e561d', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_8', 'chunkContent': 'BN Banana Yellow 1 3a9e0d6d4c67616c6ec27c67cf3e561d df5c5b76a127715bd35a5bb93b482247 2022-09-30T09:58:25.000+0000 2022-09-30T09:58:25.000+0000 Y 2022-09-30T09:58:25.000+0000 9999-12-31T00:00:00.000+0000', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_9', 'chunkContent': \"Updated records of SCD2\\n\\nSCD2 stands for unlimited history preserved. The result has two rows of 'Fiji Apple'. One row contains the new price '4.0' with ROW_LATEST_IND column value set to 'Y'. The other row contains the old version of the price '3.5' with ROW_LATEST_IND column value set to 'N'. And the 'Red Grape' is added to the table.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_10', 'chunkContent': 'ShortName Fruit Color Price key_hash data_hash dw_insert_ts dw_update_ts row_latest_ind row_valid_from_ts row_valid_to_ts FA Fiji Apple Red 3.5 18fa2df6b8fadb816dbb608ec0731274 82ac239d4ccf6603d373fe5a92df73d9 2022-09-30T09:58:25.000+0000 2022-09-30T09:59:45.000+0000 N 2022-09-30T09:58:25.000+0000 2022-09-30T09:59:45.000+0000 FA Fiji Apple Red 4 18fa2df6b8fadb816dbb608ec0731274 8a7df8a63a114cf96577fb4173d1b716 2022-09-30T09:58:45.000+0000 2022-09-30T09:59:45.000+0000 Y 2022-09-30T09:59:45.000+0000 9999-12-31T00:00:00.000+0000 GG Green Grape Green 2 4bb08b043be3da18c3e9c01fa484a30c 2725b5d2846ec3d6f79c254132719b4f', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_11', 'chunkContent': '2725b5d2846ec3d6f79c254132719b4f 2022-09-30T09:58:25.000+0000 2022-09-30T09:58:25.000+0000 Y 2022-09-30T09:58:25.000+0000 9999-12-31T00:00:00.000+0000 BN Banana Yellow 1 3a9e0d6d4c67616c6ec27c67cf3e561d df5c5b76a127715bd35a5bb93b482247 2022-09-30T09:58:25.000+0000 2022-09-30T09:58:25.000+0000 Y 2022-09-30T09:58:25.000+0000 9999-12-31T00:00:00.000+0000 RG Red Grape Red 2 d28a4e648dd37db4bee9531ceae8cdbf 65e44fd552ae20dad64a8cf50b92067b 2022-09-30T09:59:45.000+0000 2022-09-30T09:59:45.000+0000 Y 2022-09-30T09:59:45.000+0000 9999-12-31T00:00:00.000+0000', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_12', 'chunkContent': 'SCD3\\n\\nTable Structure of SCD3\\n\\nBesides the columns in SCD1, the following columns are added.\\n\\nCOLOR_OLD: the previous color of the record.\\n\\nPRICE_OLD: the previous price of the record.\\n\\nHAS_OLD_VERSION: the flag to indicate if the record has old version.\\n\\nOriginal records of SCD3', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_13', 'chunkContent': 'HAS_OLD_VERSION: the flag to indicate if the record has old version.\\n\\nOriginal records of SCD3\\n\\nShortName Fruit Color Price key_hash data_hash COLOR_OLD PRICE_OLD HAS_OLD_VERSION dw_insert_ts dw_update_ts GG Green Grape Green 2 4bb08b043be3da18c3e9c01fa484a30c 2725b5d2846ec3d6f79c254132719b4f N 2022-09-30T07:01:07.000+0000 2022-09-30T07:01:07.000+0000 FA Fiji Apple Red 3.5 18fa2df6b8fadb816dbb608ec0731274 82ac239d4ccf6603d373fe5a92df73d9 N 2022-09-30T07:01:07.000+0000 2022-09-30T07:01:07.000+0000 BN Banana Yellow 1 3a9e0d6d4c67616c6ec27c67cf3e561d df5c5b76a127715bd35a5bb93b482247 N 2022-09-30T07:01:07.000+0000 2022-09-30T07:01:07.000+0000', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_14', 'chunkContent': \"Updated records of SCD3\\n\\nSCD3 stands for limited history preservation, the table will be updated as follows:\\n\\nThe record for 'Fiji Apple' is marked as historic by setting HAS_OLD_VERSION column value as 'Y'. The previous color and price values are saved in columns COLOR_OLD and PRICE_OLD respectively. There is no new record added to the table for historical version of 'Fiji Apple'. And only one set of old values can be kept. The 'Red Grape' is added to the table.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_15', 'chunkContent': 'ShortName Fruit Color Price key_hash data_hash COLOR_OLD PRICE_OLD HAS_OLD_VERSION dw_insert_ts dw_update_ts FA Fiji Apple Red 4 18fa2df6b8fadb816dbb608ec0731274 8a7df8a63a114cf96577fb4173d1b716 Red 3.5 Y 2022-09-30T07:01:07.000+0000 2022-09-30T07:02:07.000+0000 GG Green Grape Green 2 4bb08b043be3da18c3e9c01fa484a30c 2725b5d2846ec3d6f79c254132719b4f N 2022-09-30T07:01:07.000+0000 2022-09-30T07:01:07.000+0000 BN Banana Yellow 1 3a9e0d6d4c67616c6ec27c67cf3e561d df5c5b76a127715bd35a5bb93b482247 N 2022-09-30T07:01:07.000+0000 2022-09-30T07:01:07.000+0000 RG Red Grape Red 2 d28a4e648dd37db4bee9531ceae8cdbf', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_16', 'chunkContent': 'RG Red Grape Red 2 d28a4e648dd37db4bee9531ceae8cdbf 65e44fd552ae20dad64a8cf50b92067b N 2022-09-30T07:02:07.000+0000 2022-09-30T07:02:07.000+0000', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk15_17', 'chunkContent': 'Advanced Use Cases\\n\\nBuilding SCD Type 2 for a Delta Table Created from Multiple Source Tables\\n\\nConclusion\\n\\nThis article introduces SCD solution to store and manage both current and historical dimensional data and the examples that show different SCD types. The SCD type is determined by the business requirement. SCD can be implemented using GUI-based services such as Microsoft SQL Server Integration Services (SSIS) or Mapping data Flows in Azure Data Factory. It can also be implemented with code-based approach using services such as Azure Databricks and Azure Synapse. For Storage, Azure Data Lake Storage is the de-facto choice as it supports processing data at large scale while keeping the data access cost-effective.\\n\\nReference\\n\\nFor the implementation of SCD with Databricks, see dbxscd project.\\n\\nFurther Reading\\n\\nDataOps: Data Modeling', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-modeling\\\\slowly-changing-dimension.md'}, {'chunkId': 'chunk16_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Sharing\\n\\nDefinition\\n\\nData is the foundation for every organization to thrive, optimize and profit within its own domain of expertise. Therefore, organizations have a common need to share data in a secured and governed way. The data can be shared with internal audiences, external customers and partners.\\n\\nData sharing is the combination of a well-defined process and technical capabilities. It allows organizations to define what, how, when and with whom to share their data and execute on it efficiently.\\n\\nBenefits\\n\\nBy defining and implementing the right strategy for Data Sharing, organizations can benefit from:\\n\\nNew revenue streams opportunities\\n\\nAccuracy and automation of Data Privacy controls\\n\\nIncreased efficiencies on the release of data\\n\\nIncreased ability to discover more insights\\n\\nScalability, data reliability and data processing performance improvement\\n\\nIncreased productivity of knowledge workers\\n\\nCompliance audits and automation simplification\\n\\nIncreased efficiency of feedback loops responsible to drive business decisions\\n\\nCommon Scenarios\\n\\nData Sharing incorporates a common set of scenarios that can be applied, regardless of the organization size and industry.\\nThey can be summarized as Data Consolidation, Data Distribution, Data Brokerage and Data Escrow. These scenarios can be considered as incremental evolutionary stages of an organization approach to data sharing.\\n\\nData Consolidation', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-sharing\\\\index.md'}, {'chunkId': 'chunk16_1', 'chunkContent': 'Data Consolidation\\n\\nThis scenario is often considered foundational to more comprehensive Data Sharing scenarios. In this case, the data is transformed and consolidated into one or few locations following pre-established rules. That process requires complex data engineering implementations due to:\\n\\nThe diverse nature of the source systems\\n\\nLarge number of source systems\\n\\nVolume of data\\n\\nPhysical distribution of the source data\\n\\nLogical or business ownership of data sources\\n\\nThe consolidation  enables a first level of secure, consistent and reliable consumption experience to broader audiences.\\n\\nData Distribution\\n\\nThe Data Distribution scenario builds on top of the Data Consolidation scenario. It applies to companies who need to ensure compliance with internal policies or legal regulations. Another angle might be the need to enforce access controls and apply data quality rules to the ingested datasets.\\n\\nThis scenario provides a serving layer for consumers so they can access and process the data by copy or in-place. For downstream data products, data may be virtualized abstracting details of source location and format.\\n\\nData Brokerage\\n\\nThe Data Brokerage scenario builds on the Data Distribution model. It centers on creating new derivative data products through data engineering/processing (cleansing, merging, enrichment), and data analysis. Data products are information or assets that data consumers use a stand-alone asset or as input for creating other downstream derived data products.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-sharing\\\\index.md'}, {'chunkId': 'chunk16_2', 'chunkContent': 'Data brokers also invest in metadata processing and enrichment to improve data discoverability.\\n\\nData Escrow\\n\\nThis scenario centers on businesses that require a trusted entity to manage a secure environment. The escrow entity supervises compliant data processing and usage and is responsible to tear down the environment after usage.\\n\\nMore strict requirements might lead to a slight different approach where the trusted environment may be implemented as a \"clean room\" or secure infrastructure environment. In such cases, parties enrich and experiment with obfuscated data. Data exfiltration is restricted, but derivatives can be exported.\\n\\nData Sharing Technology Options in Azure\\n\\nData Sharing is comprised of two components:\\n    - Suitable process definition for the organization.\\n    - Right technology choice for the different components of the solution.\\n\\nThere are several services is Azure that can be integrated to support a Data Sharing implementation. Below a list of the core services that can be used for such purpose:\\n\\nAzure Data Share\\n\\nMicrosoft Purview In-Place Data Sharing\\n\\nMicrosoft Purview\\n\\nTech Specific Samples\\n\\nThis sample demonstrates how to perform a fully automated data sharing process between two pre-existing Data Share accounts in Azure.\\n\\nData Share Single-Tech sample\\n\\nFurther Reading\\n\\nDataOps: Data Virtualization\\n\\nDevSecOps: Azure Trusted Research Environment', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-sharing\\\\index.md'}, {'chunkId': 'chunk17_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Virtualization\\n\\nDefinition\\n\\nMost definitions of Data Virtualization align with the following statement:\\n\\nData Virtualization is an approach to data management that provides a single/unified representation of the data, and allows for data retrieval without requiring technical details about the data such as how the data is formatted at the source or where it's physically stored. Being a single point of access to all enterprise data siloed across disparate systems, Data Virtualization allows for centralized security and governance.\\n\\nSometimes, the below mentioned is another aspect of Data Virtualization:\\n\\nThe goal of Data Virtualization is to create a single representation of data from many, disparate sources without having to copy or move the data.\\n\\nIn summary, the benefits of Data Virtualization are:\\n\\nUnified governance (access and authorization) of enterprise data sources\\n\\nAbstraction from the complexity of where and how the data is stored\\n\\nSimplified maintenance of data / no data duplication\\nNOTE:\\nPreventing data duplication is important mostly to avoid the additional maintenance costs linked to the processes that would be required to continuously move and keep the data up to date as it happens when data is moved to a centralized location.\\n\\nCommon use cases for Data Virtualization include:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-virtualization\\\\index.md'}, {'chunkId': 'chunk17_1', 'chunkContent': \"Common use cases for Data Virtualization include:\\n\\nDecentralized data management approach: different teams own and manage different data products (Example: Enterprise Data Mesh). But they might need to rely on data products from other teams to build their own products. In this scenario data will often not reside in a centralized location and might be distributed across different teams/geographies.\\n\\nData Sharing / Data Marketplace: companies might want to share data with external parties giving them access to a complete platform (storage + compute). Such access will allow external users to query data in-place minimizing data movement or duplication.\\n\\nNOTE: When data is shared only providing access to a storage location without providing access to a query engine, this isn't considered a Data Virtualization scenario. To be considered as Data Virtualization, the solution needs to include a query endpoint / compute component that allows the user to interact with data.\\n\\nData Virtualization and Federated Queries\\n\\nFederated queries are a database feature that allows an Analytical database to request data from an external data source, such as a Relational Database. The queries are federated as effectively the Analytical database is requesting the Relational database to execute the query and return its results.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-virtualization\\\\index.md'}, {'chunkId': 'chunk17_2', 'chunkContent': \"A closely related concept is the notion of query/predicate push-down. It's useful when the user wants to query two or more different systems and combine the results. Query push-down aims to move as much work as possible to the source databases. The engine might look for ways to push filtering predicates into the queries on the source systems.\\n\\nWhen data virtualization systems offer data federation, they come with connectors that offer performance optimizations. These optimizations may include caching, query push-down optimizations and column/table statistics lookup. It serves two purposes: first, it offloads computation from the virtualization layer, taking advantage of the query performance of the source. Second, it potentially reduces the quantity of data that must push across the network, a critical bottleneck for virtualization performance.\\n\\nA data virtualization system shouldn't store the data internally. Hence a simple query federation isn't considered data virtualization when not combined with predicate push-down.\\n\\nData Virtualization for Large Analytical workloads\\n\\nGiven its definition, Data Virtualization enables users to access data in place, hence potentially enabling real-time analytics on top of operational data stores.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-virtualization\\\\index.md'}, {'chunkId': 'chunk17_3', 'chunkContent': \"Analytical work loads are memory and read intensive. So, running analytical workloads on operational data stores(ODS) isn't always a recommended approach as these workloads might end up consuming all ODS resources. Applications writing to such a store might be unable to write or might be considerably slowed down. Such a slowdown compromises the operational function (write intensive) of the data store and affecting core business operations. Hence, for large analytical workloads often data can't be consumed in place and trade-offs to Data Virtualization must be considered.\\n\\nIn such situations a lightweight approach to Data Virtualization can still be employed. A simple solution to virtualize access to data can be achieved connecting the query engine directly to data (in parquet or more recently Delta format) in a storage location. Such a setup is typically combined with a continuous stream of data coming from operational stores.\\n\\nData Virtualization Options in Azure\\n\\nWhile Data Virtualization includes governance, those aspects are addressed in DataOps: Data Governance and Protection section. So, current section only focuses on the remaining features.\\n\\nThere are several solutions to Data Virtualization available on the market. The below section, focuses on how Data Virtualization can be achieved with the Platform as a Service (PaaS) solutions available in Azure.\\n\\nAzure Synapse Analytics\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-virtualization\\\\index.md'}, {'chunkId': 'chunk17_4', 'chunkContent': \"Azure Synapse Analytics\\n\\nAzure Synapse Analytics addresses Data Virtualization with Synapse Link and with External Tables. It also offers the ability to explore files stored on storage\\n\\nAzure Synapse Link\\n\\nAzure Synapse Link isn't a technology of its own. Conceptually though it allows:\\n\\ncontinuous export from an operational data source: the extracted data can be used for near real-time analytics\\n\\nminimum affect on source system as there is a continuous replication of incremental changes only\\n\\nmanaged service: it requires minimal configuration and takes care of creating and running the data pipelines feeding data to Synapse\\n\\nCurrently Azure Synapse Link can virtualize access to:\\n\\nAzure SQL Databases and SQL Server 2022\\n\\nDataverse (Dynamics 365)\\n\\nAzure Cosmos DB\\n\\nHere is how Azure Synapse Link works:\\n\\nEvery source has its own implementation of Azure Synapse Link: the source system is pushing data to Synapse and performs the required optimizations to minimize latency and resource consumption\\n\\nThe dataset is stored in updatable parquet format - which is transparent to the end user\\n\\nWhen the user queries an Azure Synapse Link table, Synapse is querying the underlying parquet dataset similar to an external table\\n\\nExternal Tables\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-virtualization\\\\index.md'}, {'chunkId': 'chunk17_5', 'chunkContent': \"External Tables\\n\\nSynapse Analytics can virtualize access to Azure Storage (Blob Storage and ADLS Gen2) by using OPENROWSET and EXTERNAL TABLE statement.\\nThe OPENROWSET function is primarily meant for quick exploration of data stored in files. For detailed information, refer to the OPENROWSET documentation.\\n\\nAfter initial exploration of a file source, one might decide that the data source deserves being exposed as a table in the database schema. External tables are the way to achieve that. For details refer to the Synapse documentation for external tables.\\n\\nSynapse Analytics offers serverless Spark Pools. These Pools can virtualize access to external data by exposing the external source metadata in the Hive metastore. These data objects are referred as external tables or un-managed tables. When such external tables are created in Serverless Spark pools, they can also be queried in Serverless SQL pools, for more information, see: Azure Synapse Analytics shared metadata tables.\\n\\nData Exploration\\n\\nSynapse offers ways other than OPENROWSET for quickly exploring content stored in files. There are other ways that aren't considered proper data virtualization options. Current page doesn't include them.\\n\\nAzure SQL Database\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-virtualization\\\\index.md'}, {'chunkId': 'chunk17_6', 'chunkContent': \"Azure SQL Database\\n\\nAzure SQL Database can help with exploration of data stored in Azure Storage (Blob Storage and ADLS Gen2) using OPENROWSET statement. For details, refer to the examples of bulk access to data in Azure Blob storage documentation.\\n\\nIt can also virtualize access to tables from other Azure SQL databases by using the EXTERNAL TABLE statement. The latter relies on the Elastic Query functionality, currently in preview. For more information, see Elastic Query overview page.\\n\\nAzure SQL Managed Instance\\n\\nAzure SQL Managed Instance can virtualize access to data stored in Azure Storage (Blob Storage and ADLS Gen2) using OPENROWSET and EXTERNAL TABLE statement. For more information and sample code, see Data virtualization with Azure SQL Managed Instance.\\n\\nAzure Data Explorer\\n\\nAzure Data Explorer (ADX) can virtualize access to data using External Tables.\\n\\nAs no caching is applied for External Tables, every time a query is executed against these tables the results aren't saved/cached.\\n\\nAzure SQL External Tables\\n\\nAzure Storage External Tables\\n\\nAzure Databricks\\n\\nDatabricks has introduced the concept of Data Lakehouse: an architecture that enables efficient and secure Artificial Intelligence (AI) and Business Intelligence (BI) directly on vast amounts of data stored in Data Lakes.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-virtualization\\\\index.md'}, {'chunkId': 'chunk17_7', 'chunkContent': \"The Lakehouse pattern offers Data Virtualization going beyond simple read access to data stored in files. It's achieved by creating a traditional data warehouse with external tables. Lakehouse also introduces support for, amongst others: ACID transactions, and fine-grained security.\\n\\nFor more information on the Data Lakehouse pattern, see Databricks Blog: What's a Data Lakehouse?.\\n\\nDatabricks is based on Spark and as such can virtualize access to data through its Hive metastore. Databricks does it by storing the external source metadata and exposing it as a table, effectively creating an external table. External tables can be created with the SQL USING format OPTIONS (path 'path to location') directive.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-distribution\\\\data-virtualization\\\\index.md'}, {'chunkId': 'chunk18_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Ingestion\\n\\nThe first stage of the data engineering lifecycle is data ingestion from source systems where data is generated. Data is imported from one or more sources into a known storage medium where it can be further accessed by downstream processes.\\n\\nThe purpose of an analytical system is to extract useful information from an organization's data for supporting and providing decisions. For that to happen, data from different systems must first be consolidated in one place for analysis. Data ingestion tools and patterns help to connect to different interfaces to pull and push data from different systems.\\n\\nTwo major types of ingestion mechanisms are - batch ingestion and stream ingestion. Batch ingestion means to process a group of datasets together. This group is either determined by a specific time interval or a certain size limit. Stream ingestion deals with continuous and unbounded datasets. That said, most of the current stream ingestion approaches use mini-batches to ingest data as it reduces the number of I/O operations.\\n\\nThe ever growing digital footprint of organizations is leading to an explosion of data. Data teams are finding it difficult to keep up with the constantly growing demand for loading data to the analytics platform.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-ingestion\\\\index.md'}, {'chunkId': 'chunk18_1', 'chunkContent': 'To deal with this demand, data teams are coming up with ideas like configuration-driven data pipelines. These pipelines are capable of ingesting data from multiple sources based on one or more parameters. There is also significant growth in self-serve data onboarding to empower people beyond data teams to ingest data.\\n\\nDataOps: Batch and Stream Ingestion\\n\\nSelf-serve Data Onboarding', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-ingestion\\\\index.md'}, {'chunkId': 'chunk19_0', 'chunkContent': 'rings:\\n  - public\\n\\nBatch and Stream Ingestion\\n\\nTwo major paradigms of ingestion mechanisms are - batch ingestion and stream ingestion. Batch ingestion means to process a group of datasets together. This group is either determined by a specific time interval or a certain size limit. Stream ingestion deals with continuous, unbounded datasets. That said, most of the current stream ingestion approaches use mini-batches to ingest data as it reduces the number of I/O operations.\\n\\nBatch Ingestion\\n\\nTraditionally data ingestion has been done in batches due to the limitations of the legacy systems. It still remains a popular way to ingest data for the simplicity of its implementation. Almost every ETL tool supports batch ingestion.\\n\\nBatch Ingestion Architectural Patterns\\n\\nPull\\n\\nMost batch ingestion data pipelines will connect to a source system and pull data from it at regular interval. There are two common patterns for a batch job to load data from a source system, full load and delta load.\\n\\nFull load: The job will load all the records from the source table.\\n\\nDelta load: The job will load only the new records added since the last execution of the job. For this approach to work, the source table needs to have a watermark column.\\n\\nAzure Blob Storage Change Feed', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-ingestion\\\\batch-stream-ingestion\\\\index.md'}, {'chunkId': 'chunk19_1', 'chunkContent': \"Azure Blob Storage Change Feed\\n\\nWhen the data source is Azure Blob Storage, the Change Feed feature implements batch pull ingestion for either the full load or delta load scenarios. The Change Feed provides transactional logs of all the changes that occur to the blobs and the blob metadata inside of a storage account, in an efficient and scalable manner.\\n\\n{% if extra.ring == 'internal' %}\\nThe following repo provides a generic example for how to use the blob storage change feed in a batch ingestion process. Azure Blob Storage Change Feed - Example\\n{% endif %}\\n\\nPush\\n\\nThough pull-based model for data ingestion is the most fault tolerant model, its biggest drawback is lack of real-time information. The push-based model aims to solve the problem as the source system sends data to the target as soon as the data is generated. This model is useful when the source system is behind a firewall and setting up the inbound network connection isn't practical.\\n\\nThis model requires the destination system to be active and available all the time. If a job fails, there should be fall-back mechanism to avoid data loss.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-ingestion\\\\batch-stream-ingestion\\\\index.md'}, {'chunkId': 'chunk19_2', 'chunkContent': \"A common example of push based model is a logging and monitoring system. Azure Monitor Agent collects monitoring data from the guest operating system and pushes it to Azure Monitor for analytics. Splunk also uses this model to collect data. Splunk Universal Forwarders collect and securely forward remote data into Splunk software for indexing and consolidation.\\n\\nLarge-Scale Data movement within Azure\\n\\nLarge-scale data movement is a topic to discuss on many projects for hot and cold data paths, regardless of the diverse intent it might serve: copying/loading data or data transfers (copying and deleting the data in the source). The following section covers many scenarios and the appropriate patterns for data movement in those scenarios - Large Scale Data movement within Azure.\\n\\nStream Ingestion\\n\\nStream ingestion is loading a data stream to a storage system for processing. The primary focus is real or near-real time ingestion. As source systems are continuously producing data, all data sources are inherently streaming data. But it's not always practical to use this pattern due to the associated technical complexity and cost of running a streaming pipeline.\\n\\nStream ingestion has many use cases and is popular where data is time-sensitive. For example, a patient monitoring system needs to present the most recent vital signs. Another example is online brokerage platforms, where a delayed stock trade can cause huge financial damage.\\n\\nStream Ingestion Architectural Patterns\\n\\nLambda\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-ingestion\\\\batch-stream-ingestion\\\\index.md'}, {'chunkId': 'chunk19_3', 'chunkContent': 'Stream Ingestion Architectural Patterns\\n\\nLambda\\n\\nWhen working with large data sets, it can take a long time to run the sort of queries that clients need. Lambda architecture is a way of processing these large datasets by creating two paths for data flow. All data coming into the system goes through these two paths:\\n\\nA batch layer (cold path) stores all the incoming data in its raw form and performs batch processing on the data. The result of this processing is stored as a batch view.\\n\\nA speed layer (hot path) analyzes data in real time. This layer prioritizes low latency at the expense of accuracy.\\n\\nFor more information, please refer to Azure Architecture Center: Lambda architecture.\\n\\nKappa\\n\\nThe kappa architecture has the same basic goals as the lambda architecture, but with an important distinction: All data flows through a single path, using a stream processing system.\\n\\nFor more information, please refer to Azure Architecture Center: Kappa architecture.\\n\\nStream Ingestion Key Consideration\\n\\nSmall File Issue', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-ingestion\\\\batch-stream-ingestion\\\\index.md'}, {'chunkId': 'chunk19_4', 'chunkContent': \"For more information, please refer to Azure Architecture Center: Kappa architecture.\\n\\nStream Ingestion Key Consideration\\n\\nSmall File Issue\\n\\nIt's common for streaming jobs to create too many small files, which can negatively affect the performance of queries and jobs reading them. One way to improve this speed is to coalesce small files into larger ones. Spark has specific command to run file compaction - Compaction (bin-packing). Databricks has an optional feature called Auto Optimize that automatically compact small files during individual writes to a Delta table - Auto Optimize.\\n\\nIncremental/Delta Load\\n\\nIn a data integration solution, incrementally (or delta) loading data after an initial full data load is a widely used scenario. One common solution in such cases is to use a watermark column. A watermark is a column that has the last updated time stamp or an incrementing key. The delta loading solution loads the changed data between an old watermark and a new watermark.\\n\\nData Factory and Synapse Analytics have out of the box support to implement incremental/delta load.\\n\\nIncremental Copy Overview (Tutorial)\\n\\nIncremental Copy using Copy Data tool (Tutorial)\\n\\nTech-Specific Samples\\n\\nStreaming-at-Scale Samples\\n\\nFurther Reading\\n\\nDataOps: Batch Processing\\n\\nDataOps: Stream Processing\\n\\nDataOps: Data Pipeline Operability\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-ingestion\\\\batch-stream-ingestion\\\\index.md'}, {'chunkId': 'chunk19_5', 'chunkContent': 'Further Reading\\n\\nDataOps: Batch Processing\\n\\nDataOps: Stream Processing\\n\\nDataOps: Data Pipeline Operability\\n\\nDataOps: Modern Data Warehouse', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-ingestion\\\\batch-stream-ingestion\\\\index.md'}, {'chunkId': 'chunk20_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Processing\\n\\nAfter data ingestion, data needs to be transformed from its original form into the required use case form. Data processing (also known as data transformation or data wrangling) is a set of techniques used to execute this transformation.\\n\\nThe first step of data processing usually involves mapping datasets into standard types and formats. For example, converting ingested string data into numeric date types. Other common data processing steps include schema changes, normalization, aggregation, etc.\\n\\nModern data processing may include one or more of the following requirements:\\n\\nAbility to process large data volumes\\n\\nHandle a large variety of data formats\\n\\nProcess data quickly (low-latency processing)\\n\\nGenerally, data processing systems optimize either for higher overall throughput or lower end-to-end latency. This difference is generally what distinguishes batch processing vs stream/real-time processing frameworks. It is important to note that this distinction is not black and white, but rather a range with various data processing frameworks sitting somewhere in this spectrum.\\n\\nDataOps: Batch Processing\\n\\nDataOps: Stream Processing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\index.md'}, {'chunkId': 'chunk21_0', 'chunkContent': 'rings:\\n  - public\\n\\nLarge-Scale Data Movement\\n\\nLarge-scale data movement is an area to address on many projects for hot and cold data paths, regardless of the diverse intent it might serve: copying/loading data or data transfers (copying and deleting the data in the source).\\n\\nWhen transferring large volumes of data for analytics or migrations, polyglot persistence is a core requirement. When it comes to costing, performance, flexibility and retention. That means that the same solution might include different data storage and engine choices to address different requirements of the data lifecycle.\\nFor example, when data is archived or retained longer for compliance reasons less expensive storage choices can be used. Additionally, deciding if and how fast that data can be available might influence the storage and database engine or compute choices.\\n\\nTech-Specific Samples\\n\\nThe next section summarizes a list of resources and assets that have successfully addressed the transfer of large volumes of data.\\n\\nLarge-Scale Data Ingestion from Azure Data Lake to Azure SQL using Databricks\\n\\nThe following resources, showcase how to use the\\nApache Spark Connector for SQL Server and Azure SQL for large-scale data loading.\\n\\nFast data loading in Azure SQL\\n\\nFast data loading in Azure SQL and optimization options\\n\\nFast Data Loading in Azure SQL using Smart Bulk Copy\\n\\nFast data loading from Azure SQL/SQL Server to Azure SQL/SQL Server\\n\\nFurther Reading', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\data-movement-azure.md'}, {'chunkId': 'chunk21_1', 'chunkContent': 'Fast data loading from Azure SQL/SQL Server to Azure SQL/SQL Server\\n\\nFurther Reading\\n\\nUse Azure Data Factory to migrate data from your data lake or data warehouse to Azure\\n\\nLoad data into Azure Synapse Analytics using Azure Data Factory or a Synapse pipeline', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\data-movement-azure.md'}, {'chunkId': 'chunk22_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Pre-Processing\\n\\nData pre-processing is a stage where some kind of processing is performed on the ingested data to make it usable for the actual processing, analytics or machine learning. Generally when processing huge volume of large files, the jobs will take good amount of time to complete. Hence, careful consideration should be given to the tools and technologies chosen. Some of the factors to consider are: autoscaling, parallel processing, ability to run container workloads, ability to fit with overall data processing pipeline, and cost effectiveness. The tools and technologies section talks about the preferred options, their limitations, challenges and best practices.\\n\\nUse Case\\n\\nData gets ingested as large bag files(ROS format) in raw zone. Before they are processed by next set of data pipelines, these bag files need to be extracted.\\n\\nTools and Technologies\\n\\nThere are multiple compute options in Azure Cloud and Azure Batch is one such option for data pre-processing. Some of the high-level features that should be considered while choosing the compute are:\\n\\nEasy integration with data pipelines (Ex: Azure Data Factory(ADF) pipelines).\\n\\nSupports running long jobs.\\n\\nSupports parallel processing.\\n\\nCan run container workloads.\\n\\nSupports auto-scaling.\\n\\nProvides options for choosing the right SKU for different workloads.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\data-pre-processing.md'}, {'chunkId': 'chunk22_1', 'chunkContent': 'Supports auto-scaling.\\n\\nProvides options for choosing the right SKU for different workloads.\\n\\nAzure Batch supports all these features, with some limitations/challenges. Here are some best practices to overcome those limitations:\\n\\nBatch application has lot of dependencies\\n\\nTo process workloads, processor code need to be deployed on the Azure Batch nodes. It involves installing dependencies that are required by the application on the batch node. Azure Batch provides a startup task, which can be used to install these dependencies. However, if the dependencies are too many, then it will delay the node readiness. Autoscaling is also impacted as every new node has to have these dependencies installed as the first step. This setup time can vary from 3-30 minutes depending on the list of required dependencies.\\n\\nIn such cases, it is best practice to containerize the application and run container work loads on Azure Batch.\\n\\nMount Blob Storage by using the Network File System (NFS) 3.0 protocol on batch nodes', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\data-pre-processing.md'}, {'chunkId': 'chunk22_2', 'chunkContent': \"Mount Blob Storage by using the Network File System (NFS) 3.0 protocol on batch nodes\\n\\nConsider the use case where there is a need to process large files. It is not optimal to download these files on to the batch nodes from storage account, extract the contents and then upload back to the storage account. To ensure nodes have enough storage attached, it may be required to do some kind of cleansing to free the space after the job is done. The downloading and uploading of contents to storage account take extra time.\\n\\nThe best practice here is, to mount the storage accounts on to the batch nodes and access the data directly. It should be noted that, NFS mounts are not supported on windows nodes. For more information, see Mounting storage accounts via NFS\\n\\nADF does not support running container workloads on Azure Batch\\n\\nContainer workloads can't be triggered directly when ADF pipelines are being used to initiate the Azure Batch processing via the custom activity. Refer to the below tech-specific sample to overcome this limitation.\\n\\nFurther Reading\\n\\nDataOps: Data Pipeline Operability\\n\\nDataOps: Batch and Stream Ingestion\\n\\nDataOps: Stream Processing\\n\\nDataOps: Modern Data Warehouse\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\data-pre-processing.md'}, {'chunkId': 'chunk23_0', 'chunkContent': 'rings:\\n  - public\\n\\nDatabricks optimization\\n\\nTo optimize Databricks speed and costing performance, developers need to understand how query planning and execution works in Spark. When a query is requested, first Spark will create a Logical plan. This plan will keep being improved until an Action step (collect(), count(), display(), etc.) is triggered. Once an action method is called, Spark prepares a Physical execution plan to implement the Logical Plan in the cluster.\\n\\nWhen executing the Physical plan, Spark respects an execution hierarchy. At the top of this hierarchy are jobs. Invoking an action within Spark triggers the launch of a Spark job.\\n\\nSpark job comprises several stages. When Spark reads a function that requires a shuffle, it creates a new stage. Functions that trigger transformations, such as join() or merge(), will trigger a shuffle and thus a new stage.\\n\\nSpark stage comprises several tasks. Every task in the stage executes the same set of instructions, each on a different subset of the data. A task is the smallest execution unit in Spark. Tasks are executed by an executor. In Databricks, each core/slot/thread of the executor will execute 1 task.\\n\\nCluster optimization\\n\\nOptimizing a cluster in Databricks generally aims to choose a cluster configuration with the most optimal speed and minimal cost.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\databricks_optimization.md'}, {'chunkId': 'chunk23_1', 'chunkContent': 'Optimizing a cluster in Databricks generally aims to choose a cluster configuration with the most optimal speed and minimal cost.\\n\\nIn Databricks, running a Spark job require at least 1 driver and 1 worker. The spark driver orchestrates the execution of the processing and its distribution among the Spark executors.\\nThe driver will have the metadata of the tasks shared with the executors. Once the tasks are completed by executors, the results are shared back with the driver. In most cases, the driver cluster does not need to be a large cluster (e.g., lots of cores and much memory). However, do note that when performing collect() action, it will bring all the data from the executors back to driver. When dealing with large dataset, this operation will generate an out-of-memory error.\\n\\nParallelism in Spark is achieved by checking if more cores are available after all tasks in a stage are assigned a core each. If there are more free cores, multiple jobs can start running simultaneously.\\nThus, choosing the right executor mainly depends on how many Tasks the query produced during the Physical execution plan. The number of cores required will depend on how many maximum Tasks need to be executed in a stage. The same logic can also be applied when choosing the number of memories per cores.\\n\\nThe Ganglia UI in Databricks is a useful tool to monitor the cluster utilization and identify opportunities to improve performance.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\databricks_optimization.md'}, {'chunkId': 'chunk23_2', 'chunkContent': 'The Ganglia UI in Databricks is a useful tool to monitor the cluster utilization and identify opportunities to improve performance.\\n\\nNotably, a larger cluster is not necessarily more expensive than a smaller one. In fact, it can be faster as the cost is primarily determined by the duration of the workload rather than the cluster size.\\n\\nPartitioning\\n\\nIn terms of optimization, the right partitioning decision applied to the dataset will help to improve the performance during reading and writing to the storage. In most cases, the bottleneck of the performance lies when reading the data needed from the storage. Check Azure Databricks data storage for details.\\n\\nQuery optimization\\n\\nWriting an efficient query is the faster mechanism that can be used to gain performance in the short term.\\nSome consideration points:\\n\\nConfigure a suitable partitioning size on the Shuffle property.\\nWhen doing data transformation (e.g., groupBy, join), Spark shuffles data between executors. By default, Spark has 200 shuffle partitions. However, in a large dataset this number might be too small. This number can be changed in the following Spark configuration: spark.conf.set (\"spark.sql.shuffle.partitions\", 768). Calculating the right size for the shuffle partitions requires some testing and knowledge of the complexity of the transformations and table sizes.\\n\\nFilter data as early as possible.\\n\\nUtilize Delta Cache.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\databricks_optimization.md'}, {'chunkId': 'chunk23_3', 'chunkContent': 'Filter data as early as possible.\\n\\nUtilize Delta Cache.\\n\\nAvoid using user-defined functions (UDF).\\n\\nConsider to use Photon as much as possible.\\nWhen Spark tasks are executed in Photon, it can be visualized in the execution plan. The nodes that are marked gold color are executed in Photon. It is also important to note that Photon only covers certain operators, expressions, and data types. Therefore, it might be possible that not entire execution plan is executed in Photon.\\n\\nMind the lazy evaluation of Spark.\\n\\nFurther reading\\n\\nDatabricks best practices\\nDatabricks Cluster best practices', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\databricks_optimization.md'}, {'chunkId': 'chunk24_0', 'chunkContent': 'rings:\\n  - public\\n\\nBatch Processing\\n\\nLarge volume data processing often requires high throughput. Batch jobs are the preferred method for high-volume data processing. A batch job runs on discrete chunks of data, whereas a streaming job processes an unbounded dataset. Batch jobs commonly run on a schedule (e.g., daily, hourly, every 15 minutes). Trigger-based batch job is also a popular pattern where a batch job is triggered on an event, like the completion on another batch job or arrival of a file.\\n\\nBatch jobs are a great choice for processing large datasets for few reasons. The technology and patterns for batch jobs are very mature and have been battle tested for decades. With the rise of cloud computing, it’s easy to spin up an on-demand compute for executing batch job(s), which drives down the operational cost. Troubleshooting and rerunning a batch job is also easier compared to a streaming job. So, if the data is not time sensitive, it’s preferred to process it as batch job(s).\\n\\nUse-cases\\n\\nData Cleansing\\n\\nData Cleansing or data cleaning takes messy, malformed data and turns it into useful, clean data. This process also includes dropping wrong or incomplete data and mapping datasets into standard types and formats. For example, converting string data from the source into numeric date types.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\index.md'}, {'chunkId': 'chunk24_1', 'chunkContent': \"In the past, the approach was to clean the data as part of the ingestion process. Due to the drop in the storage cost in recent years, the popular approach now is to load everything first and then clean the data. There are specialized tools and libraries for data cleansing that significantly reduces the time to prepare clean data.\\n\\nData Transformation\\n\\nData coming from different systems often needs to be transformed into a different form for downstream use cases. For example, a company might have multiple sales channels with each channel generating sales data in its own format. To ensure the easy use of sales data by the downstream analysts and use cases, it's a common pattern to transform them to a standard sales data format. The transformed data is often presented as tables, views, or materialized views.\\n\\nReporting & Analytics\\n\\nA large number of data initiatives revolve around reporting and analytics. Once data is stored and transformed, it's ready for generating reports or dashboards and analysis. It often involves querying thousands of records, aggregating them and getting the insight out of it. Most business logic is applied during the reporting and analytics phase.\\n\\nTools and Technologies\\n\\nMassively Parallel Processing (MPP)\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\index.md'}, {'chunkId': 'chunk24_2', 'chunkContent': \"Tools and Technologies\\n\\nMassively Parallel Processing (MPP)\\n\\nMassively Parallel Processing database products became popular in the 1980s and 1990s for analytics workloads. MPP is the coordinated processing of a program by multiple processors working on different parts of the program. Each MPP processing unit works independently with its own operating system and dedicated memory. Parallelization of tasks allows MPP databases to handle massive amounts of data and provide faster analytics based on large datasets.\\n\\nExamples of MPP Database are Azure Synapse, Snowflake, Netezza, Teradata, Redshift etc.\\n\\nHadoop MapReduce\\n\\nMore than a decade after MPP, MapReduce/Hadoop changed the world of big data processing. It was because of MapReduce's ability to use commodity hardware to build clusters and run distributed data processing jobs. Generally, MPP databases require careful up-front data modeling and query patterns before importing the data into the database’s proprietary storage format. But MapReduce/Hadoop opened up the possibility to first ingest the data, and later figuring out how to process it further.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\index.md'}, {'chunkId': 'chunk24_3', 'chunkContent': 'MapReduce is a programming model implementation for processing big data sets with a parallel, distributed algorithm on a cluster. It was designed to solve long-running processing jobs that could be scaled out to many machines with contained overall cost. Here, the time-responsiveness of the job was not as important as scaling. One disadvantage of MapReduce jobs is that after every computation step data needs to be saved to HDFS (disk) before the next step can run. These disk I/O operations can become a bottleneck because of the latency involved.\\n\\nApache Spark\\n\\nFor years, MapReduce/Hadoop was the best choice for big data analytics until Spark came along. Spark is faster than MapReduce, as its distributed memory-based architecture has fewer disk I/O operations.\\n\\nAzure Batch\\n\\nDesigned for the large-scale batch jobs, Azure Batch is a highly scalable computing solution offered by Microsoft Azure. Its parallel processing capabilities make it an ideal solution for data pre-processing, particularly for datasets that are in the multiple gigabytes or more. By utilizing Azure Batch, data processing can be performed across multiple virtual machines in a parallel manner, ensuring efficiency and cost-effectiveness. The processing can be automated and scheduled to run on demand or at a specific time. Additionally, Azure Batch integrates with other Azure services, making it easy to manage and monitor the entire data processing pipeline, from ingestion to transformation.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\index.md'}, {'chunkId': 'chunk24_4', 'chunkContent': 'An industry-specific example is the processing of data in ROS format. This format requires bagged files extraction and meta-data generation for each bag file before processing further. See DataOps: Pre-Processing with Azure Batch for more details.\\n\\nTech-Specific Samples\\n\\nFile processing and validation using Azure Functions, Logic Apps, and Durable Functions\\n\\nFurther Reading\\n\\nDataOps: Data Pipeline Operability\\n\\nDataOps: Batch and Stream Ingestion\\n\\nDataOps: Stream Processing\\n\\nDataOps: Modern Data Warehouse', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\batch-processing\\\\index.md'}, {'chunkId': 'chunk25_0', 'chunkContent': \"rings:\\n  - public\\n\\nStream Processing\\n\\nA low-latency system describes a system that is optimized to process data with minimal delay. Low-latency systems are expected to ingest and process data from a source system as soon as the data is generated. Examples of such systems are real-time and streaming systems. Compared to batch processing that generally prioritizes throughput, low latency systems prioritize end-to-end latency. Despite the term real-time, there will always be a latency between the data generation and processing times. There are many factors for this latency, like a slow network or the availability of the compute for processing. Furthermore, many data streaming frameworks use the Micro Batch approach to emulate streaming, albeit introducing some latency.\\n\\nLow latency systems are necessary when the data is time sensitive. For example, a stock trading system has to respond to the changes of stock tickers. If the system can't process the change immediately, it won't be able to make the right transaction.\\n\\nData Streaming\\n\\nData streaming is the most popular technique to build a low latency system. Streaming refers to never-ending data streams with no beginning or end that provide a constant/continuous flow of data. Streaming technologies are optimized to work on a small data volume produced with in a small time window (in milliseconds). Two popular approaches to implement a streaming platform are: micro batch, and continuous processing.\\n\\nMicro Batch\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\stream-processing\\\\index.md'}, {'chunkId': 'chunk25_1', 'chunkContent': 'Micro Batch\\n\\nMicro-batching takes a batch-oriented framework and applies it in a streaming situation. Generally a micro-batch might run anywhere from ~100ms to every minute. Spark Structured Streaming works using micro batch. Azure Stream Analytics also uses micro batches to ingest/process a data stream.\\n\\nContinuous Processing\\n\\nContinuous processing systems come with a significant overhead as each event is processed at a time. Popular industry implementations of continuous processing systems support end to end latency of ~10 ms. Apache Flink supports this model, where event computations are triggered as soon as the event is received. Spark also has a new experimental streaming execution mode called continuous processing that enables low (~1 ms) end-to-end latency.\\n\\nData Streaming Use-Cases\\n\\nStream Ingestion\\n\\nIn most data ingestion scenarios, a batch job for data ingestion is the preferred approach due to its simplicity. But if the use case demands quick availability of the data in the data warehouse or data lake, the stream of data is directly ingested to the target. Stream ingestion is a necessary component for building systems like a real-time data warehouse. Data in a real-time data warehouse is immediately queryable upon its arrival without any processing or aggregation or compaction.\\n\\nHistorically, direct stream ingestion has been challenging. But modern data warehousing technologies like Delta Lake, Azure Synapse supports high throughput stream ingestion without impacting the query performance.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\stream-processing\\\\index.md'}, {'chunkId': 'chunk25_2', 'chunkContent': 'Stream Processing\\n\\nStream processing covers a broad set of tasks that applies to the incoming data stream. Some common examples are:\\n\\nAggregation (Examples: sum, mean, standard deviation) of the data stream based on a window.\\n\\nEnrichment of the data stream by joining with another data stream or static data.\\n\\nAnalyses of the data stream to address use cases like prediction, anomaly detection, etc.\\n\\nReal-time Systems\\n\\nA real-time data processing system guarantees that the data will be acted upon within a certain amount of time, such as milliseconds. Though real-time systems may look similar to streaming systems, there is a subtle difference. Real-time system focuses on the \"tight\" deadline (milliseconds) within which it has to react to the incoming event. Whereas a stream processing system generally does analyses on the incoming data stream and more lenient when it comes to response time for the processing. Few common examples of real-time systems are air traffic control systems, process control systems, and autonomous driving systems.\\n\\nNear Real-time Analysis\\n\\nNear real-time analysis refers to the process of analyzing data, as soon as it becomes available, with a minimal delay between data collection and analysis. It is useful for applications that require timely decision-making based on current data. Examples include: system monitoring, processing sensor data, fraud detection, etc. Learn more about it in the following links:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\stream-processing\\\\index.md'}, {'chunkId': 'chunk25_3', 'chunkContent': 'Near real-time lakehouse data processing\\n\\nInteractive analysis with Azure Data Explorer\\n\\nEvent-driven Processing\\n\\nAn event-driven data processing pattern triggers the pipeline execution based on an event. An event can be the arrival of a file, or an end-user activity that pushes a message in a message queue. An event-driven architecture can use a pub/sub model or an event stream model. Details on event-driven architecture style can be found here.\\n\\nTech-Specific Samples\\n\\nProject StreamShark - StreamShark (SS) is a console application that connects and retrieves all messages from an event hub. The tool provides real-time observation of event flow patterns with an intention to shorten the application development feedback loop. When SS is executed the sequence is: The application connects to Azure Event Hubs, retrieves data from the last checkpoint, aggregates metrics about the received data in real time, displays the metrics in a terminal window.\\n\\nAzure Kusto Labs - This repository features self-contained, hands-on-labs with detailed and step-by-step instructions.  The artifacts (data, code etc.) to try out various features and integration points of Azure Data Explorer (Kusto) are also included.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\stream-processing\\\\index.md'}, {'chunkId': 'chunk25_4', 'chunkContent': 'Streaming at Scale - The samples show how to set up an end-to-end solution to implement a streaming at scale scenario using a choice of different Azure technologies. A few possible ways to implement streaming in Azure are: Kappa or Lambda architectures, a variation of them, or custom ones. Each architectural solution can also be implemented with different technologies, with its own pros and cons.\\n\\nBuild a delta lake to support ad hoc queries in online leisure and travel booking - This architecture provides an example delta lake for travel booking. Here, large amounts of raw documents are generated at a high frequency.\\n\\nServerless event processing - Reference architecture showing a serverless event-driven architecture. The sample ingests a stream of data, processes the data, and writes the results to a back-end database.\\n\\nDe-batch and filter serverless event processing with Event Hubs - A solution idea showing a variation of a serverless event-driven architecture using Azure Event Hubs and Azure Functions to ingest and process a stream of data. The results are written to a database for storage and future review once de-batched and filtered.\\n\\nAzure Kubernetes in event stream processing - Describes a variation of a serverless event-driven architecture, hosted on Azure Kubernetes with KEDA scaler. The sample ingests a stream of data, processes the data, and then writes the results to a back-end database.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\stream-processing\\\\index.md'}, {'chunkId': 'chunk25_5', 'chunkContent': 'Further Reading\\n\\nDataOps: Data Pipeline Operability\\n\\nDataOps: Batch and Stream Ingestion\\n\\nDataOps: Batch Processing\\n\\nDataOps: Modern Data Warehouse', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\analytical-systems\\\\data-processing\\\\stream-processing\\\\index.md'}, {'chunkId': 'chunk26_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Governance and Protection\\n\\nThis pillar describes the governance process of the data for enterprise systems. It also covers the security and compliance aspects of managing data so that it's reliable and meets standards. Additionally, it covers regulations and policies of the organization.\\n\\nCapability Map\\n\\nHere is a high-level capability map for Data Governance and Protection:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\index.md'}, {'chunkId': 'chunk27_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Governance\\n\\nData Governance is a set of processes, tools, technologies, people, standards/policies that enable an organization implement control over data/data assets effectively. It permits the support of its business objectives. Data Governance defines the processes, tools and responsibilities to discover and ensure the quality and security of data.\\n\\nImplementing unified data governance across any organization comes with its own challenges and the complexity multiplies as the size of the organization grows. So, it becomes crucial to think about data governance as a fundamental block of an enterprise’s data strategy.\\n\\nThere are several motivations for implementing a data governance strategy. It helps in creating a unified map of data across the entire data estate, which facilitates easy data discovery. Once the data is visible, it can be cataloged, classified, labeled, and have a common business glossary applied to it. Integrated with the catalog, the data systems can then capture the lineage of data as it flows through different systems in the data estate.\\n\\nThe following characteristics of Data Governance are discussed in context of \"data\":\\n\\nDataOps: Data Discovery\\n\\nDataOps: Data Catalog\\n\\nData Classification\\n\\nGlossary\\n\\nDataOps: Data Lineage\\n\\nDataOps: Data Versioning\\n\\nDataOps: Data Quality', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\index.md'}, {'chunkId': 'chunk28_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Catalog\\n\\nA core component of an effective Data Governance strategy is having a well-curated and complete Data Catalog. A data catalog is a central collection of metadata of the organizations data assets including related search and data management tools. Key metadata consists both technical properties of the dataset and business metadata. Technical properties include data schema and structure, physical location, type, format, approximate size, and lineage. Business metadata, on the other hand, can include:\\n    - related glossary terms\\n    - data owners\\n    - data stewards\\n    - data classifications\\n    - data sensitivity\\nand more.\\n\\nImportance of having a Data Catalog\\n\\nFor data governance and policy officers, a data catalog allows them to have a single pane of glass to govern the organization’s data estate. This fact is important for datasets that must adhere to external regulatory requirements such as GDPR. Furthermore, it can provide other insights to understand how data is being created and used across the data estate.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-catalog\\\\index.md'}, {'chunkId': 'chunk28_1', 'chunkContent': 'For data engineers and business users, it enables better discoverability of datasets, reduces dataset duplication, and clarifies dataset ownership and stewardship. This feature also allows users to quickly identify any associated considerations that are needed for appropriate dataset usage. For example, the dataset’s lineage, data sensitivity, and any associated data policies, guardrails, and controls. This feature enables better cross-team collaboration for data engineering efforts across the organization.\\n\\nFor MLOps perspective on Data Catalogs, see MLOps: Data Catalog.\\n\\nFunctions of a Data Catalog\\n\\nAt a minimum, a data catalog should enable the following features:\\n\\nOnboard dataset metadata – Ideally metadata updates happens regularly, automatically and at scale, generally through data source scanning, which would capture all technical metadata associated with each dataset including data lineage. Check DataOps: Data Lineage for more information.\\n\\nCurate and enrich – Once the dataset metadata have been onboarded, the catalog should enable dataset owners and stewards to enrich metadata with any business-specific metadata such as associated glossary terms. It should also ideally enable automatic classification of dataset at scale for effective data governance.\\n\\nEnable data discoverability and evaluation – Finally, the data catalog should enable users to efficiently search and browse for datasets across technical metadata dimensions and business semantics. It should also allow users to quickly and easily evaluate datasets to determine if they are fit for purpose. This check can be done by:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-catalog\\\\index.md'}, {'chunkId': 'chunk28_2', 'chunkContent': \"allowing dataset previews,\\n\\nshowing lineage information,\\n\\ndata sensitivity,\\n\\nrelated glossary terms\\nand more.\\n\\nData Governance in action\\n\\nMicrosoft Purview, is Microsoft’s flagship data governance tool. It's recommended for use as the main data catalog, particularly for data estates deployed on Azure. A company should typically try to have only one instance of Purview in production, as described in Microsoft Purview accounts architectures and best practices. The following samples focus around Microsoft Purview as a Data Catalog:\\n\\nMDW Repo: Data Governance - This end-to-end sample showcases how to incorporate data governance in a modern data warehouse architecture. It utilizes Microsoft Purview and Presidio while showcasing associated DevOps processes to operationalize the solution.\\n\\nPublish and Subscribe Purview events – Integration of Microsoft Purview and third-party services. using a Kafka Topic. It includes sample scripts, which demonstrates available publish and subscribe operations.\\n\\nFrom governance perspective, there is another application in Microsoft Purview called Data Estate Insights, which is purpose-build for governance stakeholders such as Chief Data Officer. This application provides actionable insights into the organization’s data estate, catalog usage, adoption, and processes. For more information, please refer to Microsoft Purview Data Estate Insights application.\\n\\nFurther Reading\\n\\nDataOps: Data Discovery\\n\\nDataOps: Data Lineage\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-catalog\\\\index.md'}, {'chunkId': 'chunk28_3', 'chunkContent': 'Further Reading\\n\\nDataOps: Data Discovery\\n\\nDataOps: Data Lineage\\n\\nDataOps: Data Quality', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-catalog\\\\index.md'}, {'chunkId': 'chunk29_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Discovery\\n\\nData discovery can be defined as the collection and evaluation of data from disparate sources. The goal of this process is to derive business value from that data.\\n\\nNowadays, most of the businesses generate and collect huge amount of data and involve multiple systems. The data discovery process helps in bringing a unified approach for collecting this data. It also centralizes the data catalog. This way, businesses can derive insights more efficiently.\\n\\nDiscovery Challenges\\n\\nThere are various challenges faced by different actors when it comes to discoverability of data. One of the challenge is the lack of automated processes for scanning and cataloging data, which makes it difficult, time-consuming and error prone. Another common issue is the lack of centrally managed data catalogs, which often results in data redundancy. Here are some extra details about these challenges:\\n\\nDiscovery challenges for data consumers\\n\\nDiscovery challenges for data producers\\n\\nDiscovery challenges for security administrators\\n\\nData Discovery Tools and Capabilities\\n\\nMicrosoft Purview\\n\\nMicrosoft Purview is a unified data-governance service. It helps in managing and governing on-premises, multi-cloud, and software-as-a-service (SaaS) data. With Purview, a holistic, up-to-date map of organization's data landscape can be created with automated data discovery, sensitive data classification, and end-to-end data lineage.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-discovery\\\\index.md'}, {'chunkId': 'chunk29_1', 'chunkContent': 'Purview provides a cloud-based service called Data Map, into which various data sources can be registered. During registration, the data remains in its existing location. A copy of its metadata is added to Microsoft Purview, along with a reference to the data source location. The metadata is also indexed to ensure each data source is easily discoverable via search and understandable to the users who discover it.\\n\\nOnce the data source is registered, it can be enriched with more metadata information such as descriptions and tags. This descriptive metadata supplements the structural metadata, such as column names and data types that are registered from the data source. When the assets like data sources or others are not recognized as a default type by Purview, custom entities can be added as explained in this tutorial.\\n\\nMicrosoft Purview Data Map powers the Microsoft Purview Data Catalog. The catalog enables business and technical users to quickly and easily find relevant data. It uses a search experience with filters based on lenses such as glossary terms, classifications, sensitivity labels and more. For more information, please refer to DataOps: Data Catalog.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-discovery\\\\index.md'}, {'chunkId': 'chunk29_2', 'chunkContent': 'An ontology is a formalized description of concepts and their relationships within a specific domain. It establishes a common vocabulary and structure for representing and organizing knowledge within that domain. This knowledge facilitates data integration and knowledge sharing. A knowledge graph represents specific instances of those concepts and relationships. The Microsoft Purview metamodel adds a semantic layer to Purview. This layer enables users to describe real-world concepts and relationships in a structured way, making them easily searchable and retrievable. This layer is similar to an ontology, as both provide a framework for organizing and representing knowledge in a formal way.\\n\\nOntologies and knowledge graphs can be expressed using a formal language such as RDF, which can be then transformed into Purview metamodel. By linking assets to the metamodel in Purview, it becomes possible to locate assets using business context. It’s also possible to examine the business context of a particular asset.\\nIf further assistance is needed to import an ontology and its knowledge graph into Purview, contact the Microsoft Purview team. They can help with the request access to an on-demand solution accelerator example.\\n\\nMetadata Synchronization', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-discovery\\\\index.md'}, {'chunkId': 'chunk29_3', 'chunkContent': \"Metadata Synchronization\\n\\nLarge enterprises may have several divisions, subsidiaries or companies with different data catalogs that use one or several technologies. It's common to see a need for data discovery across the whole enterprise. A solution is to synchronize metadata (i.e. data catalogs' data). The Azure Architecture center provides guidance on how to consolidate the metadata in Purview by providing a collection structure that hosts the result of scanned data, and imported metadata. It also proposes an architecture to implement the synchronization from external data catalogs. Other data catalog vendors can also provide similar guidance or components (e.g. Azure Purview to Collibra Integration - Collibra Marketplace).\\n\\nMachine Learning (ML) based Solutions\\n\\nThe data discovery process can be enhanced by using machine learning. By using machine learning techniques, data discovery becomes smart. ML can discover and infer relationships between data. It can also accelerate an organization’s understanding of their data.\\n\\nRead through MLOps: Data Discovery for detailed information on machine learning based data discovery.\\n\\nSQL based Data Discovery\\n\\nAzure SQL database services include Azure SQL Database, Azure SQL Managed Instance, Azure Synapse Analytics and SQL Server. These services have built-in basic capabilities for discovering, classifying, labeling, and reporting sensitive data. The goal is to protect the data and not just the database. Currently it supports the following capabilities:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-discovery\\\\index.md'}, {'chunkId': 'chunk29_4', 'chunkContent': \"Discovery & recommendations - The classification engine scans your database and identifies columns containing potentially sensitive data. It then provides you with an easy way to review and apply the appropriate classification recommendations, and to manually classify columns.\\n\\nLabeling - Sensitivity classification labels can be persistently tagged on columns.\\n\\nVisibility - The database classification state can be viewed in a detailed report that can be printed or exported to be used for compliance and auditing purposes.\\n\\nThis approach isn't aligned with the idea of having a centralized discovery and cataloging mechanism. Rather it's a specific and constrained data discovery option.\\n\\nFor more information on this topic, see the following documentation:\\n\\nData Discovery & Classification\\n\\nSQL Data Discovery and Classification\\n\\n{% if extra.ring == 'internal' %}\\n\\nTech Specific Samples\\n\\nThe RDF Import Solution Accelerator is an exemplary implementation to import an ontology and its corresponding individuals to Purview using the Purview REST API.\\n{% endif %}\\n\\nFurther Reading\\n\\nDataOps: Data Catalog\\n\\nDataOps: Data Lineage\\n\\nDataOps: Data Quality\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-discovery\\\\index.md'}, {'chunkId': 'chunk30_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Lineage\\n\\nData lineage refers to information that maps the full lifecycle of the data. It includes the origin of data, how it has been processed, and where it moves over time across the data estate.\\n\\nImportance of Data Lineage\\n\\nData Lineage provides end-to-end visibility of data through its journey from source to destination. It helps in making sure that data is coming from trusted sources. Data Lineage ensures the transformations are known and are getting loaded into specific targets. Without this visibility, it becomes challenging to accurately verify and track data.\\n\\nData Lineage can be used for different kinds of backwards-looking scenarios such as troubleshooting, tracing root cause in data pipelines and debugging. Lineage can also be used for data quality analysis, compliance and \"what if\" scenarios often referred to as impact analysis.\\n\\nHow Data Lineage works?\\n\\nAt the core of Data Lineage is the \"metadata\". Not only metadata about the source and destination, but from every transformations and operations made by every data system. Every information is then stitched together, across enterprise data systems to show an end-to-end lineage.\\n\\nThis metadata is stored into a single repository called as Data Catalog. Data Catalog is a central collection of metadata of the organizations data assets including related search and data management tools. It provides users ability to browse, search and enhance data that matters to the business.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-lineage\\\\index.md'}, {'chunkId': 'chunk30_1', 'chunkContent': \"For more information on data catalog, see DataOps: Data Catalog.\\n\\nData Lineage in Microsoft Purview\\n\\nMicrosoft Purview, is the Microsoft's flagship data governance tool. It can capture lineage for data in different parts of organization's data estate. Furthermore, it can capture data at different levels of preparation including:\\n\\nRaw data staged from various platforms\\n\\nTransformed and prepared data\\n\\nData used by visualization platforms\\n\\nOne of the platform features of Microsoft Purview is the ability to show the lineage between datasets created by data processes. Systems like Data Factory, Data Share and Power BI capture the lineage of data as it moves. Custom lineage reporting is also supported via Atlas hooks and REST API.\\n\\nHere are the Microsoft documentation links for connecting different data processing/sharing/analytics services with Microsoft Purview to track data lineage:\\n\\nAzure Data Factory Lineage\\n\\nAzure Databricks to Purview Lineage Connector\\n\\nAzure Data Share Lineage\\n\\nAzure Synapse Analytics Lineage\\n\\nPower BI Lineage\\n\\nSpark Lineage\\n\\nSQL Server Integration Services Lineage\\n\\nFor more information, see Data lineage in Microsoft Purview.\\n\\nLineage may require to create custom types that represent data Microsoft Purview doesn't know out of the box. A tutorial explains how to create custom types in Purview: Type definitions and how to create custom types in Microsoft Purview.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-lineage\\\\index.md'}, {'chunkId': 'chunk30_2', 'chunkContent': 'Further Reading\\n\\nDataOps: Data Discovery\\n\\nDataOps: Data Catalog', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-lineage\\\\index.md'}, {'chunkId': 'chunk31_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Quality\\n\\nData quality is the degree to which your data is accurate, complete, timely, and consistent with your organization's requirements. You need to constantly monitor your data sets for quality to ensure that the data applications they power remain reliable and trustworthy.\\n\\nThe quality of data is defined and measured by a set of standards called metrics. Measures of data quality include accuracy, completeness, consistency, validity, uniqueness, and timeliness.\\n\\nData Quality best practices are closely tied with the overall Data Governance strategy of an organization. The Data Owner of the datasets should set Data Quality metrics based on the identified purpose of such data.\\n\\nA related topic is data quality monitoring. It is a process that assesses the fitness of data by checking its ability to meet your business, system, and technical requirements. Details of data quality monitoring are available at DataOps: Data Quality Monitoring.\\n\\nQuality data is key to making accurate, informed decisions. Data quality issues can produce inaccurate reports, driving down operational efficiency and impacting business performance. In the world of machine learning, model trained using data of poor quality will make wrong predictions. In highly regulated industries, usage of inaccurate data can result in heavy fines for improper financial or regulatory compliance reporting.\\n\\nData quality metrics\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-quality\\\\index.md'}, {'chunkId': 'chunk31_1', 'chunkContent': 'Data quality metrics\\n\\nData quality metrics are key to assessing and increasing the quality of data products. At a global and domain level, the team needs to decide upon their quality metrics. At a minimum, the following metrics are a good starting point:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-quality\\\\index.md'}, {'chunkId': 'chunk31_2', 'chunkContent': 'Metrics Metrics definitions Completeness = % total of non-nulls + non-blanks Measures data availability, fields in the dataset that aren\\'t empty, and default values that were changed. For example, if a record includes 01/01/1900 as a data of birth, it\\'s highly likely that the field was never populated. Uniqueness = % of non-duplicate values Measures distinct values in a given column compared to the number of rows in the table. For example, given four distinct color values (red, blue, yellow, and green) in a table with five rows, that field is 80% (or 4/5) unique. Consistency = % of data having patterns Measures compliance within a given column to its expected data type or format. For example, an email field containing formatted email addresses, or a name field with numeric values. Validity= % of reference matching Measures successful data matching to its domain reference set. For example, given a country field (complying with taxonomy values) in a transactional records system, the country value of \"US of A\" isn\\'t valid. Accuracy= % of unaltered values Measures successful reproduction of the intended values across multiple systems. For example, if an invoice itemizes a SKU and extended price that differs from the original order, the invoice line item is inaccurate. Linkage = % of well-integrated data Measures successful association to its companion reference details in another', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-quality\\\\index.md'}, {'chunkId': 'chunk31_3', 'chunkContent': \"original order, the invoice line item is inaccurate. Linkage = % of well-integrated data Measures successful association to its companion reference details in another system. For example, if an invoice itemizes an incorrect SKU or product description, the invoice line item isn't linkable.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-quality\\\\index.md'}, {'chunkId': 'chunk31_4', 'chunkContent': 'Reference - CAF: Data Quality Metrics\\n\\nTechniques for Remediation\\n\\nOnce data quality issues have been identified, appropriate action should be determined on how these issues should be remediated. Generally, resolving data quality issues is highly dependent on the business and technical requirements. The following are some general techniques on how to handle data quality issues.\\n\\nRoot Cause Analysis\\n\\nOften a data quality issue is raised by an end user of the data products. To address the issue, the first step is to do a root cause analysis to find out why it happened. There is no defined list of steps for performing a root cause analysis for a data quality issue. However, here is a list from which an investigation can be started.\\n\\nLineage: start from the most upstream nodes and check for any downtime\\n\\nSchema change: check for any change in the shape of the data\\n\\nOperational environment: check for any change in the operational environment\\n\\nCode: check the code for transformation logic, edge case etc.\\n\\nInterpolation\\n\\nInterpolation, also called imputation, is a type of statistical estimation technique to fill in nulls in a time series dataset. Imputation may be applicable for certain datasets such as IoT metrics. However, it may be unacceptable for others where accuracy is paramount, such as in financial applications.\\n\\nSchema Evolution', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-quality\\\\index.md'}, {'chunkId': 'chunk31_5', 'chunkContent': \"Schema Evolution\\n\\nSchema Evolution involves dynamically adjusting the schema of the target dataset. It's done to adapt to changes in the incoming source dataset’s schema over time (schema drift). Schema evolution is especially important when the source datasets are known to change the schema frequently. Schema evolution is available in frameworks such Delta Lake and Azure Data Factory /Azure Synapse Schema drift mapping data flow.\\n\\nData Quality for Downstream Use Cases\\n\\nDespite having robust Data Quality monitoring for both data-in-flight and data-at-rest, ultimately the consumer of the dataset must decide whether the data is fit for their needs. Here metrics monitored are typically highly specific to the intended use case.\\n\\nData Quality for Machine Learning\\n\\nIn the context of machine learning, Data Quality assessment is done through feasibility studies and exploratory analysis of the datasets. This process determines if the characteristics are sufficient enough to train a viable machine learning model.\\n\\nData Quality tools and frameworks\\n\\nThe following are some of the popular tools and frameworks to monitor data quality:\\n\\nGreat Expectations - The MDW Repo: Parking Sensors sample shows how to utilize Great Expectation python framework in a larger data pipeline built with Azure Databricks and Azure Data Factory.\\n\\ndbt Unit Testing - dbt is a popular choice for modern ELT, and its tool extends the ability to add unit tests to transformed tables.\\n\\nApache Griffin\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-quality\\\\index.md'}, {'chunkId': 'chunk31_6', 'chunkContent': 'Apache Griffin\\n\\nFurther Reading\\n\\nDataOps: Data Quality Monitoring\\n\\nDataOps: Monitoring and Logging', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-quality\\\\index.md'}, {'chunkId': 'chunk32_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Versioning\\n\\nData versioning, also known as version control for data, is the practice of systematically tracking changes made to data over time. It's analogous to version control systems used in software development, such as Git, but it's applied to datasets and data-related assets. Data versioning is important for several reasons:\\n\\nReproducibility: Data versioning enables researchers, data scientists, and analysts to reproduce and validate their work by providing a historical record of changes to the data. This is crucial for ensuring the reliability and integrity of data-driven experiments and analyses.\\n\\nCollaboration: When many individuals or teams are working on the same dataset, data versioning helps manage concurrent changes and avoid conflicts. It provides a way to merge or compare different versions of the data.\\n\\nAuditing and Compliance: In regulated industries or organizations with strict data governance requirements, data versioning ensures that data changes are traceable and compliant with relevant standards and regulations.\\n\\nError Recovery: Data versioning gives you the opportunity to roll back to an earlier version of the data if there are flaws in a newer version.\\n\\nData Versioning in Machine Learning\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-versioning\\\\index.md'}, {'chunkId': 'chunk32_1', 'chunkContent': 'Data Versioning in Machine Learning\\n\\nEven with all the success in machine learning, especially with deep learning and its applications in business, data scientists still lack best practices for organizing their projects and collaborating effectively. This is a critical challenge: while ML algorithms and methods are no longer trivial knowledge, they\\'re still difficult to develop, reuse, and manage.\\n\\nTo solve these problems, you can implement data versioning using various tools, one of which is DVC (Data Version Control).\\n\\nData Version Control\\n\\nDVC is your \"Git for data\"!\\n\\nDVC is a tool for data science that takes advantage of existing software engineering tool set. It helps machine learning teams manage large datasets, make projects reproducible, and work together better.\\n\\nIf you store and process data files or datasets to produce other data or machine learning models, and you want to\\n\\ntrack and save data and machine learning models the same way you capture code;\\n\\ncreate and switch between versions of data and ML models;\\n\\nunderstand how to build datasets and ML artifacts;\\n\\ncompare model metrics among experiments;\\n\\nadopt engineering tools and best practices in data science projects;\\n\\nDVC is for you!\\n\\nSolutions', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-versioning\\\\index.md'}, {'chunkId': 'chunk32_2', 'chunkContent': \"adopt engineering tools and best practices in data science projects;\\n\\nDVC is for you!\\n\\nSolutions\\n\\nDVC lets you capture the versions of your data and models in Git commits, while storing them on-premises or in cloud storage. It also provides a mechanism to switch between these different data contents. The result is a single history for data, code, and ML models that you can traverse.\\n\\nDVC enables data versioning through codification. You produce simple metadata files once, describing what datasets, ML artifacts, etc. to track. You can put this metadata in Git instead of large files. Now you can use DVC to create snapshots of the data, restore earlier versions, reproduce experiments, record evolving metrics, etc.\\n\\nAs you use DVC, you can cache unique versions of your data files and directories in a systematic way (preventing file duplication). You separate the working data store from your workspace to keep the project light, but stays connected via file links handled automatically by DVC.\\n\\nBenefits of using the DVC approach include:\\n\\nLightweight: DVC is a free, open-source command line tool that doesn't require databases, servers, or any other special services.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-versioning\\\\index.md'}, {'chunkId': 'chunk32_3', 'chunkContent': \"Consistency: Keep your projects readable with stable file names; since the actual data files have unique pointer artifacts,\\nfile names and directories of different versions of the data need not be changed in the source code. The pointer artifact is\\nmanaged by DVC to point to the different files in storage which represent a unique dataset.\\n\\nEfficient data management: Use a familiar and cost-effective storage solution for your data and models (for example: Azure Blob, S3)—free from Git hosting constraints. DVC optimizes storing and transferring large files.\\n\\nCollaboration: Distribute your project development and share its data internally and remotely, or reuse it in other places.\\n\\nData compliance: Review data modification attempts as Git pull requests. Audit the project's immutable history to learn when datasets or models were approved, and why.\\n\\nGitOps: Connect your data science projects with the Git ecosystem. Git workflows open the door to advanced CI/CD tools, specialized patterns such as data registries, and other best practices.\\n\\n{% if extra.ring == 'internal' %}\\n\\nHow to use DVC\\n\\nA repository for a DVC tutorial can be found here.\\n\\n{% endif %}\\n\\nAzure Integration\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-versioning\\\\index.md'}, {'chunkId': 'chunk32_4', 'chunkContent': \"A repository for a DVC tutorial can be found here.\\n\\n{% endif %}\\n\\nAzure Integration\\n\\nDVC remotes give access to external storage locations to track and share your data and ML models. Usually, you will share those between devices or team members who are working on a project. For example, you can download data artifacts created by colleagues without spending time and resources to regenerate them locally.\\n\\nMain uses of remote storage:\\n\\nSynchronize large files and directories tracked by DVC.\\n\\nCentralize or distribute data storage for sharing and collaboration.\\n\\nBack up different versions of datasets and models (saving space locally).\\n\\nAzure Blob Storage\\n\\nStart with dvc remote add to define the remote. Set a name and a valid Azure Blob Storage URL:\\n\\nbash\\ndvc remote add -d myremote azure://<mycontainer>/<path>\\n\\n<container> - name of a blob container. DVC will try to create it if needed.\\n\\n<path> - optional path to a virtual directory in your bucket\\n\\nAzure Machine Learning Service\\n\\nAzure Machine Learning Service fully integrates with Azure Blob Storage to give authenticated access to data for AI training and inference workloads. AML doesn't offer a native data versioning capability, which can result in data duplication and management headaches during the AI experimentation and model generation lifecycle.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-versioning\\\\index.md'}, {'chunkId': 'chunk32_5', 'chunkContent': \"{% if extra.ring == 'internal' %}\\nDVC-AML is a custom library packaged to address versioning and de-duplication of data that acts as the source for Data Assets in an Azure Machine Learning Workspace. The solution integrates data (DVC) and code (Git) version control with AML data management APIs. It manages data assets in Azure Blob Storage without writing data twice while maintaining version history.\\n{% endif %}\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-governance\\\\data-versioning\\\\index.md'}, {'chunkId': 'chunk33_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Protection\\n\\nData Protection is the process of safeguarding digital information throughout its entire life cycle to protect it from corruption, theft, or unauthorized access. It encompasses all aspects of information security, including physical and software devices. Data protection also includes enforcing administrative control and compliance policies across the organization.\\n\\nA well-defined data protection strategy provides insights into the sensitive data and its usage, to which security practices can be applied. With control mechanisms in place, it becomes easier to enable access to the right data for the right users.\\n\\nData Encryption is also a crucial aspect of protecting data and applies to both data-at-rest and data-in-transit. The keys, certificates and secrets used for protecting data may have their own specific requirements such as:\\n\\nStoring them in hardware security modules (HSMs)\\n\\nPeriodic rotation\\n\\nOther special cases\\n\\nFor data systems, more granular control is required such as Fine-Grained access Control. The ability to secure data throughout its lifecycle is also necessary, from the time data enters the system or is generated to the eventual archival/expiration. Here is a broad categorization of the data protection capabilities:\\n\\nAccess Policies and Controls\\n\\nEntitlements\\n\\nEncryption\\n\\nMasking and Obfuscation\\n\\nAuthentication and Authorization\\n\\nFine-Grained Access Control\\n\\nData Lifecycle Management', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-protection\\\\index.md'}, {'chunkId': 'chunk34_0', 'chunkContent': 'rings:\\n  - public\\n\\nMasking and Obfuscation\\n\\nData democratization across the enterprises along with the creation of new laws and regulations for data protection is resulting in an unprecedented focus on Data privacy requirements. Having a common understanding of data privacy is important. A good definition can be found in wikipedia. It describes data privacy as \"the relationship between the collection and dissemination of data, technology, the public expectation of privacy, and the legal and political issues surrounding them.\"\\n\\nAt the core of data privacy is the type of data that needs to be protected. The first step is to classify the sensitivity of the data that is being handled and understand how that data is going to be used.\\n\\nThere are many processes that can be implemented to ensure Data Privacy is accomplished and these processes fall mainly into two categories, namely Obfuscation or Pseudonymization and Anonymization.\\n\\nWhen it comes to the technical implementation, pseudonymization techniques are different from anonymization techniques. Pseudonymization does not remove all identifying information from the data but reduces the ability to link that data with the individual identity. With anonymization, all the information that identifies an individual is scrubbed.\\n\\nObfuscation or Pseudonymization Techniques', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-protection\\\\masking-and-obfuscation\\\\index.md'}, {'chunkId': 'chunk34_1', 'chunkContent': \"Obfuscation or Pseudonymization Techniques\\n\\nData Tokenization substitutes sensitive data with a value (token) that is meaningless, and the process can't be reversed. However, the token can be mapped back to the original data. The token has no meaning outside the system that creates it and links it to other data. This technique is used to protect sensitive data and is used to protect data at rest.\\n\\nData Encryption translates the personal data into another form or code so that the data that is categorized as sensitive is replaced with data in an unreadable format. Authorized users have access to a secret key that allows them to retrieve the original data.\\n\\nAnonymization Techniques\\n\\nData Masking is a technique that can be seen as a permanent Tokenization, is used to protect data in use (not at rest) and once the data is randomized using a masking process, it cannot be reversed back to its original state. Masking techniques include:\\n\\nData Scrambling is an anonymization technique, which involves a mixing or obfuscation of characters. IMPORTANT NOTE: Such process can sometimes be reversible. It is not recommended to use it for anonymizing Personal Identifiable Information. In cases where reverting the value is allowed it can be used as a pseudonymization technique instead.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-protection\\\\masking-and-obfuscation\\\\index.md'}, {'chunkId': 'chunk34_2', 'chunkContent': 'Data Blurring is an anonymization technique that uses an approximation of data values instead of the original identifiers making  it difficult to re-identify individuals.\\n\\nBucketing or Generalization is an anonymization technique that replaces individual values of fields with a broader category.\\n\\nNulling Out is a technique that replaces sensitive data in the dataset by null values.\\n\\nCustomized anonymization uses a customized solution including one or more anonymization techniques wrapped into custom code.\\n\\nThere are assets available that might help achieving some of these goals for certain technologies and use cases. For anonymization purposes, Presidio is a popular framework, which can be considered to fall under \"customized anonymization technique\" category. Here are some of its use cases and usage examples:\\n\\nAnonymize Personal Identifiable Information using Presidio on Spark\\n\\nRun Presidio on structured / semi-structured data\\n\\nAnonymize Personal Identifiable Information entities in an Azure Data Factory ETL Pipeline\\n\\nObfuscation and Pseudonymization from an MLOps perspective\\n\\nRefer to MLOPs: Personal Identifiable Information data redaction for guidance on how these approaches can be applied on an ML project.\\n\\nFurther Reading\\n\\nDataOps: Data Protection\\n\\nDataOps: Data Security Best Practices', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-protection\\\\masking-and-obfuscation\\\\index.md'}, {'chunkId': 'chunk35_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Security Best Practices\\n\\n{% set thisPage = self._TemplateReference__context.page.url %}\\n\\nOverview\\n\\n{% set link = '../../../../code-with-devsecops/index.md' %}\\n{% if relative_path_exists(thisPage, link) %}\\nData is a fundamental target of attackers. Protecting data is a key consideration for any software design. The content under the article section builds upon the materials found in the Engineering Fundamentals and  DevSecOps sections. Refer to the relevant security content in these sections and the content presented here.\\n{% else %}\\nData is a fundamental target of attackers. Protecting data is a key consideration for any software design. The content under the article section builds upon the materials found in the Engineering Fundamentals section. Refer to the relevant security content in these sections and the content presented here.\\n{% endif %}\\n\\nMicrosoft recommends a data security strategy that follows a layered defense-in-depth approach. This approach means layering network security, access management, threat protection, and information protection to protect customer data.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-security-best-practices\\\\index.md'}, {'chunkId': 'chunk35_1', 'chunkContent': 'The Microsoft Azure Well-Architected Framework recommends a holistic approach that considers all aspects of the system architecture. Microsoft recommends considering each data service independently as well to ensure that each service is secured fully. For each Azure data service incorporated in the design, follow the recommendations posted in the Microsoft documentation linked below.\\n\\nKey Takeaways\\n\\nEnsure the overall architecture implements defense-in-depth practices.\\n\\nEvery data service in Azure has a set of security recommendations and controls that should be followed.\\n\\nEnabling Azure Defender enables automatic scanning that validates that the recommended controls are implemented correctly.\\n\\nActions\\n\\nReview the Engineering Excellence and DevSecOps playbooks.\\n\\nReview The Microsoft Azure Well-Architected Framework to ensure that the entire design follows best principals\\n\\nReview the recommendations for each service incorporated in the design\\n\\nAzure SQL\\n\\nAzure Blobs\\n\\nAzure Files (Be sure to read the related \"How-to -> Secure\" guides)\\n\\nAzure Queues and Azure Security baseline for Azure Storage\\n\\nAzure Tables\\n\\nAzure Disks\\n\\nChecklist\\n\\nEnable Azure Defender for all your storage accounts.\\n\\nTurn on soft delete for blob data.\\n\\nUse Azure AD to authorize access to blob data.\\n\\nConsider the principle of least privilege when you assign permissions to an Azure AD security principal through Azure RBAC.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-security-best-practices\\\\index.md'}, {'chunkId': 'chunk35_2', 'chunkContent': 'Consider the principle of least privilege when you assign permissions to an Azure AD security principal through Azure RBAC.\\n\\nUse managed identities to access blob and queue data.\\n\\nUse blob versioning or immutable blobs to store business-critical data.\\n\\nRestrict default internet access for storage accounts.\\n\\nEnable firewall rules.\\n\\nLimit network access to specific networks.\\n\\nAllow trusted Microsoft services to access the storage account.\\n\\nEnable the Secure transfer required option on all your storage accounts.\\n\\nLimit shared access signature (SAS) tokens to HTTPS connections only.\\n\\nAvoid and prevent using Shared Key authorization to access storage accounts.\\n\\nRegenerate your account keys periodically.\\n\\nCreate a revocation plan and have it in place for any SAS that you issue to clients.\\n\\nUse near-term expiration times on an impromptu SAS, service SAS, or account SAS.\\n\\nFurther Reading\\n\\nDataOps: Data Protection\\n\\nDataOps: Masking and Obfuscation', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-governance-and-protection\\\\data-security-best-practices\\\\index.md'}, {'chunkId': 'chunk36_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Platform Infrastructure\\n\\nThe Data Platform Infrastructure is a fundamental component that provides the foundation for managing and processing data effectively. It encompasses various elements, including storage, networking, and computing resources.\\n\\nWithin the context of the Cloud Adoption Framework (CAF), the Data Platform Infrastructure pillar offers guidance and best practices for designing and building a robust data estate on Azure. It also covers Trusted Research Environments (TRE) which are highly secure and controlled computing environments for conducting sensitive research activities.\\n\\nLastly, this pillar examines the \"Data Services on Edge\" capability, which delves into the different options Microsoft offers for processing and analyzing data closer to the sources, at the edge of the network.\\n\\nCapability Map\\n\\nHere is the high-level capability map of the Data Platform Infrastructure:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\index.md'}, {'chunkId': 'chunk37_0', 'chunkContent': \"rings:\\n  - public\\n\\nCloud Data Infrastructure\\n\\nTraditional on-premises data infrastructure has lost its popularity to modern cloud data infrastructure. It's easier and faster to deploy a cloud data infrastructure as compared to managing it on-premises.\\n\\nTypically large enterprises would have their data residing in both on-premises and Cloud. This hybrid setup gets even more complicated because of regulatory and compliance requirements. Extra planning and design are required to achieve proper security and governance.\\n\\nCloud Adoption Framework\\n\\nData Exfiltration Prevention\\n\\nTrusted Secure Environment\\n\\nData Services on Edge\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\index.md'}, {'chunkId': 'chunk38_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Services on Edge\\n\\nWith the proliferation of Internet of Things (IoT) devices and the need for real-time analytics and insights, organizations require the ability to process and analyze data closer to where it is generated. Microsoft addresses these challenges by expanding Azure's data services to edge devices and edge computing environments. This extension empowers organizations to unlock valuable insights, decrease latency, and enhance operational efficiency.\\n\\nNote: This section provides a brief overview of the available options for data processing at the edge. It doesn't contain detailed information on these options. It's highly recommended to refer to the official documentation to gain a thorough understanding of these subjects.\\n\\nAzure SQL Edge\\n\\nAzure SQL Edge is an IoT and IoT Edge optimized relational-database engine. It brings the power of Microsoft SQL Server to the edge, enabling organizations to perform real-time analytics and machine learning directly on edge devices. Azure SQL Edge is designed to run on IoT Edge devices that are often resource-constrained. Some key features of Azure SQL Edge include:\\n\\nData storage and processing: Azure SQL Edge provides a small, lightweight database engine that can run directly on edge devices. It allows users to store and process data locally, reducing the need to transfer data to a central location for analysis.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_1', 'chunkContent': 'Advanced analytics: Azure SQL Edge offers supports for in-database machine learning and advanced analytics. It allows users to train and deploy machine learning models directly on the edge devices. This capability enables real-time insights and intelligent decision-making at the edge.\\n\\nData synchronization: Azure SQL Edge supports bidirectional data synchronization between the edge and the cloud. It allows users to seamlessly move data between edge devices and Azure cloud services, ensuring data consistency and enabling hybrid scenarios.\\n\\nSecurity and compliance: Azure SQL Edge offers enterprise-grade security features to ensure data protection. It includes data encryption, secure connectivity, and access control mechanisms. In addition, Azure SQL Edge helps organizations meet their compliance requirements by integrating with Azure services like Azure Security Center and Azure Active Directory (Azure AD).\\n\\nIntegration with Azure services: Azure SQL Edge seamlessly integrates with other Azure services like Azure IoT Hub, Azure Stream Analytics, and Azure Machine Learning. This integration enables smooth data ingestion, real-time streaming analytics, and integration with the broader Azure ecosystem.\\n\\nDeployment Models\\n\\nAzure SQL Edge supports two deployment modes:\\n\\nConnected Deployment through Azure IoT Edge\\n\\nIn a connected deployment, Azure SQL Edge remains connected to the Azure cloud, enabling real-time data synchronization, management, and monitoring capabilities. Connected deployment mode is suitable when the edge device has reliable and continuous network connectivity to the Azure cloud.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_2', 'chunkContent': \"Azure SQL Edge can integrate with Azure IoT Edge to extend the capabilities of edge devices. By using Azure IoT Edge, Azure SQL Edge instances can be deployed and managed on edge devices as modules. This allows users to take advantage of Azure IoT Edge's device management, security, and connectivity features while running Azure SQL Edge for local data storage, processing, and analytics.\\n\\nIn a connected deployment, Azure SQL Edge can use Azure services like Azure Stream Analytics, Azure Machine Learning (support for models in ONNX format), and other cloud-based services. It enables real-time data ingestion, streaming analytics, and the ability to perform advanced analytics and machine learning in the cloud.\\n\\nAzure SQL Edge in a connected deployment also benefits from automatic updates, patches, and improvements delivered through the Azure cloud. This ensures that the edge devices are up to date with the latest features and security enhancements.\\n\\nData synchronization in a connected deployment is seamless and real-time, allowing users to have a synchronized view of their data across edge devices and the cloud. It enables centralized reporting, monitoring, and analysis, while still benefiting from local data storage and processing capabilities on the edge device.\\n\\nFor more information about connected deployment mode, see Deploy Azure SQL Edge.\\n\\nDisconnected (Offline) Deployment\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_3', 'chunkContent': \"For more information about connected deployment mode, see Deploy Azure SQL Edge.\\n\\nDisconnected (Offline) Deployment\\n\\nIn offline deployment, Azure SQL Edge operates in a disconnected mode without requiring a continuous connection to the Azure cloud. The disconnected mode is suitable for edge devices deployed in remote or disconnected environments, where network connectivity is limited, unreliable, or expensive.\\n\\nIn this deployment option, Azure SQL Edge runs as a self-contained database engine directly on the edge device. The Azure SQL Edge container images can be pulled from docker hub and deployed either as a standalone docker container or on a kubernetes cluster. It provides local storage, processing capabilities, and in-database analytics. You can deploy and manage the database instances directly on the edge device without relying on cloud connectivity.\\n\\nPeriodically, when connectivity is available, the data can be synchronized between the edge device and the Azure cloud to enable data backup, reporting, and centralized management. This synchronization can be done using various methods such as scheduled data transfers, intermittent connectivity, or manual batch transfers.\\n\\nFor more information, see Deploy Azure SQL Edge with Docker and Deploy an Azure SQL Edge container in Kubernetes.\\n\\nBoth deployment options offer flexibility and scalability to meet the requirements of edge computing scenarios, whether they're in remote, disconnected environments or in environments with reliable cloud connectivity.\\n\\nConsiderations\\n\\nFeature limitations\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_4', 'chunkContent': \"Considerations\\n\\nFeature limitations\\n\\nAlthough Azure SQL Edge is built on the latest SQL engine, it's crucial to recognize the feature differences between Azure SQL Edge and SQL Server. Due to Azure SQL Edge's target use cases, which involve real-time data storage, processing, and analytics, traditional services such as analysis services and reporting services that can be optionally installed on SQL Server aren't available on Azure SQL Edge.\\n\\nFor the full list of unsupported features, see the documentation.\\n\\nCost planning\\n\\nAzure SQL Edge pricing is based on a per-device model, with reserved instances available for purchase for 1 or 3 years. When selecting the SQL Edge service, it's essential to determine the number of SQL Edge instances needed for the solution.\\n\\nOperation\\n\\nSince Azure SQL Edge instances are deployed on the Edge, it's crucial to account for the operational costs and skill requirements associated with maintaining, tuning, backing up, updating, and monitoring the deployed services.\\n\\nIn summary, Azure SQL Edge empowers organizations to leverage the power of SQL Server and Azure services at the edge, enabling edge computing scenarios such as remote monitoring, predictive maintenance, and real-time analytics in environments with limited connectivity or latency requirements. For detailed information about Azure SQL Edge, see the documentation.\\n\\nAzure Stack HCI\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_5', 'chunkContent': \"Azure Stack HCI\\n\\nAzure Stack HCI is a hyperconverged infrastructure (HCI) cluster solution that hosts virtualized Windows and Linux workloads and their storage in a hybrid environment that combines on-premises infrastructure with Azure cloud services. Azure Stack HCI solutions are delivered by Microsoft's network of hardware partners, and can be used to run virtualized applications on-premises while easily connecting to Azure for hybrid scenarios such as cloud backup, cloud-based monitoring, and more.\\n\\nAzure Stack HCI allows full, direct access to the underlying hardware and the operating system running on cluster nodes. Azure Stack HCI operations can be managed the same way as any other Windows Server-based cluster, using such tools as Windows PowerShell and Server Manager. Using Windows Admin Center, for optimized experience, is recommended.\\n\\nFor detailed information, see the official documentation on Azure Stack HCI.\\n\\nSQL Server on Azure Stack HCI\\n\\nAzure Stack HCI is a highly available, cost efficient, and flexible platform to run SQL Server and supports Online Transaction Processing (OLTP) workloads, data warehouse and BI, and AI and advanced analytics over big data.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_6', 'chunkContent': \"SQL Server can be run on virtual machines (VMs) that use either Windows Server or Linux, which allows consolidation of multiple database workloads. And more VMs can be added to the Azure Stack HCI environment as needed. Azure Stack HCI also enables the integration of SQL Server with Azure Site Recovery to provide a cloud-based migration, restoration, and protection solution.\\n\\nFor deployment instructions, see Deploy SQL Server on Azure Stack HCI.\\n\\nAzure Arc\\n\\nAzure Arc is a set of services provided by Microsoft Azure that enables customers to manage and govern resources across different environments, including on-premises, multi-cloud, and edge locations. It extends Azure's management capabilities to external infrastructure and platforms, allowing organizations to centralize control, gain visibility, and apply consistent policies across their entire hybrid and distributed environments.\\n\\nAzure Arc provides the following key capabilities:\\n\\nResource Management: With Azure Arc, users can bring external resources, such as servers, Kubernetes clusters, and data services, into the Azure portal, Azure Resource Manager, and Azure Policy. This allows the management and organization of these resources alongside Azure resources, using familiar tools and processes.\\n\\nGovernance and Compliance: Azure Arc helps enforce consistent policies and governance across diverse environments. You can apply Azure Policy, Azure Security Center, and Azure Monitor to monitor and manage the compliance and security posture of the resources, regardless of their location.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_7', 'chunkContent': \"Azure Management Services: Azure Arc integrates with Azure management services, such as Azure Monitor and Azure Automation, to provide a consistent monitoring and automation experience across all environments. These services can be used to gain insights, set up alerts, automate tasks, and ensure the health and performance of the resources.\\n\\nKubernetes Management: Azure Arc extends Azure Kubernetes Service (AKS) to enable the management of Kubernetes clusters that are not running on Azure. The external Kubernetes clusters can be registered with Azure Arc and leverage the Azure portal, Azure Policy, Azure Security Center, and other Azure services to manage and govern these clusters consistently. For more information about Azure Arc-enable Kubernetes, see Azure Arc-enabled Kubernetes Clusters.\\n\\nData Services: Azure Arc enables the deployment and management of Azure data services, such as Azure SQL Managed Instance and Azure PostgreSQL Hyperscale, on any infrastructure, whether it's on-premises, in other clouds, or at the edge. It allows customers to leverage the benefits of Azure's managed data services in a hybrid or multi-cloud environment. This capability is discussed in details in the next section.\\n\\nAzure Arc-enabled Data Services\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_8', 'chunkContent': 'Azure Arc-enabled Data Services\\n\\nAzure Arc-enabled data services refer to a subset of Azure data services that can be extended and deployed outside of the Azure cloud. Azure Arc makes it possible to run Azure data services on-premises, at the edge, and in public clouds using Kubernetes and the infrastructure of their choice.\\n\\nCurrently, the following Azure Arc-enabled data services are available:\\n\\nAzure Arc-enabled SQL Managed Instance\\n\\nAzure Arc-enabled PostgreSQL (preview)\\n\\nThe following architecture diagram provides the details about the various components that make up Azure Arc-enabled data services:\\n\\nThe first step for using Arc-enabled Data Services is to deploy Kubernetes on the chosen infrastructure. There are several Kubernetes distributions that are supported such as AKS, AKS on Azure Stack, Azure RedHat OpenShift, and so on (see official documentation for complete list). Azure arc-enabled data services are deployed as a set of pods based on Microsoft Container Registry (MCR) container images in Kubernetes nodes. All services use one of the key components of the Kubernetes platform, the Kubernetes API.\\n\\nConnectivity Modes', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_9', 'chunkContent': \"Connectivity Modes\\n\\nWhen deploying Azure Arc-enabled data services components, the connectivity mode is a crucial choice to be made based on organization's business policy, government regulations or the availability of network connectivity to Azure. The following connectivity modes are currently supported:\\n\\nDirect Connected Mode: With direct connected mode, users can use the Azure Resource Manager with the Azure portal to deploy and manage Azure Arc-enabled data services. Azure Active Directory (Azure AD) and Azure Role-Based Access Control (RBAC) can be used in the directly connected mode only. Also, inventory, logs, metrics, and billing information is automatically sent to Azure.\\n\\nIndirect Connected Mode: With indirect connected mode, all deployment and management operations for Azure Arc-enabled data services are done either with the Azure command-line interface (CLI) using arcdata extension, or with tools like kubectl and Azure Data Studio. These tools interact directly with the Kubernetes API to manage Azure Arc-enabled data services. Inventory and billing can be manually exported and uploaded to Azure using the Azure CLI. Logs and metrics can be optionally uploaded to Azure Monitor.\\n\\nFor exploring the connectivity modes in detail, see Connectivity modes and requirements.\\n\\nAzure Arc-enabled SQL Server\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_10', 'chunkContent': 'For exploring the connectivity modes in detail, see Connectivity modes and requirements.\\n\\nAzure Arc-enabled SQL Server\\n\\nAzure Arc-enabled SQL Server extends the capabilities of Microsoft SQL Server to hybrid and multi-cloud environments. Azure Arc-enabled SQL Server provides a unified management experience, allowing organizations to deploy, configure, and monitor SQL Server instances across on-premises, cloud, and edge environments using familiar Azure tools. It enables centralized management, advanced services integration, and enhanced security features for SQL Server workloads, ensuring consistency and control while leveraging the power of Azure.\\n\\nHere is an illustration of the architecture of Azure Arc-enabled SQL Server.\\n\\nThe SQL Server instances can be installed in a virtual or physical machine running Windows or Linux. The Azure Connected Machine agent and the Azure extension for SQL Server establish secure connections to Azure by utilizing outbound HTTPS traffic on TCP port 443 with SSL. This allows them to communicate with multiple Azure services while ensuring data security. For SQL Server instances on Windows machines, Microsoft Defender for Cloud can be optionally configured.\\n\\nFor more information on Arc-enabled SQL Server, see Azure Arc-enabled SQL Server.\\n\\nAzure Stack HCI on Azure Arc', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk38_11', 'chunkContent': \"For more information on Arc-enabled SQL Server, see Azure Arc-enabled SQL Server.\\n\\nAzure Stack HCI on Azure Arc\\n\\nAzure Stack HCI is also supported by Azure Arc. Azure Stack HCI combines the advantages of hyperconverged infrastructure (HCI) with the flexibility and scalability of Azure. By integrating Azure Arc with Azure Stack HCI, organizations can achieve a consistent and unified management experience across their hybrid environment. Due to the Azure Arc support for Azure Stack HCI, it's possible to use Azure Arc-enabled data services, such as arc-enabled SQL Server, arc-enabled Kubernetes, and more, on the Azure Stack HCI deployment.\\n\\nThe Azure Arc agent is already installed on Azure Stack HCI cluster nodes. It is included as part of the operating system, so there is no need for manual installation. The agent is activated by registering the Azure Stack HCI cluster with Azure Arc, which automatically enables Azure monitoring, support, and billing. See the Microsoft documentation for details around managing and monitoring Azure Stack HCI-based virtualized workloads with Azure Arc.\\n\\nReferences\\n\\nMicrosoft Learn: Introduction to Azure Arc\\n\\nMicrosoft Learn: Azure SQL Edge\\n\\nMicrosoft Learn: Azure Stack HCI\\n\\nMicrosoft Learn: Introduction to Azure Arc-enabled data services\\n\\nMicrosoft Training: Introduction to Azure Arc-enabled data services\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\data-platform-infrastructure\\\\cloud-data-infrastructure\\\\data-services-on-edge\\\\index.md'}, {'chunkId': 'chunk39_0', 'chunkContent': 'rings:\\n  - public\\n\\nDevOps for Data\\n\\n{% set thisPage = self._TemplateReference__context.page.url %}\\n\\nDevOps can be defined as the union of people, process, and products to enable continuous delivery of value to the business. It\\'s an iterative process of \"Developing\", \"Building & Testing\", \"Deploying\", \"Operating\", \"Monitoring and Learning\" and \"Planning and Tracking\".\\n\\nThe application of DevOps principles to Data can be understood through the concepts of Data and CI/CD pipelines:\\n\\nData Pipelines vs CI/CD Pipelines\\n\\nData Pipeline: Also termed as \"value\" pipelines, the data pipelines convert the raw data into meaningful information, thus delivering value to the business. Data engineers generally own the data ingestion, transformation and sharing processes that are part of data pipelines. Data engineers are responsible for coding the business requirements into data pipelines.\\n\\nCI/CD Pipeline: CI/CD pipelines are one of the most critical parts of DevOps in general. In the context of data systems, CI/CD pipelines continuously update data pipelines in different environments as new ideas are developed, tested, and deployed to Production. The CI/CD pipeline is often termed as an \"innovation\" pipeline as it facilitates the change process. The Platform automation and operations team typically owns the maintenance of CI/CD pipelines.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\index.md'}, {'chunkId': 'chunk39_1', 'chunkContent': 'Unlike in the typical CI/CD scenario, here we have two different orchestrators to maintain. One is for the data pipeline and the other one for the CI/CD pipeline. The testing strategy for these two pipelines is going to differ as well. For the data pipeline, the data is changing while the CI/CD pipeline is fixed. Conversely for the CI/CD pipeline the data is fixed while the CI/CD pipeline itself is changing.\\n\\n\"DevOps for Data\" extends the existing DevOps tools and processes and puts them to work for enterprise data systems. To gain a better understanding of these topics, read the DataKitchen: DataOps Cookbook.\\n\\nCapability Map\\n\\nHere is a high-level capability map for DevOps for Data:\\n\\n{% set link = \\'../../../../code-with-devsecops/index.md\\' %}\\n{% if relative_path_exists(thisPage, link) %}\\nIn the above capability map, \"DevSecOps Capabilities\" refers to content covered in the DevSecOps section of this playbook.\\n{% endif %}', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\index.md'}, {'chunkId': 'chunk40_0', 'chunkContent': \"rings:\\n  - public\\n\\nChange Control\\n\\nChange control is about enabling fast and reliable application and infrastructure changes. Change control also manages the stability, performance, and security of different environments during these changes.\\n\\nContinuous integration (CI) process allows developers to merge their code changes in centralized repository that's used to build artifacts and run unit tests. Continuous delivery (CD) process takes these build artifacts and facilitates the deployment in different environments.\\n\\nContinuous testing (CT) process ensures that the appropriate unit, integration, regression tests etc. are run automatically during the CI/CD process.\\n\\nFor data-centric applications like data lakes, both data and CI/CD pipelines are to be managed and tested. The below links offer more information:\\n\\nDataOps: Continuous Integration\\n\\nDataOps: Continuous Delivery\\n\\nDataOps: Continuous Testing\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\index.md'}, {'chunkId': 'chunk41_0', 'chunkContent': \"rings:\\n  - public\\n\\nCI/CD for Azure Databricks\\n\\nProblem Statement\\n\\nBy using the Medallion architecture, data pipelines can be created on Azure Databricks to build a reliable and trustworthy central hub for enterprise data. In order to maintain such a system on production, it's important to test and safely deploy the underlying code.\\n\\nThis article explains how to implement a CI/CD approach to develop and deploy data pipelines on Azure Databricks using Azure Pipelines.\\n\\nArchitecture\\n\\nHere is the high-level architecture diagram of the solution:\\n\\nDatabricks\\n\\nThe data pipelines run on Azure Databricks. It is best practice to have separate Databricks workspaces for development and testing (integration), staging, and production environments.\\n\\nSource Control\\n\\nA source code repository is needed to store the data pipelines. The example uses Azure Repos, but other Git providers can be used as well. Databricks repos provide a repository-level integration with Git providers. This integration allows developers to develop and test code as notebooks and sync it with a remote Git repository.\\n\\nCI pipeline\\n\\nThe CI pipeline includes two stages: running unit tests and then publishing artifacts for CD pipelines.\\n\\nCD pipeline\\n\\nThe CD pipeline includes three stages:\\n\\nDeploy data pipelines as Databricks jobs to staging environment.\\n\\nRun integration tests over the deployed data pipelines on the staging environment.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_1', 'chunkContent': \"Deploy data pipelines as Databricks jobs to staging environment.\\n\\nRun integration tests over the deployed data pipelines on the staging environment.\\n\\nIf integration tests pass, deploy data pipelines to production environment.\\n\\nCI/CD is a general methodology, and can be implemented in different ways. Below are different implementations for some of DevOps CI/CD stages depending on whether the pipelines use Python modules/wheels or pure notebooks.\\n\\nPipeline Stage Python modules/wheels solution Notebooks solution CI Run unit tests Use Pytest to do unit tests Use Nutter to do notebook tests CI Publish artifacts Publish built Python wheel files Publish notebooks CD Deploy Databricks jobs Install Python wheel to Databricks cluster before deploy Databricks jobs Import notebooks to Databricks workspace before deploy Databricks jobs\\n\\nSample CI/CD Pipeline Overview\\n\\nLet's take Python notebooks solution and Azure CI/CD pipelines as an example to elaborate more details on the whole end-to-end DevOps solution. Below are required key steps.\\n\\nDevelop and test data pipeline notebooks in Dev Databricks workspace. If following the Medallion Architecture, create three notebooks for data ingestion, transformation, and curation respectively. Create relevant notebook tests using Nutter framework.\\n\\nAdd a Git repo and commit relevant data pipeline and test notebooks to a feature branch.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_2', 'chunkContent': 'Add a Git repo and commit relevant data pipeline and test notebooks to a feature branch.\\n\\n\"Checkout\" the Git repo in local IDE and add YAML files for Azure CI/CD pipelines.\\n\\nThe CI pipeline runs unit tests (via triggering notebooks), then publishes the notebooks as artifact.\\n\\nThis example uses Nutter CLI to trigger notebook Nutter tests\\nIf notebook tests pass, then in publishing artifact stage, all notebooks will be packed as an Azure artifact that will be used in CD pipeline.\\n\\n\\nBelow are relevant pipeline stages yaml configurations:\\n - It uses databricks CLI to import source notebooks to user account\\'s Databricks workspace, so that tests triggered by different developers with different commits won\\'t affect each other.\\n - It uses Nutter CLI to run the unit tests.\\n\\nFor more details on how to write Nutter tests, please refer Nutter notebook test framework and  modern-data-warehouse-dataops/single_tech_samples/databricks for documentation and examples.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_3', 'chunkContent': \"```yaml\\n stages:\\n - stage: UT\\n   displayName: Unit Test\\n   variables:\\n   - group: dbx-data-pipeline-ci-cd\\n   jobs:\\n   - job: NotebooksUT\\n     pool:\\n       vmImage: ubuntu-latest\\n     displayName: Notebooks Unit Test\\n steps:\\n - task: UsePythonVersion@0\\n   inputs:\\n     versionSpec: '$(pythonVersion)'\\n\\n - script: |\\n     python -m pip install --upgrade pip\\n     pip install databricks-cli\\n     pip install nutter\\n   displayName: 'Install databricks and nutter CLI'\\n\\n - script: |\\n     databricks workspace import_dir --overwrite . /Users/$(Build.RequestedForEmail)/$(Build.Repository.Name)/$(Build.SourceVersion)/\\n   displayName: Import unit test notebook to Databricks workspace\\n   env:\\n     DATABRICKS_HOST: $(DATABRICKS_HOST)\\n     DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_4', 'chunkContent': \"- script: |\\n     databricks libraries install --cluster-id $CLUSTER --pypi-package nutter\\n   displayName: 'Install nutter library on Databricks Cluster'\\n   env:\\n     CLUSTER: $(DATABRICKS_CLUSTER)\\n     DATABRICKS_HOST: $(DATABRICKS_HOST)\\n     DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\\n\\n - script: |\\n     nutter run //Users/$(Build.RequestedForEmail)/$(Build.Repository.Name)/$(Build.SourceVersion)/tests/ $CLUSTER --recursive --timeout 600 --junit_report\\n   displayName: 'Run nutter unit tests'\\n   env:\\n     CLUSTER: $(DATABRICKS_CLUSTER)\\n     DATABRICKS_HOST: $(DATABRICKS_HOST)\\n     DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_5', 'chunkContent': \"- task: PublishTestResults@2\\n   inputs:\\n     testResultsFormat: 'JUnit'\\n     testResultsFiles: '**/test-*.xml'\\n     testRunTitle: 'Publish test results'\\n   condition: succeededOrFailed()\\n\\n\\nstage: PublishArtifact\\n   condition: succeeded('UT')\\n   displayName: Publish Notebooks Artifact\\n   jobs:\\njob: PublishingNotebooks\\n     displayName: Publish artifacts\\n     steps:\\ntask: PublishPipelineArtifact@1\\n   displayName: Publish artifacts\\n   inputs:\\n     targetPath: '$(Pipeline.Workspace)'\\n     artifact: 'DBXDataPipelinesArtifact'\\n     publishLocation: 'pipeline'\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_6', 'chunkContent': '```\\n\\nThe CD pipeline includes the stages below:\\n\\nDeploy the entire data pipeline as a Databricks job to the staging environment using the Databricks CLI and dbx (Databricks CLI eXtensions) CLI, utilizing the dbx deployment.json format.\\n\\n```sh\\n   # Import source notebooks from Azure artifact to target Databricks workspace\\n   databricks workspace import_dir --overwrite \"{source_notebooks_path}\" \"{target_notebooks_path}\"\\n# Export environment variable DATABRICKS_HOST and DATABRICKS_TOKEN before running below command to configure the project environment\\n   dbx configure\\n# Define Databricks job in deployment.json file to chain those three data pipeline stages, then \\'dbx deploy\\' would help to create defined Databricks job\\n   # in target Databricks workspace; or use --assets-only options to only upload job definition to Databricks but not create actual job in workflows/jobs UI\\n   dbx deploy --deployment-file=deployment.json --assets-only --tags commit_id=$commit_id --no-rebuild', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_7', 'chunkContent': \"```\\n\\n\\nRun integration test on staging Databricks workspace.\\n\\nTrigger the data pipeline job by 'dbx launch'.\\nsh\\n # The --from-assets option tells dbx to launch a job previously deployed with --assets-only option\\n dbx launch master_data_pipeline --from-assets --tags commit_id=$commit_id --trace\\n\\n\\nRun an integration test notebook to fetch the curated data results and do some assertion.\\n\\nIf the integration test passes, do the same as the first stage but this time deploy data pipeline to Production environment instead.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_8', 'chunkContent': \"Below are yaml configures of relevant CD pipeline stages:\\n - Deploy job to STG\\n   - In this stage, source notebooks are imported to user account's workspace to segregate different pipeline runs triggered by different developers and commits.\\n   - Before deploying data pipelines, paths of referenced notebooks are updated in deployment.json file to link the right notebooks imported in previous task.\\n   - The pipeline uses --assets-only and --tags options when deploying data pipelines to Databricks. With the --assets-only option, workflow definition, relevant referenced files and core packages will be uploaded to Databricks default artifacts storage, but actual data pipelines won't be created or updated in workflows/jobs UI. Identifier tags like commit ID allow avoiding potential conflicts from different pipeline tests. For more information, see dbx deploy reference.\\n - Integration Tests\\n   - In this stage, the pipeline uses --from-assets and --tags options to locate the correct data pipeline artifacts to test. It then triggers a one-time run using the dbx launch command. For more information, see: dbx launch reference.\\n - Deploy job to PROD\\n   - In this stage, the --assets-only option is not specified for dbx deploy command to create/update data pipelines in workflows/jobs UI.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_9', 'chunkContent': 'This sample uses the dbx CLI to deploy and launch data pipelines/workflows. An alternate option is utilizing the Databricks REST Jobs APIs which will need to include logic such as running a data pipeline by REST job API, waiting for its completion, validating the test results in a single test script. This test can be run as a single task in the integration test stage.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_10', 'chunkContent': \"```yml\\n stages:\\n - stage: DeployDatabricksJob2STG\\n   displayName: Deploy job to STG\\n   variables:\\n   - group: dbx-data-pipeline-ci-cd\\n   jobs:\\n   - deployment: DeployDatabricksJob2STG\\n     displayName: Deploy workflow/job to STG\\n     environment: 'STG'\\n     strategy:\\n       runOnce:\\n         deploy:\\n           steps:\\n           - task: UsePythonVersion@0\\n             inputs:\\n               versionSpec: '$(pythonVersion)'\\n       - script: |\\n           python -m pip install --upgrade pip\\n           pip install databricks-cli\\n           pip install dbx\\n         displayName: 'Install Databricks and dbx CLI'\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_11', 'chunkContent': \"- script: |\\n           cd $(Pipeline.Workspace)/ci_artifacts/DBXDataPipelinesArtifact/s\\n           databricks workspace import_dir --overwrite . $WORKSPACE_DIR\\n         env:\\n           DATABRICKS_HOST: $(DATABRICKS_HOST)\\n           DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\\n           WORKSPACE_DIR: /Users/$(Build.RequestedForEmail)/$(Build.Repository.Name)/$(Build.SourceVersion)\\n         displayName: 'Import notebooks to STG Databricks'\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_12', 'chunkContent': '- script: |\\n           cd $(Pipeline.Workspace)/ci_artifacts/DBXDataPipelinesArtifact/s\\n           sed -i \"s|{workspace_dir}|$WORKSPACE_DIR|g\" deployment.json\\n           dbx configure\\n           dbx deploy --deployment-file=deployment.json --assets-only --tags commid_id=$(Build.SourceVersion) --no-rebuild\\n         displayName: \\'Deploy Databricks workflow/job by dbx\\'\\n         env:\\n           DATABRICKS_HOST: $(DATABRICKS_HOST)\\n           DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\\n           WORKSPACE_DIR: /Users/$(Build.RequestedForEmail)/$(Build.Repository.Name)/$(Build.SourceVersion)', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_13', 'chunkContent': \"stage: IT\\n   condition: succeeded('DeployDatabricksJob2STG')\\n   displayName: Integration Test\\n   variables:\\ngroup: dbx-data-pipeline-ci-cd\\n   jobs:\\n\\njob: NotebooksIT\\n     pool:\\n       vmImage: ubuntu-latest\\n     displayName: Notebooks Integration Test\\nsteps:\\n - task: UsePythonVersion@0\\n   inputs:\\n     versionSpec: '$(pythonVersion)'\\n\\n\\nscript: |\\n     python -m pip install --upgrade pip\\n     pip install databricks-cli\\n     pip install dbx\\n     pip install nutter\\n   displayName: 'Install databricks, dbx and nutter CLI'\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_14', 'chunkContent': \"script: |\\n     databricks libraries install --cluster-id $CLUSTER --pypi-package nutter\\n   displayName: 'Install Nutter Library on Databricks cluster'\\n   env:\\n     CLUSTER: $(DATABRICKS_CLUSTER)\\n     DATABRICKS_HOST: $(DATABRICKS_HOST)\\n     DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\\n\\n\\nscript: |\\n     dbx configure\\n     dbx launch master_data_pipelines_by_dbx --from-assets --tags commid_id=$(Build.SourceVersion) --trace --existing-runs wait\\n   displayName: 'Run deployed Databricks data pipeline/workflow'\\n   env:\\n     DATABRICKS_HOST: $(DATABRICKS_HOST)\\n     DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_15', 'chunkContent': \"script: |\\n     nutter run /Users/$(Build.RequestedForEmail)/$(Build.Repository.Name)/$(Build.SourceVersion)/integration_tests/ $CLUSTER --recursive --timeout 600 --junit_report\\n   displayName: 'Run integration tests'\\n   env:\\n     CLUSTER: $(DATABRICKS_CLUSTER)\\n     DATABRICKS_HOST: $(DATABRICKS_HOST)\\n     DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\\n\\n\\ntask: PublishTestResults@2\\n   inputs:\\n     testResultsFormat: 'JUnit'\\n     testResultsFiles: '*/test-.xml'\\n     testRunTitle: 'Publish test results'\\n   condition: succeededOrFailed()\\n\\n\\n\\n\\nstage: DeployDatabricksJob2PROD\\n   condition: succeeded('IT')\\n   displayName: Deploy job to PROD\\n   variables:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_16', 'chunkContent': \"group: dbx-data-pipeline-ci-cd\\n   jobs:\\ndeployment: DeployDatabricksJob2PROD\\n     displayName: Deploy workflow/job to PROD\\n     environment: 'PROD'\\n     strategy:\\n       runOnce:\\n         deploy:\\n           steps:\\n           - task: UsePythonVersion@0\\n             inputs:\\n               versionSpec: '$(pythonVersion)'   - script: |\\n       python -m pip install --upgrade pip\\n       pip install databricks-cli\\n       pip install dbx\\n     displayName: 'Install Databricks and dbx CLI'\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_17', 'chunkContent': \"- script: |\\n       cd $(Pipeline.Workspace)/ci_artifacts/DBXDataPipelinesArtifact/s\\n       databricks workspace import_dir --overwrite src $WORKSPACE_DIR/src\\n     env:\\n       DATABRICKS_HOST: $(DATABRICKS_HOST)\\n       DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\\n       WORKSPACE_DIR: $(PROD_WORKSPACE_DIR)\\n     displayName: 'Import notebooks to PROD Databricks'\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_18', 'chunkContent': '- script: |\\n       cd $(Pipeline.Workspace)/ci_artifacts/DBXDataPipelinesArtifact/s\\n       sed -i \"s|{workspace_dir}|$WORKSPACE_DIR|g\" deployment.json\\n       dbx configure\\n       dbx deploy --deployment-file=deployment.json --no-rebuild\\n     displayName: \\'Deploy Databricks workflow/job by dbx\\'\\n     env:\\n       DATABRICKS_HOST: $(DATABRICKS_HOST)\\n       DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\\n       WORKSPACE_DIR: $(PROD_WORKSPACE_DIR)', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk41_19', 'chunkContent': '```\\n\\nUsing the above CI/CD pipelines with unit and integration tests allows changes to data pipelines to be safely and efficiently deployed across environments.\\n\\nFor more example implementations of CI/CD on Azure Databricks, see Azure Databricks: Use CI/CD.\\n\\nFor a concrete code implementation of CI/CD on Azure Databricks, see MDW DataOps Repo: Azure Databricks CI/CD\\n\\nFurther Reading\\n\\nDataOps: Continuous Integration\\n\\nDataOps: Continuous Delivery', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\devops-for-databricks-data-pipelines-solution.md'}, {'chunkId': 'chunk42_0', 'chunkContent': 'rings:\\n  - public\\n\\nContinuous Delivery\\n\\nContinuous Delivery (CD) is the ability to get code changes into production safely and quickly in a sustainable way. Continuous delivery builds upon continuous integration (CI). It is the process of taking the build artifacts and deploys them to different environments, such as QA and Staging. CD helps in testing the new changes for stability, performance, and security.\\n\\nContinuous Delivery typically requires manual approval before the new changes are pushed to production. With continuous deployment, change to production happens automatically without explicit approval.\\n\\nFor more information on continuous delivery, see Engineering Fundamentals: Continuous Delivery.\\n\\nContinuous Delivery Tools\\n\\nThere are various software development tools available in the market that supports the orchestration of the whole CI/CD process:\\n\\nAzure Pipelines\\n\\nAzure Pipelines supports CI and CD to build, deploy and test the code. Azure Pipelines provides a YAML pipeline editor that can be used to author and edit the pipelines.\\n\\nGitHub Actions\\n\\nGitHub Actions is another great option for CI/CD.\\n\\nContinuous Delivery Process\\n\\nConfiguration Management\\n\\nConfiguration Management is the process of managing configuration settings and sensitive data across different environments. It involves handling and organizing these variables in a way that ensures proper management and control. Examples of such information are server name, usernames, passwords, api keys, SAS tokens, etc.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\index.md'}, {'chunkId': 'chunk42_1', 'chunkContent': \"The general approach is to keep secrets in separate configuration files that are not checked in to the repo. Add the files to the .gitignore to prevent that they're checked in. Once deployed, services like Azure Key Vault can be used to store the secrets.\\n\\nFor more information, see Engineering Fundamentals: Secrets Management.\\n\\nTech-Specific Samples\\n\\nThe MDW Repo: Azure SQL and MDW Repo: Azure Data Factory samples showcase how to manage secrets using pipeline variables and Azure Key Vault.\\n\\nTest Automation\\n\\nContinuous Integration typically involves performing validation steps to run a set of automated unit tests. These tests are designed to check if the application components are functioning according to their intended design and behavior. But even if all unit tests pass, there is no guarantee that the system will behave as expected after the changes are deployed.\\n\\nWith continuous Delivery, developers can automate testing beyond unit tests in order to verify application updates across multiple dimensions before deploying them to production. The types of tests may include end-to-end testing, load testing, integration testing, API reliability testing, and so on.\\n\\nFor detailed information on test automation, see DataOps: Continuous Testing.\\n\\nMulti Stage Deployment\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\index.md'}, {'chunkId': 'chunk42_2', 'chunkContent': 'For detailed information on test automation, see DataOps: Continuous Testing.\\n\\nMulti Stage Deployment\\n\\nContinuous Delivery involves deploying code changes to multiple environments, making it essential to carefully plan the pre-production stages. This planning process helps ensure a shared understanding between application engineers and business stakeholders. It includes aligning expectations regarding the number of pre-provisioned environments and defining the roles and responsibilities for approval gates. This alignment allows for effective coordination and collaboration throughout the software delivery process.\\n\\nFor more information, see Engineering Fundamentals: Modeling your Release Pipeline.\\n\\nTech-Specific Samples\\n\\nThe Azure SQL sample showcases how to create a release pipeline with multiple stages using Azure Pipelines. The MDW Repo: Parking Sensor sample demonstrates how DevOps principles can be applied to end-to-end Data Pipeline Solution built according to the Modern Data Warehouse (MDW) pattern.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\index.md'}, {'chunkId': 'chunk42_3', 'chunkContent': 'The MDW Repo: ADF CI/CD Auto Publish sample demonstrates the deployment of Azure Data Factory (ADF) using Automated Publish Method. Usually, ADF deployment requires Manual Publish setup where the developer publishes the ADF changes manually from the portal. This step generates ARM Templates that are used in deployment steps. The sample eliminates the manual publish by using a publicly available npm package @microsoft/azure-data-factory-utilities for automated publishing. Also, check the official documentation on CI/CD in ADF and Automated publishing for CI/CD.\\n\\nFurther Reading\\n\\nDataOps: Continuous Integration\\n\\nDataOps: Continuous Testing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-delivery\\\\index.md'}, {'chunkId': 'chunk43_0', 'chunkContent': 'rings:\\n  - public\\n\\nContinuous Integration\\n\\nContinuous Integration (CI) is a DevOps where the code changes made by different contributors are combined in a central place. After that, automated builds and tests are performed on the combined code.\\n\\nContinuous Integration (CI) is a DevOps practice of merging the code changes from different contributors to a centralized repository.  where the automated builds and tests are then run. CI helps in identifying bugs or issues early in the development lifecycle when they are easier and faster to fix.\\n\\nContinuous Integration systems produce deployable artifacts, which include infrastructure and apps. These artifacts are then consumed by the release pipelines in the Continuous deployment process to release new versions and fixes.\\n\\nFor detailed information and best practices around Continuous Integration, see Engineering Fundamentals: Continuous Integration.\\n\\nContinuous Integration Tools\\n\\nThere are various software development tools available in the market that support the orchestration of the whole CI/CD process. Azure Pipelines is one such tool that combines continuous integration and deployment to build, deploy and test code across different environments.\\n\\nFor more information, see Azure Pipelines.\\n\\nContinuous Integration Process\\n\\nSandbox (Dev) environment Creation', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-integration\\\\index.md'}, {'chunkId': 'chunk43_1', 'chunkContent': 'For more information, see Azure Pipelines.\\n\\nContinuous Integration Process\\n\\nSandbox (Dev) environment Creation\\n\\nSandboxing refers to the usage of virtual servers to develop and test software in an isolated environment. It allows creation of a similar environment to each individual developer in a consistent fashion. Developers have complete control over these environments, and they can make and test their changes in isolation prior to merging it to the main branch.\\n\\nDev environments on the other hand are shared and are used for integrating changes from the entire development team.\\n\\nUsing Dev Containers is the most common and convenient approach for \"Sandbox\" environments creation. For detailed information, see Engineering Fundamentals: Developing inside a Container and Engineering Fundamentals: Getting Started with Dev Containers.\\n\\nFor Data solutions, these \"sandbox\" environments generally need extra preparation steps depending on the Azure Services. Here are some of the examples:\\n\\nData Lake Gen2 Access: A common \"sandbox\" file system can be created, and each developer can then create their own folder within this filesystem.\\n\\nAzureSQL or SQL Data Warehouse: A transient database (restored from DEV) can be spun up per developer on demand.\\n\\nAzure Synapse Analytics: Git integration allows developers to make changes to their own branches and debug runs independently.\\n\\nMerging to Centralized Repository / Version Control', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-integration\\\\index.md'}, {'chunkId': 'chunk43_2', 'chunkContent': 'Merging to Centralized Repository / Version Control\\n\\nA version control system helps software engineers keep track of changes to source files and assets. It stores resources like source code, test and deployment scripts, libraries/packages, and configuration information.\\n\\nOnce the \"sandbox\" and Dev environments are ready, developers can start making code changes and testing it in their local environments. There are two popular patterns of how developers use version control - gitflow/feature branch and trunk-based development. Code review process and the life cycle of changes are different in these two patterns. For more information, see Gitflow and Trunk-based Development.\\n\\nDifferent Azure data services have their own specific ways of supporting version control, some of which are explored in the following section.\\n\\nAzure SQL Server/ SQL Server\\n\\nIn the DataOps: Code First Development section, the idea of \"code first development\" was discussed. To version control this code in a git-based system, Azure Repos or GitHub can be used.\\n\\nA sample of how to use branching and pull request in real life scenario can be found at MDW Repo: Azure SQL.\\n\\nAzure Synapse Analytics / Azure Data Factory\\n\\nGit repositories associated with Synapse Studios have three types of branches, namely Collaboration branch, Feature branch, and Publish branch.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-integration\\\\index.md'}, {'chunkId': 'chunk43_3', 'chunkContent': \"Git repositories associated with Synapse Studios have three types of branches, namely Collaboration branch, Feature branch, and Publish branch.\\n\\nCollaboration branch (usually the Main branch) holds the code that the developers are ready to deploy to the Synapse Analytics workspace. New development is done in feature branches that are created from the collaboration branch. Changes from the feature branch are merged back to the collaboration branch via pull request. Publish branch (By default, it's the workspace_publish branch for Azure Synapse and adf_publish for Azure Data Factory) holds an ARM template that represents all the workspace artifacts (data pipeline, notebooks, datasets, etc.). This ARM template is always in sync with the live Synapse Analytics workspace.\\n\\nPublishing is only supported from the collaboration branch and not from the feature branches. When the publishing is from a collaboration branch, it creates the ARM template for all the workspace artifacts (data pipeline, notebooks, datasets etc.) and pushes it to the publish branch.\\n\\nFor more information, see Source control in Azure Data Factory and Synapse control in Synapse Studio.\\n\\nAzure Databricks\\n\\nAzure Databricks integrates seamlessly with most of the version control systems. For details, see the documentation.\\n\\nThe users can work on any branch of their repository and new branches can be created inside Azure Databricks. The users can also rebase the branch during development.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-integration\\\\index.md'}, {'chunkId': 'chunk43_4', 'chunkContent': 'Azure Databricks allows the linking of a notebook to either a branch or a fork of the repository. A pull request can be created directly from the Azure Databricks workspace using the \"Create PR\" link under the \"Git Preferences\" dialog. The Create PR link displays only if the default branch of the parent repository isn\\'t being worked on.\\n\\nPull Request (PR) Validation\\n\\nPull requests are used to change, review and merge code in a repository. The users with required repository permissions can create a pull request with an intention to merge the code changes made by them to be included in the target branch. In general, a pull request undergoes a review process where one or more reviewers accesses the changes. Reviewers can provide comments, suggest modifications, and give their approval or rejection for the pull request. Depending on the governance policies, a pull request might have certain other criteria that need to be met before a merge is possible. For example: A pull request might require linking it to an existing work item that the PR is trying to address.\\n\\nMoreover, it is possible to establish a policy that mandates successful builds before completing a pull request. This policy helps minimize the occurrence of breaking changes and ensures that all test cases pass before merging the pull request. Pull requests also provides the opportunity to enforce consistent style guides via code style checks and automated linting.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-integration\\\\index.md'}, {'chunkId': 'chunk43_5', 'chunkContent': 'For more information on pull requests, see Engineering Fundamentals: Pull Requests.\\n\\nCode Style Check / Linting\\n\\nConsistent Coding Style across projects and programming languages is important to develop high-quality software. The best way to achieve this consistency is to enforce the coding style by using the right tools. The right IDE extensions enable intellisense, debugging, formatting and linting that helps to write code with consistent style. Linting check during CI build can ensure only consistent code gets merged with the main branch.\\n\\nHere is some language specific guidance to achieve consistent coding style across projects and teams:\\n\\nCode Style and Linting Guidelines Engineering Fundamentals: Python Engineering Fundamentals: Terraform Engineering Fundamentals: YAML (Azure Pipelines) Engineering Fundamentals: Bash Engineering Fundamentals: C# Engineering Fundamentals: Go Engineering Fundamentals: Java Engineering Fundamentals: JavaScript and TypeScript Engineering Fundamentals: Markdown\\n\\nBuild Process\\n\\nUnit Testing\\n\\nThe build definition consists of validation steps that run a set of automated unit tests. These tests are designed to verify if the application components meet their intended design and behave as expected.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-integration\\\\index.md'}, {'chunkId': 'chunk43_6', 'chunkContent': 'When it comes to data solutions, it is important to ensure that the data pipelines can be replayed and are idempotent. The recommended approach here is to separate the data transformation code from the data access code. By doing so, it becomes possible to write unit tests specifically for the data transformation logic, abstracting away the complexities of data access. This separation allows for thorough testing and validation of the data transformation process. For Example: the transformation code should be moved from the notebooks into packages that are imported during execution.\\n\\nHere is an example from Modern Data Warehouse repo of unit tests and the corresponding QA Pipeline that executes the unit tests on every PR.\\n\\nBuild Agent Management\\n\\nA build agent is a computing infrastructure with installed agent software that runs one job at a time. At least one agent is required to build the code or deploy your software using Azure Pipelines.\\n\\nWhen it comes of choosing a build agent on Azure, there are three primary choices:\\n\\nMicrosoft-hosted agents\\n\\nSelf-hosted agents\\n\\nAzure Virtual Machine Scale Set agents\\n\\nFor detailed information on agent management, see Azure Pipeline Agents.\\n\\nBuild Pipelines for Automation', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-integration\\\\index.md'}, {'chunkId': 'chunk43_7', 'chunkContent': 'For detailed information on agent management, see Azure Pipeline Agents.\\n\\nBuild Pipelines for Automation\\n\\nThe deployments are automated by creating build pipelines as part of Continuous Integration process. A build pipeline would typically be responsible for building the solution, running the unit tests and publishing the artifacts. The build pipeline can be triggered manually or automatically on every commit to the repository. The build pipeline can also be configured to run on a schedule. Here are some of the examples of build pipelines:\\n\\nPipeline to run unit tests - sql\\n\\nPipeline to run unit tests - python\\n\\nPipeline to publish artifacts\\n\\nPublishing Artifacts\\n\\nAzure Artifacts can be used with Azure Pipelines to deploy packages, publish build artifacts or integrate files between pipeline stages. For data solutions, the following artifacts are published and deployed:\\n\\nSQL DACPAC\\n\\nThe simplest way to deploy a database is to create data-tier package or DACPAC. DACPACs can be used to package and deploy schema changes and data. A DACPAC can be created using the SQL database project in Visual Studio.\\n\\nHere is an example of building a DACPAC from an SQL database project and publishing it as an Artifact.\\n\\nPython Wheel Packages', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-integration\\\\index.md'}, {'chunkId': 'chunk43_8', 'chunkContent': 'Here is an example of building a DACPAC from an SQL database project and publishing it as an Artifact.\\n\\nPython Wheel Packages\\n\\nA wheel is a pre-built Python package. Installing wheels is substantially faster for the end user than installing from a source distribution. A Python .whl file is essentially a ZIP (.zip) archive with a specially crafted filename that tells installers what Python versions and platforms the wheel will support.\\n\\nHere is an example of creating a wheel distribution package and publishing it as an Artifact.\\n\\nDatabricks Notebook\\n\\nNotebooks are a common tool in data science and machine learning for developing code and presenting results. In Azure Databricks, notebooks are the primary tool for creating data science and machine learning workflows and collaborating with colleagues.\\n\\nHere is an example to publish Databricks notebooks as artifacts into production Databricks workspace.\\n\\nFurther Reading\\n\\nDataOps: Continuous Delivery\\n\\nDataOps: Continuous Testing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-integration\\\\index.md'}, {'chunkId': 'chunk44_0', 'chunkContent': 'rings:\\n  - public\\n\\nContinuous Testing\\n\\nContinuous testing is a DevOps practice of testing at multiple stages of the software delivery pipeline to detect defects and quality issues early. Often, tests are automated to run as part of a larger CI/CD pipeline to ensure fast feedback to software engineers and stakeholders as rapidly as possible. In the context of DataOps and its pipeline duality, tests are required both in the value pipeline (tests for data) and the innovation/delivery pipeline (test for code).\\n\\nTests for Code\\n\\nIn this context, \"code\" refers to the conceptual system (i.e. data pipeline) that uses data to produce business value. For analytics use cases, the scope of the system is broader, involving ingestion, transformation and consumption of the dataset. The objective of the overall tests suite is to ensure this system is functioning as expected while maintaining code quality, security, and performance.\\n\\nDifferent types of tests run at different stages of the delivery pipeline. Generally, tests that verify isolated pieces of functionality, such as Unit Tests, run earlier and often in the pipeline. More complex tests, such as end-to-end tests, run at the tail end and therefore less frequently. Staggering tests are important in data systems as full end-to-end runs of analytical pipeline can take substantial time to run.\\n\\nUnit Testing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-testing\\\\index.md'}, {'chunkId': 'chunk44_1', 'chunkContent': 'Unit Testing\\n\\nThe goal of unit tests is to ensure a small, specific, isolated piece of functionality is working as expected. In data systems, it is best practice to write unit tests for the code responsible for executing business transformation logic. Business transformation code should be encapsulated such that it has no external hard dependencies or side-effects.\\n\\nA common anti-pattern is mixing data-access and data-transformation logic. This practice is commonly seen in data pipelines exclusively relying on notebooks to contain all data access and transformation code. A better approach is to encapsulate the data transformation code into a pure function and move it into a package (i.e. python wheel). This function can then be called from the notebook, which contains environment-specific data access code.\\n\\nFor more information about Unit Testing, see Engineering Fundamentals: Unit Testing.\\n\\nTech-Specific Samples\\n\\nApache Spark\\n\\nThe MDW Repo: Azure Databricks and MDW Repo: Azure Synapse samples showcase how to execute unit tests for data transformation code written in Apache Spark. It encapsulates the business logic into a python wheel package, keeping data access code in the notebook that loads the package.\\n\\nAzure Stream Analytics (ASA)\\n\\nThe MDW Repo: Azure Stream Analytics sample showcases how to execute unit tests for ASA.\\n\\nIntegration Testing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-testing\\\\index.md'}, {'chunkId': 'chunk44_2', 'chunkContent': 'The MDW Repo: Azure Stream Analytics sample showcases how to execute unit tests for ASA.\\n\\nIntegration Testing\\n\\nIntegration testing in DataOps refers to the process of testing the integration and interaction between different components, systems, or data pipelines. Generally, defects around misconfiguration (i.e. incorrect connection strings) and misaligned data interfaces are caught at this stage of testing.\\n\\nFor more information about Integration Testing, see Engineering Fundamentals: Integration Testing.\\n\\nTech-Specific Samples\\n\\nAzure Data Factory (ADF) / Azure Synapse Data Pipelines\\n\\nThe MDW Repo: ADF (Single) and MDW Repo: ADF (E2E Sample) | MDW Repo: Azure Synapse samples showcase utilizing \"pytest\" framework to trigger a set of integration tests for ADF as part of a CD pipeline.\\n\\nAzure Stream Analytics\\n\\nThe MDW Repo: Azure Stream Analytics sample showcases how to do Integration tests with ASA utilizing Node.js (TypeScript).\\n\\nTests for Data\\n\\n\"Tests for Data\" (also called Data Validation) in this context refers to mechanisms to ensure data flowing through the system is valid. Like the tests of code, different tests for data are run at different stages of the data pipeline.\\n\\nValidate early in the Pipeline', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-testing\\\\index.md'}, {'chunkId': 'chunk44_3', 'chunkContent': 'Validate early in the Pipeline\\n\\nFor Data Lake implementations, it is best practice to have a series of layers to segregate datasets at different states of processing. Common layers include \"Raw\", \"Enriched\", and \"Curated\".\\n\\nGenerally, it is best to front load most of the tests early in the pipeline, specifically between the \"Raw\" and \"Enriched\" layer. Front loading tests help downstream processing establish the known state of the data early, lowering the chance of unexpected exceptions. Typically, the tests here include schema validation, null checks, checksum, range checks, etc.\\n\\nMalformed record store\\n\\nOften, handling bad data means diverting records from the main processing pipeline and storing them in a malformed record store for later debugging.\\n\\nTech-Specific Samples\\n\\nThe MDW Repo: Parking Sensors sample utilizes the Great Expectations library to run tests for data as part of the data pipeline.\\n\\nTests for Data vs Data Quality\\n\\n\"Tests for Data\" does not cover all areas related to the larger practice of Data Quality. \"Tests for data\" refers to validating the correctness of the dataset at the point of processing. Data Quality covers a more holistic view of ensuring that datasets across the data estate are fit for their intended purpose. Data quality takes into account accuracy, completeness, consistency, validity, uniqueness and timeliness of the data.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-testing\\\\index.md'}, {'chunkId': 'chunk44_4', 'chunkContent': 'For more information on Data Quality, see DataOps: Data Quality Monitoring.\\n\\nFurther Reading\\n\\nDataOps: Continuous Integration\\n\\nDataOps: Continuous Delivery\\n\\nDataOps: Monitoring and Logging\\n\\nDataOps: Data Quality Monitoring', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\change-control\\\\continuous-testing\\\\index.md'}, {'chunkId': 'chunk45_0', 'chunkContent': \"rings:\\n  - public\\n\\nData Observability\\n\\nData observability is the ability to understand the health of data and data systems by collecting and correlating events across areas like data, storage, compute and processing pipelines.\\n\\nBuilding and operating a resilient, scalable, and performant data platform requires adopting proven DevOps-inspired processes across teams that represent functional domains. These practices enable the team to automate issue detection, prediction, and prevention in order to avoid downtime that can break production analytics and AI.\\n\\nObservability can be structured into three main pillars to allow better visibility to have a system is functioning:\\n\\nLogs - Historically, timestamped data recording regarding event happening in the system. Logs allow for looking back at past events to help understand and prevent future issues.\\n\\nMetrics - Broadly speaking they are key indicators to the health and performance of a system. Capturing metrics allow for measurable means to determine a user's experience with the system and its overall health.\\n\\nTraces - Allows for an overall timeline of events to be captured, replaced, and evaluated. It records everything that happens and can be used to find problems and areas that need to be improved. The events can also be played back so that people can see what happened and where changes are needed.\\n\\nRefer to Engineering Fundamentals: Observability for exploring the article of Observability in general.\\n\\nDataOps: Monitoring and Logging\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\index.md'}, {'chunkId': 'chunk45_1', 'chunkContent': 'Refer to Engineering Fundamentals: Observability for exploring the article of Observability in general.\\n\\nDataOps: Monitoring and Logging\\n\\nDataOps: Data Quality Monitoring', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\index.md'}, {'chunkId': 'chunk46_0', 'chunkContent': 'rings:\\n  - public\\n\\nData Quality Monitoring\\n\\nData quality monitoring is a way to check if the data is suitable for business needs, technical requirements, and system requirements. Teams should create consistent and repetitive processes and frameworks similar to DevOps and Site Reliability Engineering (SRE). This approach allows them to identify data quality issues, keep track of them in dashboards, and set up alerts to notify of any changes.\\n\\nTime to Detect (TTD), Time to Recovery (TTR), and other data quality metrics can be tracked with data quality monitoring. TTD refers to the length of time it takes for the data team to detect a data quality issue. These issues can range from freshness anomalies to schema changes that break entire data pipelines. TTR refers the length of time it takes for the team to resolve a data incident once alerted. Improving data quality is more than a technical challenge; it involves significant organizational and cultural support.\\n\\nLack of data quality monitoring can lead to wasted effort for data consumers. Unplanned and redundant work to validate and cleanse the data slows the analytics process. In other cases, poor data quality monitoring will cause data pipelines to fail, or worse, unknowingly pollute downstream datasets used to make business decisions.\\n\\nMonitoring Data In Flight', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\data-quality-monitoring\\\\index.md'}, {'chunkId': 'chunk46_1', 'chunkContent': 'Monitoring Data In Flight\\n\\nClosely related to Data Validation (tests for data), data quality monitoring involves observing the quality of a dataset as it is being processed. This monitoring includes identifying and tracking errors in the data pipeline. Other characteristics of the dataset that will be important for the business and technical teams to keep an eye on may include:\\n\\nPercentage of nulls in a field over a time period or batch of data\\n\\nStreaming anomaly detection\\n\\nLate arriving data\\n\\nChanges in schema over time\\n\\nData Quality Monitoring vs Data Validation\\n\\nData Validation (Tests for Data) is both the identification and the handling of these errors based on the business and technical requirements. It is important that fatal errors are captured and neutralized. Fatal errors are characteristics of the dataset that may cause downstream processing to fail. Examples of fatal errors include:\\n\\nUnexpected schema changes such as the dataset missing a column or field\\n\\nIncorrect data type formats\\n\\nUnexpected nulls\\nIdentifying fatal errors generally trigger a related resolution action. Resolving errors ensures that downstream consumers of the datasets can implement data processing logic with the guarantee the data is well formed.\\n\\nFor more information, please refer to DataOps: Data Validation (Tests for Data).\\n\\nAnomaly Detection for Data Pipelines', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\data-quality-monitoring\\\\index.md'}, {'chunkId': 'chunk46_2', 'chunkContent': 'For more information, please refer to DataOps: Data Validation (Tests for Data).\\n\\nAnomaly Detection for Data Pipelines\\n\\nData teams often use anomaly detection algorithms to detect behavior that is deviating from what is expected in a given pipeline. By understanding what \"good\" data looks like, it’s easier to proactively identify \"bad\" data. In a production data environment, anomaly detection is real time and applied at each stage of the data life cycle.\\n\\nMonitoring Data at Rest\\n\\nDespite monitoring data quality upstream in the data pipeline (data in flight), it is still important to monitor individual data stores for quality on a continuous basis. There may be insufficient checks at the point of processing to identify all data quality issues. In this case, monitoring data at rest becomes a feedback loop to data engineers to implement further checks upstream.\\n\\nAnother reason to monitor data at rest includes inability to detect and handle data quality issues upstream without access to the full dataset. For example, a single record at the point of processing may be allowed to have some null values but only up to a certain threshold of the overall dataset.\\n\\nData Quality Tools and Frameworks\\n\\nThe following are some of the popular tools and frameworks to monitor data quality:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\data-quality-monitoring\\\\index.md'}, {'chunkId': 'chunk46_3', 'chunkContent': 'Data Quality Tools and Frameworks\\n\\nThe following are some of the popular tools and frameworks to monitor data quality:\\n\\nGreat Expectations - The MDW Repo: Parking Sensors sample shows how to utilize Great Expectation python framework in a larger data pipeline built with Azure Databricks and Azure Data Factory.\\n\\ndbt Unit Testing - dbt is a popular choice for modern ELT, and its tool extends the ability to add unit tests to transformed tables.\\n\\nApache Griffin\\n\\nFurther Reading\\n\\nDataOps: Continuous Testing\\n\\nDataOps: Monitoring and Logging\\n\\nDataOps: Data Quality', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\data-quality-monitoring\\\\index.md'}, {'chunkId': 'chunk47_0', 'chunkContent': \"rings:\\n  - public\\n\\nObservability for Azure Databricks\\n\\nAzure Databricks is an Apache Spark–based analytics service that makes it easy to rapidly develop and deploy big data analytics. Monitoring and troubleshooting performance issues is critical when operating production Azure Databricks workloads. It is important to log adequate information from Azure Databricks so that it is helpful to monitor and troubleshoot performance issues.\\n\\nAzure Databricks supports numerous tools to collect and analyze the metrics, logs and traces from both Databricks infrastructure and applications. The amount of information that can be collected from all these tools can be overwhelming. Some tools are great for monitoring and troubleshooting issues pertaining to Databricks clusters, whereas some other tools work better for the Spark applications deployed in the Databricks workspace. So it's important to carefully plan the collection and analysis of the right information for observability.\\n\\nApproaches to Observability\\n\\nBased on the three pillars of Observability described in the Engineering Fundamentals: Observability section, the organization of Databricks' observability can be broadly outlined as follows:\\n\\nLogs\\n\\nCluster event Logs: Databricks native tool that captures cluster lifecycle events like cluster created, notebook started, cluster restarting, etc.\\n\\nCluster driver and worker logs: Databricks native tool for Spark application logs from driver and worker nodes.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_1', 'chunkContent': \"Cluster driver and worker logs: Databricks native tool for Spark application logs from driver and worker nodes.\\n\\nCluster init-script logs: Databricks native tool for logs from any scripts configured for initialization activities on clusters.\\n\\nAzure diagnostic logs: Powered by Azure Diagnostic Log, it's a comprehensive end-to-end diagnostic log of activities performed by Azure Databricks users.\\n\\nMetrics\\n\\nCluster Metrics via Ganglia: Databricks native tool for cluster metrics like memory utilization, cpu utilization etc. that can be accessed via Ganglia UI.\\n\\nCluster Metrics via OMS Agent: Collecting Databricks cluster metrics like memory utilization, cpu utilization etc. via OMS agent and streaming to Azure Monitor.\\n\\nTraces\\n\\nSpark Monitoring Library: A comprehensive library to capture detailed metrics and logs around Spark application execution (Jobs, Stages, Task, etc.) into Azure Monitor.\\n\\nCluster event logs\\n\\nThe cluster event logs display important events related to the lifecycle of a cluster. These events can be triggered either by user actions or automatically by Azure Databricks. The log is structured as - Timestamp, Event Type and Details. Such events affect the operation of a cluster as a whole and the jobs running in the cluster.\\n\\nWhat is captured\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_2', 'chunkContent': 'What is captured\\n\\nTypes of cluster events that are captured are CREATING, STARTING, RESTARTING, TERMINATING, FAILED_TO_EXPAND_DISK etc. For a full list of supported event types, see the REST API ClusterEventType data structure.\\n\\nThere are minor overlaps with Azure diagnostic logs, but Cluster event logs provide more in-depth information on cluster life cycle.\\n\\nHow to access\\n\\nCluster event logs can be accessed from the Databricks workspace UI. The Event Log tab in the cluster detail page displays this information. For detail instruction, see the View a cluster event log\\n section.\\n\\nExporting the cluster event logs to a destination like Azure Monitor is not supported by default. However, if needed, the logs can be accessed externally by invoking the Cluster Events REST API endpoint.\\n\\nWhen to use\\n\\nCluster event logs are useful to analyze cluster life-cycle events like cluster termination, cluster resizing or if the Init script execution was successful or failed.\\n\\nFurther reading\\n\\nCluster event logs\\n\\nCluster driver and worker logs\\n\\nThe logs provided are from the driver and worker nodes used in Spark.\\n\\nWhat is captured\\n\\nThe direct print and log statements from notebooks, jobs, and libraries go to the Spark driver logs. These logs have three outputs:\\n\\nStandard output\\n\\nStandard error\\n\\nLog4j logs', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_3', 'chunkContent': \"Standard output\\n\\nStandard error\\n\\nLog4j logs\\n\\nHow to access\\n\\nDriver logs can be accessed and downloaded from the Driver logs tab on the cluster details page.\\n\\nSpark worker logs can be accessed from the Spark UI. The location for delivering this log can also be configured for the cluster. It's easy to configure these logs to be delivered to DBFS using the feature referred to as Cluster Log Delivery.\\n\\nWhen to use\\n\\nThis log is used to access the log information and error messages spark developers print from the spark application.\\n\\nFurther reading\\n\\nCluster driver and worker logs\\n\\nCluster init script logs\\n\\nAn init script is a Shell script that runs during startup of each cluster node. This script gets executed before the Apache Spark driver or worker JVM are started. Here are some of the tasks that can be performed by using the init script:\\n\\nInstalling packages and libraries.\\n\\nModifying the JVM system classpath.\\n\\nSetting system properties and environment variables.\\n\\nModifying Spark configuration parameters.\\n\\nThe logs generated from the execution of the init scripts are referred to as init script logs. These logs are useful to troubleshoot cluster initialization errors.\\n\\nWhat is captured\\n\\nIt captures the logs generated from the execution of the init scripts.\\n\\nHow to access\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_4', 'chunkContent': 'What is captured\\n\\nIt captures the logs generated from the execution of the init scripts.\\n\\nHow to access\\n\\nWhen cluster log delivery is not configured, logs are written to /databricks/init_scripts. Here is an example of using a notebook to list and view the logs:\\n\\nshell\\n%sh\\nls /databricks/init_scripts/\\ncat /databricks/init_scripts/<timestamp>_<log-id>_<init-script-name>.sh.stdout.log\\n\\nIf cluster log delivery is configured, the init script logs are written to /[cluster-log-path]/[cluster-id]/init_scripts. Logs for each container in the cluster are written to a subdirectory called init_scripts/_.\\n\\nFor more information about init script logging, see Init script logs.\\n\\nWhen to use\\n\\nCluster init script logs are helpful for troubleshooting cluster initialization errors.\\n\\nFurther reading\\n\\nCluster node initialization scripts > Logging\\n\\nAzure diagnostic logs\\n\\nAzure diagnostic logs for Azure Databricks are logs generated by the services to capture information about the activities and events occurring within the Databricks environment.\\n\\nWhat is captured', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_5', 'chunkContent': 'What is captured\\n\\nDiagnostic logging captures events from many databricks services/categories like cluster, databricks file system (DBFS), jobs, notebook etc. The category (the Azure Databricks service) and operationName properties identify an event in a log record.\\n\\nFor a list of each of these types of events and the associated services, see Events. Some of the events are emitted in audit logs only if verbose audit logs are enabled for the workspace.\\n\\nHow to access\\n\\nThe interface to access the log depends on how the log delivery is configured. These logs can be enabled through the Azure portal or Azure CLI, allowing the user to configure their delivery in one of the following ways:\\n\\nArchive to a storage account\\n\\nSend to Log Analytics\\n\\nStream to an Event Hubs\\n\\nFor more information, see Configure diagnostic log delivery.\\n\\nWhen to use\\n\\nDiagnostic logging is great tool for governance of Databricks deployment. It can be used to monitor and audit the activities in the Databricks workspace. For example, it can be used to monitor who created or deleted the cluster, created a notebook, attached a notebook to cluster, scheduled job start or end and a lot more.\\n\\nFor more information about analyzing diagnostic logs in Azure Monitor, see Analyze diagnostic logs.\\n\\nFurther reading\\n\\nDiagnostic logging in Azure Databricks', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_6', 'chunkContent': 'Further reading\\n\\nDiagnostic logging in Azure Databricks\\n\\nCluster Metrics via Ganglia\\n\\nTo help with monitoring the performance of Databricks clusters, Databricks provides access to Ganglia metrics from the cluster details page. Ganglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids. It monitors telemetry pertaining to CPU, memory usage, network, etc.\\n\\nWhat is captured\\n\\nGanglia collects dozens of system metrics that included a series of CPU-, memory-, disk-, network-, and process-related values. CPU metrics are available in the Ganglia UI for all Databricks runtimes. GPU metrics are available for GPU-enabled clusters.\\n\\nHere is a list of few common metrics captured via Ganglia.\\n\\nMetric name Reporting units Description Type cpu_idle Percent Percentage of time that the CPU or CPUs were idle and the system did not have an outstanding disk IO request CPU cpu_user Percent Percentage of CPU utilization that occurred while executing at the user level CPU cpu_num Count Total number of CPUs (collected once) CPU disk_total Gb Total available disk space, aggregated over all partitions Disk disk_free Gb Total free disk space, aggregated over all partitions Disk mem_total Kb Total amount of memory Memory\\n\\nHow to access', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_7', 'chunkContent': 'How to access\\n\\nGanglia is available within Azure Databricks without any extra setup. To access the Ganglia UI, navigate to the Metrics tab on the cluster details page.\\n\\nWhen to use\\n\\nGanglia metrics help to understand if the cluster configuration is under-provisioned, over-provisioned or provisioned appropriately. We can see node level metrics such as CPU consumption, memory consumption, disk usage, network-level IO – all node level factors that can affect the stability and performance of our job. All of it makes it a great too for monitoring the performance of the cluster. Ganglia metrics do not have any insight on jobs or pipelines.\\n\\nUnfortunately, there is no built-in method to directly export these logs to a destination like Azure Monitor.\\n\\nFurther reading\\n\\nMonitoring with Ganglia\\n\\nCluster Metrics via OMS Agent', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_8', 'chunkContent': \"Further reading\\n\\nMonitoring with Ganglia\\n\\nCluster Metrics via OMS Agent\\n\\nAn important facet of monitoring is to understand the resource utilization in Azure Databricks clusters. This information is useful in arriving at the correct cluster and VM sizes. Each VM does have a set of limits (cores/disk throughput/network throughput) which play an important role in determining the performance profile of an Azure Databricks job. Utilization metrics for an Azure Databricks cluster can be obtained by streaming VM metrics to an Azure Log Analytics Workspace. To achieve this goal, the Log Analytics Agent needs to be installed on each node of the cluster.\\n\\nWhat is captured\\n\\nIt captures metrics on the utilization of different resources (memory, processor, disk etc.) of the cluster VMs.\\n\\nHow to access\\n\\nOnce the agent is successfully installed on the cluster VM, the data can be accessed from Azure Monitor.\\n\\nFor details on how to install the OMS agent in Databricks, see the Installation instructions.\\n\\nWhen to use\\n\\nCluster Metrics via OMS Agent serves as a good alternative to Cluster Metrics via Ganglia. Specially if the team is interested in using Azure Monitor which Ganglia can't use as a destination to export the log.\\n\\nRemember that OMS agents can increase cluster startup time by a few minutes.\\n\\nFurther reading\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_9', 'chunkContent': 'Remember that OMS agents can increase cluster startup time by a few minutes.\\n\\nFurther reading\\n\\nResource utilization metrics across Azure Databricks cluster\\n\\nOMS Agent for Linux\\n\\nSpark Monitoring Library\\n\\nSpark Monitoring Library extends the core monitoring functionality of Azure Databricks to send streaming query event information to Azure Monitor. It helps in capturing detailed metrics and logs related to Spark application execution. These metrics and logs are collected and stored in Azure Monitor for further analysis and monitoring.\\n\\nThe following article from Azure Architecture Center takes a deep dive into how to use this library to monitor Databricks applications - Monitoring Azure Databricks.\\n\\nWhat is captured\\n\\nSpark monitoring library enables logging of Azure Databricks service metrics and Apache Spark structure streaming query event metrics. This library provides helpful insights to fine-tune Spark jobs. Spark workloads can be easily monitored and traced at every layer, which includes tracking performance and resource utilization on the host and JVM. Moreover, it provides insights into Spark metrics and application-level logging.\\n\\nHow to access\\n\\nSpark Monitoring Library sends the log to a Log Analytics workspace. Once the library is configured,the information starts flowing into the following three tables in Log Analytics Workspace:\\n\\n— SparkMetric_CL\\n\\nSparkLoggingEvent_CL\\n\\nSparkListenerEvent_CL', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_10', 'chunkContent': '— SparkMetric_CL\\n\\nSparkLoggingEvent_CL\\n\\nSparkListenerEvent_CL\\n\\nThe library also includes ready-made Grafana dashboards that are a great starting point to monitor Azure Databricks jobs for performance issues.\\n\\nWhen to use\\n\\nSpark developers are the primary audience for Spark Monitoring Library. There are minor overlaps between Cluster Metrics and Spark Monitoring Library, but this library takes a deeper look into the spark application-specific metrics.\\n\\nBoth Azure Monitor and the Grafana dashboards can be used for troubleshooting performance issues in Spark Application.\\n\\nOne potential use case involves utilizing Cluster Metrics in a Databricks environment to detect anomalies in resource utilization. This information can then be used with the spark monitoring library to perform further analysis and pinpoint the actual issues within the application.\\n\\nAlerts\\n\\nTo efficiently analyze collected logs, users can utilize the Log Analytics user interface in the Azure portal. This interface enables them to create and test queries, facilitating the retrieval, consolidation, and analysis of data. Once the desired queries have been generated, users can either analyze the data using various tools or save the queries for future use with alerts.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_11', 'chunkContent': \"Alerts play a critical role in proactively identifying and resolving issues within one's infrastructure or applications before they have an effect on users. By utilizing Azure Monitor, users can create alerts that are triggered based on different metrics or log data sources. To learn more about creating an alert rule in Azure, see tutorial.\\n\\nAlerts in Azure Monitor\\n\\nAzure Monitor is capable of creating alert rules based on metrics, using visualization criteria as a basis. These alert rules incorporate target resources, metrics, splitting, and filter dimensions. Users can customize these settings by using the alert rule creation pane. By setting up an action group, they can receive notifications via email, SMS, voice call, or push notification whenever the alert rule is triggered.\\n\\nFor example, if one desires to receive notifications for error jobs, an alert rule can be defined using the following query:\\n\\nsh\\nSparkLoggingEvent_CL\\n| where Level == 'ERROR'\\n\\nThis query filters the logs in the SparkLoggingEvent_CL table sent by Spark monitoring library to only include entries with a log level of ERROR. By creating an alert rule based on this query, users can receive notifications whenever new error logs are detected in the specified table.\\n\\nAnother example is monitoring the CPU time of a cluster using the following query:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk47_12', 'chunkContent': 'Another example is monitoring the CPU time of a cluster using the following query:\\n\\nsh\\nSparkMetric_CL\\n| where name_s contains \"executor.cpuTime\"\\n| extend sname = split(name_s, \".\")\\n| extend executor = strcat(sname[0], \".\", sname[1])\\n| project TimeGenerated, cpuTime = count_d / 100000\\n\\nThis query filters the logs in the SparkMetric_CL table to include only entries related to executor CPU time. This query can be used to monitor and analyze the CPU time of the cluster\\'s executors.\\n\\nIn addition, alert rules can be created based on this query to receive notifications when the CPU time exceeds a certain threshold. This rule enables proactive identification and addressing of any issues related to high CPU usage in the Spark cluster.\\n\\nFor more information on alerts, see Azure Monitor alerts.\\n\\nFurther reading\\n\\nMonitoring Azure Databricks using Spark Monitoring Library', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\databricks-observability.md'}, {'chunkId': 'chunk48_0', 'chunkContent': 'rings:\\n  - public\\n\\nMonitoring and Logging\\n\\nData Observability\\n\\nObservability is a measure of how internal states of a system can be inferred from knowledge of its external outputs. An observable system would provide key insights into the overall health and performance of a system. It has become a crucial aspect of improving performance of \"distributed IT systems\" and revolves around three pillars namely metrics, logs and traces. Refer to Engineering Fundamentals: Observability for more information.\\n\\nAs part of DevOps, observability needs to be considered in the context of build and release of application. See Engineering Fundamentals: Observability of CI/CD Pipelines for more information.\\n\\nTo enable observability for data systems, platform level logs/metrics and application logs/traces are collected and analyzed. Visualizations of the collected data provide a holistic view of the system performance. Once observability is in place, appropriate actions can be taken such as sending failure notifications or tuning application components.\\n\\nPlatform Monitoring\\n\\nFoundational infrastructure for an enterprise data platform can include a mix of both provider-managed and self-managed infrastructure to enable storage and computing. DevOps engineers or infrastructure engineers need to monitor this foundational infrastructure. Such monitoring can identify and resolve system outages and performance bottlenecks that affect modern data and analytics pipelines.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\index.md'}, {'chunkId': 'chunk48_1', 'chunkContent': \"Monitoring data from databases and networking layers can improve processing throughput and minimize network latency. Teams needs tools to capture metrics, notify, track, and remediate incidents and correlate with the data and analytics issues.\\n\\nIt's highly recommended to incorporate observability-as-code into infrastructure-as-code layer. This way, monitoring instrumentation is enabled out-of-box as soon as a resource is created. Most Azure services offer out-of-box instrumentation for key resource metrics like diagnostic data.\\n\\nThere are two main components of platform monitoring namely, platform logs and platform metrics.\\n\\nPlatform logs provide detailed diagnostic and auditing information for Azure resources and the Azure platform they depend on. Although they're automatically generated, certain platform logs need to be forwarded to one or more destinations for retention.\\n\\nPlatform metrics are created by Azure resources and give visibility into their health and performance. Each type of resource creates a distinct set of metrics without any configuration required. Platform metrics are collected from Azure resources at one-minute frequency unless specified otherwise in the metric's definition.\\n\\nAzure Monitor\\n\\nAzure Monitor delivers a comprehensive solution for collecting, analyzing, and acting on telemetry from cloud and on-premises environments. It's a fundamental service for providing full observability into applications, infrastructure, and network.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\index.md'}, {'chunkId': 'chunk48_2', 'chunkContent': \"Most of the Azure data services emit logs and metrics that are automatically collected into Azure Monitor. Check here for a complete list of Azure services supported by Azure Monitor.\\n\\nThese logs and data are stored in Log Analytics Workspace. A Log Analytics workspace is a unique environment for log data from Azure Monitor and other Azure services. Each workspace has a unique workspace ID and resource ID. It's possible to bring logs from different Azure Services into the same Log Analytics workspace that facilitates consolidation and centralization of logging information.\\n\\nEnabling Diagnostic Settings\\n\\nThe recommended way to send platform logs to Azure Monitor is by enabling the diagnostics settings, either in the Azure Monitor or through the specific resource portal. More information about diagnostics settings and enabling them can be found here.\\n\\nApplication Monitoring\\n\\nObservability for Relational Databases\\n\\nMost of the relational database offerings on Azure have a list of metrics that provide crucial insights into the overall database health and performance issues. These metrics have one-minute frequency and each metric provides 30 days of history.\\n\\nDepending on the database, other server logs can also be enabled and written to Azure Monitor Diagnostic logs. Enabling the diagnostic setting allows these logs to be sent to Azure Monitor logs, Event Hubs, or Storage Account. Collected logs can be used for generating alerts and/or retained long for future analysis.\\n\\nHere is a list of available metrics and server log for different relational database services on Azure:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\index.md'}, {'chunkId': 'chunk48_3', 'chunkContent': 'Here is a list of available metrics and server log for different relational database services on Azure:\\n\\nAzure Database for MySQL - Single Server\\n\\nMetrics\\n\\nSlow Query Logs\\n\\nAudit Logs\\n\\nAzure Database for MySQL - Flexible Server\\n\\nMetrics\\n\\nSlow Query Logs\\n\\nAudit Logs\\n\\nAzure Database for MariaDB\\n\\nMetrics\\n\\nSlow Query Logs\\n\\nAudit Logs\\n\\nAzure Database for PostgreSQL - Single Server\\n\\nMetrics\\n\\nAudit Logging - pgAudit\\n\\nQuery Store\\n\\nAzure Database for PostgreSQL - Flexible Server\\n\\nMetrics\\n\\nAudit Logging - pgAudit\\n\\nQuery Store\\n\\nAzure Database for PostgreSQL - Hyperscale (Citus)\\n\\nMetrics - per node and aggregated\\n\\nAudit Logging - pgAudit\\n\\nObservability for Data Lakes/Data Warehouses\\n\\nAzure Synapse Analytics\\n\\nThe MDW repo: Parking Sensors Synapse sample describes the observability aspects for Azure Synapse.\\n\\nIt starts with describing how the diagnostics setting can be enabled for Azure Synapse using IaC template. The sample also shows how these logs can be queried in Synapse Studio or in Log Analytics Workspace using Kusto queries.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\index.md'}, {'chunkId': 'chunk48_4', 'chunkContent': \"It then provides two different ways to write custom application logs using:  Application Insights and OpenCensus library, and log4j logger.\\n\\nThough not covered in this sample, there are options to collect Apache Spark applications metrics using APIs and monitor Apache Spark applications metrics with Prometheus and Grafana.\\n\\nAzure Databricks\\n\\nAzure Databricks supports numerous tools to collect and analyze the metrics, logs, and traces from both Databricks infrastructure and applications. The amount of information that can be collected from all these tools can be overwhelming. Some tools are great for monitoring and troubleshooting issues about Databricks clusters, whereas some other tools work better for the spark applications deployed in the Databricks workspace. So it's important to carefully plan the collection and analysis of the right information for observability.\\n\\nFor detailed information, see Observability for Databricks.\\n\\nMicrosoft Fabric\\n\\nLogging Library: Logging Library\\n\\nExample with the: Logging your workload using Notebooks| Microsoft Fabric\\n\\nData science option:\\nLogging in Microsoft Fabric - Microsoft Fabric MLflow\\n\\nObservability for Serverless Systems\\n\\nServerless Streaming Pipeline\\n\\nThe MDW repo: Temperature Events sample shows how observability can be implemented for distributed systems. It deploys a streaming serverless pipeline to process incoming events. The sample also provides insight into the performance characteristics of the system through observability and load testing.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\index.md'}, {'chunkId': 'chunk48_5', 'chunkContent': 'The sample utilizes the built-in integration of Azure Event Hubs and Azure Functions with Application Insights and uses various features such as Application Maps and End-to-End Transaction details. Application Maps help in spotting performance bottlenecks or failure hotspots across all components of a distributed application.\\n\\nSetting SLAs, SLIs and SLOs\\n\\nMetrics, logs, and traces provide the foundational data required for platform and application monitoring. This information is further augmented by: defining the service thresholds from a business perspective and using data monitoring to ensure the applications/services meet the committed uptime, recovery and performance targets.\\n\\nOrganizations can adopt DevOps-style Site Reliability Engineering (SRE) practices for data monitoring. Other measures that can help organizations in reducing downtime and ensuring data reliability are:\\n\\nService level agreements (SLAs)\\n\\nService level indicators (SLIs)\\n\\nservice level objectives (SLOs)\\n\\nFor more information, see Setting SLAs, SLIs and SLOs for Data Observability.\\n\\nData Observability Maturity\\n\\nLike DevOps, Data Observability is a continuous process. Every organization has some level of maturity in terms of their ability to observe its data systems. The challenge lies in establishing a common understanding of the current state so that it can be further improved.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\index.md'}, {'chunkId': 'chunk48_6', 'chunkContent': 'The Data Observability Maturity Model is a simple matrix-based evaluation model to gain insights about the effectiveness of existing data observability capabilities and help in deciding next steps.\\n\\nFurther Reading\\n\\nDataOps: Continuous Testing\\n\\nDataOps: Data Quality Monitoring', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\data-observability\\\\monitoring-and-logging\\\\index.md'}, {'chunkId': 'chunk49_0', 'chunkContent': 'rings:\\n  - public\\n\\nDevSecOps\\n\\nDevSecOps or DevOps security is about introducing security earlier in the life cycle of application development (a.k.a shift-left). The goal is to minimize the impact of vulnerabilities and bringing security closer to development team.\\n\\nEngineering Fundamentals: Penetration Testing\\n\\nEngineering Fundamentals: Credential Scanning\\n\\nEngineering Fundamentals: Secrets Management\\n\\nDevSecOps: Overview', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\devsecops\\\\index.md'}, {'chunkId': 'chunk50_0', 'chunkContent': 'rings:\\n  - public\\n\\nPlatform Automation\\n\\nSuccessfully adopting DevOps practices can be measured by the following metrics:\\n\\nReducing lead time for changes\\n\\nDecreasing change failure rate\\n\\nIncreasing deployment frequency\\n\\nDecreasing mean time to recovery\\n\\nAutomation plays a key role in improving all four of these metrics.\\n\\nThe journey of automation begins with automating the underlying platform on top of which the application is developed. From infrastructure to the feature being developed, everything needs to be in the source control system to facilitate automation.\\n\\nDataOps: Code First Development\\n\\nDataOps: Infrastructure as Code', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\index.md'}, {'chunkId': 'chunk51_0', 'chunkContent': 'rings:\\n  - public\\n\\nCode First Development\\n\\nThe common artifacts of a data solution can include data pipelines, database schemas, stored procedures, measure definitions. Code first development manages these artifacts using either a domain-specific or general-purpose language instead of a graphical user interface.\\n\\nData solution artifacts can be divided into the following categories:\\n\\nData model and related artifacts\\n\\nETL/ELT pipelines and related artifacts\\n\\nA few common patterns and challenges of developing data solution artifacts as code are discussed below.\\n\\nData model and related artifacts\\n\\nThe data stored in a database is composed of data elements that reflect real-world entities. A data model is a representation of these entities and the relationship among them. Examples of a data model and related artifacts include database schemas, stored procedures, functions, triggers.\\n\\nThere are two popular patterns for developing data model as code, state-based approach and migration-based approach.\\n\\nState-based approach\\n\\nA state-based tool can connect to an existing database and generate scripts that modify the database from the current to the target state. The tool compares the two states and determines all the operations required to get to the target state.\\n\\nHere are some examples of how this approach can be applied to Azure data services.\\n\\nSql Server Data Tools', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\code-first-development\\\\index.md'}, {'chunkId': 'chunk51_1', 'chunkContent': \"Here are some examples of how this approach can be applied to Azure data services.\\n\\nSql Server Data Tools\\n\\nSQL Server Data Tools (SSDT) transforms database development by introducing a ubiquitous, declarative model that spans all the phases of database development inside Visual Studio. SSDT Transact-SQL design capabilities can be used to build, debug, maintain, and refactor databases. It supports working with a database project, or directly with a connected database instance on or off-premise.\\n\\nThe build output of a database project produces a DACPAC package. A data-tier application (DAC) is a logical database management entity that defines all SQL Server objects - tables, views, and instance objects, including logins - associated with a user's database. It is a SQL Server database deployment that enables developers and database administrators to package SQL Server objects into an artifact called a DAC package.\\n\\nSchema and Data Comparison feature allows the comparison of the schemas of two databases and display all discrepancies. It can also develop and run the T-SQL scripts needed to reconcile any differences discovered.\\n\\nFor an end-to-end example of how to use SSDT with DACPAC in real life scenario, see MDW Repo: Azure SQL.\\n\\nRedgate SQL Compare\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\code-first-development\\\\index.md'}, {'chunkId': 'chunk51_2', 'chunkContent': 'Redgate SQL Compare\\n\\nSQL Compare is a tool for comparing and synchronizing SQL Server database structures. SQL Compare automates the deployment and management of database changes across local, staging, and production databases.\\n\\nAzure Synapse Analytics - Dedicated SQL Pool\\n\\nSql Server Data Tools supports Dedicated SQL Pool, see Sql Server Data Tools section for the details.\\n\\nMigration-based approach\\n\\nIn migration-based approach, the developer explicitly provides the required steps to take the database from a version to the next version. All required steps for a change are grouped together, given a unique number, and stored to keep the history of all migrations. This makes it easier to rollback to an older state of the database.\\n\\nHere are some examples of how this approach can be applied to Azure data services.\\n\\nAzure SQL Server/SQL Server\\n\\nEntity Framework Core Migration\\n\\nEF Core is an object-relational mapper (ORM), which enables .NET developers to work with a database using .NET objects. Entity Framework Core Migration is an EF Core feature that lets developers create and manage the database model from application code.\\n\\nTo work with EF Core, first step is to build a model using entity classes. Here is an example of a simple model.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\code-first-development\\\\index.md'}, {'chunkId': 'chunk51_3', 'chunkContent': 'To work with EF Core, first step is to build a model using entity classes. Here is an example of a simple model.\\n\\n```csharp\\npublic class Blog\\n{\\n    public int BlogId { get; set; }\\n    public string Url { get; set; }\\n    public int Rating { get; set; }\\n    public List Posts { get; set; }\\n}\\n\\npublic class Post\\n{\\n    public int PostId { get; set; }\\n    public string Title { get; set; }\\n    public string Content { get; set; }\\n    public int BlogId { get; set; }\\n    public Blog Blog { get; set; }\\n}', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\code-first-development\\\\index.md'}, {'chunkId': 'chunk51_4', 'chunkContent': \"```\\n\\nUsing EF Migration, developers can create the database from this model and evolve the database as the model changes.\\n\\nDevelopers can use EF Core tools to write new database migrations by describing desired state. EF Core compares the current model against a snapshot of the old model to determine the differences and create migration scripts. These scripts can be tracked in project's source control like any other source file.\\n\\nOnce a new migration has been generated, it can be applied to a database in various ways. EF Core records all applied migrations in a special history table, allowing it to know which migrations have been applied and which haven't.\\n\\nRedgate SQL Change Automation\\n\\nSQL Change Automation integrates with version control to build, test, and deploy database changes. This tool uses SQL to author the changes, so it is easy for DBAs to use who might not be familiar with other programming languages. It is also clear how the changes will be applied to the database.\\n\\nETL pipelines and related artifacts\\n\\nExtract, transform, and load (ETL) pipelines transform and move data from one system to another. Often these pipelines are used to move data to an analytics-friendly state. Common examples are data pipelines, SQL/python scripts and notebooks, Spark job definitions, dataset definitions etc.\\n\\nHere are some examples of how this approach can be applied to Azure data services.\\n\\nAzure Synapse Analytics/Azure Data Factory\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\code-first-development\\\\index.md'}, {'chunkId': 'chunk51_5', 'chunkContent': \"Here are some examples of how this approach can be applied to Azure data services.\\n\\nAzure Synapse Analytics/Azure Data Factory\\n\\nBy default, Synapse Studio authors directly against the Synapse service and creates these artifacts in the workspace. To source control the artifacts, the workspace needs to be integrated with a Git repository (Azure DevOps, or GitHub). The git integration can be easily done from the Synapse Studio.\\n\\nEach Git repository that's associated with a Synapse Studio has a Collaboration Branch (main). Developers can create feature branches from the collaboration branch. Changes from the feature branch can be pushed to the collaboration branch via pull request. Synapse Analytics doesn't support publishing from the feature branches. It only supports publishing from the collaboration branch. When the collaboration branch is published, Azure Synapse builds an ARM template of all the code and pushes it to the publish branch to create the artifacts in Synapse Analytics workspace.\\n\\nFor an end-to-end example of how to use source control the artifacts using Azure DevOps in real life scenario, see MDW Repo: Parking Sensor (Synapse).\\n\\nTo learn more about source control in Synapse, see the official documentation - Source control in Synapse Studio.\\n\\nAzure Databricks\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\code-first-development\\\\index.md'}, {'chunkId': 'chunk51_6', 'chunkContent': 'Azure Databricks\\n\\nTo support best practices for data science and engineering code development, Databricks Repos provides repository-level integration with Git providers. Code developed in an Azure Databricks notebook can be synced with a remote Git repository. Databricks Repos lets you use Git functionality such as:\\n\\ncloning a remote repo\\n\\nmanaging branches\\n\\npushing and pulling changes\\n\\nvisually comparing differences upon commit\\n\\nTo learn more about source control in Databricks, see official documentation - Git integration with Databricks Repos.\\n\\nDatabricks also supports non-notebook git integration - Work with non-notebook files in an Azure Databricks repo (This feature is currently in public preview).\\n\\nFurther Reading\\n\\nDataOps: Infrastructure as Code', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\code-first-development\\\\index.md'}, {'chunkId': 'chunk52_0', 'chunkContent': 'rings:\\n  - public\\n\\nInfrastructure as Code\\n\\nInfrastructure as Code (IaC) is the management of infrastructure in a descriptive model, using the same versioning as DevOps team uses for source control. IaC creates the same environment every time it is applied and can also update the existing environment. This repeatability helps in ensuring that infrastructure deployments are consistent and repeatable.\\n\\nAzure Resource Manager (ARM) is the deployment and management service for Azure.\\nAzure provides native support for IaC via  ARM templates. For defining the infrastructure and configuration, either ARM Templates (JSON based) or Bicep language can be used. There are various third-party platforms that support IaC such as:\\n\\nTerraform\\n\\nAnsible\\n\\nChef\\n\\nPulumi\\n\\nFor example, here is a Bicep template and quickstart guide for creating a Databricks workspace resource.\\n\\nIaC for Azure Data Services\\n\\nDesigning and building a strong data platform on Azure for enterprise requirements brings its own difficulties related to governance, networking, security, and compliance. The following are general areas of friction during IaC deployment of data services on Azure.\\n\\nAzure Data Services with Virtual Network Integration\\n\\nDifferent Azure data services have different ways of restricting network access. In this section, we shall look at the common Azure data services and how these network restrictions are implemented.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\infrastructure-as-code\\\\index.md'}, {'chunkId': 'chunk52_1', 'chunkContent': \"Azure Synapse\\n\\nWhile creating Azure Synapse workspace, it can be associated to a Microsoft Azure Virtual Network (VNet). This Virtual network associated with the workspace is managed by Azure Synapse and is called a Managed workspace Virtual Network.\\n\\nThe Azure Synapse VNet recipe illustrates the IaC deployment of Azure Synapse workspace in a managed VNet configuration. This deployment's networking configuration is the most restrictive setting possible for Synapse workspace. Written in Bicep, this example also explains various challenges faced during such a deployment and provides an end-to-end implementation strategy.\\n\\nAzure Databricks\\n\\nAzure Databricks supports deployment of data plane resources using a customer provided VNet in a process called VNet Injection. This VNet is not managed by Azure. Users must explicitly handle the creation of required subnets, Network Security Groups (NSGs) and routing configuration etc.\\n\\nThe Azure Databricks VNet recipe illustrates the  IaC deployment of Azure Databricks into a VNet while still being able to connect to common Azure services such as Storage and Key Vault. This example is written using Bicep.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\infrastructure-as-code\\\\index.md'}, {'chunkId': 'chunk52_2', 'chunkContent': 'The MDW Repo: Azure Databricks sample provides detailed steps for VNet injection-based deployment that includes firewall settings, data exfiltration protection and user-defined routes. Deployment happens via a shell script deploy.sh that uses Azure CLI and ARM templates to first create the Azure resources (Databricks workspace, storage account, etc.) and then REST API calls for creating Databricks secret scope.\\n\\nMicrosoft Purview\\n\\nMicrosoft Purview is composed of three main \"sub-components\" from a networking perspective: Portal, Account, and Ingestion. When Microsoft Purview is deployed in a customer provided VNet with restricted public access, it needs the following Private Endpoint at minimum:\\n\\n\"Blob\" private endpoint to the Managed Storage (Ingestion)\\n\\n\"Queue\" private endpoint to the Managed Storage (Ingestion)\\n\\n\"Namespace\" private endpoint to the Managed EventHub (Ingestion)\\n\\n\"Portal\" private endpoint for accessing Purview Studio (Web App)\\n\\n\"Account\" private endpoint for API Access', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\infrastructure-as-code\\\\index.md'}, {'chunkId': 'chunk52_3', 'chunkContent': '\"Portal\" private endpoint for accessing Purview Studio (Web App)\\n\\n\"Account\" private endpoint for API Access\\n\\nWhen utilizing private endpoints for ingestion, Microsoft Purview requires the use of a self-hosted integration runtime. The integration runtime is used to scan the target data sources, such as Azure Blob Storage. This scanning process requires a network line-of-sight to the data sources that may themselves be behind a Private Endpoint. Lastly, Azure Key Vault is optionally required for secret storage.\\n\\nManaged VNets are also generally available for Microsoft Purview in six regions from April 2022. Also note that currently there is a limitation that any Azure IaaS, SaaS on-premises sources can\\'t be scanned using Managed Integration Runtime (IR).\\n\\nThe Microsoft Purview VNet recipe aims to address the challenges in configuring Microsoft Purview within a VNet.\\n\\nAzure Data Factory\\n\\nAzure Data Factory (ADF) usually uses an auto-resolve Integration Runtime (IR) to connect to data sources. But sometimes, customers need a private connection, especially when the data sources don\\'t allow public access.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\infrastructure-as-code\\\\index.md'}, {'chunkId': 'chunk52_4', 'chunkContent': 'ADF supports Managed VNet in order to ensure traffic between the auto-resolved IR remains private. With a managed VNet, the burden of managing the VNet can be offloaded to Data Factory. Managed private endpoints are created within the managed VNet to facilitate connectivity between IR and the target data sources.\\n\\nThe ADF portal also supports connecting over a Private Endpoint to facilitate authoring ADF pipelines over a private connection. Furthermore, Azure Data Factory typically requires a connection to Azure Key Vault in order to retrieve secrets through a private connection.\\n\\nThe Azure Data Factory VNet recipe is a sample for IaC deployment of Azure Data Factory using managed VNet. This example is written using Bicep.\\n\\nPermission Automation with Azure Data Services\\n\\nOne common challenge when automating the deployment of Azure data services is the separation between the Control Plane and the Data Plane. This separation exists in various Azure data services such as Azure Synapse, Microsoft Purview, Azure Databricks, and Azure Cosmos DB.\\n\\nAn implication of having separate data plane and control plane is that RBAC (Role-Based Access Control) roles available in the control plane will only be able to handle CRUD (Create, Read, Update, Delete) operations for the service. These operations include actions like deleting the entire service, enabling or disabling features, changing the SKU (Service Level Agreement), and similar tasks.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\infrastructure-as-code\\\\index.md'}, {'chunkId': 'chunk52_5', 'chunkContent': 'Fine grained access control instead will only be controlled through RBAC in the Data Plane. As a result, automating role assignments during deployment within CI/CD pipelines requires extra development effort when Azure CLI, Bicep, or Terraform do not provide direct support for it.\\n\\nAzure Databricks\\n\\nServices like Azure Databricks and Azure Kubernetes Service (AKS) has a dedicated CLI to handle role management. For Databricks, the friction point lies with the need to use databricks CLI in Azure Pipelines for automating access. While for AKS there is currently an attempt at removing the friction via Azure CLI and Bicep, the problem is still unsolved for Databricks.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\infrastructure-as-code\\\\index.md'}, {'chunkId': 'chunk52_6', 'chunkContent': 'The Databricks Data Plane needs to be used to control access to resources like clusters, jobs, secrets or Unity Catalog to name a few. Bicep only supports the Azure Control Plane (management.azure.com) as it is only a layer on top of Azure Resource Manager. As Unity Catalog is not exposed as a resource in the Azure Control Plane, there is no easy path to execute Unity Catalog configurations from Bicep. It can only be done by using the Databricks CLI or the Terraform provider for Databricks. When using Bicep for platform deployment, extra Bash scripts must be executed on self-hosted agents to configure anything at the Databricks Data Plane level. Only the Databricks workspace can be configured and created using ARM/Bicep.\\n\\nAzure Cosmos DB NoSQL\\n\\nFor Azure Cosmos DB NoSQL API, data plane roles can be handled via Azure CLI. Having support through Azure CLI offers a decent developer experience for permission automation in deployment pipelines as it can be achieved using AzureCLI@2 tasks.\\n\\nAzure Synapse\\n\\nAzure Synapse uses several Data Plane RBAC Roles to enforce fine-grained access control on resources in workspaces. Synapse role assignment can be automated through Azure CLI and using AzureCLI@2 tasks in deployment pipelines.\\n\\nMicrosoft Purview', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\infrastructure-as-code\\\\index.md'}, {'chunkId': 'chunk52_7', 'chunkContent': 'Microsoft Purview\\n\\nAutomating role assignments for Microsoft Purview currently requires extra development effort. Consider the following use case.\\n\\nA common use case for permission automation within CI/CD pipelines is handling the assignment an Azure AD group or an MSI to the Data Reader role with a scope limited to a specific Data Collection. This role is required to view assets in the Data Catalog. For a full list of roles refer to Access Control in Microsoft Purview.\\n\\nCurrently automating role assignments needs to be fulfilled via the Metadata Policy API. In order to achieve a use case like the one explained above, the API requires a good amount of extra development work. To overcome this problem, see this code sample that uses GitHub Action to set Purview permissions.\\n\\nAzure Cloud Scale Analytics\\n\\nThe CAF: Cloud-scale Analytics scenario provides a prescriptive data platform design coupled with Azure best practices and design principles. The Cloud-scale Analytics architecture is modular by design and allows customers to start with a small footprint and grow over time. In order to not end up in a migration project, customers should decide upfront how they want to organize data domains across Data Landing Zones. All Cloud-scale Analytics architecture building blocks can be deployed through the Azure portal and through GitHub Actions workflows and Azure Pipelines.\\n\\nThe Cloud-scale Analytics architecture consists of two core building blocks:\\n\\nCAF: Data Management Landing Zone is a subscription that governs the platform.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\infrastructure-as-code\\\\index.md'}, {'chunkId': 'chunk52_8', 'chunkContent': 'CAF: Data Management Landing Zone is a subscription that governs the platform.\\n\\nCAF: Data Landing Zone are subscriptions that host the data products.\\n\\nFurther Reading\\n\\nDataOps: Code First Development\\n\\nDataOps: Continuous Integration', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\devops-for-data\\\\platform-automation\\\\infrastructure-as-code\\\\index.md'}, {'chunkId': 'chunk53_0', 'chunkContent': 'Transactional Systems\\n\\nThis pillar contains guidance in relation to Transactional Systems. Transactional database systems are a type of database management systems that provide support for transaction-oriented applications. It can range from simple databases to complex distributed systems, and they play a vital role in ensuring the reliability and consistency of information. Relational Database Management Systems (RDBMS) and NoSQL databases are two different types of databases that are commonly used for transactional systems. The choice between them depends on the specific requirements of the system and the data it needs to handle.\\n\\nRDBMS are best suited for transactional systems that require strict consistency and relationships between data. They support ACID (Atomicity, Consistency, Isolation, Durability) transactions. The ACID support guarantees that all operations with in a transaction are completed or none of them are completed. NoSQL databases, on the other hand, are more flexible in their data storage and retrieval. NoSQL databases may not support full ACID transactions, but they can offer eventual consistency. Eventual consistency means that the data is consistent after some delay from the moment of execution.\\n\\nCapability Map\\n\\nHere is a high-level capability map for Transactional Systems:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\transactional-systems\\\\index.md'}, {'chunkId': 'chunk54_0', 'chunkContent': \"Common Capabilities\\n\\nThere are some common capabilities, which are applicable to any transactional system - RDBMS or NoSQL. While planning a transactional database system, it's important to keep these points in consideration.\\n\\nScalability\\n\\nHigh Availability/DR\\n\\nReal-time Analytics\\n\\nPerformance Tuning\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\transactional-systems\\\\common-capabilities\\\\index.md'}, {'chunkId': 'chunk55_0', 'chunkContent': 'NoSQL\\n\\nNoSQL (short for \"not only SQL\") databases are a type of database management system that provides a non-relational approach to data storage and retrieval. Unlike traditional relational databases, which are based on the SQL (Structured Query Language) framework, NoSQL databases offer flexible data models and schema-less designs. NoSQL databases are ideal for working with large volumes of unstructured, semi-structured, and structured data, which are increasingly common in modern web applications.\\n\\nThese databases are designed to address some of the limitations of relational databases such as:\\n\\nRigidity of relational schemas\\n\\nDifficulties in scaling horizontally\\n\\nLimited performance for certain types of operations\\n\\nThere are several types of NoSQL databases, each with its own strengths and weaknesses. The most common types of NoSQL databases are:\\n\\nKey-value stores: These databases store data as key-value pairs, where data is accessed using a unique key.\\n\\nDocument databases: Document databases store data in flexible JSON-like documents, allowing nested structures.\\n\\nColumn-oriented databases: These databases store data in columns rather than rows, making them suitable for analytical workloads.\\n\\nGraph databases: Graph databases are designed to store and process data for applications that rely heavily on relationships and complex network analysis.\\n\\nAzure offers various NoSQL database services such as Azure Cosmos DB, Azure Table storage, and Azure Cache for Redis.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\transactional-systems\\\\nosql\\\\index.md'}, {'chunkId': 'chunk55_1', 'chunkContent': 'Azure offers various NoSQL database services such as Azure Cosmos DB, Azure Table storage, and Azure Cache for Redis.\\n\\nDataOps: Non-Relational Model\\n\\nLatency vs Consistency\\n\\nTech-Specific Guidance\\n\\nDataOps: Graph Database Best Practices', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\transactional-systems\\\\nosql\\\\index.md'}, {'chunkId': 'chunk56_0', 'chunkContent': 'Non-relational Model\\n\\nNoSQL and Datalakes have a lot in common in the fact that both have flexibility regarding data schema; a common phrase used is data with schema evolution. Unlike OLTP and OLAP systems which are designed with strict data schemas in mind, NoSQL and Datalakes are designed to be more accommodating to schema changes. For the most part they follow the concept of schema on read rather than schema on write (which is enforced by the database engine to only allow certain data types and structures to be stored). This flexibility of course does not solve for all Analytics solutions. Allowing for a variety of data structures to be stored with limited modeling is appealing in environments where data schema changes often and there are a number of stakeholders with a variety of interests in the data. For example, data scientists wish to pull certain data fields from a file while web developers would like to dump logs into a datalake. Both sets of users could use the subset of data which they need to satisfy the needs of their data processing without having to have it stored separately or cleansed even.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\transactional-systems\\\\nosql\\\\non-relational-model\\\\index.md'}, {'chunkId': 'chunk56_1', 'chunkContent': 'This lack of cleansed data that conforms to a certain structure can be challenging however as Analytics often relies on the idea of comparing data across long periods of time so the capability to group similar data is paramount to having useful data to report. Additionally, not all NoSQL systems are similar. Datalake has been put under this header but technically it is just \"intelligent\" storage with no compute attached to it; designed to be optimized for storing data that will be processed and used for analytics.  Other NoSQL systems include technologies such as Document stores, Column-oriented (Column family), Graph, and Key-value pair. Each of these systems has a different relationship with the Analytics space.  Although there maybe use cases for Analytics within these types of NoSQL stores, they are generally more limited than that of an OLAP system or a Datalake (and usually the Datalake will have a compute layer that is similar to a traditional OLAP system).\\n\\nFurther Reading\\n\\nDataOps: Relational Modeling\\n\\nDataOps: Analytical Systems Data Modeling\\n\\nAzure Architecture Center: Non-Relational Data and NoSQL', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\transactional-systems\\\\nosql\\\\non-relational-model\\\\index.md'}, {'chunkId': 'chunk57_0', 'chunkContent': 'RDBMS\\n\\nRelational model for storing data was proposed by Edgar Codd in 1970: data is organized into relations (called tables in SQL), where each relation is an unordered collection of tuples (rows in SQL). The support for structured data storage and querying made relational database management systems (RDBMS) and SQL made them popular by mid 1980s.\\n\\nRDBMS\\'s structured view of the world fits well with modeling real-world business entities (Example: customers, orders, etc.) and the business relationships between them.\\n\\nOne of the prominent benefits relational databases provide is \"ACID\" guarantee for transactions, which stands for Atomicity, Consistency, Isolation, and Durability. Below are a few resources regarding database transactions:\\n\\nDataOps: Relational Model\\n\\nWikipedia: ACID\\n\\nWikipedia: Transaction Isolation Levels', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\transactional-systems\\\\rdbms\\\\index.md'}, {'chunkId': 'chunk58_0', 'chunkContent': 'Relational Model\\n\\nDesign decisions around relational data modeling focus mainly on performance for writing data. These systems are traditionally designed as the entry point for applications that rely heavily on low latency transactions, including but not limited to websites. Thus, these systems are designed and optimized to store and retrieve data row by row. Relational models are highly normalized (traditionally third normal form (3NF) is the standard) with little/no repetition of data. Relational modeling is designed to reduce the cost for data insertion and retrieval of specific data points/ranges.\\n\\nThe simplified OLTP design above demonstrates that queries are commonly run across a range of rows. These rows have data spread out across multiple tables in a relational design.\\n\\nFurther Reading\\n\\nDataOps: Non-Relational Modeling\\n\\nDataOps: Analytical Systems Data Modeling', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\capabilities\\\\transactional-systems\\\\rdbms\\\\relational-model\\\\index.md'}, {'chunkId': 'chunk59_0', 'chunkContent': \"Edge Data Infrastructure\\n\\nThe rise of edge computing addresses the growing need for processing data closer to its source, driven by IoT devices and high-bandwidth applications. By decentralizing data processing, edge computing reduces latency, conserves bandwidth, and improves privacy and security, enabling real-time analytics and AI-driven applications for a more efficient, connected future.\\n\\nAzure Stack Edge, a part of Microsoft's hybrid cloud solution, seamlessly integrates into the edge computing landscape by offering a robust platform for local data processing and AI-driven workloads. By deploying Azure Stack Edge devices close to the data source, users can harness the power of accelerated machine learning and real-time analytics, while benefiting from reduced latency and improved data privacy. This versatile solution caters to various industries and applications, empowering businesses to efficiently process and analyze data at the edge for enhanced decision-making and responsiveness.\\n\\nData Storage\\n\\nBlob Storage on Edge\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\contributions\\\\edge-data-infrastructure.md'}, {'chunkId': 'chunk59_1', 'chunkContent': 'Data Storage\\n\\nBlob Storage on Edge\\n\\nAzure Stack Edge Storage Account complements the edge computing scenario by providing a locally accessible, high-performance storage solution for unstructured data. It can also synchronize automatically with Storage Accounts on the Azure Cloud, enabling seamless integration with Azure services. By leveraging Blob Storage on Azure Stack Edge, businesses can efficiently handle large volumes of data generated at the edge, ensuring reduced latency, improved privacy, and streamlined data management for a wide range of applications and industries. This approach also opens up further integration possibilities to store and process data on the cloud using a wide array of Azure services.\\n\\nSMB and NFS on Edge\\n\\nAzure Stack Edge not only supports integration with Storage Accounts but also offers versatile file sharing options with SMB and NFS enabled shares. These capabilities allow businesses to store and manage data in a familiar and accessible manner while benefiting from the edge computing scenario. By utilizing SMB and NFS shares, organizations can facilitate seamless data sharing and collaboration across various platforms, ensuring low-latency access to critical data and optimized performance. This flexibility in data storage and management further enhances the value of Azure Stack Edge, making it a comprehensive solution for a wide range of applications and industries.\\n\\nCompute Options\\n\\nEdge Kubernetes Cluster', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\contributions\\\\edge-data-infrastructure.md'}, {'chunkId': 'chunk59_2', 'chunkContent': 'Compute Options\\n\\nEdge Kubernetes Cluster\\n\\nUsing Kubernetes on your Azure Stack Edge  is an excellent way to run Kubernetes workloads on your devices. With minimal configuration required, you can create a Kubernetes cluster on Azure Stack Edge, available in 1-node or 2-node configurations to accommodate high compute requirements.\\n\\nThere are multiple ways to run data workloads on Kubernetes, utilizing tools and frameworks such as:\\n\\nApache Airflow\\n\\nApache Spark\\n\\nArgo Workflow\\n\\nDapr Workflow\\n\\nThe fully Azure Arc-enabled Kubernetes Cluster simplifies the configuration process, allowing for seamless integration of GitOps, centralized monitoring, and metrics collection, among other features supported by its extension ecosystem.\\n\\nVirtual Machines\\n\\nFor workloads not yet ready to run on Kubernetes, organizations can fall back to traditional virtual machines (VMs) as a reliable and flexible alternative. Azure Stack Edge enables the deployment of VMs using ARM templates and the Azure CLI, allowing users to run their data processing workloads with ease. By leveraging the familiar and accessible environment provided by VMs on Azure Stack Edge, businesses can continue to optimize their IT resources and maintain operational efficiency while gradually transitioning towards modern containerization and orchestration solutions like Kubernetes.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\contributions\\\\edge-data-infrastructure.md'}, {'chunkId': 'chunk60_0', 'chunkContent': \"SQL Server Edge\\n\\nCapabilities\\n\\nAzure SQL Edge is designed for deployment in Edge environments, offering a range of capabilities that include:\\n\\nRunning SQL Server on Linux and container-based environments such as Kubernetes.\\n\\nHigh-performance data storage with a consistent T-SQL programming interface.\\n\\nBi-directional data synchronization between SQL Edge and Azure SQL Server.\\n\\nStream processing and analysis for both relational and non-relational data.\\n\\nMachine Learning and Artificial Intelligence support through integration with the ONNX runtime.\\n\\nConsiderations\\n\\nFeature limitations\\n\\nAlthough Azure SQL Edge is built on the latest SQL engine, it's crucial to recognize the feature differences between SQL Edge and SQL Server. For example, service native to SQL Server such as Analysis Services and Reporting Services are not available on SQL Edge.\\n\\nThe full list of unsupported features can be found here.\\n\\nCost planning\\n\\nAzure SQL Edge pricing is based on a per-device model, with reserved instances available for purchase for 1 or 3 years. When selecting the SQL Edge service, it is essential to determine the number of SQL Edge instances needed for your solution.\\n\\nOperation\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\contributions\\\\sql-server-edge.md'}, {'chunkId': 'chunk60_1', 'chunkContent': 'Operation\\n\\nSince SQL Edge instances are deployed on the Edge, it is important to plan for more operational maintenance. Developers should account for the operational costs associated with maintaining, tuning, backing up, updating, and monitoring the deployed services. Relevant documentation can be found on the product documentation pages.\\n\\nPerformance\\n\\nBackup and restore\\n\\nHigh availability\\n\\nSecurity', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\contributions\\\\sql-server-edge.md'}, {'chunkId': 'chunk61_0', 'chunkContent': \"Getting Started\\n\\nOverview\\n\\nThe use of DataOps in the section name extends beyond the established definition (see below), and includes most data engineering aspects as well.\\n\\nThe content of this page is suited for all audiences including users who are new to data projects. To jump to a particular section of interest, proceed directly to the section using links in next steps.\\n\\nDataOps is a lifecycle approach to data projects. Applied across an organization, it focuses on improving the communication, integration and data flow between data managers and data consumers. DataOps uses agile practices to orchestrate tools, code, and infrastructure to quickly deliver high-quality data with improved security.\\n\\nThe DataOps section of the playbook presents information on data systems as a set of capabilities that are available to implement a solution for a given architectural pattern, which is based on existing data architectures.\\n\\nRefer to DataOps: Value Proposition for the goals of the DataOps section. It should be noted that, DataOps section isn't a:\\n\\nDefinitive guide on data engineering/data operations. DataOps section contents are suggestions and guidance based on the past implementations and may cover only the common scenarios/major topics.\\n\\nGuide on general coding and/or development best practices. Refer to Code with Engineering Playbook for development best practices.\\n\\nGuide on Cloud adoption. Cloud adoption Framework is the document for cloud adoption.\\n\\nDataOps section prerequisites\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\getting-started\\\\index.md'}, {'chunkId': 'chunk61_1', 'chunkContent': 'Guide on Cloud adoption. Cloud adoption Framework is the document for cloud adoption.\\n\\nDataOps section prerequisites\\n\\nThis section describes the minimum expected knowledge level of the users looking to make use of the DataOps Section of the Playbook. Other prerequisites will be mentioned as part of individual sections as needed.\\n\\nIn addition to general programming concepts, this document assumes that user has basic understanding of a data platform and associated key terms like - data model, database, database table, schema, column, data base normalization and de-normalization etc. Some other terms (Clickable) users are expected to be familiar with are:\\n\\nETL,\\n\\nELT,\\n\\nRDBMS,\\n\\nNo-Sql,\\n\\nSchema-on-read and Schema-on-write,\\n\\nbig data,\\n\\n3Vs (Volume, Velocity, Variety) of big data\\n\\nData system components and data flow\\n\\nThe below diagram shows a typical logical flow of data in most modern data systems. Not every system has all the components and most systems employ a combination of offerings (capabilities, technologies, architectures). A couple of other example flows:\\n\\nReporting directly from streaming inputs and/or ingestion layer and avoiding batch processing for transformation layer.\\n\\nData prioritization and anomaly detection using streaming.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\getting-started\\\\index.md'}, {'chunkId': 'chunk61_2', 'chunkContent': 'Data prioritization and anomaly detection using streaming.\\n\\nData sources can be any applications, processes and/or resources generating transactions/operations data. It could also be another upstream data model feeding the current application. Examples include online processing systems, IOT devices, user applications, mobile devices, upstream data model etc.\\n\\nA transactional system can have one or more transactional data stores/databases, each capturing transactions for a specific application/operational area. Transactional systems allow for efficient read and write of row level data with low latencies. They also enforce schema during write operations (schema-on-write), provide ACID compliance and maintain data integrity. Preferred option in transactional systems is to use ETL to extract data from sources, apply transformations and load data in to the transactional store/s. Data stored is typically in a normalized form (3NF being most common) to enable for better CRUD operations performance and to avoid data anomalies,  data duplication. An optional operational data store (ODS) provides an aggregated view of all transactional data. Only current version/snapshot of information is persisted in an ODS with historical data constantly overwritten. An ODS is the commonly used data source for most operational reports providing insights in to the current state of operations. Also, ODSs are often found acting as intermediate points between transactional stores and analytical systems (Ex: a data warehouse or a data lake).', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\getting-started\\\\index.md'}, {'chunkId': 'chunk61_3', 'chunkContent': 'Click on transactional system for a more detailed transactional flow diagram.\\n\\nIn most data projects, the analytical system represents most of the data estate. It includes: ingestion of transactional (operational) data to combine with historical data, data transformations, and data aggregations. Analytical system is also the space where data scientists operate along side business analysts/users performing analytical querying and report generation. Most modern systems read data from transactional systems using batch processing, and may also read data directly from data sources (Ex: Anomaly detection) and stream processing. The reporting layer typically employs denormalized data (STAR schema) with ETLs populating these structured tables for efficient data retrieval.\\n\\nCompared to a transactional system where ETL, structured data, schema-on-write are common options analytical systems have ELT, structured/unstructured/semi-structured data and schema-on-read. Transactional systems prioritize storage optimization and data retrieval speeds. Analytical systems prioritize data ingestion speed and analytical query performance over avoiding data duplication.\\n\\nThe link analytical system provides a more detailed analytical flow diagram.\\n\\nVisual guide', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\getting-started\\\\index.md'}, {'chunkId': 'chunk61_4', 'chunkContent': 'The link analytical system provides a more detailed analytical flow diagram.\\n\\nVisual guide\\n\\nA typical data project contains several steps, involves multiple teams, technologies requiring varied skills, and capabilities. The following diagram provides a generic ordering of DataOps capabilities that are most likely needed during a particular phase of a data project. Clicking on the groupings or tasks in the groups will take the user to the playbook details. The circular flow indicates the iterative nature of the work involved while delivering a data project solution.\\n\\nNext steps\\n\\nUse solutions to accelerate development by making use of code samples that are based on real life implementations.\\n\\nRefer to capabilities to learn about conceptual building blocks of a data project.\\n\\nUse guidance for pointed guidance and best practices.\\n\\nRefer to industry solutions for implementation examples from different industries.\\n\\nMDW OpenHack is a good hands-on resource for understanding the key components of a modern data architecture (Modern Data warehouse[MDW]).\\n\\nFurther reading\\n\\nMicrosoft Learn: Azure DataOps\\n\\nMicrosoft learn: Data architecture\\n\\nAzure Architecture Center\\n\\nAzure Architecture Guide\\n\\nMicrosoft Cloud Adoption Framework for Azure(CAF)', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\getting-started\\\\index.md'}, {'chunkId': 'chunk62_0', 'chunkContent': \"rings:\\n  - public\\n\\nADX Use Cases and Best Practices\\n\\nAn overview of the capabilities of ADX can be found here.\\n\\nUse cases\\n\\nThe following sections describe some of the unique strengths of Azure Data Explorer.\\n\\nExploration and visualization\\n\\nADX is great for ad-hoc exploration and visualization of data. Its intuitive query language, Kusto Query Language (KQL), makes it easy to analyze and visualize data.\\n\\nAs exploration through ad-hoc queries is ADX's main goal, it is thus not designed to run long-running batch analytical queries.\\n\\nTime-series querying\\n\\nBy default, ADX partitions data by time that makes it great for filtering queries on a specific time range. There are also a wide variety of time-series functions available in KQL.\\n\\nIf not interested in querying data by time, then configuration through the partitioning policy is possible but limited.\\n\\nHot storage\\n\\nADX loads data proactively in its hot storage, i.e., local SSD. The loading is determined either by ingestion time or by a datetime column. By default, it loads the most recent data into hot storage. This behavior can be modified through hot windows.\\n\\nIf the data is queried by time, and the amount of data can be stored in hot storage, then ADX can be a great choice.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\adx-best-practices-and-use-cases.md'}, {'chunkId': 'chunk62_1', 'chunkContent': \"If data is not filtered by time and your data does not fit in hot storage, then ADX is most likely not a good choice. ADX is not efficient in querying on cold storage due to limited partitioning options. That fact means that ADX needs to load huge partitions to disk, which can be inefficient.\\n\\nLow-latency data store\\n\\nOne of the major advantages of ADX is that it is a near real-time data store. The intention is the data being available for querying shortly after ingestion.\\n\\nADX does this by storing data in a columnar store for fast querying, but in addition stores newly ingested data initially in a row-store, for quick availability in querying. Both stores are ADX internals and not exposed to the user.\\n\\nTo summarize, scenario's where ADX may not be suitable\\n\\nexecuting long running batch analytical queries\\n\\nmost-frequently queried data does not fit in hot storage\\n\\nif the use case requires partitioning on multiple columns other than time\\n\\nBest practices\\n\\nShuffle query\\n\\nThe shuffle query is used to inform ADX on how the query data should be grouped and distributed between nodes during the query execution. The shuffle query can have a significant impact on query performance. Make sure to choose a shuffle key based on the query's summarization, join, make-series and partitioning keys.\\n\\nPartitioning\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\adx-best-practices-and-use-cases.md'}, {'chunkId': 'chunk62_2', 'chunkContent': \"Partitioning\\n\\nPartitioning can improve the performance of specific queries. The scenario's when a partitioning policy is recommended can be seen here.\\n\\nPartitioning can only be done on a single string and or datetime column. However, it is possible to create an extra composite string column that contains the values of multiple columns.\\n\\nAn example of a composite column can be the concatenation of 'longitude' and 'latitude' columns through geospatial clustering.\\n\\nIngestion\\n\\nIngesting data into ADX is recommended over directly querying from external data sources as it makes the queries more efficient by:\\n\\nstoring the data in a columnar format\\n\\npartitioning the data by time\\n\\nloading most recent data into the hot storage\\n\\ncompressing the data\\n\\ntransforming and optimizing the data\\n\\nFor historical ingestion, Azure Data Factory is recommended as an orchestration tool to integrate different data stores, perform transformations on data and submit to ADX through the copy activity. Using the copy activity is recommended especially for high volume data ingestion and in production scenarios. The copy activity assures high availability and reliability by providing load-balancing, retry logic, and error handling. There's also no size limit on the data that can be ingested through the copy activity.\\n\\nGeospatial features\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\adx-best-practices-and-use-cases.md'}, {'chunkId': 'chunk62_3', 'chunkContent': 'Geospatial features\\n\\nGeospatial clustering is a technique in which geographically proximate locations are clustered into layers of grids. The grids have cells and each cell is encoded into short string.\\n\\nDuring ingestion, it is possible to define a transformation policy to enrich the data with a geospatial column. This column can then be used for filtering, which can be more efficient than filtering directly on the latitude and longitude columns.\\n\\nNote: Setting transformation policies can slow down the ingestion process significantly.\\n\\nMultiple clusters\\n\\nSplit loads for different users or applications on ADX  are possible by using an extra cluster on the same data source. To achieve that separation, the ingested database (leader database) needs to be attached to a separate ADX instance as a follower database.\\n\\nCapacity policy\\n\\nThe capacity policy is used for controlling the compute resources. These resources are allocated to specific types of data management operations in the cluster such as ingestion and extents optimization. Depending on the use-case, the  default capacity policy can be modified to improve performance.\\n\\nFor example, to prioritize ingesting data into ADX, the core utilization capacity can be modified from the default 75% to 100%.\\n\\nNote: Increasing the core utilization capacity of one data management operation can impact the performance of others.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\adx-best-practices-and-use-cases.md'}, {'chunkId': 'chunk63_0', 'chunkContent': 'rings:\\n  - public\\n\\nAzure Databricks Data Storage\\n\\nIntroduction\\n\\nWhen working on analytics projects, Azure Databricks can be used to implement the ingestion and transformations layers. As a consequence, understanding the underlying storage and specific settings is paramount accelerate performance and efficiency.\\n\\nThis section provides a brief summary of the available storage options and offers some important considerations for designing and setting up a data lake.\\n\\nDelta storage\\n\\nDelta Lake is the default storage layer of Databricks that stores data and tables. Unless otherwise specified, all tables created by Azure Databricks are Delta tables. Partitioning in Delta means that data is chunked into separate baskets (folder in blob storage). When this store is queried, just the relevant data from the baskets is loaded. Data in Delta Lake is stored as Parquet, a columnar file format. Columnar file formats are the recommended choice to speed up analytics results. These formats allow retrieval of the data only for the columns that are part of the query executed. Also, column level compression is highly effective compared to row level compression.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-storage.md'}, {'chunkId': 'chunk63_1', 'chunkContent': 'Databricks uses Spark as its engine. Ideally, data in Spark is stored in a smaller number of large files. The size ranges between 128 MB and 1 GB and allows efficient operation of the driver and worker nodes. Having the data spread over many small files will use up much of the memory when the driver tries to load all the file metadata at once. Such a load will stress the driver and slow down reading.\\n\\nFurther, Databricks Delta autoOptimize (Example: delta.autoOptimize.optimizeWrite and delta.autoOptimize.autoCompact. optimizeWrite) features aim to maximize the throughput of data being written to storage. The autoCompact feature can compact a file to the desired data size (default is 1 GB). Compaction (bin-packing) operation is idempotent i.e., running compaction again on the same dataset has no effect.\\n\\nDatabricks Delta automatically collects the information (minimum and maximum values for each column) as metadata, while saving the data as Delta files. Databricks uses this meta information to achieve faster query execution by filtering the data.\\n\\nLastly, the use of Z-Ordering (multi-dimensional clustering) feature of Databricks Delta can be used to speed up the retrieval of data. Z-Ordering co-locates related information in the same set of files.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-storage.md'}, {'chunkId': 'chunk63_2', 'chunkContent': 'The Z-Order will be automatically used by Delta during data-skipping. Data-skipping helps to reduce the amount of data Delta needs to read before finding the correct data.\\n\\nAzure Data Lake Storage Gen2\\n\\nAzure Data Lake Storage Gen2 provides a mechanism that allows the collection of objects/file to be organized into a hierarchy of directories. This feature enhances performance, management and security. For more information, see data lake storage introduction.\\n\\nAtomic directory manipulation improves performance and data consistency for operations like moving directories, or deleting expired data for a given date range.\\n\\nAzure Data Lake Storage Gen2 implements an access control model that supports both Azure role-based access control (Azure RBAC) and POSIX-like access control lists (ACLs).\\n\\nFurther reading\\n\\nDataOps: Selecting a partitioning strategy\\n\\nDelta Lake best practices\\n\\nDatabricks Blog: Processing petabytes of data with Databricks Delta', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-storage.md'}, {'chunkId': 'chunk64_0', 'chunkContent': 'rings:\\n  - public\\n\\nAzure Databricks vs Azure Synapse Analytics\\n\\nAzure Synapse Analytics is an integrated analytics service provided by Microsoft. This service combines enterprise data warehousing, big data processing, and data integration into a single platform. It provides deep integration with other Azure services such as Microsoft Power BI, Azure Cosmos DB, and Azure ML.\\n\\nAzure Databricks is a fast, scalable, and provides a collaborative analytics platform with Microsoft  Databricks. Azure Databricks is built on Apache Spark, an open-source analytics engine. It provides a fully managed, optimized environment designed for processing and analyzing large volumes of big data.\\n\\nDevelopers must decide whether to use Azure Databricks or Azure Synapse Analytics for data processing. Here we cover some ideas to help with that decision. Rather than making detailed feature comparisons we focus on broader factors that help make the decision.\\n\\nConsiderations\\n\\nFactors to include to make a choice between using Databricks or Synapse Analytics:\\n\\nStructured Streaming\\n\\nFor near-real time data processing, Databricks: Structured Streaming on Azure Databricks is a great choice. It includes tight integration with Delta Lake and \"Auto Loader\" functionality, and offers end-to-end fault tolerance with an exactly-once processing guarantee.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-vs-synapse.md'}, {'chunkId': 'chunk64_1', 'chunkContent': \"As a data warehouse, near-real time data can be ingested into Azure Synapse Analytics using Azure Stream analytics but Delta Lake isn't currently supported. As a development platform, Synapse doesn’t fully focus on real-time transformations currently.\\n\\nEnterprise Data Warehouse Capabilities\\n\\nMany times, there is a requirement for a serving layer in your data lake that has traditional data warehousing capabilities. For such cases, Azure Synapse Analytics is a great choice.\\n\\nAzure Synapse Analytics brings together enterprise data warehousing and Big Data analytics. Dedicated SQL pool refers to the enterprise data warehousing features that are available in Azure Synapse Analytics, and represents a collection of analytic resources that are provisioned when using Synapse SQL. Data is stored in relational tables with columnar storage format which significantly reduces  data storage costs, and improves query performance. Once data is stored, you can run analytics at massive scale.\\n\\nIn Azure Databricks, a Delta Lake based data warehouse is possible but it won't have the full breadth of SQL and data warehousing capabilities.\\n\\nServerless SQL Capabilities\\n\\nA running cluster is generally required to query data using Spark and results in extra cost and underutilized resources. Having serverless capabilities to run ad-hoc queries and reports is a much-desired feature.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-vs-synapse.md'}, {'chunkId': 'chunk64_2', 'chunkContent': \"Azure Synapse Analytics has this built-in capability in the form of Serverless SQL Pool. Serverless SQL pool is a distributed data processing system, built for large-scale data and computational functions. Here are some of the characteristics of Serverless SQL Pool:\\n\\nIt's serverless, hence there's no infrastructure to setup or clusters to maintain.\\n\\nA default endpoint for this service is provided within every Azure Synapse workspace.\\n\\nThe external Spark tables can be queried directly from serverless SQL pool.\\n\\nBased on pay-per-use model, there is no charge for resources reserved. Users are only charged for data processed by queries run.\\n\\nAzure Databricks, Serverless SQL provides instant compute to users for their BI and SQL workloads, with minimal management requirements. Similar to Synapse, users only pay for Serverless SQL when they start running reports or queries.\\n\\nAdmins can create serverless SQL warehouses (formerly SQL endpoints) that enable instant compute and are managed by Azure Databricks. Serverless SQL warehouses use compute clusters in the Azure subscription of Azure Databricks.\\n\\nSupported Git Providers\\n\\nCompanies often use specific Git providers across the organization. In such cases, it is important to check to see if the data service supports integration with the git repository.\\n\\nAzure Databricks supports the following Git providers:\\n\\nGitHub and GitHub AE\\n\\nBitbucket Cloud\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-vs-synapse.md'}, {'chunkId': 'chunk64_3', 'chunkContent': 'Azure Databricks supports the following Git providers:\\n\\nGitHub and GitHub AE\\n\\nBitbucket Cloud\\n\\nGitLab\\n\\nAzure DevOps Git\\n\\nAzure Databricks also offers support for enterprise git platforms like GitHub Enterprise Server, Bitbucket Server, GitLab self-managed integration. NOTE: for the integration to work, the git server must be accessible over the internet.\\n\\nAzure Synapse Analytics supports only the following Git providers:\\n\\nAzure DevOps Git\\n\\nGitHub and GitHub Enterprise\\n\\nIntegration with Microsoft Dataverse\\n\\nIf you are using Power Platforms, seamless and managed integration between Azure Synapse with Dataverse might be an important consideration. With Azure Synapse Link, Microsoft Dataverse data can be connected to Azure Synapse Analytics to get near real-time insights over the data.\\n\\nThis kind of integration of Microsoft Dataverse is not currently available for Azure Databricks. It can be achieved by using third-party connectors or writing custom integration code based on Dataverse Web APIs.\\n\\nLanguage Support\\n\\nHere is a quick summary of supported languages:\\n\\nLanguage Azure Synapse Azure Databricks PySpark (Python) Yes Yes Spark (Scala) Yes Yes Spark SQL Yes Yes .NET Spark (C#) Yes No** SparkR (R) Yes* Yes', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-vs-synapse.md'}, {'chunkId': 'chunk64_4', 'chunkContent': \"Support for R is currently under public preview as of October 18, 2022.\\n\\n**Though not supported out-of-box, the .NET for Apache Spark jobs can still be run on Databricks clusters. Check Microsoft Documentation for the details.\\n\\nSpecific Features\\n\\nThere are many features that are unique to each product. For instance, Azure Databricks has Z-Ordering technique to colocate related information in the same set of files. Z-Ordering can dramatically reduce the amount of data that Delta Lake on Databricks needs to read and thus improve overall query performance.\\n\\nAuto Loader is another unique capability of Azure Databricks that allows incremental processing of new data files as they arrive in cloud storage efficiently.\\n\\nFor workloads that process a significant amount of data (100 GB+) and include aggregations and joins, Azure Databricks has a native vectorized query engine called Photon. Directly compatible with Apache Spark APIs, Photon is developed in C++ to take advantage of modern hardware. It uses the latest techniques in vectorized query processing to capitalize on data- and instruction-level parallelism in CPUs. It does have certain limitations though, such as it doesn't support UDFs or RDD APIs. Refer to the documentation linked above for details.\\n\\nData Governance\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-vs-synapse.md'}, {'chunkId': 'chunk64_5', 'chunkContent': 'Data Governance\\n\\nMicrosoft Purview is a data governance service provided by Microsoft. It helps organizations discover, understand, and manage their data assets across various sources and locations.\\n\\nPurview has connectors to authenticate and interact with both Azure Synapse and Azure Databricks. For Azure Databricks, the Azure Databricks to Purview Lineage Connector can transfer lineage metadata from Spark operations to Microsoft Purview.\\n\\nIn addition, a Microsoft Purview account can be registered to an Azure Synapse workspace. It allows you to discover Microsoft Purview assets, interact with them through Synapse capabilities, and push lineage information to Microsoft Purview.\\n\\nFor Azure Databricks, Unity Catalog provides an alternate data governance option for data and AI assets in the Lakehouse. Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. It has built-in auditing and lineage capabilities and supports data discovery.\\n\\nIn summary, if Microsoft Purview is used as the data governance platform, Azure Synapse generally has seamless integration and more flexibility. But if you are planning to use Unity Catalog, Azure Databricks is the only choice.\\n\\nNetwork Isolation', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-vs-synapse.md'}, {'chunkId': 'chunk64_6', 'chunkContent': 'Network Isolation\\n\\nBoth Azure Databricks and Azure Synapse Analytics provides capabilities of secure network deployment but the method used is different. While Azure Synapse can be deployed using Managed workspace Virtual Networks to provide network isolation, Azure Databricks uses VNet injections with Secure cluster connectivity to achieve deployment.\\n\\nHaving said that, the \"Managed VNet\" deployment of Azure Synapse Analytics makes it much easier. Also, the public access to Synapse studio can be easily blocked using the Azure Synapse Analytics IP firewall rules. On Azure Databricks, the same can be achieved using the IP Access List API that enables Databricks admins to configure IP allow lists and block lists for a workspace.\\n\\nCheck Azure Databricks VNet recipe and Azure Synapse VNet recipe for more details and code samples.\\n\\nData Pipeline Orchestration\\n\\nMost big data solutions consist of repeated data processing operations, encapsulated in workflows. A pipeline orchestrator is a tool that helps to automate these workflows. An orchestrator can schedule jobs, execute workflows, and coordinate dependencies among tasks.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-vs-synapse.md'}, {'chunkId': 'chunk64_7', 'chunkContent': 'Azure Data Factory (ADF) is a managed service that can be used as a pipeline orchestrator. Within Azure Synapse, these pipelines are in-built (based on ADF) and are called Synapse pipelines. These pipelines can be triggered manually or configured based on a scheduled, tumbling window, storage event, or custom event. It can trigger both Databricks and Synapse jobs/notebooks.\\n\\nDatabricks also has a new capability called Delta Live Tables (DLT). DLT is a framework for building reliable, maintainable, and testable data processing pipelines. Users define transformations to perform on data, and DLT manages task orchestration, cluster management, monitoring, data quality, and error handling. You can also enforce data quality with DLT expectations. DLT for Azure Databricks requires a \"Premium Plan\".\\n\\nFor more information, see DataOps: Choosing Pipeline Orchestrator.\\n\\nCustomer Preference\\n\\nBoth Azure Databricks and Azure Synapse Analytics are first-party services on Azure. These services can be created easily using the Azure portal or other Azure tools (such as Azure CLI, PowerShell, SDK) and have full enterprise support. In addition, the technological differences between two platforms are either trivial or can be compensated for by using alternate options and/or workarounds.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-vs-synapse.md'}, {'chunkId': 'chunk64_8', 'chunkContent': \"And that's where a customer's preference(s) can help. There are various factors that can affect their decision such as:\\n\\nCustomer is already using a specific service for other projects.\\n\\nCustomer has a preference for open-source products.\\n\\nCustomer wants portability across cloud platforms.\\n\\nThere are organizational guidelines for using (or not using) particular technology.\\n\\nThe team is proficient in a certain service/programming language.\\n\\nReferences\\n\\nAlberto Cloud: Synapse and Databricks\\n\\nAzure Databricks: Delta Live Tables introduction\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\databricks-vs-synapse.md'}, {'chunkId': 'chunk65_0', 'chunkContent': \"tags:\\n  - Microsoft Fabric\\n\\nGetting Started with Microsoft Fabric\\n\\nMicrosoft Fabric is currently in PREVIEW. This information relates to a prerelease product that may be substantially modified before it's released. Reader discernment is strongly advised, and it is highly recommended not to base any production deployment decisions solely on the information provided below.\\n\\nMicrosoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. The platform is built on a foundation of Software as a Service (SaaS).\\n\\nThis article brings together various resources from both inside and outside sources to assist readers in exploring and learning about Microsoft Fabric. It's important to note that this field is constantly changing, so some of the links provided may no longer work. Please feel free to highlight any discrepancies and suggest improvements.\\n\\nMicrosoft Blog - Introducing Microsoft Fabric\\n\\nGet started by reading through the introductory blog on Microsoft Fabric by Arun Ulagaratchagan (CVP, Azure Data) where he discusses the prevailing business challenges encountered within the current Data and AI Landscape and illustrates how Microsoft Fabric effectively addresses these obstacles. Furthermore, at the conclusion of the blog, there is a compilation of additional references to explore.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\getting-started-with-microsoft-fabric.md'}, {'chunkId': 'chunk65_1', 'chunkContent': \"Web link Level Time Commitment Data analytics for the era of AI Beginner 10 minutes\\n\\nMicrosoft Build Sessions\\n\\nMicrosoft Build offered a multitude of comprehensive sessions that delve into various facets of Microsoft Fabric. Start by watching the Keynote and then proceed with the Day 1 and Day 2 events which expound upon the layers of Microsoft Fabric, complemented by various demonstrations and presentations.\\n\\nWeb link Level Time Commitment Microsoft Build 2023 - Keynote Beginner 30 minutes Microsoft Fabric Launch Digital Event (Day 1) Beginner 3 hours Microsoft Fabric Launch Digital Event (Day 2) Beginner 2 hours  45 minutes\\n\\nMicrosoft Learning Modules\\n\\nThe following Microsoft learning path is a great way to learn about the various components and features of Microsoft Fabric. It comprises several modules and includes various hands-on exercises that one can follow along in one's own Fabric environment.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\getting-started-with-microsoft-fabric.md'}, {'chunkId': 'chunk65_2', 'chunkContent': 'Web link Level Time Commitment Microsoft Fabric Learning Path Beginner 7 hours and 20 minutes Learning Modules Introduction to end-to-end analytics Beginner 21 minutes Get started with lakehouse Beginner 46 minutes Use Apache Spark Intermediate 1 hour 13 minutes Work with Delta Lake tables Intermediate 1 hour 3 minutes Use Data Factory pipelines Beginner 1 hour 22 minutes Get started with Data Activator Intermediate 1 hour 6 minutes Get started with Real-Time Analytics Beginner 53 minutes Administer Microsoft Fabric Beginner 30 minutes Ingest Data with Dataflows Gen2 Intermediate 48 minutes Get started with data science Beginner 48 minutes Get started with data warehouses Beginner 1 hour 6 minutes Data analysis using KQL Beginner 34 minutes Using real time eventstreams Beginner 1 hour 16 minutes Organize a Fabric lakehouse using medallion architecture design Intermediate 1 hour 7 minutes Ingest data with Spark and Microsoft Fabric notebooks Intermediate 48 minutes Preprocess data with Data Wrangler Beginner 1 hour 10 minutes Explore data for data science with notebooks Beginner 1 hour 27 minutes Train and track ML models with MLflow Beginner 54 minutes Generate batch predictions using a deployed model Beginner 50 minutes\\n\\nIntroduction to Microsoft Fabric - Webinar Series\\n\\nThe Fabric webinar series, designed specifically for Microsoft Build, provides detailed insights into the various features of Microsoft Fabric, allowing for a deeper understanding of the platform.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\getting-started-with-microsoft-fabric.md'}, {'chunkId': 'chunk65_3', 'chunkContent': \"Web link Level Time Commitment Microsoft Fabric Webinars - Landing Page Intermediate 2 hours Individual Episodes Introducing Unified Analytics Intermediate 21 minutes Data Factory in Microsoft Fabric Intermediate 10 minutes Power BI in Microsoft Fabric Intermediate 18 minutes Data Engineering with Microsoft Fabric Intermediate 16 minutes Real-Time Analytics in Microsoft Fabric Intermediate 15 minutes Data Warehousing in Microsoft Fabric Intermediate 15 minutes Data Science in Microsoft Fabric Intermediate 15 minutes\\n\\nEnd-to-end Tutorials\\n\\nIn this section, you'll find a collection of step-by-step tutorials available in Microsoft Fabric. These tutorials cover everything from acquiring data to using it, providing valuable guidance to help you understand the Fabric UI, different experiences in Fabric, and the options for professional and citizen developers on the platform.\\n\\nWeb link End-to-end Tutorials - Landing Page Multi-experience Tutorials Data Engineering - Lakehouse Data Science - E2E Scenario Real-Time Analytics Data Warehouse Data Activator Experience-specific Tutorials Power BI Data Factory Data Science - E2E AI samples Data Science - Price prediction with R Data Science - Flight delay prediction with R\\n\\nMicrosoft Fabric Documentation\\n\\nThe official Microsoft documentation serves as the prime resource to stay abreast of the latest advancements and offerings within Microsoft Fabric. With continuous updates and additions, the documentation keeps you updated on the latest features and capabilities.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\getting-started-with-microsoft-fabric.md'}, {'chunkId': 'chunk65_4', 'chunkContent': 'Web link Description Microsoft Fabric Documentation - Landing Page Main Landing Page. Concepts Fabric Terminology Get familiar with the Fabric Terminology. Workspaces Understand the concept of \"Workspace\". OneLake Understand the concept of \"OneLake\". lakehouse Understand the concept of \"Lakehouse\". Fabric Workloads Data Factory Learn about cloud-scale data movement and data transformation. Data Engineering Learn about how to collect, store, process, and analyze large volumes of data. Data Science Learn about Data Science experience in Microsoft Fabric. Real-Time Analytics Learn about Real-Time analytics for data loading and advanced visualization scenarios. Data Warehouse Learn more about the Data Warehousing experiences in Microsoft Fabric. Power BI Learn about Power BI to connect, visualize, discover and share data. Data Activator Learn about Data Activator, and how to create events and triggers for your data. Administration and Monitoring Microsoft Fabric Administration Learn about the Microsoft Fabric admin settings, options and tools. Developer experience Lifecycle Management Learn about the lifecycle management tools that enable efficient product development, continuous updates and fast releases. Decision Guides Lakehouse vs Warehouse Choose between lakehouse and warehouse for your workload. Copy Activity vs Dataflow vs Spark Decide whether you need a copy activity, a dataflow, or Spark for your workload.\\n\\nNext Steps\\n\\nNow that you understand Microsoft Fabric well, feel free to explore the additional resources below.\\n\\nExplore Microsoft Fabric through the Guided Tour.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\getting-started-with-microsoft-fabric.md'}, {'chunkId': 'chunk65_5', 'chunkContent': 'Now that you understand Microsoft Fabric well, feel free to explore the additional resources below.\\n\\nExplore Microsoft Fabric through the Guided Tour.\\n\\nTry Microsoft Fabric (Preview) with 60 days free trial. Please note that for public preview, the Fabric (Preview) trial requires a Power BI license. Microsoft employees can start the free trial at https://msit.fabric.microsoft.com.\\n\\nJoin Fabric CAT Teams Group. Fabric Customer Advisory Team (CAT) team is a place where you can ask questions on Fabric or Synapse and get answers from the CAT team or the thousands of members that are in the group.\\n\\nExplore Fabric CAT Portal. This portal has a great deal of in-depth content, references and contact details which would help you in your Fabric journey.\\n\\nGo through the recordings of Analytics Technical Virtual Summit where you can understand the early thinking about the product roadmap and watch the in-depth technical demonstration from the engineering teams.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\getting-started-with-microsoft-fabric.md'}, {'chunkId': 'chunk66_0', 'chunkContent': \"rings:\\n  - public\\n\\nGraph Database Best Practices\\n\\nGraph databases are one of the NoSQL options for storing data. While the concept can be easy to understand, the implementation can get complicated. Engineers and architects usually know what nodes and edges are. However, it's not always easy to know when to use a graph database to meet business needs. This article offers some best practices for graph databases in general and Cosmos DB Gremlin API in particular. The following tips can help improve the success of graph database implementations, particularly when using traversal-based queries, which offer a distinct approach to accessing data.\\n\\nDatabase Agnostic Best Practices\\n\\nEstablish Graph Use Case Validity\\n\\nMost problems can and should be solved in a relational database. Use cases suitable for graph databases require an understanding of costs and benefits trade-off of graph databases features. Possibly the most important best practice for using graph databases is to establish that you have a valid use case for a graph database.\\n\\nUsing a graph database beyond a proof of concept can be risky for many reasons:\\n\\nOften the query languages can be complex and unfamiliar\\n\\nYour team might not have the devops or infrastructure experience to support a graph database in production\\n\\nAdding another data engine to a stack increases complexity and infrastructure overhead\\n\\nPoorly designed graph data access patterns can be costly in terms of cloud computing resources\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_1', 'chunkContent': 'Poorly designed graph data access patterns can be costly in terms of cloud computing resources\\n\\nIt is recommended to consider the risks associated with a graph database against its potential value. If the goal is to ship to production, it is suggested to begin with a relational database and assess where a graph might add value.\\n\\nFor more information, see Design Hybrid Data Architecture\\n\\nAfter attempting to solve the business problem with a relational database, establish whether any of the following problems are present:\\n\\nQueries are unwieldy due to high cardinality of data\\n\\nThe data model is interconnected with complex relationships\\n\\nThe schema or DDL changes often to handle evolving relationships\\n\\nThe query logic implements repeated cyclic joins with complex conditionals\\n\\nThe data access patterns return result set unions with different object types\\n\\nIf the data access is impacted by these issues, it may be beneficial to take the risk of adding a graph database to the data architecture.\\n\\nIn some cases, a graph database might be an anti-pattern. The following issues in a graph implementation may indicate the need to reconsider a relational database instead:\\n\\nThe data model changes infrequently or never\\n\\nTraversals are simple enough to express cleanly as CTEs in SQL\\n\\nData access patterns are more property focused than path focused\\n\\nTraversals all follow the same edges (low data cardinality)\\n\\nQueries consistently return the same node/object types', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_2', 'chunkContent': \"Traversals all follow the same edges (low data cardinality)\\n\\nQueries consistently return the same node/object types\\n\\nAvoid Large Traversals\\n\\nOften, engineers new to graph databases misuse the traversal data access patterns in costly ways. It's not always clear to developers that are new to graph data engines why they need traversal data access patterns for their use case.\\n\\nA common pitfall is to query large traversals. Traversals that span super-nodes, or highly interconnected vertices, result in subgraphs that are costly in resource utilization and often lack business value.\\n\\nIt's important to remember that the Apache Tinkerpop framework offers both analytic and transactional processing. The underlying Gremlin API is transactional. Similarly, SQL Server is a transactional engine and the SQL Graph plugin is limited to the same processing. Analytic queries need to go through lots of data to find important information, and they usually return a smaller set of useful results.\\n\\nSupply Chain example\\n\\nA concrete example in the Supply Chain use case for the Retail Industry is to identify supplier risk. Retailers are facing supply chain disruptions and want to find and replace suppliers that have risks in their supply chains. If a retailer's analytics team has obtained and analyzed data on all industry supplier relationships, how can they solve this issue?\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_3', 'chunkContent': 'Suppose the retailer chooses a transactional graph and assigns risk calculations to each supplier in the industry. The direct suppliers of the retailer could be aggregated, based on their 2nd and 3rd tier relationships, by risk. While appearing suitable for a graph, this method may result in large and costly traversals aggregating risk beyond two or three hops.\\n\\nThe retailer should choose a non-graph data store for their aggregations. If they are using the Cosmos DB Gremlin API, they can switch to the columnar Cassandra API or the SQL API with Azure Synapse Link to handle their risk aggregations. After identifying risky suppliers, the analytics team can load the data into a transactional graph engine to search for substitute suppliers by querying edge or node properties and ordering them by risk.\\n\\nThe critical takeaway from this use case is to be sure to know which data engines tools are suitable for which use cases. A transactional system should not be used for an analytic use case, and vice versa.\\n\\nAvoid Relational \"Lift and Shift\"\\n\\nOne of the best ways to extract value from a graph is to use data modeling to your advantage. As most implementations of graph databases are schemaless, there is minimal risk to trying experimental data modeling designs.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_4', 'chunkContent': 'A common anti-pattern is to project the exact data model of a relational database into a graph. Approaching the data thinking about tables as entities and joins as relationships potentially misses the value of attributes describing the relationships.\\n\\nUnderstanding the core access patterns and how the business will be querying the relationships will determine the best way to model them in a graph database.\\n\\nCreatively using the following two patterns will help identify how to port tables and foreign keys into nodes and edges more valuably:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_5', 'chunkContent': 'Creatively using the following two patterns will help identify how to port tables and foreign keys into nodes and edges more valuably:\\n\\nUse joins to aggregate data into node properties\\nConsider the following example with:\\n``textusers`\\n| column    | type         |\\n|-----------|--------------|\\n| user_id   | uuid         |\\n| user_name | varchar(255) |\\nuser_groups\\n| column          | type         | foreign key   |\\n|-----------------|--------------|---------------|\\n| user_group_id   | uuid         |               |\\n| user_id         | uuid         | users.user_id |\\n| user_group_name | varchar(255) |               |', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_6', 'chunkContent': '```\\nOne common way to organize the data in a graph data model is by creating user nodes and user_group nodes. This approach might be useful for queries that ask to find connections between users in different groups.\\nHowever, if the only traversal pattern is to understand which groups users are in, then adding a user_groups property on the user nodes will suffice. A set of group names or IDs will be easy to retrieve for any given user node. Likely, analysis on this data will make use of both types of data access patterns.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_7', 'chunkContent': 'Explode columns-related tables into more atomic relationships\\nConsider the following example with vague and abstract tables:\\n``textentities`\\n| column      | type         |\\n|-------------|--------------|\\n| entity_id   | uuid         |\\n| entity_name | varchar(255) |\\n| entity_type | varchar(255) |\\nentity_relationships\\n| column            | type         | foreign key        |\\n|-------------------|--------------|--------------------|\\n| relationship_id   | uuid         |\\n| parent_entity_id  | uuid         | entities.entity_id |\\n| child_entity_id   | uuid         | entities.entity_id |\\n| relationship_type | varchar(255) |', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_8', 'chunkContent': '```\\nStoring data in a large columnar data store might be helpful, but porting\\ninto a graph would result in a rather trivial graph with one node and one edge type.\\nUsing entity_type and relationship_type to group rows into different nodes and\\nedges can create a much more useful and insightful data model to traverse. You\\ncan think about this projection as \"exploding\" the abstract relational model into\\nmore concrete objects.\\n\\nRelational data models are typically not as vague or abstract. In addition, the data access patterns that are useful in a graph database may not have been considered when designing the relational schema.\\n\\nDesign Hybrid Data Architecture\\n\\nThe role of a systems architect is to design a collection of technologies around a business problem. Not all technologies are created equal, and some, like relational databases have a long and stable history. Architects of green field projects in particular might see newer technologies like graph databases and think to use shiny bells and whistles for their innovative technology stack.\\n\\nInnovation doesn\\'t necessarily require the latest and greatest technology. Often, using tried and true technologies allows for a faster time to value by building on a stable foundational layer.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_9', 'chunkContent': 'Because graphs can be unfamiliar and new to many developers, it\\'s almost always better to balance the use of a graph with a more familiar relational database. A best practice is to target ETL of source data into a relational Data Lake first, and then project into a graph database for purpose-specific traversals.\\n\\nManaging eventual consistency between data engines is necessary in a hybrid architecture. This approach also helps identify data quality issues that could have gone unnoticed with only one engine. Having good data quality and observability is important for a hybrid data architecture.\\n\\nCosmos DB Specific Best Practices\\n\\nCosmos DB Ingestion\\n\\nA simple method to add data to Cosmos DB is by using gremlin queries to add edges (g.AddE()) and vertices (g.AddV()) directly to the data store. Adding data this way will result in decreased performance due to validation checks. These checks confirm the existence of both the \"in vertex\" and the \"out vertex\" when creating edges.\\n\\nBecause of the multi-model nature of the Cosmos DB, this server-based validation can be bypassed by using the bulk executor in either the .NET or Java SDKs.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_10', 'chunkContent': \"The developer creates the entities and relationships with standard development objects (POCO/POJO). The GraphBulkExecutor then translates the objects into the correctly structured Cosmos DB documents and inserts them directly into the document database using the Cosmos DB SQL API. Because the documents and relationships are well formed in the code, there is no need for background validation of the related vertices. This approach can yield a performance improvement of up to 100X on loading large datasets.\\n\\nIn order to support migration paths from the various sources to the different Azure Cosmos DB APIs, there are multiple solutions that provide specialized handling for each migration path.\\n\\nCosmos DB Partitioning\\n\\nOne common anti-pattern for Cosmos DB is misalignment between partitioning and traversal data access patterns. Designing a large graph data model without effective partitioning can lead to expensive and long-running queries. This guide\\nprovides an in-depth explanation on how to partition a graph in Cosmos DB. The best practices listed are:\\n\\nAlways specify the partition key value when querying a vertex\\n\\nUse the outgoing direction when querying edges whenever it's possible\\n\\nChoose a partition key that will evenly distribute data across partitions\\n\\nOptimize queries to obtain data within the boundaries of a partition\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk66_11', 'chunkContent': 'Choose a partition key that will evenly distribute data across partitions\\n\\nOptimize queries to obtain data within the boundaries of a partition\\n\\nApache Tinkerpop Gremlin query steps allow for traversals across in, out, and both directions. In Cosmos DB, outgoing edges are stored with their source\\nvertex. The diagram below, from the Cosmos DB graph partitioning documentation, uses color to show this optimization.\\n\\nOptimize Cosmos DB with Bi-Directional Edges\\n\\nIn Cosmos DB, outgoing vertex edges are stored in the same partition. A query for connected vertices that go out from a vertex only needs to search one partition. However, in a query to find vertices with incoming edges to a vertex, it has to search all vertices in the database that have an outgoing vertex to the base vertex.\\n\\nStorage is cheap, and the compute reduction from bidirectional edges should outweigh any cost in doubling the number of edges stored. Data modeling a graph should consider traversal access patterns when deciding which edges to doubly store.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\graph-database-best-practices.md'}, {'chunkId': 'chunk67_0', 'chunkContent': \"rings:\\n  - public\\n\\nGuidance\\n\\nThis section provides general guidance around various themes such as technology selection, performance benchmarking, technical workarounds etc. These themes are not tied to a capability but are contextual and technology specific.\\n\\nThe content in this section has the following characteristics:\\n\\nAs the name suggests, it's a general guidance based on experience with multiple engagements and implementations.\\n\\nSample code may not necessarily be provided with the content in this section.\\n\\nThis guidance is generic in nature and might not be applicable or work for your particular use-case. Do thorough investigation and testing before applying the guidance in a production environment.\\n\\nTechnology Choices\\n\\nAzure Databricks vs Azure Synapse Analytics\\n\\nDataOps: Azure Databricks vs Azure Synapse Analytics\\n\\nBest Practices\\n\\nAzure Data Explorer\\n\\nDataOps: ADX Use Cases and Best Practices\\n\\nAzure Synapse Analytics\\n\\nDataOps: Dedicated SQL Pool Best Practices\\n\\nAzure Databricks\\n\\nDataOps: Azure Databricks Storage Best Practices\\n\\nGraph Databases\\n\\nDataOps: Graph Database Best Practices\\n\\nSAP S4/Hana\\n\\n[DataOps: Populating SAP S/4Hana ACDOCA Table](./populating-sap-s4hana-acdoca-table.md\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\index.md'}, {'chunkId': 'chunk67_1', 'chunkContent': \"{% if extra.ring == 'internal' %}\\n\\nGetting Started Guides\\n\\nGetting Started with Microsoft Fabric\\n\\nDataOps: Microsoft Fabric\\n{% endif %}\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\index.md'}, {'chunkId': 'chunk68_0', 'chunkContent': 'rings:\\n  - public\\n\\nPopulating SAP S/4HANA ACDOCA Table\\n\\nWhile developing or testing applications based on SAP S/4HANA systems, it is necessary to have a good volume of data for stress testing or performance testing. In SAP S/4HANA systems, you cannot just create new random records in the database tables. You must use SAP APIs, or any other tooling provided by SAP to create the records to ensure integrity of the data.\\n\\nIn the finance module, ACDOCA is the table that stores the data for all financial transactions. It\\'s expected to have billions of records in production, while a typical demo system has only 250k records. This guide offers a way to create more records using the Fiori app \"Upload General Journal Entry\" for stress testing and performance testing.\\n\\nThe Upload General Journal Entry App\\n\\nThe \"Upload General Journal Entry\" is a Fiori app that allows you to upload multiple general journal entries from a spreadsheet file. The idea here is to take a batch of General Ledger line items, validate them for consistency, and create Financial documents by posting the entries in various accounts which can be used for subsequent processing.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\populating-sap-s4hana-acdoca-table.md'}, {'chunkId': 'chunk68_1', 'chunkContent': 'A template of the spreadsheet can be downloaded from the Fiori app and filled in with the journal entry information. After the upload finished successfully, the journal entries can be directly posted to the ledgers or submitted for verification. We can also search for journal entries that have been uploaded within a specific time range. This app is useful for posting mass journal entries in SAP S/4HANA.\\n\\nBenefits\\n\\nFollowing are some of the key benefits.\\n\\nThe Journal Entry (JE) excel header and item fields are mapped to the backend JE upload program. They are configured to dynamically handle the underlying structure, which validates and posts (or parks) the JE records from the file.\\n\\nA Sample Excel file exists for those records that generate errors when the Park / Post button is triggered by the user. This Excel file contains two tabs: one tab contains the actual JE records with errors; the other tab shows the error messages that were raised and their corresponding JE header records, as each error message can be identified by the “Header row no” column. Typically, the standard Journal entry functionality is limited because it only permits the creation of a single financial document. For many organizations, it is necessary to upload multiple (or large numbers of) JE documents in to SAP Finance.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\populating-sap-s4hana-acdoca-table.md'}, {'chunkId': 'chunk68_2', 'chunkContent': 'The Fiori app supports both .csv and .xls or .xlsx formats, and user can validate the file before posting (or parking) the JE records into the SAP system. It also generates an email to the approver who is responsible for the records in the file.\\n\\nAnother great feature of this Fiori JE app is when (for any reason) the position of the fields in the file is changed. This would not matter to the accuracy of the file upload. Also, to add any new fields to the file, tis simply entails adding the field to the config mapping table, and the underlying code which is written dynamically. This is handled without changing a single line of code.\\n\\nUpload Limits\\n\\nA finance document in SAP is a record of a business transaction that affects the financial statements of an organization. It consists of a header and one or more line items. The header contains information such as document number, company code, fiscal year, document date, posting date, period, reference key, document type, currency, exchange rate, etc. The line items contain information such as G/L account, cost objects, document and local currency, debit and credit amounts, tax code, etc.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\populating-sap-s4hana-acdoca-table.md'}, {'chunkId': 'chunk68_3', 'chunkContent': 'The finance document has a limit of 999 line items. The upload app supports up to 999 finance documents for each execution, i.e. approximately one million line items and it is the number of the records created at table ACDOCA.\\n\\nData Generation Rules\\n\\nThe spreadsheet provided in this guide is just the blank template provided by SAP populated with random data.\\n\\nThe data itself does not matter for the performance testing. The line items values were randomly generated. It was used valid account numbers to create some variety of accounts and appear \"nicely\" in the balance report.\\n\\nThe spreadsheet contains following sheets:\\n\\nSheet name Description Original The original template sheet downloaded from the Fiori app 2-docs Random data for 2 FI documents with 999 lines each\\n\\nThe 2-docs is the sheet with the values to be uploaded to the Fiori app, and must be the first sheet on the left.\\n\\nUpload the Spreadsheet to the Fiori app\\n\\nYou can find more information on how to execute the Fiori app in the official SAP documentation.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\populating-sap-s4hana-acdoca-table.md'}, {'chunkId': 'chunk68_4', 'chunkContent': 'You can find more information on how to execute the Fiori app in the official SAP documentation.\\n\\nThe spreadsheet provided in this guide creates 2 documents with 999 lines each. The official documentation recommends to keep the line items under 16,000 lines for good system response. By experience, it is possible to upload up to 999 documents with 999 lines each. It will create 1 million records in the table ACDOCA per execution, however you should expect very long processing time (approximately 1 hour).\\n\\nLearn More\\n\\nSAP S/4HANA\\n\\nUniversal Journal - ACDOCA\\n\\nSAP Fiori Apps Reference Library\\n\\nUpload General Journal Entries help\\n\\nUpload General Journal Entry App', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\populating-sap-s4hana-acdoca-table.md'}, {'chunkId': 'chunk69_0', 'chunkContent': \"rings:\\n  - public\\n\\nDedicated SQL Pool Best Practices\\n\\nUse this page if you are considering dedicated SQL pools (formerly SQL DW) and want to know the design considerations to keep in mind.\\n\\nIntroduction\\n\\nAzure Synapse offers multiple compute options (as explained in this quickstart), one of them is Dedicated SQL Pools.\\n\\nDedicated SQL Pools have an Massively Parallel Processing (MPP) architecture, which is different from a traditional Symmetric Multiprocessing (SMP) database. As a result, some of the concepts that work for traditional SMP databases like Azure SQL don't apply or can't be translated to an MPP architecture.\\n\\nIn an SMP database, processors share resources like storage and memory. In contrast MPP is often called a “shared nothing” architecture, as processors don't share their resources.\\nSMP systems provide increased throughput and reliability: if one processor fails, another can fill its place quickly. As a result, SMP systems can dynamically balance a workload to serve more users faster. Due to this architecture, they're ideal for situations where there is high concurrency of simple queries. Instead, MPP systems are used for large data volumes with analytical use cases as they allow scaling out computational processing across multiple nodes.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\synapse-dedicated-pools.md'}, {'chunkId': 'chunk69_1', 'chunkContent': \"In Synapse Dedicated SQL Pools compute is decoupled from storage (storage is sometimes also referred to as data distributions). The number of compute nodes depends on the Service Level Objective (SLO), or the number of Data Warehouse Units (DWU), while the data distributions are independent of the SLO and are fixed to 60. As a result, the number of distributions per nodes varies with the SLO. For understanding how the ratio changes with different service levels, see Synapse Service Levels.\\n\\nDedicated SQL Pools don't have auto scaling capabilities, nor auto pause. But these operations can be scheduled/automated to some extent. For examples see how to pause and resume dedicated SQL pools with Synapse Pipelines and Quickstart: Scale compute in dedicated SQL pool.\\n\\nFurther reading\\n\\nDetails on the Synapse SQL Architecture\\n\\nOverview of best practices with Dedicated SQL Pools\\n\\nData Ingestion\\n\\nWhen data is ingested into Dedicated SQL Pools, the data is sharded into 60 distributions to optimize the performance of the system. The sharding pattern to distribute data can be selected when you define the table. As a rule of thumb, the best distribution strategy should minimize the amount of data movement to fulfill query demands.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\synapse-dedicated-pools.md'}, {'chunkId': 'chunk69_2', 'chunkContent': \"When loading large amounts of data, it is a good practice to group INSERT statements into batches: by developing one process that writes to a file, and then another process to periodically load from this file: Group INSERT statements into batches.\\n\\nPolyBase is generally the best choice when you are loading or exporting large volumes of data, or you need faster performance. You can use PolyBase in CTAS statements or directly from Azure Data Factory pipelines.\\n\\nIf you are loading data to a hash-distributed table, your ingested data shouldn't be ordered by the hash-distribution key (see Table Design section).\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\synapse-dedicated-pools.md'}, {'chunkId': 'chunk69_3', 'chunkContent': \"Data loading might affect reads or conversely reads might affect data loading: loading data to a table requires an exclusive lock to the table. A loading operation will have to wait until all locks, including read locks, on the table are released. At the same time, when a loading operation is executed, all other operations on that table, including reads, will have to wait. For this reason, partition switching is the recommended approach. The main reason for such recommendation being that partition switching is just a metadata operation and as such tends to be executed quickly. Using partition switching reduces the duration of table lock and as a result reduces impact on reads. In summary, the practice recommends loading data to a staging table that has identical table definition and boundaries to the production table. After the load is completed on the staging table, the partition can be switched to the production table. For an example on partition switching, see the GitHub sample.\\n\\nFurther reading\\n\\nData loading best practices for dedicated SQL pools\\n\\nTable Design\\n\\nIn Dedicated SQL Pools, regular tables (for example, tables that aren't temporary) store data in Azure Storage and by default are distributed.\\nDedicated SQL Pools support 3 ways of distributing tables: Round-Robin, Hash, Replicated.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\synapse-dedicated-pools.md'}, {'chunkId': 'chunk69_4', 'chunkContent': \"Replicated tables aren't distributed across multiple nodes but are instead, as the name implies, replicated across all nodes. With a replicated table all nodes will have a copy of the full table available locally. As a result, replicated tables are recommended when join conditions would otherwise require data movement. Replicated tables are also a good choice when the table size on disk is less than 2 GB. For more information, see Design guidance for replicated tables.\\n\\nDistributed tables can be either round-robin or hash distributed. With hash distribution, you can guarantee that rows with equal distribution key values will be on the same node. For round-robin tables, the distribution of rows is random. For more details on how to choose the best distribution: Distributed tables design guidance.\\n\\nFurther reading\\n\\nWhen to hash distribute tables\\n\\nTable partitions\\n\\nDesigning tables with Synapse SQL\\n\\nFor assessing whether tables are storing data in a healthy way, see the GitHub sample\\n\\nQuery Performance\\n\\nYou can improve query performance by choosing the right distribution method (as explained also above) and sometimes by partitioning tables.\\n\\nClustered Columnstore Index (CCI), partitioning the table will result into partitioning the CCI too. For optimal performance, CCI  should always have at least 1 million rows per partition and distribution. For more information, see\\n\\nPartition Sizing.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\synapse-dedicated-pools.md'}, {'chunkId': 'chunk69_5', 'chunkContent': \"Partition Sizing.\\n\\nWhen analyzing query performance, you might find that you need to change table design (like distribution for example).  For more information, see query performance troubleshooting/tuning techniques: How to minimize data movements (Compatible and Incompatible Joins).\\n\\nColumnstore Indexes and table Statistics are essential to query performance. For detailed information, see Best Practices for Maintaining Statistics and Statistics in Synapse SQL.\\n\\nFor query performance, it's also important to remember that memory and resource limits are determined by the chosen SLO (number of DWU): when you are short of other options, you might need to increase the SLO to achieve a better performance. SLO also impacts the maximum number of concurrent queries. As a reference, there is a maximum of 128 concurrent queries that can run at any time with the largest SLO (DW30000c). For more information, see Memory and Concurrency Limits.\\n\\nDedicated SQL Pools have a concept of resource classes to determine the performance that users/processes can use for queries. Concurrency limits combined with resource classes in a given SLO have an impact on the maximum performance that a resource class can use:  Concurrency maximums for resource classes.\\n\\nFurther reading\\n\\nOverview of table partitioning in SQL Dedicated Pools\\n\\nFor detailed information on resource classes, see Resource classes for workload management\\n\\nGuidance on query performance tuning\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\synapse-dedicated-pools.md'}, {'chunkId': 'chunk69_6', 'chunkContent': 'For detailed information on resource classes, see Resource classes for workload management\\n\\nGuidance on query performance tuning\\n\\nAssessing performance of tables\\n\\nMonitoring and Observability\\n\\nMonitoring and observability are essential for ensuring that your workloads and queries can run and are in healthy state. The Azure Synapse Toolbox is a collection of useful tools and scripts to help you manage and monitor the health of your Azure Synapse Analytics workspaces.\\n\\nEnabling alerts enables one to detect workload issues earlier, providing an opportunity to take action sooner and minimize the impact on end users. For more information, see the community article.\\n\\nThere are many options available for monitoring your Dedicated SQL Pool. For a walkthrough of implementing a monitoring solution for Azure Synapse Analytics, see the community article.\\n\\nSecurity\\n\\nWhen it comes to security, Dedicated SQL Pools can be secured like any other Azure SQL database. For an extensive overview, see Secure a dedicated SQL pool.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\guidance\\\\synapse-dedicated-pools.md'}, {'chunkId': 'chunk70_0', 'chunkContent': \"Background\\n\\nData Mesh is a socio-technical architectural paradigm in which data is treated as a product and is owned by domain experts. These challenges will help you understand the key concepts of Data Mesh and how to apply them to a real-world scenario.\\n\\nWhile we haven't offered specific guidance on the tools and technologies to use, we highly recommend considering Microsoft Fabric as your implementation's underlying platform. Microsoft Fabric is a comprehensive analytics solution for enterprises, encompassing data movement, data science, real-time analytics, and business intelligence.\\n\\nWith the ability to create shortcuts, define domains, and use workspaces as data product boundaries, Microsoft Fabric is a perfect fit for Data Mesh. Built on a SaaS foundation, Fabric allows customers to focus on solving business problems, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience.\\n\\nWith that, let's get started!\\n\\nIntroducing Southridge Video\\n\\nSouthridge Video started off as a movie streaming and DVD company. To create a better touch experience, they acquired a brick and mortar media store called 'FourthCoffee'. To help with decision making, the company developed a modern data warehouse where they ingested data from different sources, developed a dimensional data model and published dashboards to share insights.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\00-background.md'}, {'chunkId': 'chunk70_1', 'chunkContent': 'To develop the central data warehouse, Southridge Video combines data from a variety of sources such as Azure SQL Databases, Cosmos DB and Azure Storage Account. These sources contribute various datasets, encompassing DVD rentals, movie streaming, movie catalog, actors, and customer information.\\n\\nAll data from the source systems are ingested into the Enterprise Data Lake by the central data processing pipeline infrastructure. Data is first loaded into a Raw layer where it is grouped by source systems. This raw data goes through data standardization and data quality checks and moves to Conformed layer. Finally, the data is transformed into a star schema for business to consume. The entire data infrastructure is managed by a single data engineering team at Southridge.\\n\\nThe following architecture diagram shows the high-level components and data flow of the existing data warehouse:\\n\\nChallenges and opportunities\\n\\nThe company has been thriving, experiencing significant growth in recent times with their streaming platform performing exceptionally well. To refocus efforts in movie streaming, they have expanded into movie production. However, fierce competition is putting pressure on both the movie production team and the sales team to act and adapt quickly. Their current centralized MDW system is struggling to meet the demands of their flourishing business. The centralized data engineering team faces challenges such as prolonged turnaround times to fulfill requests and difficulties in providing support and troubleshooting due to lack of domain expertise.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\00-background.md'}, {'chunkId': 'chunk70_2', 'chunkContent': 'Recognizing the challenges of a centralized system in a fast-moving world, Southridge Video has made the decision to redesign their centralized architecture and adopt a more decentralized domain-driven design. This new approach involves splitting the monolithic data warehouse into multiple data domains, with each domain having its own dedicated team responsible for managing the data assets specific to that domain. Additionally, prescribed governance measures will be put in place to ensure consistency and coherence across domains.\\n\\nDesired outcomes\\n\\nDuring this upskilling hack, the focus will be on redesigning the data platform while addressing a crucial business request from Southridge Video. This request entails the implementation of an advanced movie search functionality. Southridge Video aims to offer its customers a personalized search experience by exclusively presenting relevant movies for each search query. The new data architecture will be used to support the development of this functionality.\\n\\nReferences\\n\\nDataOps for the modern data warehouse\\n\\nMicrosoft Solutions Playbook: Modern Data Warehouse\\n\\nExternal Blog: Moving Beyond a Monolithic Data Lake\\n\\nMicrosoft Solutions Playbook: Data Mesh Architecture\\n\\nMicrosoft Learning Path: Get started with Microsoft Fabric', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\00-background.md'}, {'chunkId': 'chunk71_0', 'chunkContent': 'A Tale of Two Domains\\n\\nData Mesh context\\n\\nCreating a Data Mesh begins with the business examining its data estate and identifying the business domains. These business domains are then mapped to the data domains. Within each domain, there can be one or more data products.\\n\\nTypically, there is a one-to-one correspondence between business and data domains. However, in some cases, a business domain can be divided into multiple data domains, such as when data ownership dictates the split, although this is relatively uncommon.\\n\\nAfter identifying the data domains and data products, ownership of these domains and products should also be considered by the business. If data is to be treated as a product, a \"Product Owner\" needs to be assigned. Similarly, domain ownership should be defined, and other roles may need to be specified as part of the Data Mesh implementation.\\n\\nKeep in mind that this is a business exercise, not a technical one. At this stage, there\\'s no need to worry about the technology platform or its implementation. The primary focus should be on understanding the business domains, data domains, and data products, as well as their relationships from a business perspective.\\n\\nBackground story\\n\\nSouthridge Video has two major business domains:\\n\\nMovie Production: This division is responsible for managing the film production operations of the company, as well as curating and preserving the extensive library of movies that they have acquired over time.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\01-tale-of-two-domains.md'}, {'chunkId': 'chunk71_1', 'chunkContent': \"Sales::This division is responsible for managing all rental and streaming records (watched movies history), as well as managing customer data and information.\\n\\nOver time the centralized data warehouse approach proved to be inefficient in responding quickly to the needs of these two major business functions. Few key limitations that are hampering the business today are:\\n\\nLack of domain expertise: The centralized data team doesn't have the expertise and context of domain experts who have deep knowledge about the data and its meaning within their respective domains. For example, the centralized data team does not deeply understand nuances of how different revenue streams of rentals and streaming sales should be handled in different customers geographies resulting in long lead time to build relevant and accurate sales reports.\\n\\nDependency on centralized teams: There is an increasing heavy reliance on the central data team for data integration, modeling, and analytics. This dependency can slow down decision-making and hinder the autonomy and agility of domain experts. For example, the Filmmaking team is experiencing long lead times in onboarding new Movies to the Movie Catalog.\\n\\nDue to these challenges, Southridge Video is looking to adopt a decentralized data architecture to empower the domain teams to rapidly adapt to business requirements. However, as a consequence of decentralization, the number of places and teams who provide data increases. This results in data silos. To address this problem, Southridge decided to adopt the concept of data product.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\01-tale-of-two-domains.md'}, {'chunkId': 'chunk71_2', 'chunkContent': \"Southridge also planned to establish a centralized governance solution to make sure data products from different domains are developed and shared consistently. This centralized governance solution also plays a crucial role in maintaining the security of sensitive information. As the databases contain customer data including phone numbers and physical addresses, Southridge Video is particularly concerned about safeguarding their customers' information. Their priority is to ensure that such data is always protected and accessible only to authorized personnel with legitimate business justifications.\\n\\nTo reduce disruption, Southridge Video leadership has decided to start a greenfield data project. The new data platform will pull data directly from the source systems, avoiding the legacy data warehouse.\\n\\nTechnical details\\n\\nAs already mentioned, there are two major business domains: Movie Production and Sales. This information is critical while defining the domains ownership of data while designing the decentralized architecture.\\n\\nThe goal of decentralizing the data architecture is to delegate responsibility to individuals who are closest to the data, promoting ongoing adaptability and scalability. It's a common pattern to follow organizational units to define data domains. This article covers the details of domain ownership with example - Data Mesh Principles and Logical Architecture > Domain Ownership.\\n\\nFor Southridge the major business domains and their data ownership is clearly mentioned. Try to define the data domains from it.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\01-tale-of-two-domains.md'}, {'chunkId': 'chunk71_3', 'chunkContent': \"For Southridge the major business domains and their data ownership is clearly mentioned. Try to define the data domains from it.\\n\\nEach data domain will have one or more data products. A data product is designed to provide a readily accessible and curated collection of data that can be effortlessly utilized by teams or individuals throughout an organization. A data product generally provides a 360-degree view of a specific entity, like Customers or Movies. The data products for each data domain can be derived from the data assets mentioned above.\\n\\nData product encapsulates three structural components required for its function, providing access to the domain's analytical data as a product. The three components are\\n\\nData & Metadata\\n\\nCode\\n\\nInfrastructure\\n\\nThis article covers the details of data product with example - Data Mesh Principles and Logical Architecture > Data as a product.\\n\\nThere are two major data sources: Southridge Video and FourthCoffee. Here is a quick summary of different types of datasets in each of these data sources.\\n\\nIn real life, datasets like these (sales, customer data, etc.) can be quite large. However, for this hackathon, the dataset is significantly smaller and contains only a few thousand records.\\n\\nIt's highly recommended to read the first two articles from References section to complete this challenge.\\n\\nSuccess criteria\\n\\nYou understand and can discuss the high-level challenges of a centralized data architecture.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\01-tale-of-two-domains.md'}, {'chunkId': 'chunk71_4', 'chunkContent': 'Success criteria\\n\\nYou understand and can discuss the high-level challenges of a centralized data architecture.\\n\\nYou understand and can discuss the concept of business domains, data domains, and data products and their relationship.\\n\\nYou understand and can discuss various ownership roles in a Data Mesh architecture.\\n\\nYou have defined the data domains and data products based on the datasets mentioned above. Please remember this is a business task and there is no need to worry about the technology platform or low-level details at this stage.\\n\\nKnowledge check\\n\\nQuestion 1\\n\\nWhich of the following options is an example of a decentralized domain-driven data architecture?\\n\\na. Data Hub\\n\\nb. Data Mesh\\n\\nc. Data Fabric\\n\\nd. Medallion\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 2\\n\\nWhich of the following options are correct?\\n\\na. A data domain can have one or more data products.\\n\\nb. A data product can belong to multiple data domains.\\n\\nc. A data product can reference data from a data product from another domain.\\n\\nd. A data product can be owned by multiple domain teams.\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 3\\n\\nWhich of the following option is NOT a component of a data product?\\n\\na. Data & Metadata\\n\\nb. Code\\n\\nc. Data Governance', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\01-tale-of-two-domains.md'}, {'chunkId': 'chunk71_5', 'chunkContent': 'a. Data & Metadata\\n\\nb. Code\\n\\nc. Data Governance\\n\\nd. Infrastructure\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nReferences\\n\\nExternal Blog: Moving Beyond a Monolithic Data Lake\\n\\nExternal Blog: Data Mesh Principles\\n\\nMicrosoft Solutions Playbook: Data Mesh Architecture\\n\\nMicrosoft Solutions Playbook: Storage on Microsoft Fabric\\n\\nCloud Adoption Framework - What is Data Mesh?\\n\\nMicrosoft Learn: Fabric Domains', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\01-tale-of-two-domains.md'}, {'chunkId': 'chunk72_0', 'chunkContent': \"Brave New World\\n\\nData Mesh context\\n\\nOnce the data domains, data products, and their ownerships have been identified, the next step is to implement the data products. This involves the business handing over the requirements to the data engineers and architects.\\n\\nThe data engineers and architects are then tasked with comprehending the business requirements and the available data to design and construct the data products. They commence by connecting to various source systems and examining the various datasets available.\\n\\nThe data products are meant to be self-contained and autonomous. This means that the data products should be able to ingest, transform, and serve data without affecting other data products. This is a key aspect of the Data Mesh architecture. For a greenfield project, it's a good idea to start by focussing on a single data product and build it end-to-end. This will help in understanding the challenges and also the best practices. Once the first data product is built, the same approach can be applied to other data products.\\n\\nFinally, the data products should be able to serve data to other data products and also consume data from other data products. This is achieved by defining the consumption interface for the data products. The consumption interface is the mechanism by which the data products can be consumed by other data products. The consumption interface can be in the form of a dataset, SQL Table, JSON file, REST API and so on.\\n\\nBackground story\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\02-brave-new-world.md'}, {'chunkId': 'chunk72_1', 'chunkContent': \"Background story\\n\\nSouthridge leadership team is thrilled about the design of the new decentralized data architecture and its potential impact. Based on the available datasets and business requirements, the team has proposed the following data products:\\n\\nDomain Data Products Movie Production Movie Catalog (includes Actors) Sales Customers Sales\\n\\nThough it's possible to come up with different data product breakdown, for this hackathon we shall stick to this proposal.\\n\\nThey have made an urgent request to determine their current movie portfolio to refine their strategy on future film production. To respond to this request, Southridge Movie Production team needs to consolidate Movie Catalog data which includes the movies from Southridge and FourthCoffee.\\n\\nThe leadership team is also keen to test the capabilities of the new architecture by providing an answer to this question by developing a data product. A traditional workflow includes loading the data from operational data sources, passing it through transformation layers and applying data quality verification. A subset of the data from this process is published through a consumption interface for other domains to consume.\\n\\nImagine you are the Southridge Movie Production Team. In this challenge, you will set up the data products in the Movie Production domain and publish the data for external-to-the-domain consumption.\\n\\nTechnical details\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\02-brave-new-world.md'}, {'chunkId': 'chunk72_2', 'chunkContent': \"Technical details\\n\\nTo support data as a product that domains can autonomously serve or consume, Data Mesh introduces the concept of data product. Data product encapsulates three structural components required for its function, providing access to the domain's analytical data as a product. The three components are:\\n\\nData & Metadata: the data and associated metadata to make it usable for consumers\\n\\nCode: code for data pipeline, code for serving data and code for compliance\\n\\nInfrastructure: infrastructure for running data product's code, as well as storage\\n\\nIn the previous challenge, the team has defined the domain breakdown and partially the data products. In this challenge, the data products for the Movie Production domain need to be expanded to cover all three components of a data product. It's important to keep in mind that a data product is an independent entity and any sharing of an artefact with another data product needs to be carefully planned.\\n\\nThe consumer of a data product shouldn't be aware of all the components of a data product. The data product owner should expose only a subset of the data as a consumption interface for external-to-domain consumers.\\n\\nSouthridge and FourthCoffee both have their own catalog of movies and actors/actresses. A movie mapping table has been created for FourthCoffee to join movies which exist in both Southridge and FourthCoffee movie catalog.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\02-brave-new-world.md'}, {'chunkId': 'chunk72_3', 'chunkContent': 'Here is a quick summary of the different data sources for Southridge and FourthCoffee:\\n\\nSouthridge resources\\n\\nSouthridge Video uses an Azure SQL Database to store information about video streaming. The team will note that there is a Hidden schema in the databases. This data is for future use and is not meant to be looked into during this challenge. Credentials to access the Azure SQL Database are provided below. Alternatively, the team may set the Azure Active Directory Admin to one of the provided accounts - Configure and manage Azure AD authentication with Azure SQL.\\n\\nThe username for the Azure SQL databases is southridge. For password, refer to the login instructions provided separately.\\n\\nTheir movie catalog data with actors/actresses data is separately stored in an Azure Cosmos DB document collection. Access keys for Azure Cosmos DB are available from within the Azure portal - Secure access to data in Azure Cosmos DB.\\n\\nFourthCoffee resources\\n\\nFourthCoffee uses a storage account to store its data.\\n\\nCredentials to access the storage account are available from the Azure portal - Manage storage account access keys.\\n\\nPlease note that the dataset OnlineMovieMappings.csv is intended for joining FourthCoffee with Southridge movies using the OnlineMovieID column.\\n\\nSuccess criteria\\n\\nYou have completed the design of the data products in Movie Production domain.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\02-brave-new-world.md'}, {'chunkId': 'chunk72_4', 'chunkContent': \"Success criteria\\n\\nYou have completed the design of the data products in Movie Production domain.\\n\\nConnect to the different source systems and explore the source datasets using appropriate tooling.\\n\\nUnderstand the relationship between the different datasets related to the Movie Catalog data product.\\n\\nDefine & create logical container for each data product.\\n\\nYou have ingested 'movie' related data from source systems to the appropriate data product.\\n\\nFor Southridge, pull related data from Cosmos DB collection.\\n\\nFor FourthCoffee, pull related data from the storage account.\\n\\nWhile storing the data from different sources in the target storage, make sure data is grouped according to their source systems.\\n\\nYou have transformed and integrated data from source systems to build the consumption interface for Movie Catalog data product.\\n\\nMake sure the source system's datasets have been transformed to use consistent data types and formats. For example, if source systems use different data types or formats for a date, the final dataset stores all dates in a single, consistent data type and format.\\n\\nAvoid record duplication and create only a single record per movie in the final dataset i.e., same movie can exist in more than one source system. In such cases, consolidate the movie information into a single record with the following priority order:\\nSouthridge\\nFourthCoffee\\n\\nFor each movie record in the above step, create a unique identifier.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\02-brave-new-world.md'}, {'chunkId': 'chunk72_5', 'chunkContent': 'For each movie record in the above step, create a unique identifier.\\n\\nAdd the source system movie identifiers as additional columns. This would be required when you use this data product as input to other data products. In such cases, you would need these columns for movie lookup.\\n\\nMake sure the original extracted data is preserved, and the transformation jobs don\\'t change it.\\n\\nThough Actors data is part of the Movie Catalog data product, it\\'s optional to include it in this challenge.\\n\\nDefinitions and explanations\\n\\nConsumption interface\\n\\nThe term consumption interface refers to the mechanism of exposing a data product. In a Data Mesh architecture, one important aspect of Data Products is that they are easy to discover and consume. It means that they are cataloged and have a well-defined interface which exposes them to the consumers. This interface can be in the form of a dataset, SQL Table, JSON file, REST API and so on.\\n\\nKnowledge check\\n\\nQuestion 1\\n\\nWhy is autonomy an important characteristic of data products in Data Mesh?\\n\\na. To increase dependencies on other data products\\n\\nb. To simplify data operations\\n\\nc. To ensure they require constant maintenance\\n\\nd. To make them harder to access\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 2\\n\\nWhat is the purpose of a consumption interface for data products in Data Mesh?\\n\\na. To prevent data sharing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\02-brave-new-world.md'}, {'chunkId': 'chunk72_6', 'chunkContent': 'Question 2\\n\\nWhat is the purpose of a consumption interface for data products in Data Mesh?\\n\\na. To prevent data sharing\\n\\nb. To limit access to data\\n\\nc. To define how data products interact with each other\\n\\nd. To categorize data by color\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 3\\n\\nIn a greenfield project, what is the recommended initial focus regarding data products?\\n\\na. Develop as many data products as possible simultaneously\\n\\nb. Ignore data products and focus on hardware\\n\\nc. Concentrate on building a single data product end-to-end\\n\\nd. Outsource data product development to other companies\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nReferences\\n\\nMicrosoft Fabric: OneLake shortcuts\\n\\nMicrosoft Fabric: ADF Copy activity\\n\\nMicrosoft Fabric: Moving and transforming data with dataflows and data pipelines\\n\\nMicrosoft Fabric: Tutorial on prepare and transforming data in Lakehouse\\n\\nAzure Databricks: Medallion Lakehouse Architecture\\n\\nMicrosoft Fabric Blog: Lakehouse Sharing and Access Permission Management\\n\\nMicrosoft Solutions Playbook: Data Lake', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\02-brave-new-world.md'}, {'chunkId': 'chunk73_0', 'chunkContent': \"The Bucket List\\n\\nData Mesh context\\n\\nOnce the first data product has been built, the learnings can be applied to other data products. One of the great things about Data Mesh is that all data products can be built and maintained independently. This means that the team can work on multiple data products simultaneously. It's an excellent opportunity to divide the work among yourselves and gain hands-on experience.\\n\\nHowever, dependencies do exist. Data products can, and should, rely on other data products to prevent data duplication. At the same time, consuming data from certified data products means you can trust the data. This is where the concept of a consumption interface comes into play. It enables data products to be consumed by other data products without the need to delve into the internal details of the data product.\\n\\nWhen consuming data from a different data product, it's ideal to avoid data duplication, meaning it's better to consume data directly from a data product rather than copying it. This approach ensures that if the source data product is updated, the consuming data product will always have the latest information. It's also a great way to maintain data consistency across the organization. There is more to explore on this topic in the next challenge.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\03-bucket-list.md'}, {'chunkId': 'chunk73_1', 'chunkContent': \"Once the team has built a few data products, the importance and the need for a self-serve data platform become more evident. For example, there should be a consistent logging and monitoring mechanism for all data products. Additionally, security and compliance guardrails should be applied to all data products. It's the responsibility of the Data Platform team to enable these capabilities in a self-serve manner, allowing the data product teams to focus on building their data products.\\n\\nFor the sake of simplicity, these capabilities are not addressed in the challenges. However, it's highly recommended to explore these Data Mesh capabilities to build a robust data platform.\\n\\nBackground story\\n\\nLeadership is highly satisfied with the turnaround time for the movie catalog data product. It's now evident that domain experts can significantly contribute to quickly iterating on an idea.\\n\\nThis timing aligns well with the Sales team's plans to develop a Customer 360 dashboard to capture their customer base and the corresponding sales transactions from rentals and streaming services across Southridge Video and FourthCoffee. Inspired by the success of the Movie Production team, they are eager to adopt the same decentralized paradigm for customer and sales data in their projects.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\03-bucket-list.md'}, {'chunkId': 'chunk73_2', 'chunkContent': \"To prevent data duplication, the Sales team aims to consume data from the recently published movie catalog data product, ensuring that all movie data originates from this officially endorsed source. Thanks to the consumption interface provided by the Movie Production team, the Sales team can now self-serve data and work independently.\\n\\nImagine you are the Sales Team. In this challenge, you need to build the data products related to the Sales domain while consuming from a different domain’s data product.\\n\\nTechnical details\\n\\nThe data required to fulfill the business request encompasses the following categories:\\n\\nCustomers (with Address)\\n\\nMovie rentals\\n\\nMovie streaming\\n\\nIn Challenge 1, you'll find a comprehensive summary of all available datasets from the two business units, Southridge and Fourthcoffee. Feel free to revisit it if you have any questions about locating specific data.\\n\\nIn challenge 2, you'll find all the technical details related to Southridge and Fourthcoffee infrastructure resources.\\n\\nSuccess criteria\\n\\nYou have completed the design of the data products in Sales domain.\\n\\nConnect to the different source systems and explore the source datasets.\\n\\nUnderstand the relationship between the different datasets related to 'Customers' and 'Sales' data products.\\n\\nDefine and create logical container for each data product.\\n\\nYou have ingested 'Customers', 'Streaming Transaction' and 'Rental Transaction' related data.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\03-bucket-list.md'}, {'chunkId': 'chunk73_3', 'chunkContent': \"You have ingested 'Customers', 'Streaming Transaction' and 'Rental Transaction' related data.\\n\\nFor Southridge, pull related data from Azure SQL Database.\\n\\nFor FourthCoffee, pull related data from the storage account.\\n\\nWhile storing the data from different sources in the target storage, make sure data is grouped according to their source systems.\\n\\nYou have transformed and integrated data from source systems to build the consumption interfaces for 'Customers' and 'Sales' data products.\\n\\nMake sure the source system's datasets have been transformed to use consistent data types and formats. For example, if source systems use different data types or formats for a date, the final dataset stores all dates in a single, consistent data type and format.\\n\\nFor each customer record in the above step, create a unique identifier.\\n\\nAdd the source system identifiers as additional columns.\\n\\nMake sure the original extracted data is preserved, and the transformation jobs don't change it.\\n\\nYou have developed a 'Customer 360' dashboard that highlights the following insights:\\n\\nTop performing movies by year and month.\\n\\nMost valuable customers by year and month.\\n\\nGeography (state) wise sales breakdown.\\n\\nKnowledge check\\n\\nQuestion 1\\n\\nHow do data products in a Data Mesh architecture typically consume data from other data products?\\n\\na. They don't consume data from other data products.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\03-bucket-list.md'}, {'chunkId': 'chunk73_4', 'chunkContent': 'a. They don\\'t consume data from other data products.\\n\\nb. Through a complex web of dependencies.\\n\\nc. Via well-defined consumption interfaces\\n\\nd. By bypassing other data products entirely.\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 2\\n\\nWhich functionality of Microsoft Fabric makes it quick and easy to consume data from other data products without any data duplication?\\n\\na. Storing data in Delta Parquet format\\n\\nb. Copy activity\\n\\nc. Fabric capacity\\n\\nd. OneLake shortcuts\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 3\\n\\nWho is responsible for providing the self-serve capabilities for the teams to build data products?\\n\\na. Data platform owners\\n\\nb. Data product owners\\n\\nc. Data product consumers\\n\\nd. Data engineers and architects\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nReferences\\n\\nMicrosoft Fabric: OneLake shortcuts\\n\\nMicrosoft Fabric: ADF Copy activity\\n\\nMicrosoft Fabric: Moving and transforming data with dataflows and data pipelines\\n\\nLakehouse Tutorial: Prepare and transform data in the lakehouse\\n\\nLakehouse Tutorial: Building reports in Microsoft Fabric', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\03-bucket-list.md'}, {'chunkId': 'chunk74_0', 'chunkContent': \"The Rise of Governance\\n\\nData Mesh context\\n\\nOne of the design principles for Data Mesh is to reuse the existing data products. Every new data product needs one or more data sources. These data sources can be other data products or external data sources. Before a new data product is created, the following questions should be asked:\\n\\nIs there a similar data product already that exists? The last thing the team wants is to have multiple data products with the same data. If the answer is yes, what are the gaps, and can these gaps be addressed by extending the existing data product? For instance, an existing data product might contain most of the required data, but additional columns are needed for your use case. Alternatively, you may prefer to present the data in JSON format rather than a tabular one. In such cases, it's better to extend the existing data product rather than creating a new one. This decision is a matter for business conversation among the product owners.\\n\\nIs there an existing data product that can be used as a data source? If yes, how do we ensure that it's trustworthy and well-maintained?\\n\\nHow can we ensure that changes to the data product you plan to reuse don't break your data product?\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\04-governance.md'}, {'chunkId': 'chunk74_1', 'chunkContent': \"How can we ensure that changes to the data product you plan to reuse don't break your data product?\\n\\nThis is where the concept of Data Product Catalog comes in. A data product catalog is a registry of all the data products in the organization. It contains the information about the data product, such as the data product's consumption interface, the data product's SLA, and the data product's owner. In short, data product catalog makes a data product discoverable, trustable, and reusable.\\n\\nPart of the governance story also includes Data Lineage. Within a data product, there should be end-to-end visibility into how data is ingested, transformed, and published. Data lineage represents the journey of data from source to destination, aiding in issue resolution, impact analysis, and enabling business leaders to understand PII data flow.\\n\\nAlso, having a Data Access Control mechanism in place is important. It helps to ensure that only the right people have access to the right data at the right time. It also helps to ensure that the data is not misused or abused.\\n\\nBackground story\\n\\nSouthridge is progressing very well on their journey towards decentralized data architecture with two domains and several data products already in place. But this new architecture is also opening up new challenges.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\04-governance.md'}, {'chunkId': 'chunk74_2', 'chunkContent': \"The two domains have been built independently with a collection of data products with independent lifecycle by two different teams. In that process, they have learned few important lessons such as:\\n\\nThe 'sales' product is using 'movies' product as input dataset. To avoid any impact, the product owner(s) of 'sales' needs to ensure that the schema of 'movies' dataset doesn't get changed without their knowledge.\\n\\nTeams can build multiple data products but it's hard for other people to know if these products are properly managed and have the right governance controls in place i.e., it's hard for the consumers to know the legitimacy of the data products that they want to consume.\\n\\nSouthridge leadership team believes there should be a set of rules which are applied to all data products and their interfaces - to ensure a healthy and interoperable ecosystem.\\xa0They are thinking of creating a centralized governance team to make sure the different domains develop data products consistently.\\n\\nImagine you are the central governance team, who sets controls that help enable data product teams within the business to comply with metadata documentation, data cataloging, data classification, and data quality monitoring.\\n\\nTechnical details\\n\\nCentralized governance in a decentralized data architecture is a broad topic which encompasses multiple components. In this challenge, we shall limit our focus to only a few components, though there are other crucial aspects of governance.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\04-governance.md'}, {'chunkId': 'chunk74_3', 'chunkContent': \"First, consumers of data products need an easy way to search and find available datasets. It's important to make sure the consumers can differentiate different types of datasets according to their accuracy and maturity level. Owner of the data products should endorse/ certify the datasets which are meant for external-to-domain consumption. In a decentralized data architecture, the data catalog is the map for the consumers to explore and find the required datasets.\\n\\nAnother important feature for consumers is the lineage of the data. Lineage captures the journey of the source datasets through multiple transformation steps which eventually creates the curated dataset for consumption. Lineage helps consumers to understand data and troubleshoot various issues with data. Another reason for leadership team to be interested is, they want to know how different PII data (customer name, address, phone number etc.) are moving through the data pipelines.\\n\\nThe owners of the data domains need to be able to control access to the data at a granular level. The developers of the data product should be able to create/update the datasets, whereas others may only need read access to the data. Defining and assigning the different roles is a key aspect of governance.\\n\\nSuccess criteria\\n\\nYou have selected and implemented a cataloging solution that captures:\\n\\nDataset schemas\\n\\nRelated metadata (e.g., data owner, last refresh, etc.)\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\04-governance.md'}, {'chunkId': 'chunk74_4', 'chunkContent': 'Dataset schemas\\n\\nRelated metadata (e.g., data owner, last refresh, etc.)\\n\\nDataset maturity levels (e.g., certified, endorsed, raw, etc.)\\n\\nYou have certified the relevant datasets for each data product to build trust among consumers.\\n\\nYou have selected and implemented a data lineage solution that:\\n\\nAutomatically captures lineage as datasets change over time\\n\\nCaptures column-level lineage\\n\\nCaptures related metadata\\n\\nYou have implemented a data access control mechanism that:\\n\\nAllows data product developers to create/update datasets\\n\\nAllows data product consumers to read datasets\\n\\nAllows data product owners to manage access to datasets\\n\\nKnowledge check\\n\\nQuestion 1\\n\\nWhat is the primary goal of data cataloging in a Data Mesh architecture?\\n\\na. To complicate data accessibility.\\n\\nb. To make data products completely independent.\\n\\nc. To centralize all data into a single repository.\\n\\nd. To enhance discoverability and usability of data assets across the organization.\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 2\\n\\nIn a Data Mesh context, what does data lineage help with regarding PII (Personally Identifiable Information) data?\\n\\na. It encourages the exposure of PII data.\\n\\nb. It hides the movement of PII data.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\04-governance.md'}, {'chunkId': 'chunk74_5', 'chunkContent': 'a. It encourages the exposure of PII data.\\n\\nb. It hides the movement of PII data.\\n\\nc. It ensures transparency and traceability of PII data flows.\\n\\nd. It increases the complexity of PII data handling.\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 3\\n\\nWhat are the key attributes of a data product that cater to the needs of data consumers in a Data Mesh architecture?\\n\\na. Scalable, Interoperable, Customizable\\n\\nb. Discoverable, Trustable, Consumable\\n\\nc. Secure, Responsive, Personalized\\n\\nd. Hidden, Limited, Inaccessible\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nReferences\\n\\nMicrosoft Fabric: Endorsements\\n\\nMicrosoft Fabric: Promote or certify items\\n\\nMicrosoft Fabric: Scanning Power BI from Microsoft Purview\\n\\nMicrosoft Purview: Metamodel\\n\\nMicrosoft Purview: Manage assets with metamodel\\n\\nMicrosoft Solutions Playbook: Data governance', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\04-governance.md'}, {'chunkId': 'chunk75_0', 'chunkContent': \"Search Party\\n\\nData Mesh context\\n\\nData Mesh is centered around instilling agility in both business and data teams. The goal is to empower business teams for quicker data-driven decision-making, while data teams should rapidly respond to business demands by furnishing the necessary data products.\\n\\nThis dynamic gives rise to the establishment of multiple domains and data products. However, the utility of data products lies in their ability to deliver business value. If a data product doesn't contribute to the business, its existence is questionable.\\n\\nThis raises interesting questions. How do we gauge the contribution of a data product to the business? How can we measure its success? The answers to these questions are not straightforward. In a Data Mesh context, organizations should evaluate a data product's alignment with business objectives, track user adoption, and assess its impact on decision-making to determine its value.\\n\\nOne simple metric to gauge a data product's success is to identify the number of other data products or applications consuming it. If multiple other data products or applications are using a data product, it's a strong indicator of its value to the business. These applications and services can vary, encompassing dashboards, reports, complex machine learning models, or microservices.\\n\\nBackground story\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\05-search-party.md'}, {'chunkId': 'chunk75_1', 'chunkContent': 'Background story\\n\\nSouthridge leadership is very impressed with the team\\'s work! Finally, all the data products with the required data are ready for consumers. Excitement is growing; the data scientists are very excited to start working on it.\\n\\nSouthridge\\'s streaming website presents a search interface on the homepage for the users to search for movies. The search functionality plays a significant role in generating traffic and driving movie views.\\n\\nThe existing search functionality on the website currently relies solely on lexical search, which often fails to provide accurate results. To address this limitation, there is a desire to introduce a sophisticated semantic search. This upgraded search mechanism will cater to complex scenarios, allowing users to search for terms such as \"airplane films\" and receive relevant suggestions like \\'Top Gun.\\'.\\n\\nMoreover, the search results will take into account additional factors such as the user\\'s location, and previously viewed films, ensuring a more personalized and tailored experience.\\n\\nSouthridge leadership team is also very eager to see if the modern LLM technologies can be used to help with this use case, rather trying to train their own model for this purpose.\\n\\nImagine you are the data science team, working on implementing the new version of the search functionality.\\n\\nTechnical details\\n\\nThis challenge can be divided into two main areas, Semantic search and Personalization.\\n\\nSemantic search', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\05-search-party.md'}, {'chunkId': 'chunk75_2', 'chunkContent': 'Technical details\\n\\nThis challenge can be divided into two main areas, Semantic search and Personalization.\\n\\nSemantic search\\n\\nThe semantic search can be implemented in multiple ways. But we highly recommend exploring Azure OpenAI service to complete this challenge.\\n\\nMovie title and other metadata as genre and synopsis can be used to implement the similarity search. OpenAi Similarity Embeddings models are very good at capturing semantic similarity between two or more pieces of text. We can generate the embeddings of the movie title and other related data and compare that against the embedding of the search query.\\n\\nPersonalization\\n\\nBeyond similarity search, the overall search experience can be improved by providing a personalized experience. From the rental and streaming data we can find out the previously watched movies for the customer. This information can be used to further refine the search results.\\n\\nWe also have customer addresses. This information can be used to retrieve what movies are popular in a particular geography and based on that we can refine the search results.\\n\\nSuccess criteria\\n\\nYou have extended the architecture presented in the Technical details section to include the following details:\\n\\nChoose an OpenAI model for generating embeddings.\\n\\nIdentify the attributes/columns from the Movie Catalog data product to use for generating embeddings.\\n\\nDescribe the logic for applying personalization to the results of the similarity search.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\05-search-party.md'}, {'chunkId': 'chunk75_3', 'chunkContent': \"Describe the logic for applying personalization to the results of the similarity search.\\n\\nMaintain a simple architecture without introducing unnecessary components. Consider using an in-memory approach for storing and searching embeddings.\\n\\nYou have implemented the similarity search.\\n\\nYou have implemented personalization on the search results by using at least one of the following strategies:\\n\\nCustomer's previously watched movies\\n\\nCustomer's geography\\n\\nDefinitions and explanations\\n\\nLexical search\\n\\nLexical search refers to a type of search method that focuses on the literal or exact matching of words or phrases within a given text or database. It involves searching for specific terms or expressions without considering their context or meaning. In a lexical search, the emphasis is on finding exact matches rather than interpreting the underlying semantics or relationships between words. This type of search is commonly used in information retrieval systems, text analysis, and linguistic research.\\n\\nSemantic search\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\05-search-party.md'}, {'chunkId': 'chunk75_4', 'chunkContent': 'Semantic search\\n\\nSemantic search is an advanced search technique that aims to understand the intent and meaning behind a user\\'s query rather than relying solely on literal keyword matching. It goes beyond the surface level of words and takes into consideration the context, relationships, and concepts associated with the search query. By utilizing natural language processing (NLP) and machine learning algorithms, semantic search attempts to comprehend the user\\'s query and provide more relevant and accurate search results. It focuses on understanding the semantics, or meaning, of the search query and the content being searched, enabling it to deliver more contextually appropriate and insightful results. Semantic search enhances the search experience by considering concepts, synonyms, related terms, and contextual information to provide a deeper understanding of user intent and deliver more precise and valuable search results.\\n\\nKnowledge check\\n\\nQuestion 1\\n\\nWhat role do embeddings play in generative AI models like GPT-3?\\n\\na. They determine the model\\'s hardware configuration.\\n\\nb. They define the physical characteristics of AI-generated content.\\n\\nc. They represent contextual information and knowledge about words and concepts.\\n\\nd. They control the model\\'s internet connection.\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 2\\n\\nWhat is the primary purpose of personalization in online services and recommendations?\\n\\na. To provide a one-size-fits-all experience for all users.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\05-search-party.md'}, {'chunkId': 'chunk75_5', 'chunkContent': 'a. To provide a one-size-fits-all experience for all users.\\n\\nb. To tailor content and recommendations to individual user preferences.\\n\\nc. To maximize advertising revenue.\\n\\nd. To limit user choices and diversity in content.\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 3\\n\\nWhich of the following models can be used to generate embeddings using Azure OpenAI Service?\\n\\na. text-embedding-ada-002\\n\\nb. gpt-4\\n\\nc. dalle2\\n\\nd. davinci-002\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nReferences\\n\\nOpenAI: Embeddings\\n\\nOpenAI Repo: Python notebook for semantic text search using embeddings\\n\\nMicrosoft Solutions Playbook: Azure OpenAI\\n\\nMicrosoft Solutions Playbook: Working with Large Language Models\\n\\nAzure AI Services: Tutorial on exploring embeddings and document search\\n\\nCognitive Search: Add vector fields to a search index\\n\\nCognitive Search: Create and use embeddings for search queries and documents\\n\\nCognitive Search: Semantic search in Azure Cognitive Search\\n\\nCognitive Search: Vector search within Azure Cognitive Search', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\05-search-party.md'}, {'chunkId': 'chunk76_0', 'chunkContent': \"The Quality Awakens\\n\\nData Mesh context\\n\\nHaving data that can be trusted in the business is paramount. This means that the data needs to be ready for the business to use when making decisions. Data quality can take many forms depending on the business need. This may mean normalizing time-series data to standardized time windows (10 minute data being compared with 15 minute data), it may be alerting and removing impossible values (refrigerator temperature reading 300).\\n\\nProblems with data quality don't just show up from incorrect data; consistency matters too. Having the means to publish and endorse a dataset that is shared across the company can help solve this data consistency problem by helping eliminate the duplicated data.\\n\\nEnsuring data quality within a data product is a complex topic and there are many different ways to approach it. A common method is to categorize data quality into two main categories:\\n\\nData validation: This ensures data is valid and ready to be processed, for example, checking if mandatory columns are present or if data types are correct.\\n\\nBusiness validation: This confirms that the data makes sense from a business standpoint, like checking if a column's value falls within an acceptable range.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\06-data-quality.md'}, {'chunkId': 'chunk76_1', 'chunkContent': \"Business validation: This confirms that the data makes sense from a business standpoint, like checking if a column's value falls within an acceptable range.\\n\\nLastly, any errors in data or business validations should be reported to the business so they can take appropriate actions. Depending on the use-case, this information can be presented in various ways, such as through a Power BI report, a dashboard, or alerts. An advanced implementation would also have the capability to take automated actions, such as sending an email to the data owner or initiating a workflow to rectify the data.\\n\\nBackground story\\n\\nThe implementation of Data Mesh architecture is coming along nicely. The team has successfully implemented few domains and corresponding data products. The initial data has been ingested and the data products are ready for consumption. The enhanced search functionality of the new implementation is already providing tangible benefits. But business has raised some concerns about the data quality as new data is being ingested. There are two main challenges that the team is facing:\\n\\nAt times, the new data being ingested has schema mismatches with results in the failure of the data ingestion process. For example: a mandatory column is missing, the data type of a column is different and so on. There is no way for the team to know about such inconsistencies so they are not able to take appropriate actions.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\06-data-quality.md'}, {'chunkId': 'chunk76_2', 'chunkContent': 'Business doesn\\'t know weather these issues are because of bad data (For example: missing mandatory column) or failure of business validation rules (For example: a column value can be in a certain range).\\n\\nTechnical details\\n\\nFor this challenge, the team is going to focus on \"Movies\" data product only. The initial movie data for Southridge Video came from the Azure CosmosDB collection. Now, there is another collection which represents the incremental data that needs to be ingested. The team needs to make sure that the new data is ingested successfully and the data quality is maintained. They also need to highlight any issues with the data quality to the business.\\n\\nAs part of business validations, the following rules need to be applied:\\n\\nThe release year of the movie cannot be less than 1900 or more than 2023.\\n\\nThe valid movie ratings are \"G\", \"PG\", \"PG-13\", \"R\", and \"M\".\\n\\nHere are the details of the new data:\\n\\nThe new data is available in new-movies collection of the movies database in the same Azure CosmosDb account.\\n\\nThere are 10 new documents in the collection. Some of these may contain invalid records (data validations) and some may have invalid values (business validations).\\n\\nSuccess criteria\\n\\nYou have defined a \"schema\" for the \"Movies\" data product.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\06-data-quality.md'}, {'chunkId': 'chunk76_3', 'chunkContent': 'Success criteria\\n\\nYou have defined a \"schema\" for the \"Movies\" data product.\\n\\nTypically, this information is included as part of data contracts along with a lot of other attributes. In this case, we are not writing a full-fledged data contact. Rather, we are just focusing on the schema information which is required for data validation.\\n\\nCreate a file (preferably JSON/YAML) to define the schema of the \"Movies\" data product. Include the following information:\\nColumn names\\nColumn data types\\nIs required?\\nBusiness constraints (if any)\\n\\nStore this file in appropriate location within the data product.\\n\\nWhat are the pros and cons of storing a data contract within the data product itself?\\n\\nYou have processed the new data and ingested it into the \"Movies\" data product.\\n\\nRead the new incoming movie data.\\n\\nApply the \"data validations\" and \"business validations\" on this data. Please feel free to use any open-source library. You can also use the schema file that you created in the previous step.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\06-data-quality.md'}, {'chunkId': 'chunk76_4', 'chunkContent': 'Follows the following processing rules:\\nIf the data is invalid because of data validations, then log the error and move on to the next document. Also, store the failed record in a separate location.\\nIf the data is invalid because of business validations, then log the error and move on to the next document. Also, store the failed record in a separate location.\\nIf the data is valid, then ingest the document into the consumption-layer (aka gold layer) of the data product.\\n\\nYou have created a data quality report for the \"Movies\" data product.\\n\\nPublish a report that contains the following information:\\nTotal count of ingested records.\\nTotal count of failed records because of data validation.\\nTotal count of failed records because of business validation.\\n\\nPublish another report that contains the following information:\\nThe details of the failed records so that business can take appropriate actions.\\n\\nThe above reports can be in any format (Power BI report, text, HTML, PDF, etc.). Goal is to create a simple report that can be used by the business to understand the data quality.\\n\\nKnowledge check\\n\\nQuestion 1\\n\\nWhich of the following examples are instances of business validation? There may be more than one correct answer.\\n\\na. The order date shouldn\\'t be greater than return date.\\n\\nb. The order quantity should be a float.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\06-data-quality.md'}, {'chunkId': 'chunk76_5', 'chunkContent': 'a. The order date shouldn\\'t be greater than return date.\\n\\nb. The order quantity should be a float.\\n\\nc. Any return order should have a description.\\n\\nd. The order table should have a primary key.\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 2\\n\\nWhat is the recommended approach for monitoring data quality in a Data Mesh architecture for a data product?\\n\\na. Implement continuous data monitoring with alerting mechanisms.\\n\\nb. Have no monitoring in place to maintain data autonomy.\\n\\nc. Rely solely on manual data quality checks by data engineers.\\n\\nd. Use data catalogs to document metadata for data lineage.\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nQuestion 3\\n\\nWhat is the key difference between data validation and business validation?\\n\\na. Data validation focuses on technical standards, while business validation ensures data is free from errors.\\n\\nb. Data validation checks data relevance, while business validation checks data completeness.\\n\\nc. Data validation verifies data integrity, while business validation enforces business-specific rules.\\n\\nd. Data validation is concerned with data usability, while business validation is all about data structure.\\n\\n??? note \"Answer! (Click to expand)\"\\n\\nReferences\\n\\nMicrosoft Solutions Playbook: Data quality monitoring\\n\\nMicrosoft Solutions Playbook: Malformed record store', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\06-data-quality.md'}, {'chunkId': 'chunk76_6', 'chunkContent': 'References\\n\\nMicrosoft Solutions Playbook: Data quality monitoring\\n\\nMicrosoft Solutions Playbook: Malformed record store\\n\\nGreat Expectations: Getting started\\n\\nPandera: Data validation API for dataframe-like objects\\n\\nYoutube Video: Fully Utilizing Spark for Data Validation (Databricks)\\n\\nYoutube Video: Implementing a Data Quality Framework in Purview\\n\\nCerberus: Data validation for Python\\n\\nMicrosoft Learn: Using Azure Monitor with Azure Synapse Analytics\\n\\nPlease note that these links are for reference only. You are free to use any other resources that you find useful.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\06-data-quality.md'}, {'chunkId': 'chunk77_0', 'chunkContent': \"Wrap Up\\n\\nCongratulations on the completion of the Data Mesh Hack! We hope you enjoyed the experience and learned a lot from it. In this section, the hack challenges will be summarized, and some pointers for the next steps will be provided.\\n\\nChallenges summary\\n\\nThe hack began with a real-world situation where the centralized data platform faced difficulties meeting the requirements of the growing business needs of Southridge Video. Consequently, the organization decided to investigate a decentralized, domain-centric data architecture known as Data Mesh.\\n\\nIn the first phase (challenge 1), the organization delved into its business operations, and mapped the business domains to data domains and data products.\\n\\nMoving on to the second phase (challenge 2), the team concentrated on implementing the 'Movie Catalog' data product, making it their initial target. The goal was to learn from the challenges and mistakes encountered in this process and apply these lessons to future data products. The implementation of data product followed the traditional data engineering process, which involved data ingestion, data transformation, and data consumption.\\n\\nUpon successfully implementing the 'Movie Catalog' data product, the team progressed to the third phase (challenge 3), where they implemented the remaining data products in a different domain. This phase also involved exploring how other data products could be utilized by one another.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\07-wrap-up.md'}, {'chunkId': 'chunk77_1', 'chunkContent': \"The fourth phase (challenge 4) presented a significant challenge due to the complexity of Data Governance. Decentralized data governance added an extra layer of difficulty. The team explored various aspects of data governance including cataloging, lineage, and access control.\\n\\nThe fifth phase (challenge 5) proved to be the most intriguing. Creating data products is only valuable when they contribute to the business. In this phase, the team developed a semantic search application capable of providing meaningful recommendations to end users based on their search queries.\\n\\nThe sixth and final phase (challenge 6) revolved around the concept of data quality monitoring. It involved managing data validations and business validations for newly ingested data. Additionally, the team explored the option of having data quality dashboards for monitoring at the data product level.\\n\\nTarget architecture\\n\\nThe envisioned Data Mesh architecture resembles the diagram provided below. It's important to note that this diagram is only meant to provide a high-level overview of the architecture. Your implementation may differ from this diagram, which is perfectly acceptable.\\n\\nClean up resources\\n\\nBefore you wrap up, make sure to clean up the resources created for this hack. You can do so by following the steps below:\\n\\nDelete the resource group that you created for this hack as part of Infrastructure as Code (IaC) setup for the existing data estate. You can do so from the Azure Portal or by running the following az command:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\07-wrap-up.md'}, {'chunkId': 'chunk77_2', 'chunkContent': \"sh\\naz group delete --name <resource-group-name> --yes --no-wait\\n\\nConsider deleting the Microsoft Fabric Workspace(s) created for this hack, if you don't need it anymore.\\n\\nAlso consider cleaning up the following items, if you created these as part of the hack. Please note that these actions require elevated permissions.\\n\\nMicrosoft Entra Security Groups\\n\\nMicrosoft Fabric Domains\\n\\nProvide feedback\\n\\nYour feedback is crucial for our ongoing improvement of content, tools, and the overall experience for future participants. Please use the following links to provide feedback:\\n\\nHack Content Feedback\\n\\nProduct Feedback\\n\\nOpen a new Product Feedback work item as a child link\\n\\nImprove the hack content directly in the Microsoft Solutions Playbook by opening a PR. Refer the Contributing Guide for more details.\\n\\nNext steps\\n\\nNow that you have completed the Data Mesh Hack, here are some recommendations for the next steps in your learning journey:\\n\\nExplore the Artificial Intelligence: Large Language Model Lab which has a similar format to the Data Mesh Hack.\\n\\nFamiliarize yourself with Microsoft Fabric by reading the public documentation.\\n\\nGain hands-on expertise in Microsoft Fabric by following the Get started with Microsoft Fabric learning path.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\07-wrap-up.md'}, {'chunkId': 'chunk78_0', 'chunkContent': \"Introduction\\n\\nThis hack is specifically crafted to facilitate learning the concepts of Data Mesh through practical, hands-on experience (Hack) via challenges rooted in a fictional customer scenario. Within this hack, participants engage in a progressive process of solving a series of challenges, ultimately constructing a Data Mesh. The order in which these challenges are tackled holds significance, as each challenge builds upon the results of the preceding one(s).\\n\\nIt's worth noting that these challenges are not accompanied by explicit step-by-step instructions for solving them. However, each challenge is furnished with comprehensive technical explanations and reference materials to empower teams in resolving the challenges and gaining valuable learning experiences.\\n\\nPresently, there are two available delivery models for conducting the hack. Should you require any assistance in organizing the hack, we encourage you to reach out to us.\\n\\nFeedback plays a pivotal role in enhancing both our products and the hack experience. We kindly request that you allocate some time to gather and share your valuable feedback.\\n\\nInfrastructure setup\\n\\nIn order to complete the challenges, there are some resources that are required to be setup and include the data that you will be using in the challenges. The infrastructure of the fictitious customer is represented by a set of Azure resources. The Infrastructure as Code (IaC) scripts for setting up the customer's Azure infrastructure can be found in the dataops-code-samples repository.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\index.md'}, {'chunkId': 'chunk78_1', 'chunkContent': 'The participants would also require Microsoft Fabric license. A few success criteria from the challenges require administrative privileges. But if such elevated roles cannot be granted, the hack can still be executed.\\n\\nThe following is a list of common Azure resources that are deployed during the hack.\\n\\nAzure resource Resource Providers Azure Cosmos DB Microsoft.DocumentDB Azure SQL Database Microsoft.SQL Azure Storage Microsoft.Storage Azure Data Lake Store Microsoft.DataLakeStore Azure Key Vault Microsoft.KeyVault Microsoft Purview Microsoft.Purview\\n\\nEnsure that these services are not blocked by Azure Policy. The services that attendees can utilize are not limited to this list. Subscriptions with a tightly controlled service catalog may run into issues, if the service an attendee wishes to use is disabled via policy.\\n\\nChallenges\\n\\nThere are six challenges that gradually construct the Data Mesh and ultimately address a specific business need. Here is a concise overview of these challenges.\\n\\nChallenge Purpose Background Story Covers the customer story behind the challenges Challenge 1: A Tale of Two Domains Design domains and data products Challenge 2: Brave New World Implement the first data product Challenge 3: The Bucket List Deep dive into data products Challenge 4: The Rise of Governance Implement governance from Data Mesh perspective Challenge 5: Search Party Consume data product to address a business challenge Challenge 6: The Quality Awakens Implement data/business validations to maintain data quality\\n\\nIntroduction and walkthrough', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\index.md'}, {'chunkId': 'chunk78_2', 'chunkContent': 'Introduction and walkthrough\\n\\nIf you would like more background on Data Mesh and how the challenges reflect the Data Mesh approach, please check out the video here.\\n\\nFor a introductory walkthrough of using Microsoft Fabric, please view the video here. There are also resources for this video here.\\n\\nNote: This video was created during the public preview and some elements may have changed as we near GA.\\n\\nDelivery models\\n\\nCurrently the hack can be offered in two models.\\n\\nCoach-guided hack\\n\\nIn this model, the event organizers handle infrastructure provisioning and appoint a coach for each team. There can be multiple teams, each consisting of an ideal number of participants, ranging from four to six. The coach assists the team in comprehending the challenges and providing guidance when they encounter obstacles. This setup promotes collaboration and learning within a mentored environment. Additionally, participants should be prepared to commit two consecutive days to this endeavor.\\n\\nSelf-serve hack\\n\\nIn this model, individuals or small teams have the opportunity to undertake the hack independently. The participants are responsible for both provisioning the infrastructure (as outlined in the infrastructure Setup section) and executing the challenges.\\n\\nWhile individuals or teams have the freedom to determine their approach for conducting the hack, the Data Channel in SolutionOps team can be a resource when looking for help from others who have completed or coached this hack in the past.\\n\\nPrerequisites', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\index.md'}, {'chunkId': 'chunk78_3', 'chunkContent': \"Prerequisites\\n\\nBelow are the prerequisites for the hack:\\n\\nFundamental Azure Administration know-how\\n\\nUnderstanding of core Data Mesh concepts\\n\\nBasic familiarity with Microsoft Fabric\\n\\nA good grasp of SQL and Python\\n\\nIt's ok if you don't have all of these skills. You can still participate in the hack and learn as you go. The hack is designed to be a learning experience and provide enough guidance and references to help you along the way.\\n\\nFeedback\\n\\nYour feedback is crucial for our ongoing improvement of content, tools, and the overall experience for future participants. Please use the following links to provide feedback:\\n\\nHack Content Feedback\\n\\nProduct Feedback\\n\\nOpen a new Product Feedback work item as a child link\\n\\nImprove the hack content directly in the Microsoft Solutions Playbook by opening a PR. Refer the Contributing Guide for more details.\\n\\nFAQs\\n\\nIs completing MDW OpenHack a prerequisite for this hack?\\n\\n<!-- lychee >\\n  While prior experience with the Modern Data Warehouse OpenHack, also known as Cloud-based Data Warehousing, is not mandatory to take on these challenges, it can certainly assist participants in grasping the customer story more effectively.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\hackhub\\\\data-mesh-hack\\\\index.md'}, {'chunkId': 'chunk79_0', 'chunkContent': \"tags:\\n    - Financial Services\\n    - Healthcare\\n    - Manufacturing\\n    - Retail\\n\\nIndustry Solutions\\n\\nSimilar to technology solutions, industry solutions are also an opinionated engineering approach that's designed to meet the specific needs and requirements of a particular industry. These solutions are tailored to address the unique challenges, regulations, and workflows of a specific industry. Example industries include- healthcare, finance, manufacturing, or retail. These industry solutions can include a wide range of Azure services and often cut across different technologies.\\n\\nThis section provides various industry solutions with primary focus on DataOps. The emphasis is on collaboration, automation, and agile practices in the management and deployment of data.\\n\\nDataOps for Manufacturing\\n\\nIn manufacturing industry, data is a critical component as it offers insights into every aspect of the production process. With the help of data analytics tools, manufacturers can: monitor production in real-time, identify inefficiencies and bottlenecks, and optimize operations to increase productivity and reduce costs. Data can also be used to track equipment performance, predict maintenance needs, and reduce downtime, thereby improving overall equipment effectiveness (OEE).\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\index.md'}, {'chunkId': 'chunk79_1', 'chunkContent': 'In addition, data can help manufacturers improve quality control by identifying defects and preventing them from reaching the market. With the rise of the Industrial Internet of Things (IIoT) and smart factories, data has become an essential component of modern manufacturing. It allowed manufacturers to make data-driven decisions and stay competitive in a rapidly evolving market.\\n\\nFor more information, see DataOps for Manufacturing.\\n\\nSolutions\\n\\nDataOps: Agile Plant Accelerator\\n\\nDataOps for Healthcare\\n\\nHealthcare organizations are dealing with an increasing amount of data. Some of the data sources include: electronic health records, medical imaging records, clinical trials and research studies. The implementation of DataOps can streamline the process of data integration, ensure data quality and security. It can also enable real-time insights that improve patient outcomes, reduce costs, and optimize operations. DataOps in healthcare emphasizes collaboration, automation, and agile practices in the management and deployment of data.\\n\\nFor more information, see DataOps for Healthcare.\\n\\nSolutions\\n\\nDataOps: DICOM Metadata Analytics\\n\\nDataOps: Azure TRE\\n\\nDataOps for Automotive', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\index.md'}, {'chunkId': 'chunk79_2', 'chunkContent': 'Solutions\\n\\nDataOps: DICOM Metadata Analytics\\n\\nDataOps: Azure TRE\\n\\nDataOps for Automotive\\n\\nData platforms for the automotive industry are becoming increasingly important as the industry shifts towards more data-driven decision making. These platforms are designed to collect, analyze, and manage large amounts of data from various sources within the automotive ecosystem: including vehicles, dealerships, suppliers, and customers. DataOps enable automakers and their partners to gain valuable insights into consumer behavior, market trends, and vehicle performance. All these help users to optimize production processes, improve product quality, and enhance the overall customer experience. A few common use cases from automotive industry where DataOps is a critical component are: connected fleet management, autonomous vehicle platform, price prediction for ride sharing companies etc.\\n\\nSolutions\\n\\nDataOps: Autonomous Vehicles Platform\\n\\nReferences\\n\\nAzure Industry Solutions\\n\\nAzure for Manufacturing\\n\\nAzure for Healthcare', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\index.md'}, {'chunkId': 'chunk80_0', 'chunkContent': 'tags:\\n    - Automotive\\n\\nDataOps for Autonomous Vehicles Platform\\n\\nAutonomous driving systems depend on sensors, complex algorithms, machine learning systems and processors to run software. They create and maintain a map of their surroundings based on a variety of sensors situated in different parts of the vehicle like radar, cameras, and LiDAR (Light Detection and Ranging).\\n\\nSoftware then processes all this sensory input, plots a path, and sends instructions to the vehicle’s actuators, which control acceleration, braking, and steering. Rules, obstacle avoidance algorithms, predictive modeling, and object recognition help the software learn to follow traffic rules and navigate obstacles. But to do all this, there are complex processing challenges with respect to the data collected for building this software.\\n\\nMicrosoft’s AVOps reference architecture provides a comprehensive set of cloud, edge, vehicle, and AI services that enable an integrated, end-to-end workflow for developing, verifying, and improving automated driving (AD) functions. The AVOps platform needs a scalable and cost-effective solution that can potentially store and process petabytes of data for building and training ML models. The solution in this article is aligned with Microsoft’s AVOps reference architecture. It addresses the concerns and constraints related to developing a cloud-based data processing solution for AVOps (Autonomous Vehicle Operations).', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_1', 'chunkContent': 'This article also comes with AVOps Solution Kit, which is a git repository containing the code samples and configurations to deploy the large scale data processing platform for AVOps.\\n\\nData Collection Workflow\\n\\nThe AVOps platform needs diverse set of data on the roads for training the ML models. This data could be about driving the car in different traffic conditions, different weather conditions, lane cutting, etc.\\n\\nAdmins create a list of tasks which map to these conditions and assign the tasks to the drivers.\\n\\nWhen the conditions are correct for the task at hand, for example, driving on a sunny day in a heavy traffic situation, then the driver starts the task by starting the recording in the car.\\n\\nAt the end of the day, the test vehicles come back to their base where this data is copied from the vehicles to cloud. The data collected from test vehicles going around the city are in the Rosbag file format. A Rosbag, or bag, is a file format in ROS for storing ROS message data. Rosbag is an effective way to store all the collected sensor data in a format that can be easily used. Each Rosbag could range from 2-10 GB in size. Each vehicle would collect a maximum of 100 TB of data.\\n\\nAfter collection, this data will be uploaded to Azure for further processing.\\n\\nData Processing Challenges', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_2', 'chunkContent': \"After collection, this data will be uploaded to Azure for further processing.\\n\\nData Processing Challenges\\n\\nAzure's recommended storage for building a data lake is ADLS Gen2 which has a soft limit of 5 petabytes per storage account. An AVOps platform collecting data from multiple test vehicles will soon reach this limit.\\n\\nUploading terabytes of data every day to ADLS Gen2 takes hours if not days. So uploading via internet is not practical as it takes a long time for the data to be available in the cloud and the bandwidth cost also becomes significant.\\n\\nStorage costs would significantly increase as the size of the data and the duration of storage increases.\\n\\nProcessing cost would also increase given the extremely large volume of data.\\n\\nCapturing data lineage is challenging as this solution uses some non-traditional file formats and processing technologies. The data lineage is very important for tracking and auditing once ML model training starts.\\n\\nArchitecture\\n\\nThe Metadata, data catalog and lineage store is a special component of the architecture that is connected to all the stages of the data flow. It has two parts - a metadata store (Azure Cosmos DB) and metadata API.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_3', 'chunkContent': 'Metadata store (Azure CosmosDB): It stores all the metadata and lineage information related to any data movement or transformation activity. For example, it stores the lineage of data as it goes through each process of extraction, down sampling, synchronization, enrichment and scene detection.\\n\\nMetadata API: This API provides a user-friendly tool to search the Metadata store to access measurements, datastreams, Lineage, and scenes and find out which data is stored where. The DataStream record type in the Metadata Store contains the location of the ADLS folder, which points to the exact data source. The Metadata API thus becomes the storage layer manager which can spread data across storage accounts and helps the developer finding out data location using metadata based search.\\n\\nData Loading to ADLS Gen2\\n\\nData is copied from vehicle to Azure Data Box or Azure Data Box (Edge). Once copied, it is sent to Azure data center for loading to the Landing Zone in Azure data lake.\\n\\nTopic Extraction with Validation\\nThere are three main data zones here - Landing, Raw and Extracted. Each zone maps to an ADLS Gen 2 storage account.\\n\\nOnce data is available in the Landing Zone, it triggers the Azure Data Factory pipelines for extracting the data from Landing zone to Raw Zone.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_4', 'chunkContent': 'Once data is available in the Landing Zone, it triggers the Azure Data Factory pipelines for extracting the data from Landing zone to Raw Zone.\\n\\nAzure batch uses the mounted folder from Azure data lake to extract the data in a canonical folder location in Extracted Zone of Azure data lake. Each topic like camera, GPS, LiDAR, radar, etc. gets its own folder.\\n\\nAZCopy is used on Azure Batch to copy files across storage accounts as it is found to be more performant and cost-effective in this case.\\n\\nIn each storage account, data is stored in a hierarchical structure. For example, in the raw storage account, it is\\nraw/YYYY/MM/DD/VIN/MeasurementID/DatastreamID\\n\\n2 Hz Down Sampling\\n\\nFrom the extracted zone, Azure Data Factory pipeline down sample the data to reduce the amount of data to label/annotate. Code for down sampling runs on Azure Batch.\\n\\nSynchronization (algorithmic)\\n\\nAzure Data Factory pipeline for Synchronization (algorithmic) of data across sensors where the data in the various topics will be synchronized by their timestamps. Alternatively, the synchronization could have happened at the device level itself.\\n\\nEnrichment', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_5', 'chunkContent': 'Enrichment\\n\\nAzure Data Factory pipeline is used for further enriching the data with weather, maps or objects. Data generated can be kept in Parquet files to relate with the synchronized data. Metadata about the enriched data is also stored in Metadata store.\\n\\nScene Detection\\n\\nAzure Data Factory pipeline for scene detection. Scenes are various scenarios like lane changes, blocked roads, pedestrians, traffic lights, and traffic signs. ADF pipeline will be used to label and detect these scenes in the data. Scene Metadata is kept in the metadata store while scenes themselves as objects can be stored in Parquet files.\\n\\nData Exploration by ML Engineers\\n\\nAzure Databricks / Azure Synapse to connect with Metadata API and access the azure data lake storage and research on the data. This enables the ML engineers or data scientists to search for interesting images across the datastreams and select them for further processing.\\n\\nData Model\\n\\nMicrosoft’s Common Data Model provides a shared data language for business and analytical applications to use. This solution is deeply inspired by the Common Data Model approach and utilizes one of the core automotive pillar models viz. Measurement (called DeviceMeasurement in the Common Data Model).\\n\\nMeasurement', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_6', 'chunkContent': 'Measurement\\n\\nA measurement corresponds to a collection of data files generated at the vehicle when it went to perform a specific task. It contains all the information about the drive like the task, vehicle information, driving conditions and information of the Rosbag files recorded during the drive.\\n\\nMeasurement has the following content:\\n\\nRosbag files\\n\\nThe Rosbag files contain the actual raw data of various sensors (LiDAR, radar, camera, etc.) which was recorded during a vehicle run. The data is kept in the form of segregated topics of specific sensors in a Rosbag file.\\n\\nFor better operations, Rosbag files are created in chunks of 2 GB size.\\n\\nManifest File\\n\\nThe manifest file contains the metadata information of a measurement, which mainly contains the following details:\\n\\nVehicle (Vehicle identifier, type, make etc.)\\n\\nDriver (Driver personal details)\\n\\nTask information (Unique task identifier, driving area, co-relation with vehicle and driver)', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_7', 'chunkContent': 'Rosbag metadata (A list of Rosbag file\\'s metadata, which contains the path, name, checksum, file creationTime)\\nHere is an example manifest file.\\njson\\n{\\n    \"id\": \"1e211e94-b27b-11ed-afa1-0242ac120002\",\\n    \"metadata\": {\\n        \"additionalProp1\": \"string\",\\n        \"additionalProp2\": \"string\",\\n        \"additionalProp3\": \"string\"\\n    },\\n    \"vehicle\": {\\n        \"id\": \"KA01LB1234\",\\n        \"name\": \"TestCar\",\\n        ...\\n        ...\\n    },\\n    \"driver\": {\\n        \"id\": \"john.doe\",\\n        \"firstName\": \"John\",\\n        \"lastName\": \"Doe\",\\n        \"email\": \"john.doe@gmail.com\",\\n        \"isActive\": true\\n        ...', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_8', 'chunkContent': '\"isActive\": true\\n        ...\\n        ...\\n    },\\n    \"createdAt\": 1663916425,\\n    \"description\": \"string\",\\n    \"task\": {\\n        \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\\n        \"title\": \"heavy traffic in rainy day\",\\n        \"description\": \"description of the task\",\\n        \"state\": \"UNASSIGNED\",\\n        \"conditions\": [\\n            {\\n                \"name\": \"area\",\\n                \"value\": \"Delhi\"\\n            }\\n        ],\\n        \"isActive\": true,\\n        ...\\n        ...\\n    }\\n    ,\"files\" : []\\n}', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_9', 'chunkContent': 'Datastream\\n\\nDatastream is at the heart of the AVOps platform. It is an immutable stream of data and the fundamental unit to work within the data pipelines. Datastream can be considered as the first footprint of vehicle data in the cloud.\\nIt is more understood with the types of Datastreams viz. Raw and Derived.\\n\\nRaw Datastream\\n\\nWhen the Measurement is first copied to the landing zone in the cloud, system creates a raw Datastream, which points to a physical location of Rosbag files and manifests in the cloud (possibly an ADLS folder link). Raw Datastream contains its sources as an empty list and a lineage for traceability. You would read about how this solution keeps the lineage of the data records (Datastream) in a later section.\\n\\nDerived Datastream', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_10', 'chunkContent': 'Derived Datastream\\n\\nAny operation performed on the raw Datastream generates another immutable Datastream, called Derived Datastream. Not only does the Raw Datastream generate Derived ones but a Derived Datastream or the multiple combinations of the Datastreams can generate another Derived Datastream. The solution provides full flexibility in the Datastream creation. For example, extraction, synchronization, annotation, and curation operations on the data create various subtypes of \"Derived Datastream\" viz EXTRACTED, SYNCHRONIZED, ANNOTATED and CURATED. Derived Datastream contains the list of Datastreams (which are its sources), the lineage associated with it and other metadata.\\n\\nLineage\\n\\nEvery Datastream inherently keeps a lineage information. Whenever a pipeline process (e.g., extraction, curation etc.) likes to produce a Datastream, it calls a Metadata API, which requires source Datastream/s and producer metadata as input. Producer metadata contains Datastream name/type, and the process’ version.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_11', 'chunkContent': 'In the below diagram, \"Data Stream 101\" is a raw Datastream, and it can be tracked back from the final product \"Data Stream 106\" after Labelling through a GetLineage API that is exposed via the Metadata API. LineageAPI takes a Datastream unique id as an input and returns a tree of Datastream hierarchy.\\n\\nDesign Considerations\\n\\nStorage\\n\\nADLS Gen2 is used a de-facto storage for large Rosbag files. ADLS Gen2 is the first choice because it supports hierarchical namespace, which was a primary use case for organizing the unstructured data and data discovery purposes.\\n\\nAs solution requires to process the large scale (in Petabytes) data, it requires fast copy when the data flow is happening across zones. ADLS Gen2 supports NFS 3.0, that solves the high throughput requirements. Azure Batch job starts a docker container that mounts the storage account over NFS and copies the data. NFS 3.0 enablement enhances the copy performance by multifold.\\n\\nSolution also supports the data lifecycle inherently. This helps in managing the cost of storing large scale data.\\n\\nData in \"Landing\" zone is archived as soon as it’s moved to the \"RAW\" zone for processing.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_12', 'chunkContent': 'Data in \"Landing\" zone is archived as soon as it’s moved to the \"RAW\" zone for processing.\\n\\nSimilarly, there are rules set up in the ADLS blob storage for moving the data from \"Hot\" to \"Cold\" tier as data travels to \"DERIVED\" from \"RAW\" Zone.\\n\\nSolution also provides flexibility in terms of configurations for data lifecycle to meet the customer needs.\\n\\nCosmos DB stores the structured data of this solution, that is the foundational units of this solution viz. Measurement and Datastream json files.\\n\\nThe lineage information for Datastream is finally stored in the Cosmos DB. An API (Metadata API) is exposed over Cosmos DB to query the structured data and that also helps in tracking the data lineage.\\n\\nCosmos DB serves multiple purposes for the AVOps platform. Not only does it store the structured metadata of Rosbag files, but it also allows multiple clients to work with the system through a storage layer manager (Metadata API).', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_13', 'chunkContent': 'Key requirement with lineage tracking was to be able to reach to specific measurement/Rosbag file from processed data (or a Datastream). Decision of keeping lineage in Cosmos DB with Mongo graph query made it easy and simple to kick-start traceability support. Although, Purview could be the first choice for tracking the data lineage, it could have made the solution little more expensive, bulkier and involved some headache of writing custom APIs on top of cosmos for lineage (to integrate with it).\\n\\nCompute\\n\\nAzure Batch is the heart of compute in this solution. Azure Batch natively supports massively parallel processing, which fine-tunes the performance and turn-around time of data pipelines.\\n\\nFor a sample Rosbag extraction process, multiple topics (lidar, radar, camera1, camera2 etc.) need to be extracted. It is designed to extract all the topics in with multiple jobs running in parallel.\\n\\nAzure Batch supports Azure spot VM instances, which reduces the cost of the Batch workloads. Setting up an auto-scale formula helps further in cost optimization.\\n\\nSolution provides flexibility in terms of hosting the compute for the API/services exposed.\\n\\nAzure Function can help optimize cost of burst kind of traffic scenarios, when services are idle for most of the time.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk80_14', 'chunkContent': \"Azure Function can help optimize cost of burst kind of traffic scenarios, when services are idle for most of the time.\\n\\nAKS (Azure Kubernetes Service) can be also in consideration when there is a requirement of multiple microservice's communication.\\n\\nIf it’s about the ease of usability, App service/ App service with containers can be used.\\n\\nReferences\\n\\nMicrosoft's AVOps reference architecture\\n\\nAVOps Solution Kit: AVOps solution kit is a Dataops platform which helps customers to kick-start quickly on high scale data ingestion requirements.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\dataops-avops-platform.md'}, {'chunkId': 'chunk81_0', 'chunkContent': 'tags:\\n    - Automotive\\n\\nDataOps for Automotive\\n\\nData platforms for the automotive industry are becoming increasingly important as the industry shifts towards more data-driven decision making. These platforms are designed to collect, analyze, and manage large amounts of data from various sources within the automotive ecosystem, including vehicles, dealerships, suppliers, and customers. They enable automakers and their partners to gain valuable insights into consumer behavior, market trends, and vehicle performance, which can be used to optimize production processes, improve product quality, and enhance the overall customer experience. Some common use cases from automotive industry where DataOps is a critical component are connected fleet management, autonomous vehicle platform, price prediction for ride sharing companies etc.\\n\\nDataOps for Autonomous Vehicles Platform\\n\\nAutonomous driving systems depend on sensors, complex algorithms, machine learning systems and processors to run software. They create and maintain a map of their surroundings based on a variety of sensors situated in different parts of the vehicle like radar, cameras, and LiDAR (Light Detection and Ranging).\\n\\nSoftware then processes all this sensory input, plots a path, and sends instructions to the vehicle’s actuators, which control acceleration, braking, and steering. Rules, obstacle avoidance algorithms, predictive modeling, and object recognition help the software learn to follow traffic rules and navigate obstacles. But to do all this, there are complex processing challenges with respect to the data collected for building this software.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\index.md'}, {'chunkId': 'chunk81_1', 'chunkContent': 'Microsoft’s AVOps reference architecture provides a comprehensive set of cloud, edge, vehicle, and AI services that enable an integrated, end-to-end workflow for developing, verifying, and improving automated driving (AD) functions. The AVOps platform needs a scalable and cost-effective solution that can potentially store and process petabytes of data for building and training ML models. The solution in this article is aligned with Microsoft’s AVOps reference architecture. It addresses the concerns and constraints related to developing a cloud-based data processing solution for AVOps (Autonomous Vehicle Operations).\\n\\nThis article also comes with AVOps Solution Kit, which is a git repository containing the code samples and configurations to deploy the large scale data processing platform for AVOps.\\n\\nFor details of this solution, please read through DataOps for Autonomous Vehicles Platform.\\n\\nReferences\\n\\nMicrosoft’s AVOps reference architecture', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-automotive\\\\index.md'}, {'chunkId': 'chunk82_0', 'chunkContent': \"tags:\\n    - Healthcare\\n\\nDICOM Metadata Analytics\\n\\nDigital Imaging and Communications in Medicine (DICOM) is the standard for the communication and management of medical imaging information and related data. It is used worldwide to store, exchange, and transmit medical images. According to dicomstandard.org:\\n\\n!!! note\\n    DICOM® — Digital Imaging and Communications in Medicine — is the international standard for medical images and related information. It defines the formats for medical images that can be exchanged with the data and quality necessary for clinical use.\\n\\nDICOM dataset doesn't only come with large images, but also metadata and quantitative measurements. Analytics on this metadata can uncover great insights on imaging technique, the patient’s body part examined and modality of study etc. One of many applications of this analytics is to quickly create cohorts of patients for further study.\\n\\nThis pattern covers how to develop the infrastructure needed for ingesting and storing DICOM and clinical data to support requirements around searching, filtering and access management.\\n\\nArchitecture\\n\\nThe following diagram outlines the architecture:\\n\\nKey components of the architecture are:\\n\\nAzure Data Factory (ADF): ADF is used as an orchestrator to manage the copy, validation, and ingestion of all DICOM and clinical data.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-healthcare\\\\dicom-metadata-analytics.md'}, {'chunkId': 'chunk82_1', 'chunkContent': 'Azure Data Explorer (ADX): ADX is used as the serving layer storage for the DICOM metadata and clinical data considering its flexibility, performance, and price point.\\n\\nAzure Function App: A function app is used to ingest data into ADX. It is bound to an event grid which allowed the function app to auto-scale in response to the number of incoming trigger events.\\n\\nMicrosoft DICOM PaaS Service: The DICOM service is used to store the raw DICOM files.\\n\\nTerraform and Azure DevOps: Terraform and Azure DevOps is used to deploy the infrastructure as code.\\n\\nDataflow\\n\\nThe following steps represent the numbers in the architecture diagram:\\n\\nDICOM data is transferred from the DICOM data source to Blob Storage in the target tenant by the data owner.\\n\\nDICOM data is copied into the landing zone Blob Storage by the Data Factory data copy pipeline using a Hybrid Runbook Worker.\\n\\nBlob Storage events are pushed using Azure Event Grid.\\n\\nThe ingestion is done using an Azure Function App. It is bound to the event grid which allows the Function App to auto-scale in response to the number of incoming trigger events.\\n\\nThe function app uses ADX queued ingestion which decouples the data ingestion process from the ADX engine service improving performance and reliability.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-healthcare\\\\dicom-metadata-analytics.md'}, {'chunkId': 'chunk82_2', 'chunkContent': 'The function app uses ADX queued ingestion which decouples the data ingestion process from the ADX engine service improving performance and reliability.\\n\\nData is ingested into DICOM PaaS using the DICOM Store REST API.\\n\\nDesign Considerations\\n\\nServing Layer Storage\\n\\nAzure Data Explorer (ADX) is used as the storage layer to serve the data to end user to support the requirements around searching, filtering and access management. After evaluating other options (Cosmos, Azure Search, Azure SQL) ADX offered the flexibility, performance, and price point that best aligned with the overall projects functional and non-functional requirements. In addition, ADX’s column store, text indexing, and data sharding technologies worked well with the large number of DICOM metadata tags, the hierarchical nature of the DICOM data and could easily support future data source requirements.\\n\\nData Integration Service\\n\\nAzure Data Factory (ADF) is used as the orchestrator, managing the full ingestion flow of both the DICOM and clinical data sets. ADF is able to handle the initial validation, movement, triggering of the underlying data processing steps, and verify the end state.\\n\\nMicrosoft DICOM Service', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-healthcare\\\\dicom-metadata-analytics.md'}, {'chunkId': 'chunk82_3', 'chunkContent': 'Microsoft DICOM Service\\n\\nThe DICOM Service supports search through the DICOM Search REST API but integrating external data is not supported. In addition, by using ADX there was no practical limit on the number of DICOM tags that could be included in the search criteria. The DICOM Service plays an important role of storing and providing access to the DICOM images.\\n\\nTechnical Samples\\n\\nTo be added\\n\\nReferences\\n\\nAzure Data Factory\\n\\nAzure Data Explorer\\n\\nMicrosoft DICOM Service', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-healthcare\\\\dicom-metadata-analytics.md'}, {'chunkId': 'chunk83_0', 'chunkContent': \"tags:\\n    - Healthcare\\n\\nDataOps for Healthcare\\n\\nAs the amount of healthcare data continues to grow, it is becoming more challenging for healthcare organizations to manage, integrate, and analyze their data effectively. DataOps for healthcare aims to provide a solution to this challenge by streamlining the process of data integration, ensuring data quality and security, and enabling real-time insights that can help healthcare organizations improve patient outcomes, reduce costs, and optimize operations.\\n\\nDICOM Metadata Analytics\\n\\nDigital Imaging and Communications in Medicine (DICOM) is the standard for the communication and management of medical imaging information and related data. It is used worldwide to store, exchange, and transmit medical images.\\n\\nDICOM dataset doesn't only come with large images, but also metadata and quantitative measurements. Analytics on this metadata can uncover great insights on imaging technique, the patient’s body part examined and modality of study, etc. One of many applications of this analytics is to quickly create cohorts of patients for further study. This solution will cover how to develop the infrastructure needed for ingesting and storing DICOM and clinical data to support requirements around searching, filtering and access management.\\n\\nFor details of this solution, please read through DataOps: DICOM Metadata Analytics.\\n\\nAzure Trusted Research Environments (Azure TRE)\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-healthcare\\\\index.md'}, {'chunkId': 'chunk83_1', 'chunkContent': 'Azure Trusted Research Environments (Azure TRE)\\n\\nThe Azure Trusted Research Environment project is an accelerator to assist Microsoft customers and partners who want to build out Trusted Research environments on Azure. This project enables authorized users to deploy and configure secure workspaces and researcher tooling without a dependency on IT teams.\\n\\nFor details of this solution, please read through DataOps: Azure TRE.\\n\\nReferences\\n\\nAzure for Healthcare\\n\\nHDR UK: Trusted Research Environments', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-healthcare\\\\index.md'}, {'chunkId': 'chunk84_0', 'chunkContent': 'tags:\\n    - Healthcare\\n\\nTrusted Research Environment\\n\\nA trusted research environment (TRE) in healthcare is a secure and controlled environment that facilitates the collection, sharing, and analysis of sensitive healthcare data for research purposes. A TRE is designed to protect the privacy of patients and their data while enabling researchers to access and analyze data to generate insights and advance medical knowledge.\\n\\nBy ensuring that healthcare research is conducted in a trusted research environment, healthcare organizations can build trust with patients, researchers, and regulatory authorities, while also promoting innovation and advancing medical knowledge. Trusted research environments help to ensure that the benefits of healthcare research are realized while minimizing the risks to patients and their privacy.\\n\\nFurther information on TREs in general can be found in many places, one good resource is HDR UK.\\n\\nAzure Trusted Research Environment project\\n\\nThe Azure Trusted Research Environment project is an accelerator to assist Microsoft customers and partners who want to build out Trusted Research environments on Azure. This project enables authorized users to deploy and configure secure workspaces and researcher tooling without a dependency on IT teams.\\n\\nThis project is typically implemented alongside a data platform that provides research ready datasets to TRE workspaces. Currently it supports the following workspace services:\\n\\nAzure ML\\n\\nGitea\\n\\nGuacamole\\n\\nInnerEye\\n\\nMLFlow\\n\\nHealth Services\\n\\nAzure Databricks', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-healthcare\\\\trusted-research-environment.md'}, {'chunkId': 'chunk84_1', 'chunkContent': 'Guacamole\\n\\nInnerEye\\n\\nMLFlow\\n\\nHealth Services\\n\\nAzure Databricks\\n\\nFor detailed information about setting up a new TRE environment on Azure, please visit the Azure TRE documentation site.\\n\\nReference\\n\\nAzureTRE Github Repo', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-healthcare\\\\trusted-research-environment.md'}, {'chunkId': 'chunk85_0', 'chunkContent': 'tags:\\n    - Manufacturing\\n\\nAgile Plant Accelerator\\n\\nManufacturing industry provides enormous opportunities for technology to provide value both for manufacturers and ultimately end customers. One of the key scenarios revolves around building agile factories of the future – streamlining the whole manufacturing process through improvements on factory process efficiency, supply chains, automation and workforce productivity.\\n\\nMany of the related problem spaces are data centric – be it data acquisition from factory devices or processes, contextualizing data, storing data and ultimately making decisions based on the data.\\n\\nAgile Plant Accelerator (APA) initiative vision is to build a collection of independent and composable building blocks, which can be leveraged to reduce complexity and time in creating custom manufacturing solutions. From an end-to-end use case perspective, the initiative aims to build accelerators for a \"Manufacturing Control tower\" scenario, creating a path to calculation and visualization of performance indicators, factory asset status and alerts, based on ingested, normalized and cleansed data.\\n\\nAPA building block designs are deeply rooted in real customer scenarios and are developed following CSE Engineering Fundamentals and Azure Well Architected Framework practices.\\n\\nAPA Stream Ingestion\\n\\nAPA Stream Ingestion block is the first deliverable from APA initiative.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\agile-plant-accelerator.md'}, {'chunkId': 'chunk85_1', 'chunkContent': 'APA Stream Ingestion\\n\\nAPA Stream Ingestion block is the first deliverable from APA initiative.\\n\\nStream data ingestion to target storage is a recurring cloud side pattern for Agile Plant scenarios. Within the high-level pattern, past customer engagements have had variable designs and requirements around for example (not an exhaustive list): data sources and format, cloud ingestion service, target storage, end to end latency, throughput, tenancy model, cost and whether the system only leverages future or also historical data.\\n\\nAPA Stream Ingestion block focus is initially on a cold path analytics enabler solution, where:\\n\\nOPC-UA formatted data (representing factory asset related values) is ingested to cloud from a simulated edge environment.\\n\\nWhen looking at the manufacturing data landscape, there is a variety of protocols and data formats that a complete solution often needs to deal with. OPC-UA is one of the attempts to standardize the data access and while the adoption doesn’t cover the complete picture, it acts as a good realistic reference format and has been used in several production solutions.\\n\\nData is stored to target storage with partitioning that supports multiple tenants (for example customers or organizations within a company) to further leverage the data and tenants possibly having data in storage from multiple sub-tenants (for example factories).', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\agile-plant-accelerator.md'}, {'chunkId': 'chunk85_2', 'chunkContent': 'Solution components are deployed into a restricted network and every tenant has unique credentials associated with the system.\\n\\nMultiple deployments of the solution can be leveraged to serve a larger load from a single tenant or multiple tenants.\\n\\nEnd to end latency from cloud ingress to target storage is < 10 minutes and throughput for a deployment is 10 MB/s.\\n\\nThe solution consists of deployable code and observability artifacts as well as associated documentation. These help the adopter understand design principles and options, how to customize the solution and how to optimize for tenancy, scale and cost requirements in their own setting.\\n\\nTechnical Samples\\n\\nThe following Azure DevOps repository contains the implementation details about the Stream Ingestion block of APA:\\n\\nAgile Plant Accelerator Stream Ingestion\\n\\nReferences\\n\\nOPC-UA', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\agile-plant-accelerator.md'}, {'chunkId': 'chunk86_0', 'chunkContent': 'tags:\\n    - Manufacturing\\n\\nDataOps for Manufacturing\\n\\nManufacturing Industry Landscape\\n\\nThe manufacturing industry is broad and generally includes businesses that take materials or components and transform them into products. The manufacturing industry can be divided into two verticals: Discrete Manufacturing and Process Manufacturing.\\n\\nDiscrete Manufacturing\\n\\nDiscrete manufacturing companies manufacture products from parts and components. The finished product can be disassembled back to its original components.\\n\\nStarting with an engineering design, everything, from airplanes and smart phones to microchips and industrial machinery, is assembled through a defined process along the assembly line.\\nThe output is consumer products, or products used in other manufacturing processes.\\n\\nProcess Manufacturing\\n\\nThe process manufacturing industry is composed of chemical and agrochemical companies that produce industrial chemicals, converting raw materials into more than 70,000 different products, such as seeds, fertilizer, skin care, rubber, or plastics.\\n\\nProduction uses formulas rather than a bill of materials. The finished product cannot be disassembled back to its original state because its components were irreversibly changed during production.\\n\\nMost of these products are part of a comprehensive value chain delivered to other industries like consumer-packaged goods, agriculture, or discrete manufacturing.\\nChemical and agrochemical companies are seeking ways to increase end-product yield, reduce waste, and lower energy and water usage, while maintaining world-class performance.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk86_1', 'chunkContent': 'Industry 4.0 and Smart Manufacturing\\n\\nWhat’s the future of manufacturing? Many people will say the answer is ‘Industry 4.0’ or ‘Smart Manufacturing’.\\n\\nIndustry 4.0 is a new phase in the industrial revolution that introduce intelligent networking of machines and processes for industry with the help of information and communication technology.\\n\\nIndustry 4.0, digital transformation, and smart manufacturing are about using disparate information to drive automated decisions from machinery to the cloud and putting more information in the hands of business decision makers when and where they need it. Companies make this transformation by adopting key technologies listed below:\\n\\nInternet of Things (IoT) - Connected devices communicate with each other to share data, automate processes, and increase efficiency.\\n\\nArtificial Intelligence (AI) - Machine learning algorithms automate complex decision-making processes, resulting in improved performance and reduced costs.\\n\\nCyber-physical systems - Physical devices and processes are monitored and controlled by integrated computer systems.\\n\\nBig Data and Analytics - The ability to collect and analyze large amounts of data enables more informed decision-making, improved performance, and greater efficiency.\\n\\nCloud Computing - The use of cloud-based services enables the centralization of data storage and computing power, making it easier to access and analyze data.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk86_2', 'chunkContent': 'Augmented Reality (AR) - AR technology enhances physical products and services with digital information, creating new opportunities for engagement and interaction.\\n\\nAutonomous Systems - The ability to automate physical and cognitive processes results in greater efficiency, improved performance, and reduced costs.\\n\\nDigital Twins - Virtual representations of physical devices, systems, and processes enable real-time monitoring, diagnosis, and optimization.\\n\\nWhy DataOps for Manufacturing\\n\\nIn the journey of adopting Industry 4.0, digital transformation and smart manufacturing, companies find it’s challenging due to the inconsistent industrial data format, big data volume and complicated data flows.\\n\\nAs the diagram shows above, before Industry 4.0, industrial data architecture had evolved over many years into a layered approach defined in the Purdue Model or ISA-95. This architecture consists of multiple layers, where data flowed from sensors to various systems, such as SCADA or ERP, with reduced data resolution at each layer. Proprietary communication protocols were developed to connect each layer, with OPC being one of the open protocols.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk86_3', 'chunkContent': 'However, Industry 4.0 and new target applications like IoT platforms, data lakes, and machine learning applications, have made industrial data architectures exponentially more complex. The outdated, layered approach to processing data is no longer effective with the increase in data volume, the flexibility of data flows and the need for data at all layers in the modern digital landscape of Industry 4.0 and smart manufacturing. Pushing excessive data through systems can slow down processing and increase security vulnerability.\\n\\nThe above challenges of inconsistent industrial data format, big data volume and flexible data flows have led to the development of DataOps solutions for industrial environments, we can also call it Industrial DataOps.\\n\\nIndustrial DataOps is a category of software solutions that address the data architecture needs of industrial companies as they adopt Industry 4.0, Digital Transformation, and Smart Manufacturing. DataOps solutions perform data contextualization and standardization and provide flexible and secure data flows to the various consuming applications running at the Edge, on-premises or in the Cloud. Industrial DataOps mainly covers the following capabilities:\\n\\nData transformation and contextualization\\n\\nA factory could have hundreds of PLCs and machine controllers from multiple vendors, each with varying data points that were created for use by Operational Technology (OT) applications. To gain insights from this data, it must be examined across the scope of machinery, processes, and products.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk86_4', 'chunkContent': 'A DataOps solution must incorporate standard models to handle the sheer volume of machines and controllers, correlate the data, and make it accessible to Information Technology (IT) applications.\\n\\nAzure services: Azure Databricks, Azure HDInsight, Azure Synapse\\n\\nSample code:\\n\\nDataOps - Parking Sensor Demo\\n\\nConfig-Driven Data Pipeline\\n\\nConnection to OT and IT systems\\n\\nIndustrial devices (or systems) and IT systems communicate in different ways. The former uses many proprietary protocols, but there is a growing adoption of OPC UA and other open protocols. IT systems use their own protocols to communicate with extensive usage of APIs and bespoke integrations.\\n\\nTo be effective, a DataOps solution must be able to integrate seamlessly with devices and data sources at the operations layer by leveraging industry standards, while providing value to business applications that conform to current IT best practices.\\n\\nAzure services: Azure Data Factory, Azure Event Hub, Azure IoT Hub, Azure Data Explorer\\n\\nSample code/articles:\\n\\nDataOps - Parking Sensor Demo\\n\\nConfig-Driven Data Pipeline\\n\\nData flow management', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk86_5', 'chunkContent': 'Sample code/articles:\\n\\nDataOps - Parking Sensor Demo\\n\\nConfig-Driven Data Pipeline\\n\\nData flow management\\n\\nData flows must be contained and managed within a system where they can be identified, enabled, disabled, and modified. It is critical to know what data is moving among systems—and be able to turn it off. Because many vendors want machine data to provide enhanced service, the operations team needs to be able to control that data flow and set conditions and frequency. Additionally, customers would like to know the metadata and the lineage of the data, to ensure the correct results of the downstream analysis.\\n\\nAzure services: Azure Data Factory, Azure Logic Apps, Azure Purview\\n\\nSample code:\\n\\nDataOps - Parking Sensor Demo\\n\\nIndustrial-level scalability and security\\n\\nIndustrial data differs from the typical transactional data found in most IT systems due to its extensive scope and size. It requires immediate contextualization and delivery at a resolution tailored to each specific usage, usually within a time frame of milliseconds to seconds after it is generated.\\n\\nMoreover, industrial data contains the confidential information and intellectual property of a manufacturing plant, which must be secured and protected.\\n\\nAzure services: Azure security services for operations, applications, storage, networking, compute, and identity\\n\\nSample code/articles:\\n\\nSecure MLOps solutions with Azure network security\\n\\nUse of the edge computing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk86_6', 'chunkContent': 'Sample code/articles:\\n\\nSecure MLOps solutions with Azure network security\\n\\nUse of the edge computing\\n\\nIndustrial data for analytic and visualization purposes can be processed either locally near the machinery (on edge devices), in an on-site data center, or in the cloud. The Industrial DataOps solution, however, may need to operate near the data source and supplies applications as per the desired frequency or conditions. The data analysis or even machine learning model training can be done on local powerful edge devices, to avoid uploading sensitive or large volume data to the cloud. And in this way, it is more effective to share data models across the factory and enterprise for data management.\\n\\nAzure services: Azure IoT Edge, Azure IoT Hub, Azure Event Hub\\n\\nSample code/articles:\\n\\nEnable machine learning inference on an Azure IoT Edge device\\n\\nMicrosoft Cloud for Manufacturing\\n\\nTechnically, DataOps can be deployed on-site, in a combination of on-premises and cloud infrastructure, or entirely within the cloud. By utilizing the cloud, DataOps can benefit from features like scalability, improved efficiency, and enhanced security. As one of the key technologies in Industry 4.0, cloud computing is in a critical role of digital transformation for manufacturing.\\n\\nThe Microsoft approach to Industrials and Manufacturing considers market insights and the competitive landscape, and positions the Microsoft value proposition via the Industry Priority Scenarios (IPSs):', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk86_7', 'chunkContent': 'Microsoft Cloud for Manufacturing accelerates the digitalization of manufacturing, while empowering the workforce to secure a more resilient and sustainable future.\\n\\nBuild More Agile Factories\\n\\nAn interconnected network of machines, communication mechanisms, and computing power, the agile (smart) factory is a cyber-physical system that uses advanced technologies such as artificial intelligence (AI) and machine learning to analyze data, drive automated processes, and learn as it goes.\\n\\nBy leveraging Industrial IoT and AI, we can drive safe and secure production with reliable quality and yield, optimizing resource utilization within the factory. Also, we can drive for lean, agile, fully automated, and sustainable manufacturing with an empowered workforce.\\n\\nAzure Services: Azure Databricks, Azure Event Hub, Azure Machine Learning\\n\\nSample code:\\n\\nAgile Plant Accelerator (APA) is an initiative to provide an accelerator sample solution which is composed of several independent and composable building blocks with the aim of bringing agility and reducing complexity in the plant automation process.\\n\\nProject AKRI is a Cloud Native Computing Foundation (CNCF) Sandbox project which lets you easily expose heterogeneous leaf devices (such as IP cameras and USB devices) as resources in a Kubernetes cluster, while also supporting the exposure of embedded hardware resources such as GPUs and FPGAs. Akri continually detects nodes that have access to these devices and schedules workloads based on them.\\n\\nTransform the Workspace', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk86_8', 'chunkContent': 'Transform the Workspace\\n\\nDigitally empower people and teams’ workforce with the skills and tools (e.g. secure collaboration and information management) to safely keep up with the new complexities and speed of digital manufacturing.\\n\\nAzure Services: Azure Databricks, Azure Machine Learning, Azure Synapse Analytics\\n\\nEngage customers in new ways\\n\\nIncrease customer satisfaction with new digital experiences across marketing, sales, and service channels.\\n\\nAzure Services: Azure Databricks, Azure Machine Learning, Azure Synapse Analytics\\n\\nCreate more resilient supply chains\\n\\nReduce risk with a secure, flexible, and resilient end-to-end supply chain through real-time visibility, intelligent planning, and execution. Drive for autonomous and sustainable supply chain.\\n\\nAzure Services: Azure Databricks, Azure Machine Learning, Azure Synapse Analytics, Azure IoT Hub, Azure Digital Twins\\n\\nUnlock innovation and deliver new services\\n\\nDiscover and engineer new business value with sustainable products, operations, and digital services. Embrace new design and Industrials and Manufacturing paradigms. Achieve faster time to market with reduced design cycles.\\n\\nAzure Services: Azure Databricks, Azure Machine Learning, Azure Synapse Analytics, Azure IoT Hub, Azure Digital Twins, Azure High-performance Computing\\n\\nConclusion', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk86_9', 'chunkContent': \"Conclusion\\n\\nDataOps is a good tool for us when we want to adopt Industry 4.0/Smart Manufacturing. It enables the operations team to efficiently provide data to systems and business users by defining standard models and managing integrations. This approach enables the operations team to adapt to changes in the factory, add new applications, and respond to shifts in business partnerships with external vendors. It also grants the operations team control over data access, speeds up analytics and visualization initiatives, and maintains the factory's ability to adjust or add new equipment over time.\\n\\nMicrosoft Cloud for Manufacturing is designed to deliver capabilities that support the core processes and requirements of the industry. These end-to-end manufacturing cloud solutions include released and new capabilities that help securely connect people, assets, workflow, and business processes, empowering organizations to be more resilient.\\n\\nReferences\\n\\nMicrosoft Cloud for Manufacturing\\n\\nIndustry 4.0 and Smart Manufacturing\\n\\nHow Industrial DataOps is Shaping Industry 4.0\\n\\nDataOps: Fundamental for Industrial Transformation\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\industry-solutions\\\\dataops-for-manufacturing\\\\index.md'}, {'chunkId': 'chunk87_0', 'chunkContent': \"rings:\\n  - public\\n\\nSolutions\\n\\nA solution is an opinionated engineering approach that brings together a set of capabilities to solve a business problem. It provides guidance, insights, and best practices on how to develop complete functional solution to address an end-to-end business scenario. All solutions listed have been successfully applied and validated by multiple customers.\\n\\nHere is a list of available solutions:\\n\\nData: Modern Data Warehouse - The Modern Data Warehouse is a versatile architectural pattern to build scalable analytical data pipelines in a cloud-first environment.\\n{% if extra.ring == 'internal' %}\\n\\nData: Data Mesh - Data Mesh is a relatively newer architectural paradigm based on a federated approach to build domain-oriented data products.\\n\\nData: Analytics and ML for Enterprise Business Applications - This solution presents the reusable patterns on how to implement advanced analytics and machine learning (ML) on top of enterprise business applications. Enterprise business applications include Enterprise Resource Planning (ERP) systems, Customer Relationship Management (CRM) systems, Marketing automation technologies, and more.\\n\\nData: Enterprise Data Sharing - This solution focuses on how Data Sharing scenarios can be built respecting Enterprise constraints for both internal and external customers or partners.{% endif %}\\n\\nSolutions Roadmap\\n\\nIn addition to the solutions mentioned above which have already been published, there are several others that are in the roadmap:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\index.md'}, {'chunkId': 'chunk87_1', 'chunkContent': \"In addition to the solutions mentioned above which have already been published, there are several others that are in the roadmap:\\n\\nData Marketplace\\n{% if extra.ring == 'public' %}\\n\\nData Mesh\\n\\nAnalytics and ML for Enterprise Business Applications\\n\\nEnterprise Data Sharing\\n{% endif %}\\n\\nData Modernization\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\index.md'}, {'chunkId': 'chunk88_0', 'chunkContent': 'Customer 360 View using Microsoft Dynamics 365\\n\\nCustomer 360 view is a comprehensive, 360-degree view of customer interactions and experiences with a business. It is an integrated and holistic view of all customer data, including past purchases, engagement history, preferences, social media interactions, demographics, and other relevant information.\\n\\nMicrosoft Dynamics 365 is a cloud-based business application platform that provides a suite of integrated solutions for customer relationship management (CRM) and enterprise resource planning (ERP), as well as other business operations such as marketing automation, sales, finance, and operations. Dataverse is the data platform that underlies Dynamics 365, as well as other Microsoft applications such as Power Apps and Power BI. It is a cloud-based storage that provides a secure and scalable environment for storing and managing data, enabling users to create, share, and manage applications and data with ease.\\n\\nThis architecture will outline how to build a comprehensive customer 360 view using Azure data services and Microsoft Dynamics 365 Customer Insight. Azure data services will be used to ingest data from multiple sources and process them, whereas Customer Insight will be used for building the unified customer profile including measures, segments, and enrichment.\\n\\nArchitecture Overview\\n\\nThis architecture presents a fictitious scenario of building a customer 360 view. Generally complex use cases require data from multiple sources. In this simplified version of this example scenario, data is coming from two source systems:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\analytics-business-app\\\\customer360-d365.md'}, {'chunkId': 'chunk88_1', 'chunkContent': 'Customer Details: Name, address and other PII information are available in Microsoft Dynamics 365 Sales application.\\n\\nCustomer Finance: Customer income, spend and other financial information is available in as a CSV file in an ADLS Gen2 container.\\n\\nThere are two major Azure data services that can be used to collect data from source systems, process and curate it for downstream use cases - Azure Synapse Analytics & Azure Databricks. Customer Insight pulls this curated data and derives unified customer profile including measures, segments, and enrichment.\\n\\nHere is the high level architecture diagram of the solution. Please note that for data processing, either Azure Synapse Analytics or Azure Databricks can be used.\\n\\nThis solution follows a medallion architecture which describes a series of data layers that denote the quality of data stored in the lakehouse. The terms bronze (raw), silver (validated), and gold (enriched) describe the quality of the data in each of these layers. ADLS Gen2 is used as the underlying storage layer.\\n\\nData from dataverse to the Bronze layer is loaded using Synapse Link for Dataverse. Azure Synapse Analytics or Azure Databricks jobs will process the data and move it to Silver and then Gold layer.\\n\\nCustomer Insights connects to customer data from the Gold layer and implements the unified customer profile.\\n\\nKey Design Considerations\\n\\nUse of Azure data services for data processing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\analytics-business-app\\\\customer360-d365.md'}, {'chunkId': 'chunk88_2', 'chunkContent': 'Key Design Considerations\\n\\nUse of Azure data services for data processing\\n\\nMicrosoft Dynamics 365 Customer Insight can pull data from multiple sources and do some data processing (deduplication, unification etc.). In many scenarios using Customer Insight instead of a proper analytics tool (Azure Synapse Analytics or Azure Databricks) is a perfectly valid approach.\\n\\nThe main reasons which drove the decision of using Azure Synapse Analytics/Azure Databricks are:\\n\\nIf the data from multiple sources are going through complex transformation (encryption/decryption, complex aggregations, large joins etc.) which are not suitable for Customer Insight, it\\'s recommended to use a proper analytics environment.\\n\\nIt is easy keeping a historical track of all the data changes using Synapse Analytics. Historical data is important for implementing ML use cases. This can\\'t be achieved by Customer Insight.\\n\\nAppend Only mode for Synapse Link for Dataverse\\n\\nNo data in the Bronze layer will be updated or deleted, data will be only added to this layer. While pulling data from dataverse, \"Synapse Link for Dataverse\" is using Append Only mode. In this mode, when a row in a dataverse table is deleted, it is not hard deleted from the destination. Instead, a row is added and set as isDeleted=True to the file in the corresponding data partition in Azure Data Lake.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\analytics-business-app\\\\customer360-d365.md'}, {'chunkId': 'chunk88_3', 'chunkContent': \"Use of Synapse Analytics Lake Database (applicable for Azure Synapse Analytics)\\n\\nThere are multiple options to build a database in Synapse Analytics, like dedicated SQL pool, lake database and serverless SQL pool. The Azure Synapse connector for Customer Insight only supports lake database to pull data. So the Gold layer is served using a lake database in Synapse Analytics.\\n\\nUse of parquet file format (applicable for Azure Databricks)\\n\\nThere are multiple analytics friendly file types to store data in ADLS Gen2, like parquet, delta etc. Customer Insight doesn't support delta file format, so parquet is used in this implementation for storage while using Azure Databricks.\\n\\nTechnical Samples\\n\\nThe following code samples showcase concrete implementations of the presented architecture:\\n\\nCustomer 360 View - Microsoft Dynamics 365 and Azure Synapse Analytics\\n\\nCustomer 360 View - Microsoft Dynamics 365 and Azure Databricks\\n\\nLimitations\\n\\nCurrently it's not possible to set up the Synapse Link for Dataverse programmatically. Thus, automation of this step is not possible at this stage, and it must be performed manually from the User Interface.\\n\\nFurther Reading\\n\\nDataOps: Modern Data Warehouse\\n\\nCreating Azure Synapse Link for Dataverse\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\analytics-business-app\\\\customer360-d365.md'}, {'chunkId': 'chunk89_0', 'chunkContent': 'Analytics and ML for Enterprise Business Applications\\n\\nIntroduction\\n\\nEnterprise Business Applications have traditionally been used for managing and automating core business processes and operations. These applications are designed to integrate and streamline various business functions such as finance, accounting, human resources, supply chain management, and customer relationship management.\\n\\nDue to the nature of these applications, the data they generate is a great asset to drive insight on how to streamline operations, optimize processes, improve decision making and enhance customer experience. For example, ERP systems can provide organizations with a wealth of data on their operations and processes. By analyzing this data, organizations can identify areas of improvement and optimize their processes to increase efficiency. Another example is, by gaining insights into customer data through the CRM systems, organizations can provide more personalized experiences to their customers.\\n\\nModern analytics and data warehousing tools are used to collect, process and analyze data from these applications. This is further improved by doing predictive analytics using machine learning techniques.\\n\\nThere are some common architectural patterns of deployment and usages of the enterprise business applications across different organizations which results in reusable patterns of how to do advanced analytics and machine learning on top of these applications. The purpose of this solution is to curate these reusable patterns.\\n\\nCommon Challenges\\n\\nLimited Analytical Capabilities', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\analytics-business-app\\\\index.md'}, {'chunkId': 'chunk89_1', 'chunkContent': \"Common Challenges\\n\\nLimited Analytical Capabilities\\n\\nSome business applications come with reporting and analytics capabilities. But their native analytics capabilities are very limited compared to modern state-of-the-art machine learning and analytics tools. They come short when trying to address complex use cases like churn prevention, fraud detection or predictive maintenance.\\n\\nLegacy Applications\\n\\nThere are many legacy business applications still in use which were developed a long time ago on top of old technology stack. Legacy technology stacks often lack proper ETL connectors and limited to no API support, which can make it challenging to extract data from these systems.\\n\\nRestricted Network Connectivity\\n\\nMany business applications are deployed on the on-premise infrastructure with limited network connectivity to external applications which sometimes makes it very difficult to integrate ETL tools with them.\\n\\nHandling PII Data\\n\\nIt's trivial for business applications to capture and store vast amount of PII data. Meeting compliance requirements (e.g., GDPR, CCPA, HIPAA etc.) for PII data in the analytics platforms is critical to ensure that sensitive information is protected and to avoid legal and financial penalties. Generally additional measures such as data encryption, access controls, and auditing are implemented while handling PII data.\\n\\nLogical Architecture\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\analytics-business-app\\\\index.md'}, {'chunkId': 'chunk89_2', 'chunkContent': 'Logical Architecture\\n\\nThere is a large set of business applications and different vendors offer the applications in slightly different flavor. But regardless of the type of the applications, analytics and machine learning on top of these systems will involve some common steps, like data ingestion from multiple sources, standardizing the data (data cleansing, data quality etc.) and developing a curated layer for consumption (data modelling, aggregation etc.). DataOps provides the best practices around the ingestion, transformation and serving of the data.\\n\\nThe curated layer of data is used to develop reports and dashboards to address use cases like one view of customer, profit and loss dashboards etc. For more complex use cases, machine learning models are trained using the data. Some examples of such use cases are customer churn prediction, predictive maintenance, fraud detection etc. Training ML models and using them for inference within reports/dashboards involves additional steps that include data exploration by ML engineers/ data scientists, feature engineering and model training,\\n\\nIn many cases, these reports/dashboards are published to the business apps, so that the end users can access them easily.\\n\\nThe following diagram presents a traditional logical architecture of doing DataOps and MLOps on top of two business apps - CRM and ERP.\\n\\nReal life implementation can be different from this architecture depending on the choice of the technology and requirements.\\n\\nArchitectural Patterns', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\analytics-business-app\\\\index.md'}, {'chunkId': 'chunk89_3', 'chunkContent': 'Real life implementation can be different from this architecture depending on the choice of the technology and requirements.\\n\\nArchitectural Patterns\\n\\nArchitectural patterns refer to reusable solutions that can be applied to common problems in software development. The following section presents sample implementations of analytics and machine learning for specific enterprise business applications.\\n\\nThough the samples focus on a specific use case (customer 360 view), the implementations are quite generic and can be easily extended for other use cases.\\n\\nCustomer 360 View using Microsoft Dynamics 365\\n\\nReferences\\n\\nDataOps: Modern Data Warehouse\\n\\nMLOps: Automating and Monitoring Model Training', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\analytics-business-app\\\\index.md'}, {'chunkId': 'chunk90_0', 'chunkContent': 'Data Mesh\\n\\nIntroduction\\n\\nData Mesh is an architectural pattern that aims to provide a self-service, decentralized model for data management and access within an organization. The concept of a Data Mesh encompasses data, technology, processes, and organization. An excellent overview can be found in Zhamak Dehghani’s 2019 paper How to Move Beyond a Monolithic Data Lake to  Distributed Data Mesh, where she coined the term.\\n\\nData Mesh is a democratized approach to managing data where various domains operationalize their own data. Typically, a domain can be thought of as a business unit that has overall accountability for an area within an organization. For example, financial services may have domains like customers, transactions, applications, fraud, etc.\\n\\nData Mesh challenges the idea of conventional centralization of data and data management:\\n\\nRather than looking at data as one repository, Data Mesh considers the decomposition of a data estate into independent data products.\\n\\nAs per Data Mesh concepts, data products are the primary areas for managing data. The approach presents a challenge in enforcing a minimum set of centralized governance policies and rules.\\n\\nThe shift from centralized to federated ownership is backed by a modern and self-service data platform. The platform is typically designed using cloud-native technologies.\\n\\nCase for Data Mesh: Mesh vs. Monolith', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_1', 'chunkContent': 'Case for Data Mesh: Mesh vs. Monolith\\n\\nSimilar to DevOps, adopting a Data Mesh approach requires more than just technology; it necessitates a different organizational structure and culture. The reasons are outlined in Key Principles section.\\n\\nWhen a Data Mesh approach may be appropriate:\\n\\nAn organization has many autonomous teams that need to access and use data but with no central authority or process for managing data access and governance.\\n\\nAn organization undergoing rapid growth or change with a need to adapt to new business needs and requirements quickly and flexibly.\\n\\nAn organization looking to move to a more data-driven decision-making model with a need to establish clear ownership and accountability for data quality and usage.\\n\\nThe scale of the organization is such that it has a growing need for self-service data onboarding and distribution. It allows decoupling from central IT management in turn speeding-up time to insights.\\n\\nWhen a Data Mesh approach may not be appropriate:\\n\\nThere are only a few teams/users who need access to data and there is already a robust, centralized process in place for access and governance.\\n\\nData volumes are small.\\n\\nNo requirement for self-service onboarding and data distribution.\\n\\nThere are unresolved barriers for data sharing within the organization. For example - highly regulated domains might find it simpler to implement a tightly controlled, centralized model.\\n\\nAccompanying Repository', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_2', 'chunkContent': 'Accompanying Repository\\n\\nMany of the practical aspects of implementing a Data Mesh have been applied in a recent Microsoft engagement, captured in a Microsoft internal repo. The repo is intended for illustrative purposes only and can be found in Code Hub.\\n\\nThis repository contains Infrastructure as Code (IaC) components to create the Mesh Context and configurable Data Products. It also includes an example data product that consumes publicly available data concerning upcoming rocket launches.\\n\\nFor space enthusiasts, the Code Hub Wiki contains detailed information on many publicly available datasets. It also contains APIs for space, in particular for launches and satellite tracking.\\n\\nKey Principles\\n\\nData as a Product', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_3', 'chunkContent': \"Key Principles\\n\\nData as a Product\\n\\nEach domain operates its data end to end, creating Data Products. Accountability lies with the data owner within the domain. Data pipelines (as seen in traditional ETL/ELT patterns) in this respect become a first-class concern of the domains themselves. From a technical perspective, a data product is a software component that's built to deliver a specific data-related capability to its users. This capability can include things like providing access to a specific dataset, performing a certain data transformation, or delivering real-time analytics. The focus on the needs of users is the key characteristic of a data product. Also, a data product is designed to be independently deployable and operable. In other words, a data product is a purpose-built software component to deliver data-related functionality to users in an easy-to-use and understandable way.\\n\\nFederated Data Governance\\n\\nAn enterprise data governance body must be established to ensure trust between data owners and share data products. This governance body will implement: data quality, central visibility of data ownership, data access management, and data privacy policies.\\n\\nDomain-oriented Data Ownership\\n\\nThe enterprise should ideally define and model each data-domain product within the mesh by applying the principles of domain-driven design (see appendix).\\n\\nSelf-serve Data Platform\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_4', 'chunkContent': \"Self-serve Data Platform\\n\\nA self-serve data platform is required for a Data Mesh to allow users to focus on their individual data use cases and abstract the technical complexity.\\n\\nData Mesh Roles\\n\\nThe main roles involved with a Data Mesh framework are explained in the sections below.\\n\\nData product owners\\n\\nData product owners are the teams or individuals who are responsible for the data products within the mesh. They manage the data, ensure its quality and accuracy, and make sure it's being used effectively.\\n\\nData platform owners\\n\\nData platform owners are the teams or individuals who are responsible for the overall infrastructure and management of the Data Mesh. They oversee the data governance and make sure that the data products are integrated and interoperable.\\n\\nData consumers\\n\\nData consumers are the teams or individuals who use the data products within the mesh. They may be analysts, engineers, or other stakeholders who use the data to make decisions or build applications.\\n\\nIn most organizations, these roles will interact with other core IT teams where there is an enterprise function. Below sections lists some of the teams and their roles.\\n\\nIT/DevOps teams\\n\\nIT/DevOps teams are responsible for the infrastructure and automation of the Data Mesh. They may be responsible for deploying new data products, ensuring best practices are in place, and handling any technical issues that arise.\\n\\nSecurity teams\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_5', 'chunkContent': 'Security teams\\n\\nSecurity teams are responsible for ensuring the security and compliance of the Data Mesh. They may be responsible for managing identities, access control, and data encryption.\\n\\nData Mesh Architectures\\n\\nIn this section we will look at the Data Mesh approach from several perspectives:\\n\\nSystem Context: Where a Data Mesh may sit in relation to other systems within an enterprise.\\n\\nMesh Context: This context will outline typical components and resources that are shared across the mesh, and where \"data products\" fit.\\n\\nData Product Context: Defining a Data Product, its core components and how various users can interact with them.\\n\\nInfrastructure Abstraction Context: Defining the abstractions that allow teams to use templates to build data products, saving time and reducing the need for extensive cloud expertise.\\n\\nSystem Context\\n\\nLike a standard modern data warehouse, a Data Mesh is intended to play a fundamental role in the information architecture of an enterprise. Ingesting data from operational systems into data products, performing the relevant transformations and analysis, and then supporting decision intelligence across the organization.\\n\\nWhen implemented correctly, Data Mesh should enable teams to easily and independently onboard data, facilitate the analytical enrichment through creation of new data products. A Data Mesh should also make it easier for data consumers to discover, trust and consume the data product. A Data Mesh should be easily governed and administered through centralized policy and governance controls.\\n\\nMesh Context', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_6', 'chunkContent': 'Mesh Context\\n\\nA Data Mesh architecture is intended to provide the following capabilities:\\n\\nDiscovery tools to quickly see available data and understand its content.\\n\\nData Lineage to understand the history and dependencies of data.\\n\\nComprehensive system and data monitoring to give a complete view of system health.\\n\\nData Product encapsulation for cost management.\\n\\nSecurity and Policy management.\\n\\nThese capabilities are enabled by services (or sets of services) that are typically across all data products. For example, security and policy management can be enforced and inherited across all data products that are created or onboarded into the mesh.\\n\\nFor Data Producers\\n\\nThe Data Mesh should enable Data Producers to easily build and publish data products. At the platform level it would require:\\n\\nSelf-service ingestion: Facilitate self-service onboarding of datasets into the mesh including ongoing deltas.\\n\\nData Infrastructure: Appropriate access to necessary services (storage and compute) and environments to build Data Products.\\n\\nData Distribution: Ability to publish and distribute Data Products including any necessary access controls.\\n\\nFor details, see the Data Product Context section.\\n\\nFor Administrators\\n\\nMonitoring and Observability\\n\\nEnable Monitoring and Observability (M&O) across all data products in the mesh. We should structure M&O to operate at both mesh and data product levels.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_7', 'chunkContent': 'Mesh resources will have visibility of and be able to monitor the health of all assets within the mesh to support platform administrators and data stewards.\\n\\nCommonly used services that support M&O are Azure Monitor, Application insights and Log Analytics.\\n\\nServices that execute business logic and deal with data movement and data transformation are typically the concern of data products. for more information, see Data Product Context.\\n\\nGovernance Controls and Policy\\n\\nPolicies enforce behaviors and control the data products should adhere to. These policies are controlled at the mesh level.\\n\\nIn a Data Mesh architecture, policies can be used to:\\n\\nDescribe properties about a data product.\\n\\nEnforce data products to use Azure resources only in the allowed regions.\\n\\nRequire resources to send diagnostic logs to a Log Analytics workspace.\\n\\nMonitor and manage access to data products, including discovery.\\n\\nAzure policies are one way to ensure governance and can help in:\\n\\nProviding consistent standards across data products.\\n\\nEnsuring quality of data.\\n\\nEnsuring costs are managed.\\n\\nProhibiting certain actions on data product resources.\\n\\nEnforcing any other rules for the data product resources.\\n\\nSecurity Best Practices in A Data Mesh', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_8', 'chunkContent': 'Enforcing any other rules for the data product resources.\\n\\nSecurity Best Practices in A Data Mesh\\n\\nManaged Identities remove the overhead of managing secrets, access policies for secrets, connection strings and secret rotation. For this reason, it’s best to use Managed Identities with Azure role-based access control (RBAC) instead of Service Principals. Managed Identities also provide an access lifecycle where permission is automatically removed with any clean-up of a resource. Another advantage of using Managed Identities is that it limits exposure to leaking/publishing secrets to code repositories.\\n\\nCost Management\\n\\nCost management is the process of controlling the expenses associated with running the Data Mesh. The consumption cost of each Data Product, and the mesh resources, will contribute to the total cost to run the system. Tags can be used to group costs within a data domain, which may include one or more data products.\\n\\nComponents and data requirements will vary across Data products, meaning the cost of each will depend on several factors:\\n\\nThe number of resources associated.\\n\\nThe specific type of resources, and configurations.\\n\\nHow often the Data Product is utilized.\\n\\nFor a Data Mesh implemented in Azure, it’s recommended to:\\n\\nLogically separate each Data Product into isolated resource groups to enable admins to track their cost and monitor usage.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_9', 'chunkContent': \"Logically separate each Data Product into isolated resource groups to enable admins to track their cost and monitor usage.\\n\\nIntegrate Azure Advisor to present tailored cost recommendations such as oversized resource compared to its usage pattern.\\n\\nUse Budgets to configure alerts based on actual or forecasted cost so spend for the Data Product doesn't exceed a set limit.\\n\\nData Product Context\\n\\nIn the context of a Data Mesh, data is treated as a product - something that fulfills a business need, as opposed to an asset.\\n\\nA data product can include, among others, components like: APIs, data lakes, data warehouses, or machine learning models. The components of a data product depend on the needs and goals of the team responsible for it. Equally important are the needs of the users who will be accessing and using the data.\\n\\nIn general, a data product should be designed to be highly reliable, scalable, and easy to use. Clear documentation and support materials should accompany it to help users on data access and effective use of data. Data products should also be subjected to ongoing governance and oversight, as discussed in the Mesh Context section.\\n\\nBuilding a data product requires a data product owner and team that:\\n\\nHave domain knowledge and good understanding of data and processes around it.\\n\\nAre technically skilled to build and maintain data artifacts.\\n\\nAre responsible for data quality, relevance, and usefulness.\\n\\nDesigning a Data Product\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_10', 'chunkContent': \"Are responsible for data quality, relevance, and usefulness.\\n\\nDesigning a Data Product\\n\\nData Products should be designed around a business or domain context.\\n\\nA recent McKinsey's report mentions the creation a data operating model that treats data as a product. The report also highlights how data product owners can: provide easy and repeatable data solutions for various business challenges, reduce the time and cost of delivering new capabilities.\\n\\nIt's best practice to tie the product to one or more related business domains, following domain-driven design (see appendix). The team that builds and manages the data product should have the knowledge and the context to be able to produce a high-quality output.\\n\\nThe technical requirements for business domains may vary. For example, some require near real-time processing of sensor data, others may require infrastructure tuned to execute machine learning models. A distributed platform is preferred as opposed to a monolithic platform that centralizes capability or a design that forces business domains to conform to specific infrastructure. The approach outlined here allows data product owners to design data products according to their domains.\\n\\nInfrastructure Encapsulation\\n\\nTo support the business domains, a data product infrastructure can be considered a sub-architecture within a Data Mesh. It's recommended that the design should follow the dataOps best practices outlined in the Modern Data Warehouse section of this playbook.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_11', 'chunkContent': \"To ensure granularity of the management of data products, they should be logically separated. One method to accomplish this separation in Azure is to encapsulate data products into dedicated resource groups.\\n\\nInfrastructure encapsulation makes it easier for teams by:\\n\\nManaging security boundaries through Role-Based Access Control (RBAC) so that one team doesn't affect other team's resources.\\n\\nFlexibility to have CI/CD deployment scoped to the intended node without affecting other products.\\n\\nManaging a scalable governance model. For example, Product A policies may differ from Product B policies.\\n\\nSimplifying permission management of resources.\\n\\nSimplifying cost management for each product by selecting the right scope.\\n\\nData Product Components\\n\\nA data product will have common foundations such as orchestration, compute and storage. The specific implementation of these foundations will differ based on the domain’s requirements.\\n\\nIt should also contain all Infrastructure as Code (IaC) components to abstract, automate and manage the data product’s lifecycle.\\n\\nAn outline of orchestrators, compute and storage options, and Iac can be found in the Modern Data Warehouse section.\\n\\nData Contracts\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_12', 'chunkContent': \"An outline of orchestrators, compute and storage options, and Iac can be found in the Modern Data Warehouse section.\\n\\nData Contracts\\n\\nData Contracts are a code artifact that specifies rules for data entering a system, and a means to test against them. When designing a system, there are expectations on data: its structure, the low-level types, and in some cases specific rules on the values themselves (data quality measures). They can also be implemented against a data product’s output to ensure that any changes to the data product conform with the mesh’s expectations. Contracts can be expanded to include Service Level Objectives (SLOs) which can be measured using data product telemetry. Alerts can be triggered where Service Level Agreements (SLAs) aren't met.\\n\\nContracts should be versioned in the mesh repository. Both the live contracts and malformed data should be accessible through the data product’s API.\\n\\nValidating early in the pipeline is recommended. Validated data should be allowed through while malformed data should be quarantined. Alerts should be set on malformed data for remediation either manually or programmatically. Validation and alerting ensures that data adheres to a format that consumers expect while minimizing poor inputs for downstream analysis.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_13', 'chunkContent': \"Validation frameworks and rules can be integrated with Azure Monitor. Once integrated, it can report on the health of data within the mesh and alert product owners for remediation.\\n\\nTwo examples of python frameworks that provide this capability are:\\n\\nGreat Expectations is a validation, documentation and profiling tool. It can be configured to run validation rules over data periodically and produce reports based on batches. It has many advanced features such as geo rules, advanced statistical tests and bivariate checks.\\n\\nCerberus provides powerful yet simple and lightweight data validation. It's designed to be easily extensible, allowing for custom validations.\\n\\nData Virtualization\\n\\nData Virtualization abstracts retrieval and manipulation of data from source systems. If the output of a data product is a data store, it can be virtualized to a central component of the mesh for simplicity. A different option needs to be considered for an engine or analytic output. Data virtualization could be in addition to/or instead of serving data via an API. In side a Data Mesh, virtualization depends on how a product is implemented.\\n\\nAn example of data virtualization is using Azure Synapse Link to access data from products across the mesh in Synapse Studio. Here, data product owners and data consumers can use commonplace skills (Examples: SQL, spark) to access data, without complex data movement.\\n\\nData Product Health\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_14', 'chunkContent': \"Data Product Health\\n\\nAlthough the mesh context manages most of the system-level health and monitoring, individual data products must have mechanisms to capture and make this data available to users and the broader mesh.\\n\\nData product health should be defined by adherence to SLAs and component uptime. Data product health will vary by data product and should form part of the design process. Providing a clear picture of data product health allows administrators to be alerted when issues arise. It also helps data consumers to understand if there are limitations on their data as a result. The importance of monitoring has already been covered in depth in Monitoring and Logging.\\n\\nData quality reports should be accessible by consumers using the data product API.\\n\\nIn Azure, data validation records can be sent to Application Insights to be surfaced in Azure Monitor. Alerts can be sent to administrator when service level expectations aren't being met. Data stewards can then contact data product owners to discuss anomalies in the feed.\\n\\nInfrastructure Abstraction Context\\n\\nAs discussed in the Data Product Context section, all data products will have some common requirements (or at least themes across requirements). In addition, data products often conform to organizational policies set by the mesh as discussed in the Mesh Context section.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_15', 'chunkContent': \"Administrators can create a Data Product Template (DPT) using IaC tooling such as Bicep or Terraform. Data product owners and developers can use these DPTs to limit the time spent on infrastructure setup and accelerate development. DPTs also allow for mesh level policies and practices such as: using Managed Identities to be implemented at point of creation, limiting risks, and enhancing compliance. DPTs can be extended to include network connectivity to other data products and external sources by default, where required and permitted.\\n\\nFurther, automation of infrastructure and application code can be accomplished using services like Azure Pipelines or GitHub Actions that offer a consistent approach to CI/CD. This allows automatic deployment of infrastructure code changes while ensuring best practices are in place such as code quality, security policies validation.\\n\\nFurther Reading\\n\\nModern Data Warehouse\\n\\nWhat's a Data Mesh?\\n\\nHow to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh\\n\\nData Mesh Principles and Logical Architecture\\n\\nAppendix\\n\\nDomain-driven Design\\n\\nDomain-driven design is a design approach that focuses on understanding and addressing the specific needs and requirements of a particular business domain or industry. The approach involves working closely with domain experts and stakeholders to identify and prioritize the key problems and opportunities within the domain. It also involves designing solutions that are tailored to the specific needs and constraints of the domain.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk90_16', 'chunkContent': 'The goal of domain-driven design is to create more effective and efficient solutions by aligning them closely with the underlying business and technical context in which they will be used. This design approach can involve:\\n\\nDesigning custom solutions or adapting existing solutions to better fit the domain.\\n\\nDeveloping domain-specific languages or frameworks.\\n\\nTools to support solution development and use.\\n\\nFor more information, here is a useful article from Microsoft Best Practice Introduction to Domain-Driven-Design', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\data-mesh\\\\index.md'}, {'chunkId': 'chunk91_0', 'chunkContent': 'Enterprise Data Sharing\\n\\nDefinition\\n\\nData is the foundation for every organization to thrive, optimize and profit within its own domain of expertise, therefore there is a common need to be able to share data in a secure and governed way to internal audiences, external customers, and partners.\\n\\nData sharing is defined as the combination of a well-defined process and technical capabilities with the aim of making the right data available to interested parties. It allows organizations to define what, how, when and with whom to share their data and execute on it efficiently.\\n\\nThe Enterprise dimension of Data Sharing entails that this set of capabilities and processes need to conform with principles that assure the robust, secure and resilient behavior of the end-to-end solution for both Data providers and Data consumers.\\n\\nEnterprise Data Sharing incorporates a common set of scenarios that can be applied, regardless of the organization size and industry. They can be summarized as:\\n\\nData Consolidation is considered the foundational scenario.\\n\\nData Distribution aims to consolidate and focus on distributing data for larger audiences.\\n\\nData Brokerage extends the previous scenario to create new data products and improve data discoverability.\\n\\nData Escrow requires an independent and trusted entity to supervise the data being shared.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_1', 'chunkContent': \"Data Escrow requires an independent and trusted entity to supervise the data being shared.\\n\\nThese scenarios can be considered as incremental evolutionary stages of an organization's approach to data sharing.\\nScenarios that aim to sell data to customers, partners or individuals are known as Enterprise Data Sharing Monetization Scenarios. Data Brokerage and Data Escrow belong to this category.\\n\\nCurrently, there is no end-to-end unique capability that implements all the necessary components to bring such solutions to life. However, the solution can be broken down into individual capabilities that are modular and can be integrated together depending on the customer scenario.\\n\\nKey Principles\\n\\nThe present section aims to highlight what aspects are normally addressed when creating an Enterprise Data Sharing solution. The principles are the north star guidance to achieve a secure, resilient, and robust end-to-end solution for both Data providers and Data consumers.\\n\\nAn Enterprise Data Sharing solution can address the totality, or a subset of the key principles listed below, and each principle implementation can be done differently in accordance with different requirements, technology, and customer preferences.\\n\\nSecurity\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_2', 'chunkContent': \"Security\\n\\nFor the Data providers, the data needs to be stored and made available within the organization's security guard rails to internal and external audiences using security mechanisms like authentication, authorization, and access management. For data consumers, it is imperative that the data they have been granted access to is subjected to additional measures of protection and security, in compliance with the security paradigms and regulations set forth by their respective organizations. This is particularly critical when the data is copied or replicated into the consumer environment..\\n\\nData Privacy\\n\\nData needs to be correctly classified for access management and data privacy. Data protection mechanisms like RLS (Row Level Security), CLS (Column Level Security), control and data plane permission management, data anonymization/obfuscation can be leveraged to achieve this principle.\\n\\nAutomation\\n\\nManual interventions in the end-to-end process should be minimal or preferably non-existent.\\nFor more details, please refer to the Operability principle.\\n\\nExtensibility\\n\\nThe possibility of extending the Data Sharing solution with a different set of Data Sharing requirements with no major changes required in the end-to-end solution. A second aspect is to provide the possibility for the Data Consumers to extend and enrich the data received with other relevant data sources.\\n\\nData Discoverability\\n\\nHaving a centralized and up to date Data Catalog that reflects new and updated datasets and respective metadata to improve the ability of the users to find the relevant information.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_3', 'chunkContent': \"Upon sharing the data and in situations where the data provider also provides the infrastructure for exploration, it may become necessary for the provider to reset and reinforce information at the catalog and data exploration levels to ensure the accurate discoverability of the data, as well as consistency with the exploration tools.\\n\\nVersioning and Schema Evolution\\n\\nThe ability of the solution to absorb and deal with schema changes and reflect and share those changes into different co-existing versions if required.\\n\\nData Quality\\n\\nFrom the data consumer's perspective, the datasets to be received should have been through some level of curation and these are expected to be aligned with the legal agreement and designated entitlements.\\n\\nFrom the provider side, making that level of curation before the datasets are ready to be shared is paramount to guaranteeing high quality of the data.\\n\\nReliability\\n\\nWhen sharing data through the method of copying, it is essential to establish a reliable refresh mechanism that abides by the agreed service level agreement (SLA), ensuring that the frequency of data updates from the source to the destination is properly maintained.\\n\\nWhen using sharing by reference, the availability of the data share endpoint needs to be in line with the agreed SLA.\\n\\nResiliency\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_4', 'chunkContent': 'Resiliency\\n\\nThe data sharing solution should be able to deal with failures at any given point in such a manner that the whole process is idempotent i.e., it can restart or resume the process and attain the exact same result as if there were no problems, within the agreed SLA.\\n\\nObservability\\n\\nThe solution incorporates observability mechanisms in key pieces of the solution, so alerts can be triggered and acted upon when problem occurs with the main goal of fixing the problem and meeting the agreed SLAs.\\n\\nTesting\\n\\nThere are several aspects of testing that should be addressed:\\n\\nTesting the code base (unit testing)\\n\\nTesting the orchestration of the end-to-end solution (integration testing)\\n\\nPerformance testing might be necessary in scenarios that deal with large amounts of data volumes. For example, the data provider should make sure that the shared datasets can be joined on the relevant columns for the typical usage of the data consumers and guarantee that the results are back in a timely fashion.\\n\\nIn situations where the data provider also furnishes the infrastructure for data exploration, it is also crucial to conduct thorough testing of the deployment mechanism.\\n\\nOperability', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_5', 'chunkContent': 'Operability\\n\\nRelated with the automation aspect, all the evolution on the data and the code base of the Data Sharing solution (data updates, deployment, and CI/CD) should aim to be fully automated with minimal or no manual intervention. On the Data provider side, development, staging and production environments should be maintained.\\n\\nWhen a new data consumer enrolls to receive datasets offered by the data provider, the onboarding process should be automated, repeatable, and fully integrated with the solution operations.\\n\\nMonetization\\n\\nWhen the data consumer audience is an external audience, the Data Sharing process becomes a revenue source for the Data provider. In such cases, there needs to be a legal agreement between the two parties involved as well as an automated setup and charging mechanisms that are made available through a Data Marketplace.\\n\\nAudience simplification\\n\\nWhen a Data provider shares similar data for both internal and external consumers there should be no distinction between internal and external audiences except for, in some cases, financial transaction. This principle ensures that compliance and auditability are consistent across all audiences and simplifies the management by avoiding exceptions.\\n\\nMulti-Tenancy\\n\\nSharing data between different tenants is a common requirement.\\n\\nEnterprise Data Sharing Functional Capabilities\\n\\nData Marketplace', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_6', 'chunkContent': 'Sharing data between different tenants is a common requirement.\\n\\nEnterprise Data Sharing Functional Capabilities\\n\\nData Marketplace\\n\\nA Data Marketplace is the first pillar on which monetization scenarios are based.\\nThe principle behind a data marketplace foresees a mapping between the available Data Shares/datasets to the offerings available in the Marketplace. Once the provider and consumer agree on the legal terms and the data consumer buys the specific offer, the data provider authorizes the data consumer subscription to access the Managed application that has been published on the Marketplace.\\n\\nThe data consumer can now access and use that application to deploy the data exploration infrastructure while the provider can trigger the data share invitations and data snapshots automation upon the deployment success.\\n\\nData Sharing\\n\\nThe mechanics of Data Sharing\\n\\nThe Data Sharing mechanism is the core function that makes available the requested/bought datasets in a secured manner from the provider to the consumer.\\n\\nThere is an invitation triggered by the provider to the consumer who can accept it.\\nThere are two main patterns to sharing the data:\\n\\nShare by Copy provider makes a copy/snapshot of the data at a given point in time and that is what is shared with the consumer. It will run on a defined schedule, and can be full or incremental.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_7', 'chunkContent': 'Share by Reference the data is shared as-is, the consumer will gain access for a given time frame to the data. The data might change during the time period when it is available to the consumer.\\n\\nThe datasets can be shared in different formats. The format needs to be agreed upfront and different file formats might be part of different offerings.\\n\\nThe simplest Data Sharing solution could be implemented just as above when the Data exploration infrastructure is already in place and doesn’t require to be deployed from the Data provider perspective. In this case consumers just need to get access to the datasets from the Data provider and proceed on their own unique exploration environments.\\n\\nThis process can be manual or fully automated for both provider and consumer and there is a sample that implements this behavior using Azure Data Share using full data snapshots and its available here.\\n\\nData Versioning and Schema evolution in Data Sharing\\n\\nThe responsibility of managing data versioning and schema evolution lies with the data provider, and it is important to establish a clear agreement with data consumers regarding how many versions will be shared, such as whether to provide only the latest version or the last two major versions along with the latest one. This definition should be included in the agreement between the data provider and consumers.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_8', 'chunkContent': 'In addition, it is advisable to include a notification mechanism to alert data consumers when a particular version becomes deprecated. Furthermore, if a Data Catalog is utilized in the Data exploration infrastructure, the deprecation information should be updated in the Catalog as part of the data update process.\\n\\nThere might be major and minor versions of the datasets and there might be breaking changes to the schema evolution. In either case, an approach can be to maintain the necessary versions in different containers or folders (v1, v2, v2.1) and to have a metadata file file per version that describes the schema that that version is conforming with.\\n\\nData Exploration\\n\\nThe third functional capability pillar for Enterprise Data Sharing is the Data Exploration layer.\\nIt is a necessary layer, but is not mandatorily delivered by the Data provider. The Data consumer might just be interested in the datasets from the data provider. To perform data exploration, they may choose to use their existing in-house capabilities.\\n\\nHowever, there are many cases where the Data consumer is not interested in managing the necessary data platform infrastructure to host the data or they don’t have that knowledge. Rather, they prefer an out-of-the-box full integrated solution to perform the exploration and the data provider can offer that capability as part of the offering.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_9', 'chunkContent': 'A second situation might happen when the Data consumer have the necessary infrastructure, services, and knowledge to host and explore the data, but the Data provider is providing complex transformation logic, configuration, and integration enhancements that the Data consumer would like to take advantage of.\\n\\nIn the image below you can find an example of an exploration zone deployed by a Data provider into a Data consumer subscription. The Data provider offers a deep integration between diverse Data, Monitoring and other Services that allow for consistency and freshness of the data and aligned with the metadata.\\n\\nThe format of the files shared needs to be supported by the exploration infrastructure provided. In case of using in-house infrastructure to perform the exploration, the data consumer needs to guarantee that the file format being received is aligned with the exploration tools that the organization uses.\\n\\nEnterprise Data Sharing Architectures\\n\\nThe goal of Enterprise Data Sharing architecture is to represent how to share Data across different audiences (internal or external) in a secure, reliable, and robust way with the final goal of making available, usable data that can be further used to:\\n\\ncreate business relevant data products or\\n\\nmonetizing the data\\nusing Enterprise Data sharing capabilities.\\n\\nMonetization Scenarios Architecture\\n\\nA possible Enterprise Data Sharing architecture for Monetization scenarios is illustrated below:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_10', 'chunkContent': 'Monetization Scenarios Architecture\\n\\nA possible Enterprise Data Sharing architecture for Monetization scenarios is illustrated below:\\n\\nThis architecture is well aligned with the key principles for an Enterprise Data Sharing solution and is designed to serve external audiences using the Marketplace to help onboard different data consumers. As explained before, there are three main areas that underpins this architecture: Data marketplace, Data Sharing and Data Exploration that can be modularized and executed together or separately.\\n\\nRegarding the contractual agreement, there are two options available:\\n\\nStep 1 needs to be negotiated and closed as a first step. In this case, the Managed App is made available to the consumer subscription.\\n\\nThe consumer makes the purchase directly on the Marketplace.\\n\\nInternal Distribution Scenarios Architecture\\n\\nThe Marketplace mechanism is a great asset to sharing assets externally and to monetize data. However, when sharing data globally at scale within the own organization (internal data sharing) is the use case at hand, a similar approach can be followed by publishing a Managed application into an internal Service Catalog.\\n\\nIn this case, monetization is not a requirement, but the data provider can use cross-charge mechanisms to distribute the cost among internal business units.\\n\\nA possible representation of such scenario is described below:\\n\\nThere are three main areas that underpin this architecture: Internal Service Catalog, Data Sharing and Data Exploration that can be modularized and executed together or separately.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_11', 'chunkContent': 'Hybrid Scenarios Considerations\\n\\nIn certain situations the Data providers might require to provide an hybrid scenario: a combination of internal and external data sharing on the same assets or domains. In this particular case, is advised to follow the Audience Simplification principle. By unifying the underlying processes, exceptions are avoided and consistency and simplicity are enabled on the following procedures:\\n\\nmonetization: by norm internal scenarios use charge back mechanisms, but monetizing the data internally is also possible.\\n\\nauditing processes: auditing over the same infrastructure and operations for both internal and external audiences is advised.\\n\\ncompliance checks: same as previous.\\n\\ninfrastructure management: there is one infrastructure and unified management and governance tools and procedures that avoid exceptions and help to accelerate deployment, testing, troubleshooting and root cause analysis.\\n\\nIt is important to highlight that the ramification of possible hybrid scenarios is large and there might be specific dependencies that need to be adapted and addressed differently depending on the requirements.\\n\\nIntegration with other Data Architectures\\n\\nAn interesting aspect to mention is the possible integration with other existing Data architectures for both Data providers and Data consumers.\\n\\nFor the Data provider, the integration points are outputs from Data integration and transformation processes. These can be:\\n\\ncurated datasets coming from gold or silver layers from a Modern Data Warehouse architecture or,\\n\\nfinal Data product from an internal Data Mesh implementation or,', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk91_12', 'chunkContent': 'final Data product from an internal Data Mesh implementation or,\\n\\npre-existing datasets as Data marts or Data vaults  that are created by other Data architectures.\\n\\nData consumer’s integration points can be:\\n\\nshared datasets landing in the bronze layer of a Modern Data Warehouse architecture, where they can be further integrated with in-house data.\\n\\ndata sets feeding into a specific node of an Internal Data Mesh implementation for further Data products creation.\\n\\nTech Specific Samples\\n\\nWill be available soon:Data Marketplace\\n\\nData Sharing mechanism\\n\\nData Exploration\\n\\nFurther Reading\\n\\nDataOps: Modern Data Warehouse\\n\\nDataOps: Data Mesh\\n\\nDataOps: Data Sharing\\n\\nAzure Data Share\\n\\nAzure Storage in-place data sharing with Microsoft Purview\\n\\nAzure Managed Applications\\n\\nPerformance and Scalability for Blob Storage', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\enterprise-data-sharing\\\\index.md'}, {'chunkId': 'chunk92_0', 'chunkContent': 'rings:\\n  - public\\n\\nModern Data Warehouse\\n\\nThe Modern Data Warehouse (MDW) is a common architectural pattern to build analytical data pipelines in a cloud-first environment. The MDW pattern is foundational to enable advanced analytical workloads such as machine learning (ML) alongside traditional ones such as business intelligence (BI).\\n\\nTraditional Data Warehouse vs Modern Data Warehouse\\n\\nThe modern data warehouse unlocks advanced capabilities related to analytics that would otherwise be difficult to achieve with traditional data warehousing architectures. In a traditional data warehouse, data pipelines and the relevant dimensional model (star schema) are built based on known reporting requirements. So, analytics requirements on a traditional data warehouse can be achieved using a top-down (deductive) approach. For advanced analytical requirements in machine learning use cases, reporting outputs are unknown at the start. This requires an iterative exploratory analysis phase by data scientists. This phase helps uncover insights in raw datasets and their relevance specific to the outcome. Hence, implementation of advanced analytics can be described as a bottom-up approach (inductive).\\n\\nThe following diagram shows the different types of analytics that can be done using both traditional and modern data warehouses:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\index.md'}, {'chunkId': 'chunk92_1', 'chunkContent': 'The following diagram shows the different types of analytics that can be done using both traditional and modern data warehouses:\\n\\nCompared to a traditional RDBMS data warehouse, a data lake is the primary means of data storage in an MDW. Data lakes support storage of both structured and unstructured datasets, which is required for advanced analytics use cases. Data lake also enables schema-on-read access, which is crucial for exploratory analysis. The RDBMS data warehouse is still an important component of the MDW architecture but is now used as a serving layer to enable traditional business intelligence reporting.\\n\\nThe mechanism of loading data is also different. While extract-transform-load (ETL) (SSIS is an example) is preferred in traditional data warehousing, extract-load-transform (ELT) is preferred in MDW. In MDW with ELT, data is first ingested into the data lake as-is and then transformed.\\n\\nOverview of the Modern Data Warehouse\\n\\nThe following are the four stages in an MDW architecture:\\n\\nIngest–Different data sources are ingested and persisted into the data lake.\\n\\nTransform–Data is validated and transformed into a pre-determined schema.\\n\\nModel–Data is then modeled into a form optimized for consumption (i.e., Star schema).\\n\\nServe–Data is exposed for consumption. It includes enabling visualization and analysis by end users.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\index.md'}, {'chunkId': 'chunk92_2', 'chunkContent': 'Serve–Data is exposed for consumption. It includes enabling visualization and analysis by end users.\\n\\nThe following functional components of the architecture enables these four stages:\\n\\nStorage–The main concern for this component is to serve as the primary storage for the data lake and for the relevant serving layers. For details, see DataOps: Data Lake section.\\n\\nCompute–Includes compute for the ingest, transform, and serve stages. Common data computing frameworks include: Apache Spark (available through Azure Synapse Spark pools or Azure Databricks), ADF data flows and Azure Synapse SQL dedicated pools (particularly for the serving layer).\\n\\nOrchestrator–Responsible for end-to-end orchestration of the data pipeline. Azure Data Factory and Azure Synapse data pipelines are common data orchestrators.\\n\\nThe non-functional components that need to be considered are:\\n\\nSecurity - Includes platform, application, and data security. For more information, see DataOps: Security.\\n\\nData Governance - Ensures datasets are governed and cataloged along with their lineage captured. For more information, see DataOps: Data Catalog and DataOps: Data Lineage.\\n\\nOperability (DevOps) - Refers to DevOps practices to ensure efficient operations of the data system, including CI/CD, automated testing, and monitoring. For more information, see DataOps: Data Pipeline Operability and DataOps: DevOps for Data.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\index.md'}, {'chunkId': 'chunk92_3', 'chunkContent': \"Logical Architecture\\n\\nThe below diagram shows the logical MDW architecture along with its functional components:\\n\\nThe below diagram shows the logical MDW architecture with corresponding Azure services. The list of Azure services is non-exhaustive.\\n\\nHere are a few samples of implementing a MDW architecture:\\n\\nUsing Azure Databricks and Azure Data Factory:\\n\\nUsing Azure Synapse:\\n\\nLimitations\\n\\nThe MDW architectural principles are versatile in building analytical data pipelines in a cloud-first environment. However, they don't offer comprehensive guidance in the following areas:\\n\\nEnterprise-wide data platform architecture.\\n\\nEnterprise data governance.\\n\\nFederated data architectures (Example: Data Mesh).\\n\\nData sharing.\\n\\nOn-premises data workloads.\\n\\nTransactional data workloads.\\n\\nDetailed guidance in pure streaming and event-driven data pipeline architectures.\\n\\nModern Data Warehouse Best Practices\\n\\nThe following section elaborates more on the specific stages and components within the MDW architecture along with key considerations and best practices.\\n\\nData Lake\\n\\nA primary component of MDW architecture is a data lake storage that acts as the source of truth of different datasets. It's recommended to use ADLS Gen2 for the data lake storage. For more information on storage, see DataOps: Storage.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\index.md'}, {'chunkId': 'chunk92_4', 'chunkContent': \"It's a best practice to logically divide the data lake into multiple zones corresponding to increasing levels of data quality. Data lake zones typically map directly to different data ingestion, transformation and serving outputs of your data pipeline activities. At least three zones (Bronze/Silver/Gold or Raw/Enriched/Curated) are recommended:\\n\\nRaw Layer - Datasets are kept as similar to the source dataset as possible with little to no transformations applied. Raw datasets give the ability to replay data pipelines if there are production issues. It also means that data pipelines should be designed to be replayable and idempotent.\\n\\nEnriched - Datasets have data validation applied and standardized to a common type, format and schema. It's common to use parquet or delta as the storage format for this layer.\\n\\nCurated - Datasets have been optimized for consumption in the form of Data Product. It's common to have these datasets with dimensional modeling applied and eventually loaded into a data warehouse. This data lake zone forms part of the Serving layer in the MDW architecture. The common storage format for this layer are parquet and delta.\\n\\nFor detailed information, see CAF: Data Lake Zones and Containers.\\n\\nIngestion\\n\\nFirst, identify which data sources need to be ingested. For each of the identified data sources, determine:\\n\\nLocation.\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\index.md'}, {'chunkId': 'chunk92_5', 'chunkContent': 'First, identify which data sources need to be ingested. For each of the identified data sources, determine:\\n\\nLocation.\\n\\nSource system (FTP, storage account, SAP, SQL Server, etc.).\\n\\nData format, if applicable (CSV, Parquet, Avro) and corresponding closest matching destination format.\\n\\nExpected volume and velocity (frequency, interval).\\n\\nSecond, identify the ingestion mechanism i.e., batch or streaming. The ingestion mechanism would determine appropriate technologies to be used. Azure Data Factory or Azure Synapse data pipelines are common services for ingesting batch datasets. Both provide an integration runtime (IR) for ingesting datasets on different networking environments (on-premises to cloud). While Azure Event Hubs and Azure IoT Hub are common ingestion points for streaming when paired with a streaming service such as Azure Steam Analytics. For more information, see: DataOps: Batch and Stream Ingestion.\\n\\nGenerally, the ingestion pipeline will require a one-time historical load and a recurring delta load. For the latter, determine how to identify data deltas for each run. Two commonly used methods include change data capture and utilizing a dataset attribute to identify modified records. An external state store may be needed to keep track of the last loaded dataset in the form of a watermark table.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\index.md'}, {'chunkId': 'chunk92_6', 'chunkContent': \"Consider using a metadata-driven approach for large-scale ingestion use cases such as loading multiple datasets. For more information, see DataOps: Config-driven Data Pipelines and ADF Metadata-driven copy jobs.\\n\\nAt times, data pre-processing is required after data ingestion and before data transformation. In certain cases where large number of huge files (in gigabytes or more) are being ingested, pre-processing steps can get complicated. An industry-specific example is the processing of data in ROS format.  ROS format requires the extraction of bagged files and metadata generation for each bag file to make it available for further processing.\\n\\nFor handing such scenarios, Azure Batch can be a great compute option. See DataOps: Pre-processing with Azure Batch for more details.\\n\\nTransform\\n\\nFollowing data ingestion into Raw zone of a data lake, the data needs to be validated and then transformed into a standard format and schema in the Enriched zone. Data validations should be performed at this point. It's best practice to maintain a malformed record store to track records that failed validations to help with debugging issues.\\n\\nCommon services used at this stage include Azure Synapse Spark pools, Azure Databricks and data flows (ADF/Synapse).\\n\\nFor more information on data transformation:\\n\\nDataOps: Batch Processing\\n\\nDataOps: Stream Processing\\n\\nFor more information on data validation:\", 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\index.md'}, {'chunkId': 'chunk92_7', 'chunkContent': 'DataOps: Batch Processing\\n\\nDataOps: Stream Processing\\n\\nFor more information on data validation:\\n\\nDataOps: Continuous Testing (Test for Data)\\n\\nModel\\n\\nData modeling goes hand-in-hand with data transformation. Here, data modeling refers to both the standardized data model in the enriched zone and the consumer-optimized data model in the curated zone. While related, both data models serve fundamentally different purposes. The goal of the enriched zone data model is to provide a common data model without a specific business use case in mind. The priority is completeness, data standardization, validity and uniformity across disparate sources from the raw zone. On the other hand, datasets in the curated zone are designed to be easily consumable. The steps may include data filtering, aggregations, and use case-specific transformations depending on the specific consumer of the dataset. The curated zone therefore may contain many derivative data products produced from datasets in the enriched zone.\\n\\nFor more information on Data Modeling:\\n\\nDataOps: Data Modeling\\n\\nServe\\n\\nThe serving layer of the architecture functions primarily to serve the data to downstream consumers. The curated zone in the data lake forms part of the serving layer. Other components such as a data warehouse, an API, or dashboard are also commonly used. Depending on the number of consumers and their requirements, same datasets may use multiple serving mechanisms.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\index.md'}, {'chunkId': 'chunk92_8', 'chunkContent': 'Common services used at serve stage include Azure Synapse dedicated SQL pool, Azure SQL, and Microsoft Power BI.\\n\\nTechnical Samples\\n\\nThe following are technical samples showcasing concrete implementations of the Modern Data Warehouse pattern:\\n\\nMDW repo: Parking Sensor (Azure Synapse)\\n\\nMDW repo: Parking Sensor (Azure Databricks and Azure Data Factory)\\n\\nFurther Reading\\n\\nAzure Architecture Center: DataOps for the Modern Data Warehouse\\n\\nDatabricks: ETL using Azure Databricks (Tutorial)\\n\\nDataOps: Data Pipeline Operability\\n\\nDataOps: DevOps for Data', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\index.md'}, {'chunkId': 'chunk93_0', 'chunkContent': 'rings:\\n  - public\\n\\nModern Data Warehouse Backlog Guidance\\n\\nObjectives\\n\\nThis content aims to provide guidance on how to support the creation of a foundational backlog based on the Parking Sensor sample. The backlog can be extended to different services and other requirements that target other use cases.\\n\\nThe aim is NOT to create a detailed task list, but a high-level breakdown of the main areas to target. These areas be extended to other areas that are not implemented in the Modern Data Warehouse main sample.\\n\\nThe elements that are included on the Parking Sensor sample are identified with a sample tag whilst the others are tagged with extension referring to possible extensions.\\n\\nProject Scope\\n\\nDefine the scope of your project from the below options:\\n\\nLocation\\n\\nsample - Cloud\\n\\nextension - Edge computing\\n\\nextension - Cloud and edge computing\\n\\nMode\\n\\nsample - Batch\\n\\nextension - NRT (Near Real Time)\\n\\nextension - Batch and NRT (Near Real Time)\\n\\nThe scope of the sample is Cloud + Batch, however there is an opportunity to extend to other fields applying similar techniques and concepts.\\nThe areas that concern the sample can be listed as below, and these areas can be extended as identified by the ext tag.\\n\\nOperational Foundation\\n\\nArchitectural Components\\n\\nGoal', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_1', 'chunkContent': 'Operational Foundation\\n\\nArchitectural Components\\n\\nGoal\\n\\nThe goal of the Architectural Components area in the sample solution is to mirror the necessary architecture components into the deployment for the scope defined. The architecture should include all or a subset of the components described in the following sections.\\n\\nConsiderations\\n\\nData lake Storage Service aspects to consider:\\n\\nsample/extension - Plan for the storage of the Data Lake by considering the structure your lake will have to adopt.\\n\\nsample - Need of one or more Storage accounts for the Data Lake function. The split into different accounts or containers might depend on the environment, security, cross-charge, or organizational requirements.\\n\\nextension - Define the partition strategy in the Data Lake that most benefits the usage of the data in the use case at hand.\\n\\nsample/extension - Define what is the default file format that is going to be used to ingest the data and land in the Data Lake (e.g.: parquet, delta, json, etc.).\\n\\nsample - Consider the need of a storage account for internal use of some services (Azure Synapse for example).\\n\\nDatabases\\n\\nOther databases can be considered in the process, either as a data source or as a Data Warehouse database to serve the final reporting layer.\\n\\nsample - Azure Synapse SQL Dedicated Pool, used to feed the reporting layer.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_2', 'chunkContent': 'sample - Azure Synapse SQL Dedicated Pool, used to feed the reporting layer.\\n\\nextension - Any other database (operational, historical, business, etc.) that needs to be ingested to contextualize or transform the data that lives in the data lake.\\n\\nextension - Use another database that would be used in the serving layer for reporting purposes.\\n\\nData Governance can be composed of different tools and integrations:\\n\\nextension - Microsoft Purview service and artifacts like for example: Glossary initial import, Metadata model assets initial import, Custom entities initial import.\\n\\nextension - Other Data Catalogs might or might not include integration/federation with Microsoft Purview.\\n\\nData Transformation\\n\\nsample - Azure Synapse service (Synapse workspace and selected pools), and artifacts (Data sources, Linked Services, Scripts, Pipelines, Notebooks, Triggers, Packages (wheel/jar)).\\n\\nsample - Azure Databricks service (Workspace, cluster) and artifacts (Notebooks, Scripts, Packages).\\n\\nsample - Azure Data Factory and Azure Synapse pipelines, can also be used as a mechanism to perform data transformation beyond the notebook calls.\\n\\nextension - Spark Job definitions could be included in the artifact lists if there is a requirement or need for it.\\n\\nData pipeline Orchestrator', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_3', 'chunkContent': 'Data pipeline Orchestrator\\n\\nsample - Azure Data Factory service\\nsample - Azure Synapse pipelines\\nextension - ADB - Jobs\\nextension - ADB - Delta Live Tables\\n\\nObservability\\n\\nsample - Infrastructure observability\\n\\nMonitoring default metrics with Azure Monitor\\nSynapse: pipeline and pool metrics\\n\\nsample - Data pipeline observability\\n\\nMore observability can be implemented using the integration of certain libraries like Great Expectations and App Insights at the notebook level.\\n\\nSecret Management\\n\\nsample - Azure Key vault, a secret management service, is necessary to store all the sensitive information at deployment time that can be later accessed for the CI/CD process or for the data pipelines functionality.\\n\\nsample - In order to simplify the security layer, Integrated Security should be considered whenever possible.\\n\\nServing Data Layer\\n\\nextension - Power BI service (chances are that PBI is already enabled in the organization).\\n\\nsample - Synapse artifacts like scripts or exploration notebooks.\\n\\nsample - DataBricks scripts or exploration notebooks.\\n\\nextension - Third-party tools as required per the Customer.\\n\\nSecurity\\n\\nextension - Collect Security Customer requirements. Be sure to cover aspects like:\\n\\nAccess control requirements.\\n\\nPrivacy requirements (or other compliance requirements).\\n\\nNetwork requirements.\\n\\nAfter collecting all security requirements, consider each architecture component and:', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_4', 'chunkContent': 'Privacy requirements (or other compliance requirements).\\n\\nNetwork requirements.\\n\\nAfter collecting all security requirements, consider each architecture component and:\\n\\nsample/extension - Implement access management automation through a combination of control plane, ACLs, and data plane (where appropriate) of each.\\n    Note: in sample, just some RBAC roles are assigned in the context of the user that deploys the sample, but ACLs are not part of the implementation.\\n\\nextension - Implement vNet integration automation when network isolation of the components is required.\\n\\nextension - Implement specific data protection mechanisms when required like data encryption or data obfuscation.\\n\\nServices integration\\n\\nConsider how to integrate the deployed services in the IaC process.\\nExamples might include:\\n\\nextension - Azure Synapse and Microsoft Purview integration\\n\\nextension - Azure Synapse and Power BI integration\\n\\nSuccess Criteria\\n\\nSuccessfully deploy all the components from the architecture and initial artifacts in an automated way and end-2-end.\\n\\nCover all the environments needed - DEV, STG, PROD.\\n\\nSuccessfully configure integration points between services in an automated way.\\n\\nDesign the Data Lake Structure\\n\\nGoal\\n\\nIn modern Data Warehouse architectures,  data coming from other systems can land into a Data Lake within a pre-defined structure.\\n\\nConsiderations', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_5', 'chunkContent': 'Considerations\\n\\nOrganize your data lake, according to these considerations:\\n\\nsample - Define how many layers you need in the medallion architecture (normally three layers: Bronze, Silver, and Gold - but it needs to fit your particular use case. Alternative zones can be used that might not need any transformation or pre-processing before they are used on the reporting logic).\\n\\nextension - Explore the data formats and expected usage to decide which partitioning strategies to explore.\\n\\nextension - Explore privacy and security requirements (CLS, RLS, Access Management, Control and Data Plane) and map the data lake design to the security need when required.\\n\\nextension - Explore Data Quality requirements that might be impacted by the Data Lake design, like Data duplication.\\n\\nSuccess Criteria\\n\\nBenchmark ingestion and query times based on the selected partitioning strategies (or without it).\\n\\nAgree on a final design for the data lake.\\n\\nSuccessfully integrate the data lake initial deployment and configurations on the IaC process following the design decisions and structure made for the current use case.\\n\\nDesign the Data Warehouse structure\\n\\nGoal', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_6', 'chunkContent': 'Design the Data Warehouse structure\\n\\nGoal\\n\\nIn a modern Data Warehouse architecture, data coming from other systems can land into a Data Lake that needs to be well structured. As the data progresses through the layers, it might be required that it finally lands on a classic Data Warehouse. The Data Warehouse can be used as a serving layer for reporting or custom applications beyond the gold layer.\\nThe Data Warehouse needs to be designed to receive the transformed data into a required schema (star schema or snowflake schema for example) that successfully serves the end users’ needs.\\n\\nConsiderations\\n\\nsample/extension - Understand the flow of the data from data source to the Data Warehouse final sink and understand the transformations that need to be applied at each stage.\\n\\nSuccess Criteria\\n\\nModel the Data Warehouse schema that will receive the ultimate transformed data.\\n\\nEmbed the schema creation into the IaC process.\\n\\nCI/CD\\n\\nGoal\\n\\nThe goal of the CI/CD area in the sample solution is to fully implement the CI/CD process integrating with every and each component of the defined architecture.\\n\\nConsiderations\\n\\nsample Implement the Git Integration with each applicable component of the solution (only on DEV).', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_7', 'chunkContent': 'Considerations\\n\\nsample Implement the Git Integration with each applicable component of the solution (only on DEV).\\n\\nsample Define the branching strategy for the development of the overall solution. Consider each component of the solution individually as different services might work differently and need a different approach. For example, in the sample solution ADF and Azure Synapse use the concept of collaboration and publish branches to promote the artifacts between environments.\\n\\nsample Consider what validations and builds need to be triggered with a PR for each environment stage. For example: unit tests, linting, code wheel or jar builds and dacpac builds.\\n\\nsample Define the approval gates between environments: they might be manual.\\n\\nsample Define necessary testing between environments:\\n\\nsample Unit testing (at the package level that is imported in the notebook)\\n\\nsample Integration testing (triggered between STG and PROD environment)\\n\\nextension Notebook testing\\n\\nextension Loading testing\\n\\nextension Performance testing\\n\\nextension Consider implementing CI/CD for the reporting layer (report development) - it might not be in scope.\\n\\nextension Consider implementing CI/CD for the Governance layer (if in scope).\\n\\nSuccess Criteria\\n\\nSuccessfully implement the automation of the CI/CD process (devops pipeline) that:\\n\\nDeploys and updates all the services in the architecture and all the included artifacts.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_8', 'chunkContent': 'Deploys and updates all the services in the architecture and all the included artifacts.\\n\\nUpdates services and artifacts between all the environments needed (from DEV to STG, from STG to PROD).\\n\\nImplement the necessary approval gates (from DEV to STG, from STG to PROD).\\n\\nIncorporate data schema changes and versioning handling into the CI/CD process.\\n  Note: this step might be done later in the process, or it might be omitted if versioning is not in scope.\\n\\nSuccessfully test the end-2-end deployment and promotion between environments.\\n\\nData\\n\\nData Ingestion\\n\\nGoal\\n\\nThe goal is to understand your data at the source. Ask the Customer the following questions:\\n\\nWhich sources exist?\\n\\nWhat are the expected formats? Parquet, Delta, csv, avro, delta, txt, other?\\n\\nWhere is the data located? On-premises or in the cloud?\\n\\nWhat is the latency requirement for ingestion? Batch, NRT?\\n\\nAre they Privacy constraints or requirements to be aware of? What are they?\\n\\nAre there other constraints or requirements to consider?\\n\\nNOTE: is imperative that the Customer is available and fully onboard as early as possible in the process to answer the questions.\\n\\nConsiderations', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_9', 'chunkContent': 'NOTE: is imperative that the Customer is available and fully onboard as early as possible in the process to answer the questions.\\n\\nConsiderations\\n\\nsample/extension - Based on format, location, and latency requirements, you might use different ingestion mechanisms.\\n\\nConsider writing an ADR to explore different ingestion options and document pros and cons. Examples of ingestion mechanisms are: sample ADF (Azure Data Factory), sample Azure Synapse pipelines, Azure Data Share, Microsoft Purview In-place sharing, Azure Copy, etc.\\n\\nextension - Consider if the ingestion process is required to handle versioning of the schemas or datasets and make the appropriate changes in the data lake design or Data Warehouse design.\\n\\nSuccess Criteria\\n\\nWrite an ADR and document the decision on the Ingestion option or options.\\n\\nEquip the solution with one or a combination of ingestion mechanisms as decided in the ADR. Consider all formats and latencies from the Data Source list which aligned with the Customer requirements.\\n\\nSuccessfully automate the ingestion from all data sources to bronze layer as designed in data lake design.\\n\\nData Transformation\\n\\nGoal', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_10', 'chunkContent': 'Successfully automate the ingestion from all data sources to bronze layer as designed in data lake design.\\n\\nData Transformation\\n\\nGoal\\n\\nThe goal is to understand the data journey after landing in the bronze layer. Understand the final Data product and the personas using or iterating with the data in the silver and gold layers. By other words define your data desired state for each layer and how is the data going to be used/explored.\\n\\nConsiderations\\n\\nsample/extension - Consider what format is going to be primarily used for the transformations (for example: parquet, delta, other). If the source data is on another format, you might consider transforming the original format to the expected file type and format for the silver and gold layers.\\n\\nsample - If Data pipelines are being considered for the ingestion, consider the following:\\n\\nFor notebooks:\\nPlan to use wheel or jar packages to encapsulate the code in the notebook.\\nIntegrate observability in the notebooks from the start.\\n\\nextension - Consider investigating what optimization techniques could be applied (cluster, code, configuration).\\n\\nSuccess Criteria\\n\\nDecision on the format you want to land the data in the silver layer.\\n\\nDecision on the format you want to land the data in the gold layer.\\n\\nDefine which personas will use the silver layer: data scientists, others and how the layer needs to be structured for such use case.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_11', 'chunkContent': 'Define which personas will use the silver layer: data scientists, others and how the layer needs to be structured for such use case.\\n\\nDefine which personas will use the gold layer, for example: business users, product owners, end users, and how the layer needs to be structured for such use case: Power BI report, REST API, Custom App, other.\\n\\nWrite transformation logic through notebooks and/or pipelines that implement current to desired state.\\n\\nUnit test the packages and incorporate them in the DevOps pipeline.\\n\\nTest the transformation logic end-2-end.\\n\\nBenchmark results and overall configurations before and after applying isolated or combined optimization techniques.\\n\\nIncorporate artifacts deployment (pipelines, packages, Data Sources definitions) into IaC and CI/CD processes.\\n\\nData Quality\\n\\nGoal\\n\\nIncorporating Data Quality into the solution is a standard activity that should be incorporated in different stages of the data pipeline. They are driven by two different pillars:\\n\\nextension - Standard DQ validations\\n\\nextension - Business validations\\n\\nConsiderations\\n\\nThe Standard dimensions of Data Quality are summarized as follows:\\n\\nCompleteness\\n\\nUniqueness\\n\\nTimeliness\\n\\nCorrectness\\n\\nConsistency\\n\\nValidity\\n\\nConformity', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_12', 'chunkContent': 'Uniqueness\\n\\nTimeliness\\n\\nCorrectness\\n\\nConsistency\\n\\nValidity\\n\\nConformity\\n\\nAt any point of the Data pipeline lifecycle, all or a subset of these dimensions can be checked with rules.\\n\\nBusiness validations can be considered as Data Quality validations in certain aspects. They are related to a use case. Consider some examples to think about the use case at hand and how it can benefit from business rules validations.\\n\\nConsider for example:\\n\\nextension - A data pipeline is working with unstructured image data and for the final report to have a certain level of quality, the images are required to have a minimum resolution. This property can be checked when ingesting the data, or later the transformation phase.\\n\\nextension - Large data gaps during data ingestion will affect the quality of the business Machine Learning models, leading to poor decision making.\\n\\nSuccess Criteria\\n\\nDecide which Data quality metrics should be implemented in the solution.\\n\\nImplement the rules considering a DQ existing tool that can integrate with your pipeline or developing custom rules and checks in the pipeline.\\n\\nDecide with the Customer what remediation actions can respond to such rules and if they can be automated or not.\\n\\nData Monitoring and Observability\\n\\nGoal', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_13', 'chunkContent': 'Data Monitoring and Observability\\n\\nGoal\\n\\nIncorporate observability in the Data pipeline lifecycle. The aim is to report on health and progress of the ingestion, transformations, and serving processes.\\n\\nConsiderations\\n\\nThe approach to observability should be as comprehensive as possible by including:\\n\\nsample - Infrastructure monitoring\\n\\nsample - Data Pipelines health and progress metrics\\n\\nsample - Notebook observability\\n\\nSuccess Criteria\\n\\nIdentify and prioritize which metrics and failure points need to be monitored.\\n\\nImplement/configure monitoring and observability on all components of the architecture (infrastructure). Azure Monitor can be applied for Azure native and first party components.\\n\\nImplement observability using Application Insights existing integrations (with Synapse Spark for example).\\n\\nImplement custom observability at the notebook level using appropriate libraries and frameworks. Application insights can also be used.\\n\\nWrite kql targeted queries that can be used to query the metrics and custom logs. The aim is to reveal useful information about the workloads that are being run.\\n\\nTesting\\n\\nGoal\\n\\nThe goal is to incorporate testing across the entire solution. Different types of testing might be included in different stages of the project.\\n\\nConsiderations\\n\\nConsider if the solution you are designing might benefit from the following testing types. If so, include the relevant tasks in earlier activities:\\n\\nextension - IaC Testing', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_14', 'chunkContent': 'extension - IaC Testing\\n\\nsample - Unit Testing\\n\\nextension - Notebook Testing\\n\\nsample - Integration Testing\\n\\nextension - Loading Testing\\n\\nextension - Performance Testing\\n\\nextension - End-To-End Testing\\n\\nSuccess Criteria\\n\\nIaC Testing (related with IaC)\\n\\nPerform an end-2-end deployment of the IaC pipeline and guarantee that the results are consistent with the expected results: services deployed, access control and initial artifacts. It is not enough that the deployment runs without errors, it is necessary to test against the access.\\n\\nUnit Testing (related with CI/CD)\\n\\nSuccessfully incorporate unit testing against the wheel and jar packages to be included in the solution.\\n\\nNotebook Testing (related with Data Transformation)\\n\\nPerform an ADR to investigate notebook testing options, weight pros and cons and document the decision.\\n\\nIntegration Testing (related with CI/CD and Data Transformation)\\n\\nSuccessfully run integration tests under the scope of the release pipeline to test integration points between components.\\n\\nLoading Testing (related with Data Ingestion)\\n\\nIf ingestion times are required to meet certain thresholds, it is advised to benchmark, and test the loading time for different partitioning strategies.\\n\\nAgree on an acceptable threshold for the ingestion of the data sources.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk93_15', 'chunkContent': 'Agree on an acceptable threshold for the ingestion of the data sources.\\n\\nAgree on the data volume that the benchmark should run against per data source.\\n\\nImplement monitoring and alerting that detects when the threshold of the ingestion process exceeds the agreed value.\\n\\nPerformance Testing\\n\\nIf query times are required to meet certain thresholds, benchmark and test different types of queries is advised. The recommendation for benchmarking is to execute 3 query runs and use the average execution time. The runs should be executed against an agreed data set with a relevant data volume.\\n\\nAgree on an acceptable threshold for the queries per query type.\\n\\nAgree on the data volume that the benchmark should run against.\\n\\nIf the threshold of the ingestion process exceeds the agreed value, proceed with query, engine, or configuration optimizations. Testing should be repeated until threshold is achieved.\\n\\nEnd-2-End Testing\\n\\nSuccessfully test the entire solution:\\nIaC deployment followed by,\\na data Ingestion pipeline run, where observability is active and triggers alerts based on default thresholds. Select a low threshold to force that the trigger is fired, followed by:\\na release pipeline where unit, notebook and integration tests are triggered, followed by,\\na performance test on pre-defined queries that log the query execution time and triggers an alert when performance is degrading from initial thresholds.', 'source': '..\\\\data\\\\docs\\\\code-with-dataops\\\\solutions\\\\modern-data-warehouse\\\\mdw-backlog-guidance.md'}, {'chunkId': 'chunk94_0', 'chunkContent': \"rings:\\n  - public\\n\\nDevSecOps Playbook\\n\\nThe DevSecOps Playbook provides enterprise software engineers with solutions, capabilities, and code developed to solve real-world problems. Everything in the playbook is developed with, and validated by, some of Microsoft's largest and most influential customers and partners.\\n\\n{% if extra.ring == 'internal' %}\\nYou are invited to share your enterprise-grade production solutions as well. Refer to Contributing to the Solutions Playbook.\\n\\n{% endif %}\\nThis Playbook section provides DevSecOps solutions focused on the following areas:\\n\\nApply advanced DevOps and DevSecOps practices to shift security left in the software development lifecycle\\n\\nProactively establish a security strategy\\n\\nIterate delivery automation toward more secure software\\n\\nAccelerate velocity by reducing time from build to release\\n\\nCollaborate with compliance teams to understand and improve the organization’s security posture\\n\\nDevSecOps solutions\\n\\nImprove release artifact and workload integrity in Kubernetes via a secure software supply chain\\n\\nInfrastructure as Code(IaC) Orchestration & Testing for Enterprise Customers\\n\\nSecrets Management\\n{% if extra.ring == 'internal' %}\\n\\nCloud Native Application Bundle(CNAB) for Azure Trusted Research Environment(TRE)\\n{% endif %}\\n\\nAbout the Data Playbook\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\index.md'}, {'chunkId': 'chunk94_1', 'chunkContent': \"About the Data Playbook\\n\\nThese Playbook solutions employ good engineering practices to accelerate real-world application development. Common themes include:\\n\\nImproving application design and developer productivity by sharing code and knowledge developed by experts for Microsoft customers.\\n\\nUsing automation to make repetitive tasks faster, more reliable, and auditable\\n\\nMaking application deployments and operations secure and observable.\\n\\nSecuring applications by following security best practices at all stages of the engineering lifecycle.\\n\\nWriting portable solutions that run in multiple locations, including edge devices, on-premises data centers, the Microsoft cloud, and competitor's clouds.\\n\\nIntegrated solutions\\n\\nPlaybook solutions span multiple Microsoft products and services and focus on creating integrated end-to-end solutions often using a range of open-source software libraries.\\n\\nProven with real customers\\n\\nAll code linked from playbook solutions and capabilities was created working with our customers to develop production solutions. This documentation and code is generalized to remove confidential details.\\n\\nReferences\\n\\nAzure DevOps\\n\\nMicrosoft Security Engineering\\n\\nOWASP DevSecOps Maturity Model\\n\\nLean Security\\n\\nDevSecOps - DoD Cyber Exchange\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\index.md'}, {'chunkId': 'chunk95_0', 'chunkContent': 'rings:\\n  - public\\nms.author: mrenard\\n\\nCloud infrastructure provisioning: best practices for IaC\\n\\nIntroduction\\n\\nInfrastructure-as-Code (IaC) is a common DevOps practice that enterprises use to provision and deploy IT infrastructure. Enterprises applying IaC and continuous integration/continuous delivery (CI/CD) pipelines can maintain high availability and manage risk to their cloud environments at scale. However, multi-environment challenges, manual processes, and fragmented guidance still lead to configuration drift, errors, and inconsistencies. These issues are likely to result in downtime, security vulnerabilities, and inefficient resource utilization.\\n\\nSuccessful enterprises establish and institute best practices early in the planning phase to reduce the probability of downstream deployment issues. This article provides a set of patterns and best practices that can be applied to help enterprises enhance reliability, security, and cost-effectiveness of their Azure cloud environments.\\n\\nPractices that ensure secure, repeatable and reliable cloud infrastructure provisioning and deployments with IaC\\n\\nEnsure consistency across environments', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk95_1', 'chunkContent': 'Practices that ensure secure, repeatable and reliable cloud infrastructure provisioning and deployments with IaC\\n\\nEnsure consistency across environments\\n\\nEnsuring consistency from the development environment through production reduces the risk of a failed deployment. A good approach to managing multiple configurations is through consistent folder structures that represent the environments. The structure should support easy identification of the different environment configurations. The same deployment pipeline can be used by merely swapping configuration values using pipeline parameters. An example would be having a folder structure with the environments as the top-level folder. Each folder can contain the same set of files per environment. New environments could be created by copying and renaming existing environments.\\n\\nFor example, with a dev, test, and prod environment, the folder structure could resemble the illustration below:\\n\\ndiagram\\n📂env\\n  📂dev\\n    📜01-init.json\\n    📜02-sql.json\\n    📜03-web.json\\n  📂test\\n    📜01-init.json\\n    📜02-sql.json\\n    📜03-web.json\\n  📂prod\\n    📜01-init.json\\n    📜02-sql.json\\n    📜03-web.json', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk95_2', 'chunkContent': 'Starting with a reliable pipeline enables the team to deploy required infrastructure resources to target environments and reduces errors from misconfiguration. The following guidance helps to ensure deployments are secure and repeatable.\\n\\nSet up idempotent pipelines\\n\\nIaC pipeline executions should be idempotent, and therefore produce identical results for each execution of the same IaC code. Deployed infrastructure should not be affected unless there is a change to the IaC code. To achieve this behavior use the state tracking mechanism of the IaC tool in use. Terraform uses a state file, while Bicep checks the current runtime state of the infrastructure.\\n\\nModularize deployments\\n\\nWe recommend splitting the deployment into independent modules for complex infrastructures. Each module is responsible for a specific set of resources and configurations related to a single logical component. This approach allows for more modular, flexible and extendable deployments as the solution grows.\\n\\nEmploy a robust testing and quality check strategy\\n\\nEven the smallest change can have a major impact on the deployed environment. When multiple teams collaborate on the same solutions, it is critical to validate all changes and verify that those changes do not break environments.\\n\\nCI/CD pipelines unlock the ability to automate almost any task. Organizations extract more value from IaC deployments by automating code quality checks, and testing, to ensure minimal deployment disruption.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk95_3', 'chunkContent': \"When determining which tests to integrate and automate, focus on changes that can cause a deployment to fail. Checks and tests include:\\n\\nIaC Test/Check Description Template validation Enable users to detect invalid code early and shorten the development iteration loop. Organizations standards validation Validate and enforce standards specific to the organization. Desired state tests Verify that the IaC deployment was successful. Preview Resources Prevent unintended deletions. Security Scanning Identify known vulnerabilities and security misconfigurations.\\n\\nTo give teams more confidence that changes do not introduce risk to production, tests and checks should be integrated into a CI/CD pipeline.\\n\\n{% if extra.ring == 'internal' %}\\n\\nFor security scanning, the Credential Scanning in an IaC Pipeline capability can be used to prevent secrets leaking into IaC pipelines.\\n\\n{% endif %}\\n\\nLinting and Validation\\n\\nImplementing code linting and validation tools in a pipeline for IaC projects helps to identify and fix common issues in the code. To ensure code is always in a healthy state, linting and validation processes can be automated and integrated into a CI/CD pipeline. The linting process can also be used to enforce default organizational practices and coding standards.\\n\\nUse code validation to ensure that code is internally consistent, semantically correct, and always deployable.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk95_4', 'chunkContent': \"Use code validation to ensure that code is internally consistent, semantically correct, and always deployable.\\n\\nThis practice improves readability, comprehension, and review, thereby reducing errors and improving maintainability of assets.\\n\\nThe following tools are options for linting and validating code. Depending on each team's evaluation of the stated advantages and disadvantages, team should pick the tool that best meets project requirements and fits workflow needs.\\n\\nFor Terraform:\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk95_5', 'chunkContent': 'For Terraform:\\n\\nLinting Tool Description Language Advantages Disadvantages Terraform Lint (tflint) A Terraform linter focused on possible errors, best practices, etc. Go Pluggable, supports custom rules, actively maintained Some configuration needed Checkov A static code analysis tool for infrastructure-as-code. Python Supports multiple IaC tools (Terraform, CloudFormation, etc.), checks for security best practices, supports custom checks Might have false positives/negatives Terrascan Detects security vulnerabilities and compliance violations. Go Supports multiple IaC tools, extensive policy library (supports OPA) Can be slow on large codebases Tfsec A security scanner for your Terraform code. Go Fast, checks for security best practices, easy to use Limited to security checks Terraform Validate A built-in command in Terraform for checking the syntax of your code. Go (built into Terraform) No additional installation needed, checks for syntax errors Limited, only checks syntax, not security or best practices Terraform-compliance A lightweight, compliance-focused, open-source tool. Python Scenario-based tests, easy to read, supports custom checks Only for behavior testing, not a comprehensive linter\\n\\nAnd for Bicep:', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk95_6', 'chunkContent': 'And for Bicep:\\n\\nLinting Tool Description Language Advantages Disadvantages Bicep CLI A command line tool for validating Bicep files. Go Built into Bicep, no additional installation needed Only checks syntax, not security or best practices Bicep VS Code Extension Provides linting, autocompletion, code navigation, and more. TypeScript Integrated into VS Code, easy to use, supports IntelliSense Limited to users of VS Code ARM Template Test Toolkit (arm-ttk) Although not specifically designed for Bicep, it can be used to lint compiled ARM templates. PowerShell Can test for best practices, errors, etc. Requires Bicep code to be compiled to ARM templates first\\n\\nOptimize resources\\n\\nResource efficiency and right-sizing cloud deployments is a major challenge as organizations attempt to scale solutions. Manually managing resources in different environments becomes increasingly difficult as the number of teams and projects increase. Insufficient resource expiration and destruction processes leads to considerable sprawl, making it difficult for engineering teams to manually identify and dispose of obsolete resources. This results in inflated resource cost and introduction of potential security risks.\\n\\nTo dispose of obsolete environments, an effective best practice is to implement a \"destroy\" pipeline.\\n\\nThe destroy pipeline is either:', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk95_7', 'chunkContent': 'To dispose of obsolete environments, an effective best practice is to implement a \"destroy\" pipeline.\\n\\nThe destroy pipeline is either:\\n\\nTriggered periodically or on a scheduled basis. Obsolete resources can be identified by using a combination of tags and metadata contained in the environment. For example, the pipeline could filter resources marked with a specific tag to determine whether the environment is still needed. If the environment is no longer needed, the pipeline disposes of it.\\n\\nOr triggered automatically by an action that marks an environment as obsolete. An example of an action could be merging a pull request that was validated in that environment. Alternate flows are supported by using tags that mark environments to be preserved. Preserved environments may be used to investigate or analyze defects.\\n\\nDestroying an environment may be more complex than simply deleting the resource group with the nested resources that it contains. External dependencies could have also been configured during the provisioning exercise. For example, connections with external virtual networks, or the deployment of Directory objects. Such dependencies may not be cleaned up when the resource group is deleted. An effective destroy pipeline should be able to handle compensating resource management principals. Resource state management could be highly beneficial for this scenario.\\n\\nExplore Ephemeral Environments', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk95_8', 'chunkContent': 'Explore Ephemeral Environments\\n\\nMaintaining long-term resources for testing purposes is both time-consuming and resource-intensive. This challenge can be addressed with ephemeral environments in the pull request process. Ephemeral environments are temporary and disposable, allowing for easy and efficient testing without the need to maintain and secure long-term resources. Using ephemeral environments also mitigates the risks of breaking existing environments during deployment or testing. They can be used to replicate the target production environment, including the same infrastructure, software, and configuration. Ephemeral environments enable more accurate testing and reduce the risk of unexpected issues when deploying code changes.\\n\\nRemote State (Terraform specific)\\n\\nIn a multi-team or multi-developer environment, managing the state of Terraform resources is a challenge. Using a local state file to track the deployed resources state or source configurations can lead to drifts or conflicts in the infrastructure resources, creating delays and increased costs. Terraform remote state should be configured to allow multiple team members to safely share deployed infrastructure configurations in a read-only manner. This setup can be done without relying on extra configuration storage. Concurrent executions of Terraform commands against the same deployed resources are also supported.\\n\\nReference Example', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk95_9', 'chunkContent': 'Reference Example\\n\\nThe best practices in this article are codified and included in a repository referred to as Symphony. This quick start tool efficiently configures a new project with the best practices pre-implemented and ready to use or customize.\\n\\nSymphony includes templates and workflows to help with:\\n\\nAutomating deployment of resources using IaC\\n\\nIncorporating engineering fundamentals for IaC\\n\\nResource validation\\n\\nDependency management\\n\\nTest automation and execution\\n\\nSecurity scanning\\n\\nReferences and Citations\\n\\nWhat is Infrastructure as code (IaC)?\\n\\nDevSecOps for Infrastructure as code (IaC)\\n\\nCI/CD for IaC on multiple orchestrators', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\articles\\\\infrastructure\\\\best-practices-infrastructure-pipelines.md'}, {'chunkId': 'chunk96_0', 'chunkContent': 'Overview\\n\\nIn the DevSecOps section, the Capabilities Map serves as a guide for navigating the design and implementation of DevSecOps principles, practices and tools throughout the development phases of complex IT applications and systems. For simplicity, the content in this section differ from the Capabilities Map in that content is segmented into four standard DevOps pipeline phases - Plan, Develop, Deliver, Operate. These phases are continuous, dependent on each other and not role-specific. DevSecOps secures each phase of a continuous DevOps pipeline as security is designed, integrated, and validated throughout. While every pipeline is unique, similar workflows are reflected in most organizations application.\\n\\nThe phases, Plan, Develop, Deliver, Operate, in use align with public facing Microsoft references for consistency. See reference list below.\\n\\nThe next section describes the chronological phases in detail.\\n\\nCapability Phases\\n\\nPlan: It all starts with planning. This phase involves collaboration, discussion, review, and strategy of security analysis. Teams define security requirements, perform a security analysis and create a plan that outlines where, how, and when security testing will be done.\\n\\nDevelop: This phase includes all aspects of code writing, testing, reviewing, and the integration of code by team members as well as building that code into build artifacts that can be deployed into various environments.\\n\\nDeploy: This phase is where software moves from testing to live production which encompasses securing deployment and compliance.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\index.md'}, {'chunkId': 'chunk96_1', 'chunkContent': 'Deploy: This phase is where software moves from testing to live production which encompasses securing deployment and compliance.\\n\\nOperate: This phase involves maintaining, monitoring, and troubleshooting applications in production environments coupled with rich telemetry, actionable alerting, and full visibility into applications and the underlying system.\\n\\nReferences\\n\\nWhat is DevOps\\n\\nDevOps Solutions on Azure\\n\\nDevSecOps - DoD Cyber Exchange', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\index.md'}, {'chunkId': 'chunk97_0', 'chunkContent': 'Plan Phase\\n\\n```mermaid\\nflowchart LR\\n    plan{{PLAN}}==>dev{{DEVELOP}}==>deploy{{DEPLOY}}==>operate{{OPERATE}}\\n    style plan fill:lightgray, text:black', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\01-Plan\\\\index.md'}, {'chunkId': 'chunk97_1', 'chunkContent': \"```\\n\\nShifting security left begins with the inclusion of DevSecOps practices in the Plan Phase. Traditionally the performance of security related activities have been relegated to a post development assignment, resulting in reduced deployment velocity, and enforcing the siloed-divide between developers and security experts.\\n\\nAligning security practices with development requires a shift towards DevSecOps culture in which everyone takes responsibility for security and fosters collaboration, discussion, review and analysis of the overall security strategy.\\n\\nIn the plan phase of DevSecOps, the team will develop a plan to integrate security into the software development process.The plan may also include strategies for responding to security incidents and conducting regular security assessments. The goal of the plan phase is to ensure that security is considered at every stage of the development process, from design and implementation to testing and deployment.\\n\\nActivities during this phase typically includes but is not restricted to:\\n\\nCreating a road map for implementing security measures.\\n\\nIdentifying potential risks and vulnerabilities.\\n\\nEstablishing protocols for testing and verifying the security of the software.\\n\\nStrategies for responding to security incidents and conducting regular security assessments.\\n\\nPerforming security analysis of the planned and existing architecture.\\n\\nConsidering countermeasures that could affect the application's design, meet the organization's security objectives, and reduce risk.\\n\\nSettling on orchestration and configuration management processes.\\n\\nCreating a change management plan\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\01-Plan\\\\index.md'}, {'chunkId': 'chunk97_2', 'chunkContent': 'Settling on orchestration and configuration management processes.\\n\\nCreating a change management plan\\n\\nTool Guidance\\n\\nDifferent organizations have different needs and preferences therefore tooling choices will differ on an organizational level. However, here are a few tools that are commonly used during the plan phase of DevSecOps:\\n\\nTool Supported Features Benefits Example Collaboration Suite Central place to host team meetings,design discussions, and store related documentation. Improved communication efficiency, organized teamwork and version controlled documents Microsoft Teams and Slack. Threat modeling tool Document system security design and analyze and define mitigations potential security issues. Supports the early identification of, and the mitigation of potential security issues. Microsoft Threat Modeling Tool and STRIDE. Integrated Issue tracking system Common orchestration system that addresses defect management, alongside code, and workflow pipelines that build and test code. Feature planning, prioritization and change management. Catching security vulnerabilities early, track and remediate security flaws identified in the same tools that developers are already working with daily. Azure DevOps, GitHub, Trello and JIRA. Security assessment tools These tools are used to test the security of software, identify weaknesses. Provides  and recommends early remediation strategies. Burp Suite, OWASP ZAP, and AppScan.\\n\\nReferences\\n\\nThreat Modeling\\n\\nSecurity Engineering - Threat Modeling', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\01-Plan\\\\index.md'}, {'chunkId': 'chunk98_0', 'chunkContent': 'Develop Phase\\n\\n```mermaid\\nflowchart LR\\n    plan{{PLAN}}==>dev{{DEVELOP}}==>deploy{{DEPLOY}}==>operate{{OPERATE}}\\n    style dev fill:lightgray, text:black', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\index.md'}, {'chunkId': 'chunk98_1', 'chunkContent': \"```\\n\\nThe Develop Phase includes all aspects of coding, testing, reviewing, and the integration of code artifacts generated by build systems into various deployed environments. This phase can encompass several sub-phases, such as Build, Test, Start and Debug.\\n\\nDevelop phase tools support the development activities that convert requirements into source code. The source code itself may consist of :\\n\\nApplication code.\\n\\nTest scripts.\\n\\nInfrastructure as Code scripts and definitions.\\n\\nSecurity and Policy scripts.\\n\\nDevSecOps workflow scripts and definitions.\\n\\nDatabase Scripts, queries and procedures.\\n\\nEach of the above may store information that could be used by an attacker to gain access and insight into the software and security systems that an organization relies on. DevSecOps teams, therefor have to take extra precautions during the Develop Phase to avoid high-risk development practices.\\n\\nThe development team may rely on a single modern integrated development environment (IDE) or disparate tools, specific to each task above. Committing to reducing the number of tools used by the team or having those tools integrated, presents opportunities for software developers to get early feedback while developing software that helps to improve the organization's overall security posture from the ground up.\\n\\nSome of the benefits of a modern IDE such as Visual Studio [^1] or Visual Studio Code [^2] include but are not limited to:\\n\\nLinting tools to improve code speed and quality.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\index.md'}, {'chunkId': 'chunk98_2', 'chunkContent': 'Linting tools to improve code speed and quality.\\n\\nSecret and Credential Scanners with pre-commit hooks [^3] that prevent accidentally leaking secrets in checked in source code.\\n\\nStatic Code Analysis plugins to catch code errors and recommend fixes before checking code into a central source repository.\\n\\n[^1]: Visual Studio information page\\n[^2]: Visual Studio Code information page\\n[^3]: GitHub Pre-Commit hooks\\n\\nTool Guidance\\n\\nTool Supported Features Benefits Examples Code Quality Scanner Identify defects and allows remediation and review. Automates review and identification of common mistakes. Fortify, Checkmarx CxSAST, WhiteSource and Nexus IQ. Secret Scanner Scans all code changes for sensitive information and blocks suspected commits. Prevents leakage of secrets and avoids the need for emergency key rotation. GitLeaks,GitHub Secret Scanning, Trufflehog, GitGuardian, GitRob, RepoRobber and Yelp Detect Secrets. Static Code Analyzer Scans and analyzes code being written and flags security vulnerability and suggests remediation steps. Catching security vulnerabilities early prevents defects from slowing down delivery velocity to remediate security flaws identified later in the process. SonarQube, Checkmarx and Veracode.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\index.md'}, {'chunkId': 'chunk99_0', 'chunkContent': \"Local Development Environment\\n\\nAs engineers work on adding features and removing bugs it can be helpful to have a common development environment that is shared across the machines (both real and virtual) that engineers are using as they contribute code to a project. Regardless of the developer's selected environment there are tooling and configuration options that can ensure code can run across the various OS and system types.\\n\\nDevContainers\\n\\nDevContainers are an option for sharing configuration across developer machines and more general guidance on these can be found here, however there are additional security steps that can be taken to protect code and assets from attackers.\\n\\nSecrets\\n\\nSecrets are a primary source of access to many systems. Although our guidance highlights alternatives to secrets and ways to secure them we also acknowledge that secrets remain a development time requirement. This often requires additional steps to ensure that secrets are not checked into source repositories while still allowing access to secrets needed during development time.\\n\\nAs DevContainers have become an industry goto for sharing configuration of code and IaC across development teams we have found additional complexity in securely accessing secrets during the development work. To help solve this problem while improving the time required for an engineer to have a working instance of the code under development for feature work we suggest including details of required secrets in the DevContainer as a potential pathway to accounting for secrets when configuring a secure development environment.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Development-Environment\\\\index.md'}, {'chunkId': 'chunk100_0', 'chunkContent': 'Secrets Detection\\n\\nOverview\\n\\nApplication and infrastructure source codes contain exposed secrets that can pose a severe security risk and requires attention during development. Utilizing robust Secret Detection tools help mitigate this risk by probabilistically identifying whether strings in application code contain secrets. Early and automated detection of secrets in IaC and application code is critical and ideally should occur before code is committed to a repository and part of its history. Effective secret detection tools share several essential characteristics.\\n\\nCharacteristics of effective Secret Detection Tools\\n\\nCharacteristic Description Precision The tool rarely produces false positive results. False positive results directly impact the level of trust that developers have for the tool. Recall The tool rarely misses a secret. This reduces runtime and operational remediation. Configurable False positives can be addressed in the code through annotation and does not slow developers down. Additionally, new patterns can be introduced to detect custom secrets not previously detectable to the tool\\n\\nSample sequence - Early detection\\n\\nIn the example below, a secret detection tool with a pre-commit configuration option prevents the unintentional check-in of a secret in the repository.\\n\\n```mermaid', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\index.md'}, {'chunkId': 'chunk100_1', 'chunkContent': '```mermaid\\n\\nsequenceDiagram\\n   Developer-x Scanner: Check in Code ( git commit )\\n   Scanner->> Developer: Secret Found - Checkin Blocked\\n   Developer->> Scanner : Remediated\\n   Scanner->> Repository: (git commit)\\n   Repository->> Developer: Check in succesful', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\index.md'}, {'chunkId': 'chunk100_2', 'chunkContent': '```\\n\\nSample sequence - Post Check-in detection\\n\\nIn the example below, the secret detection tool has been implemented as a step in a Continuous Integration (CI) pipeline and only detects the secret once the code is checked in. This is a far more involved workflow and demonstrates a potential remediation path that could prevent the leaked credential from being used maliciously by an attacker.\\n\\n```mermaid\\n\\nsequenceDiagram\\n   Developer->> Repository: Check in Code ( git commit )\\n   Repository->> CI (Scanner): Trigger CI Workflow\\n   CI (Scanner)->> CI (Log): Secret Found\\n   CI (Log)->> BTS: Create Urgent Bug\\n   BTS->> Operations : Kick Off Secret Rotation\\n   BTS-->> Developer: Remediation Instructions', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\index.md'}, {'chunkId': 'chunk100_3', 'chunkContent': '```\\n\\nAct on the results\\n\\nIt is important to act on the results of the scan immediately.\\n\\nTriage the output from the scans to identify false positives and false negatives.\\n\\nIt is recommended that results determined as true secrets be marked as high-severity bugs/issues which must be remediated with urgency.\\n\\nDocument and track the remaining issues.\\n\\nSecret rotation is a compulsory step once a secret has been exposed.\\n\\nConsiderations\\n\\nThere is no single tool that can detect all possible types of credentials. Implementing this type of credential scanning tool is helpful, but does not replace code reviews by other members of the team.\\n\\nMultiple tools should be evaluated in order to detect different types of credentials and select the one that best fits the needs of the project and the organization.\\n\\nConsider tuning and configuring the credential scanning tool before integrating it into a pipeline. This will help reduce the occurrence of false-positives and avoid false-negative results.\\n\\nTool Selection Matrix\\n\\nThe table below has been populated with a selection of tools that have both high precision and recall as stated in the below table. It is important to note that other factors may influence your choices, such as the availability of an official GitHub Action that can be used in a Workflow or its ability to be used from a shell, among other things.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\index.md'}, {'chunkId': 'chunk100_4', 'chunkContent': 'Tool Command Line Pre-commit hook GitHub Action Azure DevOps Extension License Repo Service GitHub Secrets Scanning No No No No commercial, required GitHub Enterprise Cloud with a license for GitHub Advanced Security Yes Yelp Detect-Secrets Yes (as python library) Yes Yes (community, MIT) No Apache 2.0 No TruffleHog Yes (cli to download from releases) Yes - OSS (official, AGPL 3.0) - Enterprise (official, required TruffleHog Enterprise) No AGPL 3.0 No GitLeaks Yes (cli to download from releases) Yes - Gitleaks (official, commercial for organizations) - Gitleaks Scanner (community, MIT) Yes (community, MIT) MIT No\\n\\nCode Sample\\n\\nHere is a sample on how to use GitLeaks to detect secrets during the build process. The link below will direct you to an implementation sample that showcases the proper implementation of its capabilities.\\n\\nIntegrating Gitleaks into GitHub Workflow or Azure Pipeline\\n\\nAn implementation of the automated secret detection pattern on an IaC pipeline is available in this repository.\\nIn this case, the validation step of the pipelines are using GitLeaks to scan the repository for leaked credentials and publish the results in as SARIF (Static Analysis Results Interchange Format) file, so it can be consumed by other static analysis tools.\\n\\nAdditional Resources\\n\\nOWASP Secrets Detection Guide', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\index.md'}, {'chunkId': 'chunk101_0', 'chunkContent': \"summary: Integrating Gitleaks into GitHub Workflow or Azure Pipeline\\nauthors:\\n  - Dariusz Porowski\\ndate: 2022-08-12\\ntags:\\n  - Gitleaks\\n  - Secrets Detection\\n  - Secrets Scanning\\n\\nGitleaks\\n\\nOverview\\n\\nIn this article, you can find information on integrating Gitleaks into your GitHub Workflow or Azure Pipeline as part of the PR and Build process.\\n\\nWhat is Gitleaks?\\n\\nGitleaks is an Open-Source tool released under MIT License. It's a SAST tool for detecting and preventing hardcoded secrets like passwords, API keys, and tokens in git repos. Gitleaks is an easy-to-use, all-in-one solution for detecting secrets, past or present, in your code.\\n\\nGitleaks produces a scan report as SARIF (Static Analysis Results Interchange Format) file, which is OASIS standard used to streamline how static analysis tools share their results.\\n\\nGitHub Workflows\\n\\nIn this section, you can find example recipes for GitHub Workflow.\\n\\nOption 1 - Gitleaks with official GitHub Action\\n\\nGitleaks officially released its GitHub Action called Gitleaks Gitleaks Action to perform secret detection, and it's recommended and the easiest solution to use Gitleaks in the GitHub Workflow.\\n\\nNOTE\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_1', 'chunkContent': 'NOTE\\n\\nGitleaks Action is released under a commercial license!\\n\\nIf you are scanning repos that belong to an organization account, you will need to obtain a license key.\\n\\nIf you are scanning repos belonging to a personal account, no license key is required.\\n\\nFor more information about licensing, go to the official page: gitleaks.io\\n\\nGitleaks result (SARIF file) may be uploaded to the GitHub Code Scanning service to see code scanning alerts from third-party tools.\\n\\nNOTE\\n\\nGitHub Code Scanning integration it\\'s only available for Organizations that use GitHub Enterprise Cloud and have a license for GitHub Advanced Security.\\n\\n{% raw %}\\n\\n```yaml\\nname: Gitleaks-GHA-official\\n\\non:\\n  push:\\n    branches: [\"main\"]\\n  pull_request:\\n    branches: [\"main\"]\\n  workflow_dispatch:\\n\\njobs:\\n  secrets-detection:\\n    name: Secrets Detection\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Checkout repo\\n        uses: actions/checkout@v3\\n        with:\\n          fetch-depth: 0', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_2', 'chunkContent': '```', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_3', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nTo adjust Gitleaks Action configuration, follow documentation: https://github.com/marketplace/actions/gitleaks#environment-variables\\n\\nOption 2 - Gitleaks with unofficial GitHub Action\\n\\nThere are a couple of unofficial Gitleaks GitHub Actions on the GitHub Marketplace. In most cases, they do not require a commercial license and are released under open-source licenses.\\n\\nEach Action has a different approach for Gitleaks usage and scanning, so there is not one answer to what is the best - it depends on scenario requirements.\\n\\nNOTE\\n\\nVery often, unofficial Actions are community-powered and may have undefined code maintenance and support approaches.\\n\\nOne community-based GitHub Action has been selected for this example, which has similar results and experience as the official one - Gitleaks Scanner Action.\\n\\n{% raw %}\\n\\n```yaml\\nname: Gitleaks-GHA-unofficial\\n\\non:\\n  push:\\n    branches: [\"main\"]\\n  pull_request:\\n    branches: [\"main\"]\\n  workflow_dispatch:', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_4', 'chunkContent': 'jobs:\\n  secrets-detection:\\n    name: Secrets Detection\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Checkout repo\\n        uses: actions/checkout@v3\\n        with:\\n          fetch-depth: 0', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_5', 'chunkContent': '```', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_6', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nTo adjust Gitleaks Scanner Action configuration, follow documentation: https://github.com/marketplace/actions/gitleaks-scanner#inputs\\n\\nOption 3 - Gitleaks as command line\\n\\nYou can run Gitleaks natively as a command line without using any GitHub Actions. In this scenario, you have complete control, but you are responsible for code maintenance based on Gitleaks breaking changes in future releases. Moreover, you have to setup Gitleaks on the agent as well.\\n\\nThe following example contains the setup step that downloads and installs the latest version of the Gitleaks from the official repository and the step with Gitleaks execution to detect secrets in the code.\\n\\n{% raw %}\\n\\n```yaml\\nname: Gitleaks-CMD\\n\\non:\\n  push:\\n    branches: [\"main\"]\\n  pull_request:\\n    branches: [\"main\"]\\n  workflow_dispatch:', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_7', 'chunkContent': 'jobs:\\n  secrets-detection:\\n    name: Secrets Detection\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Checkout repo\\n        uses: actions/checkout@v3\\n        with:\\n          fetch-depth: 0', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_8', 'chunkContent': '```', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_9', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nTo adjust Gitleaks configuration, follow documentation: https://github.com/zricethezav/gitleaks#usage\\n\\nGitHub References\\n\\nGitleaks Action (official)\\n\\nGitleaks Scanner Action (unofficial)\\n\\nAzure Pipelines\\n\\nIn this section, you can find example recipes for Azure Pipelines.\\n\\nNOTE\\n\\nOfficial Azure Pipelines extension for Gitleaks does not exist.\\n\\nOption 1 - Gitleaks with unofficial Azure Pipelines extension\\n\\nCurrently, there is only one Azure Pipelines extension for Gitleaks under Visual Studio Marketplace, and it is an open-source project licensed under MIT License.\\n\\nNOTE\\n\\nVery often, unofficial Tasks are community-powered efforts and vary code maintenance and support approaches.\\n\\nGitleaks by Foxholenl has been selected for this Pipeline example.\\n\\nTask automatically uploads SARIF report to Pipeline Artifacts, and the result can be visualized using the SARIF SAST Scans Tab extension.\\n\\n{% raw %}\\n\\n```yaml\\ntrigger:\\n- main\\n\\npr:\\n- main\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n- checkout: self\\n  displayName: Checkout repo', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_10', 'chunkContent': 'pool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n- checkout: self\\n  displayName: Checkout repo\\n\\ntask: Gitleaks@2\\n  displayName: Run Gitleaks\\n  inputs:\\n    configtype: default\\n    scanmode: all\\n    verbose: true', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_11', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nOption 2 - Gitleaks as command line\\n\\nYou can run Gitleaks natively as a command line without using any Azure Pipelines extension. In this scenario, you have complete control, but you are responsible for code maintenance based on Gitleaks breaking changes in future releases. Moreover, you have to setup Gitleaks on the agent as well.\\n\\nThe following example contains the setup step that downloads and installs the latest version of the Gitleaks from the official repository and the step with Gitleaks execution to detect secrets in the code.\\n\\n{% raw %}\\n\\n```yaml\\ntrigger:\\n- main\\n\\npr:\\n- main\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n- checkout: self\\n  displayName: Checkout repo', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_12', 'chunkContent': 'pool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n- checkout: self\\n  displayName: Checkout repo\\n\\nscript: |\\n    curl -sSLO $(curl -sSL https://api.github.com/repos/zricethezav/gitleaks/releases/latest | jq --compact-output --raw-output \\'.assets[] | select( .name | contains(\"linux_x64\") ) | select( .name | endswith(\".tar.gz\") ).browser_download_url\\')\\n    filename=$(find . -maxdepth 1 -type f -regex \\'^.\\\\/linux_x64*.tar.gz\\')\\n    tar -xf \"${filename}\" gitleaks\\n    mv -f gitleaks /usr/local/bin/\\n    rm -f \"${filename}\"\\n  displayName: Setup Gitleaks', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_13', 'chunkContent': 'script: |\\n    set +e\\n    report_name=\"gitleaks-report\"\\n    report_format=\"sarif\"\\n    command=\"gitleaks detect --redact --verbose --report-format=${report_format} --report-path=$(Pipeline.Workspace)/${report_name}.${report_format} --log-level debug\"\\n    echo \"Running Gitleaks $(gitleaks version)\"\\n    echo \"##[command]${command}\"\\n    OUTPUT=$(eval \"${command}\")\\n    exitcode=$?\\necho \"##vso[task.setvariable variable=gitleaks_command]${command}\"\\necho \"##vso[task.setvariable variable=gitleaks_exitcode]${exitcode}\"\\nif [ ${exitcode} -eq 0 ]; then\\n  GITLEAKS_RESULT=\"SUCCESS! Your code is good to go\"\\n  echo \"##[command]${GITLEAKS_RESULT}\"\\nelif [ ${exitcode} -eq 1 ]; then\\n  GITLEAKS_RESULT=\"STOP! Gitleaks encountered leaks or error\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_14', 'chunkContent': 'GITLEAKS_RESULT=\"STOP! Gitleaks encountered leaks or error\"\\n  echo \"##vso[task.logissue type=error]${GITLEAKS_RESULT}\"\\nelse\\n  echo \"##vso[task.logissue type=error]Gitleaks unknown error\"\\n  exit ${exitcode}\\nfi\\necho \"${OUTPUT}\"\\necho \"# ${GITLEAKS_RESULT}\" >$(Pipeline.Workspace)/summary.md\\necho \\'\\' >>$(Pipeline.Workspace)/summary.md\\necho \\'json\\' >>$(Pipeline.Workspace)/summary.md\\necho \"${OUTPUT}\" >>$(Pipeline.Workspace)/summary.md\\necho \\'\\' >>$(Pipeline.Workspace)/summary.md\\ncat $(Pipeline.Workspace)/summary.md\\necho \"##vso[task.uploadsummary]$(Pipeline.Workspace)/summary.md\"\\necho \"##vso[task.setvariable variable=gitleaks_report]$(Pipeline.Workspace)/${report_name}.${report_format}\"\\nexit ${exitcode}\\n  name: gitleaks\\n  displayName: Run Gitleaks', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_15', 'chunkContent': \"task: PublishPipelineArtifact@1\\n  displayName: Upload SARIF report to Pipeline Artifacts\\n  condition: and(always(), eq(variables.gitleaks_exitcode, '1'))\\n  inputs:\\n    targetPath: $(gitleaks_report)\\n    artifact: CodeAnalysisLogs\\n    publishLocation: pipeline\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk101_16', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nAzure DevOps References\\n\\nGitleaks - Azure Pipelines Extension (unofficial)\\n\\nSARIF SAST Scans Tab\\n\\nReferences\\n\\nGitleaks', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Detection\\\\Recipes\\\\Gitleaks.md'}, {'chunkId': 'chunk102_0', 'chunkContent': 'tags:\\n  - Secrets Store\\n  - Secrets Storage\\n  - Keys Store\\n  - Keys Storage\\n  - Security\\n\\nSecrets Store\\n\\nOverview\\n\\nA secrets store is a secure location for storing your project\\'s secrets and making them available as required to various parts of a project. This may include usage in apps, services, APIs, or deployment processes.\\n\\nProblem Statement\\n\\nAlmost every application is part of a larger ecosystem that connects with other services like databases, APIs, etc. Before the \"Secure Secret Store\" era, secrets used for connection to other services were stored in code, configuration files, initialization parameters, version control repositories, external files, databases, etc. That approach experiences \"Secrets Sprawl\", and refers to when an organization stores secrets in many different places without access control, audit, logging, and many cases without encryption.\\n\\nSolution\\n\\nThe best practice is not to include secrets in source code and to make sure not to store secrets in source control. All sensitive pieces of configuration should be kept in a secure place in a managed way. The Secret Store solves the problem and additionally brings many benefits such as:\\n\\nencryption/decryption when used\\n\\naudit/logging - who accessed the secret and when\\n\\na central place storing secrets, keys, or certificates to support long-term scalability\\n\\neasy access to automation like DevOps workflows to eliminate certain users\\' dependencies\\n\\nTool Selection Matrix', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\index.md'}, {'chunkId': 'chunk102_1', 'chunkContent': \"easy access to automation like DevOps workflows to eliminate certain users' dependencies\\n\\nTool Selection Matrix\\n\\nThe table below has populated a selection of tools covering similar functionality.\\n\\nIt is important to note that other factors may influence your choices, such as the availability of an official Azure DevOps Extension / GitHub Action that can be used in a Workflow or its ability to be used from a shell, among other things.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\index.md'}, {'chunkId': 'chunk102_2', 'chunkContent': 'Tools Use Case Command Line GitHub Action Azure DevOps Extension / Task Service Azure Key Vault Securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets. Azure CLI / PowerShell No / Yes (via Azure CLI Action ) Azure Key Vault Task Cloud-managed ( Azure ) HashiCorp Vault Securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets. Vault CLI Vault Secrets Action No / Yes (via Vault CLI ) Self-managed / Cloud-managed ( HashiCorp Cloud Platform (HCP) ) GitHub Encrypted Secrets Securely store tokens, passwords, API keys, and other secrets for GitHub Workflows. GH CLI Native Support No / Yes (via GH CLI ) Cloud-managed ( GitHub ) Azure DevOps Secure Files Securely store certificates, provisioning profiles, keystore files, SSH keys for Azure Pipelines. N/A N/A Download Secure File Task Self-managed ( Azure DevOps Server ) / Cloud-managed ( Azure DevOps Services )\\n\\nImplementation examples\\n\\nThe links below will direct you to implementation examples that showcase common scenarios.\\n\\nAzure Key Vault usage examples with GitHub Workflows\\n\\nAzure Key Vault usage examples with Azure DevOps Pipelines\\n\\nHashiCorp Vault usage examples with GitHub Workflows\\n\\nHashiCorp Vault usage examples with Azure DevOps Pipelines\\n\\nGitHub Encrypted Secrets usage examples with GitHub Workflows', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\index.md'}, {'chunkId': 'chunk102_3', 'chunkContent': 'HashiCorp Vault usage examples with Azure DevOps Pipelines\\n\\nGitHub Encrypted Secrets usage examples with GitHub Workflows\\n\\nSecure Files usage examples with Azure DevOps Pipelines\\n\\nAzure Key Vault usage examples with Edge Kubernetes Clusters', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\index.md'}, {'chunkId': 'chunk103_0', 'chunkContent': \"summary: Azure Key Vault usage examples with Azure DevOps Pipelines\\nauthors:\\n  - Dariusz Porowski\\ndate: 2022-11-29\\ntags:\\n  - Azure Key Vault\\n  - Key Vault\\n  - Secrets Store\\n  - Azure Pipelines\\n  - Security\\n\\nKey Vault and Azure DevOps\\n\\nTwo common scenarios exist for Key Vault usage in Azure Pipelines. Directly in the pipeline or with Variable Groups as a proxy to Kay Vault. The most significant difference between them is reusability. It's a factor that determines your choice.\\n\\nAzure Key Vault Task is suitable for a one-time job. For this case, Variable Group configuration is not needed.\\n\\nVariable Group linked to Key Vault brings more value if any need exists to reuse data (or it's a plan to reuse it in the future) from the Kay Vault across multiple pipelines or jobs.\\n\\nAzure Key Vault Task\\n\\nFollow the documentation with an example of how to use Azure Key Vault Task in your pipeline: Use Azure Key Vault secrets in Azure Pipelines\\n\\nVariable Group linked to Key Vault\\n\\nFollow the documentation with an example of how to link Azure Key Vault with Variable Group and use it in your pipeline: Link secrets from an Azure Key Vault to the Variable Groups\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-ADO.md'}, {'chunkId': 'chunk104_0', 'chunkContent': 'summary: Azure Key Vault usage examples with GitHub Workflows\\nauthors:\\n  - Dariusz Porowski\\ndate: 2022-11-29\\ntags:\\n  - Azure Key Vault\\n  - Key Vault\\n  - Secrets Store\\n  - GitHub Workflows\\n  - Security\\n\\nKey Vault and GitHub\\n\\nOfficial GitHub Action for getting secrets form Key Vault exists[^1] but is deprecated. The recommended alternative is to use the Azure CLI Action and pass a custom script to access Azure Key Vault.\\n\\n[^1]: Azure Key Vault Action\\n\\nGet secrets from Key Vault using Azure CLI Action\\n\\nFollow the documentation with an example of using Azure Key Vault in your GitHub Workflow: Use Key Vault secrets in GitHub Actions workflows with one exception. Documentation shows an example workflow with the deprecated Azure Key Vault Action mentioned above. Instead of providing the YAML code from docs, use the below with Azure CLI Action.\\n\\n{% raw %}\\n\\n```yaml\\nname: Example Key Vault flow\\n\\non:\\n  push:\\n    branches:\\n      - main\\n\\npermissions:\\n  id-token: write\\n  contents: read', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-GH.md'}, {'chunkId': 'chunk104_1', 'chunkContent': 'permissions:\\n  id-token: write\\n  contents: read\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n    steps:\\n      # checkout the repo\\n      - uses: actions/checkout@v3', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-GH.md'}, {'chunkId': 'chunk104_2', 'chunkContent': \"```\\n\\n{% endraw %}\\n\\nNOTE\\n\\nOIDC\\n\\nOpenID Connect (OIDC) is used in this example. It's a preferred authentication method and allows your GitHub Actions workflows to access resources in Azure without needing to store the Azure credentials as long-lived GitHub secrets. Read more about Configuring OpenID Connect in Azure.\\n\\nMasking\\n\\nThe example above uses the GitHub masking feature that prevents a string or variable from being printed in the log. Each masked word separated by whitespace is replaced with the * character. Read more about Masking a value in log.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-GH.md'}, {'chunkId': 'chunk105_0', 'chunkContent': \"Key Vault and Kubernetes on the Edge\\n\\nManaging secrets for Edge/Kubernetes clusters in DDIL environments (Denied, Disrupted, Intermittent, and Limited) can present unique challenges, particularly due to the potential loss of connectivity to these clusters. Utilizing standard patterns such as Pod Identities or accessing secrets via Key Vault SDKs may not function reliably if the Kubernetes cluster experiences disconnection from Azure.\\n\\nOther methods of managing secrets, such as storing encrypted secrets in code repositories through use of tools such as Sealed Secrets, are also not ideal, as they lack the key features needed to maintain the integrity of secrets, such as:\\n\\nEnforcement of secret rotation and expiry\\n\\nFine-Grained access control\\n\\nThis document covers an implementation for injecting secrets from a Key Vault into an edge/on-prem cluster which may only have occasional connection to Azure Services, while still maintaining an Azure Key Vault as a central repository for all secrets management.\\n\\nKubernetes encompasses a Secret Store CSI Driver, which permits the integration of secrets from external repositories directly into pods as a volume. Upon attaching the volume to the pod, the information can be mounted into the container's filesystem and accessed accordingly. An Azure Key Vault Provider for the Secret Store CSI Driver is available, which enables the injection of secrets from Azure Key Vault into pods using the CSI Driver.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-Kubernetes.md'}, {'chunkId': 'chunk105_1', 'chunkContent': \"There are 2 main scenarios in which one would access secrets in a Kubernetes Context:\\n\\nKey Vault and Kubernetes on the Edge\\n\\nReferencing Secrets from an application running inside a pod\\n\\nReferencing secret values in config files\\n\\nPlease note that the following guidance makes use of Cluster Extensions for Arc Enabled Kubernetes, this requires your Kubernetes Cluster to be onboarded with Azure Arc\\n\\nReferencing Secrets from an application running inside a pod\\n\\nIn this scenario you would have a pod that's been deployed in your cluster, and needs secrets to be mounted into the pods volume. The existing documentation describing how to deploy the Key Vault Secrets Provider into an Arc-enabled Kubernetes Cluster can be followed. There is however, one recommended exception, and that is to enable Secret Rotation when deploying the Kubernetes Extension:\\n\\n```bash\\n\\n!/bin/bash\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-Kubernetes.md'}, {'chunkId': 'chunk105_2', 'chunkContent': '```bash\\n\\n!/bin/bash\\n\\naz k8s-extension create --cluster-name $CLUSTER_NAME \\\\\\n--resource-group $RESOURCE_GROUP \\\\\\n--cluster-type connectedClusters \\\\\\n--extension-type Microsoft.AzureKeyVaultSecretsProvider \\\\\\n--name $EXTENSION_NAME \\\\\\n--configuration-settings secrets-store-csi-driver.enableSecretRotation=true\\n--configuration-settings secrets-store-csi-driver.rotationPollInterval=2m', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-Kubernetes.md'}, {'chunkId': 'chunk105_3', 'chunkContent': '```\\n\\nReferencing secret values in config files\\n\\nIn this scenario, you would have a config file that needs to reference a Kubernetes Secret. You cannot directly use the secrets provider as done in the previous scenario, as a config file does not have the capability to access a mounted volume. As part of the Secrets Store CSI Driver, there is a feature to create Kubernetes Secret Objects from secrets ingested by the provider.\\n\\nIn order to achieve this, it is first recommended to the follow the existing documentation, but with 2 minor modifications:\\n\\nWhen installing the Key Vault Secrets Provider Extension, enable Key Rotation, and Sync Secrets:\\n```bash\\n!/bin/bash\\naz k8s-extension create --cluster-name $CLUSTER_NAME \\\\\\n--resource-group $RESOURCE_GROUP \\\\\\n--cluster-type connectedClusters \\\\\\n--extension-type Microsoft.AzureKeyVaultSecretsProvider \\\\\\n--name akvsecretsprovider \\\\\\n--configuration-settings secrets-store-csi-driver.enableSecretRotation=true \\\\\\n--configuration-settings secrets-store-csi-driver.rotationPollInterval=2m \\\\\\n--configuration-settings secrets-store-csi-driver.syncSecret.enabled=true', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-Kubernetes.md'}, {'chunkId': 'chunk105_4', 'chunkContent': '```', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-Kubernetes.md'}, {'chunkId': 'chunk105_5', 'chunkContent': 'After following the existing documentation, create a Kubernetes deployment that references this secrets provider and mounts the required secrets into its volume. This will trigger the creation of Kubernetes Secrets for everything ingested by the secrets provider, allowing it to then be referenced in various configuration files:\\nyaml\\n  apiVersion: apps/v1\\n  kind: Deployment\\n  metadata:\\n    name: secrets-sync-agent\\n    labels:\\n      app: secrets-sync-agent\\n  spec:\\n    replicas: 1\\n    selector:\\n      matchLabels:\\n        app: secrets-sync-agent\\n    template:\\n      metadata:\\n        labels:\\n          app: secrets-sync-agent\\n      spec:\\n        volumes:\\n          - name: secrets-store-inline\\n            csi:\\n              driver: secrets-store.csi.k8s.io\\n              readOnly: true', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-Kubernetes.md'}, {'chunkId': 'chunk105_6', 'chunkContent': 'readOnly: true\\n              volumeAttributes:\\n                secretProviderClass: $SECRET_PROVIDER_CLASS_NAME\\n              nodePublishSecretRef:\\n                name: secrets-store-creds\\n        containers:\\n          - name: alpine\\n            image: alpine:latest\\n            imagePullPolicy: IfNotPresent\\n            command:\\n              - \"/bin/sleep\"\\n              - \"10000\"\\n            volumeMounts:\\n              - name: secrets-store-inline\\n                mountPath: \"/mnt/secrets-store\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-Kubernetes.md'}, {'chunkId': 'chunk105_7', 'chunkContent': 'mountPath: \"/mnt/secrets-store\"\\n                readOnly: true', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Azure-KeyVault-Kubernetes.md'}, {'chunkId': 'chunk106_0', 'chunkContent': 'summary: GitHub Encrypted Secrets usage examples with GitHub Workflows\\nauthors:\\n  - Dariusz Porowski\\ndate: 2022-11-30\\ntags:\\n  - GitHub Encrypted Secrets\\n  - Secrets Store\\n  - GitHub Workflows\\n  - Security\\n\\nGitHub Encrypted Secrets and GitHub\\n\\nEncrypted Secrets allow you to store sensitive information in your organization, repository, or repository environments. Secrets are represented as environment variables.\\n\\nActions secrets\\n\\nActions secrets are environment variables that are encrypted. Anyone with collaborator access to this repository can use these secrets for Actions.\\n\\nOrganization secrets\\n\\nFor secrets stored at the organization level, you can use access policies to control which repositories can use organization secrets. In addition, organization-level secrets let you share secrets between multiple repositories, reducing the need to create duplicate secrets. Updating an organization secret in one location also ensures that the change takes effect in all repository workflows that use that secret.\\n\\nRepository secrets\\n\\nRepository secrets are specific to a single repository and all environments used there.\\n\\nEnvironment secrets\\n\\nFor secrets stored at the environment level, you can enable required reviewers to control access to the secrets. A workflow job cannot access environment secrets until required approvers approve. You can use environment secrets if you have secrets that are specific to an environment.\\n\\nUsing encrypted secrets in a workflow', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\GitHub-Encrypted-Secrets.md'}, {'chunkId': 'chunk106_1', 'chunkContent': 'Using encrypted secrets in a workflow\\n\\nFollow the documentation with an example of using Encrypted Secrets in your GitHub Workflow: Using encrypted secrets in a workflow\\n\\nAdditional Resources\\n\\nEncrypted Secrets', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\GitHub-Encrypted-Secrets.md'}, {'chunkId': 'chunk107_0', 'chunkContent': \"summary: HashiCorp Vault usage examples with Azure DevOps\\nauthors:\\n  - Dariusz Porowski\\ndate: 2022-12-29\\ntags:\\n  - HashiCorp\\n  - Vault\\n  - Secrets Store\\n  - Azure DevOps\\n  - Security\\n\\nHashiCorp Vault and Azure DevOps\\n\\nIn the absence of the official Azure DevOps Tasks Extension, the preferred integration method is Azure Auth Method - it allows authentication against Vault using Azure Managed Service Identity (MSI). This method supports authentication for System-assigned and User-assigned managed identities. The limitation of this method is it's only available to private Azure DevOps agents running on Azure. Configuration for this scenario is pretty well documented on HashiCorp sites. Please follow this guide: Azure Auth Method.\\n\\nIf in your solution, you would like to use hosted agents or private agents not run on Azure (e.g., on-premises) then the AppRole auth method is your answer. This guide shows you how to use AppRole[^1] auth method to extract secrets from HashiCorp Vault.\\n\\n[^1]: HashiCorp Vault AppRole Auth Method\\n\\nGet secrets from HashiCorp Vault using AppRole auth method\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-ADO.md'}, {'chunkId': 'chunk107_1', 'chunkContent': '[^1]: HashiCorp Vault AppRole Auth Method\\n\\nGet secrets from HashiCorp Vault using AppRole auth method\\n\\nThe example below uses Azure DevOps Generic Service Connection to store AppRole access details to the Vault, and community extension (Authenticated Scripts) to utilize Generic Service Connection data inside a Bash script.\\n\\n{% raw %}\\n\\n```yaml\\ntrigger:\\n  - main\\n\\nname: Example HashiCorp Vault flow\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n  - script: |\\n      wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\\n      echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\\n      sudo apt update && sudo apt install vault\\n      vault --version\\n    displayName: Setup Vault CLI', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-ADO.md'}, {'chunkId': 'chunk107_2', 'chunkContent': 'task: AuthenticatedBash@1\\n    displayName: Get secrets from Vault\\n    inputs:\\n      serviceConnection: \"Vault Service Connection\"\\n      targetType: \"inline\"\\n      script: |\\n        # Set Vault URL\\n        export VAULT_ADDR=\"${AS_SC_URL}\"\\n# Get AppRole token and set Vault token\\nexport VAULT_TOKEN==$(vault write auth/ado-approle/login role_id=\"${AS_SC_USERNAME}\" secret_id=\"${AS_SC_PASSWORD}\" -format=\"json\" | jq -rc \\'.auth.client_token\\')\\n\\n# Get secrets\\ncontainerUsername=$(vault kv get -field=containerUsername myProject/acr)\\ncontainerPassword=$(vault kv get -field=containerPassword myProject/acr)\\n\\n# Set secrets for next steps\\necho \"##vso[task.setvariable variable=containerUsername;issecret=true]${containerUsername}\"\\necho \"##vso[task.setvariable variable=containerPassword;issecret=true]${containerPassword}\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-ADO.md'}, {'chunkId': 'chunk107_3', 'chunkContent': 'script: |\\n      docker login myregistry.azurecr.io --username $(containerUsername) --password $(containerPassword)\\n    displayName: Login to Docker Registry', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-ADO.md'}, {'chunkId': 'chunk107_4', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nThe script below will bootstrap your environment with an example configuration that is required for the above Azure Pipelines.\\n\\nVault configuration with a AppRole auth\\n\\nVault KVv2 store with examples secrets\\n\\nAzure DevOps extensions setup\\n\\nAzure DevOps Generic Service Connection setup\\n\\n{% raw %}\\n\\n```bash\\n\\n!/bin/bash\\n\\nVariables\\n\\nAZURE_DEVOPS_ORGANIZATION_URL=\"https://dev.azure.com/MyAdoOrg\"\\nAZURE_DEVOPS_PROJECT_NAME=\"MyAdoProject\"\\nAZURE_DEVOPS_SERVICE_ENDPOINT_NAME=\"Vault Service Connection\" # Azure DevOps Service Connection Name\\n\\nBasic Vault config\\n\\nexport VAULT_ADDR=\"https://myvault.mycompany.com:8200\" # Vault address\\nexport VAULT_TOKEN=\"MyToken\"                           # Vault token\\n\\nSecrets config', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-ADO.md'}, {'chunkId': 'chunk107_5', 'chunkContent': 'Secrets config\\n\\nVAULT_SECRETS_ENGINE_NAME=\"myProject\" # Vault secrets engine name\\nACR_STORE_NAME=\"acr\"                  # ACR Store name\\nACR_USERNAME=\"myAcrUsername\"          # ACR username\\nACR_PASSWORD=\"myAcrPassword\"          # ACR password\\n\\nHelper for Azure CLI DevOps Extension check/setup\\n\\nfunction _setup_az_cli_ado_extension() {\\n    if ! az extension show --name azure-devops >/dev/null; then\\n        az extension add --name azure-devops\\n    else\\n        az extension update --name azure-devops\\n    fi\\n}\\n\\nHelper for Azure DevOps Extensions', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-ADO.md'}, {'chunkId': 'chunk107_6', 'chunkContent': 'Helper for Azure DevOps Extensions\\n\\nfunction _setup_ado_extensions() {\\n    # Install Authenticated Scripts extension (https://marketplace.visualstudio.com/items?itemName=cloudpup.authenticated-scripts)\\n    az devops extension install --publisher-id cloudpup --extension-id authenticated-scripts --organization \"${AZURE_DEVOPS_ORGANIZATION_URL}\" --project \"${AZURE_DEVOPS_PROJECT_NAME}\"\\n}\\n\\nEnable Vault Secrets Engine and put ACR creds into secrets store\\n\\nfunction setup_vault_secrets_engine() {\\n    # Enable Vault Secrets Engine\\n    vault secrets enable -version=2 -path=\"${VAULT_SECRETS_ENGINE_NAME}\" kv\\n\\nEnable AppRole auth method and create role for Azure DevOps\\n\\nfunction setup_vault_approle() {\\n    # Enable AppRole auth method for # Vault address\\n    vault auth enable -path=ado-approle approle\\n\\nRead-only permission on \\'${VAULT_SECRETS_ENGINE_NAME}/data/*\\' path', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-ADO.md'}, {'chunkId': 'chunk107_7', 'chunkContent': 'Read-only permission on \\'${VAULT_SECRETS_ENGINE_NAME}/data/*\\' path\\n\\npath \"${VAULT_SECRETS_ENGINE_NAME}/data/*\" {\\n  capabilities = [ \"read\" ]\\n}\\nEOF\\n\\n{\\n  \"data\": {},\\n  \"name\": \"${AZURE_DEVOPS_SERVICE_ENDPOINT_NAME}\",\\n  \"type\": \"Generic\",\\n  \"url\": \"${VAULT_ADDR}\",\\n  \"authorization\": {\\n    \"parameters\": {\\n      \"username\": \"${role_id}\",\\n      \"password\": \"${secret_id}\"\\n    },\\n    \"scheme\": \"UsernamePassword\"\\n  },\\n  \"isShared\": false,\\n  \"isReady\": true\\n}\\nEOF\\n}\\n\\nCreate Azure DevOps Generic Service Connection for Vault\\n\\nfunction setup_ado_service_connection() {\\n    _setup_az_cli_ado_extension\\n    _setup_ado_extensions\\n\\nExecute\\n\\nsetup_vault_secrets_engine\\nsetup_vault_approle\\nsetup_ado_service_connection', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-ADO.md'}, {'chunkId': 'chunk107_8', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nNOTE\\n\\nBefore proceeding, you must plan your security strategy to ensure that access tokens are only allocated predictably. The attached script does only basic configuration for example purposes. Read more about Vault AppRole security and design before you run in production.\\n\\nAppRole Usage Best Practices\\n\\nHow (and Why) to Use AppRole Correctly in HashiCorp Vault\\n\\nAdditional Resources\\n\\nAppRole Pull Authentication\\n\\nAuthenticating Applications with HashiCorp Vault AppRole', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-ADO.md'}, {'chunkId': 'chunk108_0', 'chunkContent': 'summary: HashiCorp Vault usage examples with GitHub Workflows\\nauthors:\\n  - Dariusz Porowski\\ndate: 2022-11-30\\ntags:\\n  - HashiCorp\\n  - Vault\\n  - Secrets Store\\n  - GitHub Workflows\\n  - Security\\n\\nHashiCorp Vault and GitHub\\n\\nOfficial GitHub Action for getting secrets from HashiCorp Vault exists[^1], and this example shows you how to retrieve and use secrets in your GitHub Workflow.\\n\\n[^1]: HashiCorp Vault Action\\n\\nGet secrets from HashiCorp Vault using HashiCorp Vault Action with OpenID Connect (OIDC)\\n\\nOfficial GitHub documentation contains an example configuration[^2], but in some places may need clarification. Therefore, the below code extends it for your convenience.\\n\\n[^2]: Configuring OpenID Connect in HashiCorp Vault\\n\\n{% raw %}\\n\\n```yaml\\nname: Example HashiCorp Vault flow\\n\\non:\\n  push:\\n    branches:\\n      - main\\n  workflow_dispatch:\\n\\nvariables:\\n  - VAULT_ADDR: https://myvault.mycompany.com:8200', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-GH.md'}, {'chunkId': 'chunk108_1', 'chunkContent': 'variables:\\n  - VAULT_ADDR: https://myvault.mycompany.com:8200\\n\\njobs:\\n  retrieve-secret:\\n    runs-on: ubuntu-latest\\n    permissions:\\n      id-token: write\\n      contents: read\\n    steps:\\n      # checkout the repo\\n      - uses: actions/checkout@v3', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-GH.md'}, {'chunkId': 'chunk108_2', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nThe script below will bootstrap your Vault configuration with a basic setup (OIDC GitHub auth for HashiCorp Vault Action) to use with GitHub Workflow above.\\n\\n{% raw %}\\n\\n```bash\\n\\n!/bin/bash\\n\\nBasic Vault config\\n\\nexport VAULT_ADDR=\"https://myvault.mycompany.com:8200\"\\nexport VAULT_TOKEN=\"MyToken\"\\n\\nVariables\\n\\nGH_ORG=\"myCompany\"               # GitHub User name or Organization name\\nGH_REPO=\"myRepo\"                 # GitHub Repository name\\nSECRETS_ENGINE_NAME=\"myProject\"  # Vault secrets engine name\\nACR_STORE_NAME=\"acr\"             # ACR Store name\\nACR_USERNAME=\"myAcrUsername\"     # ACR username\\nACR_PASSWORD=\"myAcrPassword\"     # ACR password\\n\\nEnable Vault Secrets Engine\\n\\nvault secrets enable -version=2 -path=\"${SECRETS_ENGINE_NAME}\" kv\\n\\nPut ACR creds into secrets store', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-GH.md'}, {'chunkId': 'chunk108_3', 'chunkContent': 'Put ACR creds into secrets store\\n\\nvault kv put \"${SECRETS_ENGINE_NAME}\"/\"${ACR_STORE_NAME}\" containerUsername=\"${ACR_USERNAME}\" containerPassword=\"${ACR_PASSWORD}\"\\n\\nEnable JWT/OIDC auth method for GitHub\\n\\nvault auth enable -path=github jwt\\n\\nConfigure JWT/OIDC auth method for GitHub\\n\\nvault write auth/github/config bound_issuer=\"https://token.actions.githubusercontent.com\" oidc_discovery_url=\"https://token.actions.githubusercontent.com\"\\n\\nCreate read policy for GitHub with read-only access to secrets engine\\n\\nvault policy write github-policy - <<EOF\\n\\nRead-only permission on \\'${SECRETS_ENGINE_NAME}/data/*\\' path\\n\\npath \"${SECRETS_ENGINE_NAME}/data/*\" {\\n  capabilities = [ \"read\" ]\\n}\\nEOF\\n\\nCreate role for GitHub JWT/OIDC auth method', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-GH.md'}, {'chunkId': 'chunk108_4', 'chunkContent': 'Create role for GitHub JWT/OIDC auth method\\n\\nvault write auth/github/role/github-role - <<EOF\\n{\\n  \"role_type\": \"jwt\",\\n  \"user_claim\": \"actor\",\\n  \"bound_claims\": {\\n    \"repository\": \"${GH_ORG}/${GH_REPO}\"\\n  },\\n  \"policies\": [\"github-policy\"],\\n  \"ttl\": \"15m\"\\n}\\nEOF', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-GH.md'}, {'chunkId': 'chunk108_5', 'chunkContent': \"```\\n\\n{% endraw %}\\n\\nNOTE\\n\\nBefore proceeding, you must plan your security strategy to ensure that access tokens are only allocated predictably. To control how your cloud provider issues access tokens, you must define at least one condition so that untrusted repositories can't request access tokens for your cloud resources. For more information, see Configuring the OIDC trust with the cloud.\\n\\nAdditional Resources\\n\\nUsing OIDC With HashiCorp Vault and GitHub Actions\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\HashiCorp-Vault-GH.md'}, {'chunkId': 'chunk109_0', 'chunkContent': 'summary: Secure Files usage examples with Azure DevOps Pipelines\\nauthors:\\n  - Dariusz Porowski\\ndate: 2022-11-29\\ntags:\\n  - Secure Files\\n  - Secrets Store\\n  - Azure Pipelines\\n  - Security\\n\\nAzure DevOps Secure Files\\n\\nSecure files allow you to store files you can share across pipelines. Use the Secure Files library to store files like:\\n\\nsigning certificates\\n\\nApple Provisioning Profiles\\n\\nAndroid Keystore files\\n\\nSSH keys\\n\\nThese files can be stored on the server without committing them to your repository.\\n\\nUse Secure Files\\n\\nFollow the documentation with an example of using Secure Files in your pipeline: Use secure files', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\02-Develop\\\\Secrets-Store\\\\Recipes\\\\Secure-Files-ADO.md'}, {'chunkId': 'chunk110_0', 'chunkContent': 'Deploy Phase\\n\\n```mermaid\\nflowchart LR\\n    plan{{PLAN}}==>dev{{DEVELOP}}==>deploy{{DEPLOY}}==>operate{{OPERATE}}\\n    style deploy fill:lightgray, text:black', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\index.md'}, {'chunkId': 'chunk110_1', 'chunkContent': '```\\n\\nThe Deploy Phase is focused on delivering and integrating assembled solutions into live production environments. The objective of DevSecOps is to ensure the delivery of resilient software systems at the speed of relevance and integrating security at every step. DevSecOps strives for faster and more secure software and solution delivery while achieving governance and control. It is therefore in the automation/deployment zone where confidence is built in the DevSecOps lifecycle giving credence to the \"software, safer, sooner\" motto, which, through continuous improvement is able to deliver secure software without impacting delivery velocity in a negative way.\\n\\nThe Continuous Deployment feedback loop iterates across the Plan, Develop, and Deploy phases of the DevSecOps lifecycle. Deployment is therefore the formal act of pushing one or more features into production in an automated\\nfashion, introducing control gates powered by tools that catch vulnerabilities and prevents defective software from being deployed in production while offering developers transparent and actionable feedback in the CI/CD workflow.\\n\\nAutomated processes and tools focused on the Deploy Phase results in accelerated vulnerability remediation, simplifies compliance and limits the window of time that a threat actor has to take advantage of vulnerabilities. This allows security teams to focus on critical issues while repeatable and adaptable workflows monitor the code, software and environments.\\n\\nThis phase may contain workflows that string together a collection of tools, each responsible for contributing towards an improved security posture and higher quality code.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\index.md'}, {'chunkId': 'chunk110_2', 'chunkContent': 'This phase may contain workflows that string together a collection of tools, each responsible for contributing towards an improved security posture and higher quality code.\\n\\nTool Guidance\\n\\nThe table below provides a non-exhaustive list of options which may be considered to continuously improve the deploy phase.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\index.md'}, {'chunkId': 'chunk110_3', 'chunkContent': 'Tool Supported Features Benefits Secret Scanner Scans all checked in code for secrets and sensitive information. Automates review automated key rotation. Test coverage Measures how much code is exercised while the automated tests are running. Shows the fidelity of the test results. License checker Creates an inventory of license from dependencies. Provides Software license compliance and software asset management reporting. SAST Tooling Performs Static Application Security Test. Provides a static code scan report and recommended mitigation or auto mitigates risks. SCA Tools Performs software composition analysis. Identify and track compliance risk that may result from the use of OSS and third-party code in applications. DAST Tooling Dynamic Application Security Test allows testing of the software on the underlying operating system and perform testing with fuzz inputs. Identifies vulnerabilities and dynamic code defects and generates recommended remediation steps. Integration Test Tests the interaction of software with the focused units it will interact with once deployed. Reports whether integrated units perform as expected. Container Scanning Tools Test containers and nested dependencies. Identifies vulnerable container images and recommends remediation steps. Penetration Testing Evaluate the security of a system by injecting authorized simulated cyber-attacks. Reports security vulnerabilities and remediation and mitigation steps. IaC Scanning Tools Scans infrastructure as Code definitions commonly found in Terraform and Bicep scripts, Helm Charts, etc to identify misconfiguration. Provides a report of potential misconfigurations and deviations from set policies. Policy and compliance tools Ensures software', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\index.md'}, {'chunkId': 'chunk110_4', 'chunkContent': 'etc to identify misconfiguration. Provides a report of potential misconfigurations and deviations from set policies. Policy and compliance tools Ensures software and dependencies align to defined organization and security policies. Provides policy violation report and remediation steps. Windows Binaries Scanning Tools Perform security analysis on windows binaries. Identity security issues of windows binaries.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\index.md'}, {'chunkId': 'chunk110_5', 'chunkContent': 'References\\n\\nSecurity governance\\n\\nSecurity best practices for IaaS workloads in Azure', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\index.md'}, {'chunkId': 'chunk111_0', 'chunkContent': \"Container Scanning in the CI/CD Pipeline\\n\\nProblem Statement\\n\\nPopularity of containerized deployment is growing and container images are becoming the standard application delivery model in today's cloud-native environments. Using containers, developers can deploy applications more quickly and reliably by transforming them into self-contained container images (learn more about containers). These images become an appealing target for malicious actors because a single vulnerable or compromised container could potentially become a point of entry into an organization’s broader environment. One of the most common container security threats is malware that is embedded in a container image. Even container images from well known and trusted vendors often have vulnerabilities.\\n\\nIn the containerized deployment model, the development team is responsible for ensuring that their container images are secure. This is a challenging task because the development team is not always aware of the vulnerabilities in the tools added to their images, as well as the vulnerabilities in the base images that their container images are based on.\\n\\nSolution\\n\\nContainer scanning is an efficient way for developers to ensure their containers are secure. Security scanners find vulnerabilities within your images, as well as within the images that your container images are based on.\\n\\nThe scope of this page includes the static analysis of container images for vulnerabilities, after the image is built in the CI/CD pipeline and before being pushed to a registry for deployment. Scanning in the CI/CD pipelines prevents container images containing critical vulnerabilities from being stored in your registries and deployed into production.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\index.md'}, {'chunkId': 'chunk111_1', 'chunkContent': 'Tools\\n\\nThe tools in this category scan for vulnerabilities by comparing the components of a container to the Mitre CVE database. You can find more information about Common Vulnerabilities and Exposures (CVE) database at CVE.org.\\n\\nSelect a tool that meets your requirements and budget. Like other scanning tools, consider one that produces low false-positive/false-negative outputs.\\n\\nHere are a few free tools to consider:\\n\\nTrivy\\n  Trivy, by Aqua Security, is a simple and comprehensive scanner. It is an open source tool and designed to be used within a CI/CD process to scan for vulnerabilities before sending an image to a container registry or deploying it to a target environment. It scans:\\n\\nFor vulnerabilities in container images, file systems, Git repositories and for configuration issues. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and language-specific packages (Bundler, Composer, npm, yarn, etc.).\\n\\nInfrastructure as Code (IaC) files such as Terraform, Dockerfile and Kubernetes, to detect potential configuration issues that expose your deployments to the risk of attack.\\n\\nFor hardcoded secrets like passwords, API keys and tokens.\\n\\nComplete documentation for Trivy can be found here.\\n\\nClair\\n  Clair is an open source tool that perform static analysis of container vulnerabilities.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\index.md'}, {'chunkId': 'chunk111_2', 'chunkContent': 'Clair\\n  Clair is an open source tool that perform static analysis of container vulnerabilities.\\n\\nIt works with OCI and Docker containers. Clair ingests many vulnerability data sources such as Debian Security Bug Tracker, Ubuntu CVE Tracker and Red Hat Security Data.\\n\\nSince Clair consumes so many CVE databases, its auditing is comprehensive.\\n\\nTo get started with Clair, see the Clair documentation page.\\n\\nHow to choose a tool\\n\\nTrivy is a more suitable scanning tool for CI/CD pipelines because it does not require additional services and is ready to scan immediately by automatically fetching the required CVEs database on the fly during the scans.\\n\\nClair was originally not designed to be used within CI/CD pipelines, it needs several workarounds in order to make it work. It also requires a PostgreSQL database server to be running. With an empty database, it could take about 20 to 30 minutes to fill it up with CVEs. The workaround is to use a pre-filled database that is updated daily and published as a third party container image to Docker Hub.\\n\\nCode Examples and Recipes\\n\\nFor sample GitHub Workflows, Azure DevOps Pipelines, and general usage of selected tools please follow listed articles:\\n\\nRunning Trivy on Azure DevOps\\n\\nRunning Clair on Azure DevOps\\n\\nAnalyze Scan Results\\n\\nReview and resolve the container scan results.\\n\\nActions', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\index.md'}, {'chunkId': 'chunk111_3', 'chunkContent': \"Running Clair on Azure DevOps\\n\\nAnalyze Scan Results\\n\\nReview and resolve the container scan results.\\n\\nActions\\n\\nTriage the output from the scans to identify false-positives and false-negatives.\\n\\nDocument and track the remaining issues.\\n\\nPrioritize work based on the Severity rating generated by the tool, if available. You can also use the Severity rating on the National Vulnerability Database (NVD) by searching for the CVE. The NVD provides a CVSS Score for each of the vulnerabilities in the database.\\n\\nProvide Governance and Training\\n\\nEnforce the proper use of container scanning in the CI/CD pipeline and ensure that your organization's compliance policies are followed.\\n\\nActions\\n\\nDocument the process as a component of the software development lifecycle.\\n\\nTrain users on how to configure and operate the tool.\\n\\nResources\\n\\nCommon Vulnerabilities and Exposure (CVE)\\n\\nCommon Vulnerability Scoring System (CVSS)\\n\\nNIST National Vulnerability Database (NVD)\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\index.md'}, {'chunkId': 'chunk112_0', 'chunkContent': 'Running Clair on Azure DevOps\\n\\nObjective\\n\\nConfigure and run Clair on an Azure DevOps CI/CD pipeline.\\n\\nActions\\n\\nClair runs as a centralized scanning service but there are options to run it on the pipeline. Clair Local Scan is an implementation that provides a Clair CLI and a local (pre-populated) CVE database.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\Recipes\\\\Clair-ADO.md'}, {'chunkId': 'chunk113_0', 'chunkContent': 'Running Trivy on Azure DevOps\\n\\nObjective\\n\\nConfigure and run Trivy on an Azure DevOps CI/CD pipeline. Compared to Clair, this integrates better on pipelines.\\n\\nActions\\n\\nThere are three options how to run Trivy in an Azure DevOps pipeline:\\n\\nRun Trivy using the official Trivy pipeline task\\n  An easy option to run, but you have to install Trivy extension from the Marketplace to your Azure DevOps organization and it is not possible to configure the severity level of the issues to be scanned. As a result, the pipeline will fail if any issue is found.\\n\\nRun Trivy as a Docker container\\n  The easiest and flexible option to run. It is possible to configure the severity level of the issues to be scanned and fail a pipeline if, for example, only high or critical severity issues are found.\\n\\nRun Trivy as a CLI tool\\n  This is the hardest option to run. You have to install and configure the Trivy CLI tool on the build agent. It is flexible as the Docker option mentioned above.\\n\\nAs you can see above the easiest and more efficient way is to run Trivy as a Docker container.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\Recipes\\\\Trivy-ADO.md'}, {'chunkId': 'chunk113_1', 'chunkContent': 'As you can see above the easiest and more efficient way is to run Trivy as a Docker container.\\n\\nIn the example below, we will run Trivy twice, once for low and medium severity, and again for high and critical severity issues. They are separated because we want to see all issues, but we only want to fail the build in case of high or critical issues. The severity levels are textual representation of the numeric Base, Temporal and Environmental scores of the vulnerability. The possible values are LOW, MEDIUM, HIGH and CRITICAL. These levels are defined by the Common Vulnerability Scoring System (CVSS).\\n\\n```yaml\\nvariables:\\n  # Container image to scan\\n  container_image: openjdk:17-jdk-slim', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\Recipes\\\\Trivy-ADO.md'}, {'chunkId': 'chunk113_2', 'chunkContent': 'jobs:\\n- job: TrivyDockerScan\\n  displayName: Scan image $(container_image)\\n  steps:\\n  # Pull the Trivy image from Docker Hub\\n  - script: docker pull aquasec/trivy\\n    displayName: Pull Trivy image\\n  # Run Trivy for low and medium severity issues\\n  - script: docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v $HOME/Library/Caches:/root/.cache/ -v $(Agent.TempDirectory):/atd aquasec/trivy image --severity LOW,MEDIUM --format template --template \"@contrib/junit.tpl\" -o /atd/junit-report-low-med.xml $(container_image)\\n    displayName: Trivy scan (Low/Med)\\n  # Run Trivy for high and critical severity issues', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\Recipes\\\\Trivy-ADO.md'}, {'chunkId': 'chunk113_3', 'chunkContent': 'displayName: Trivy scan (Low/Med)\\n  # Run Trivy for high and critical severity issues\\n  - script: docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v $HOME/Library/Caches:/root/.cache/ -v $(Agent.TempDirectory):/atd aquasec/trivy image --severity HIGH,CRITICAL --format template --template \"@contrib/junit.tpl\" -o /atd/junit-report-high-crit.xml $(container_image)\\n    displayName: Trivy scan (High/Crit)\\n  # Publish the Trivy low and medium severity scan results as JUnit-style test results\\n  - task: PublishTestResults@2\\n    inputs:\\n      testResultsFormat: JUnit\\n      testResultsFiles: $(Agent.TempDirectory)/junit-report-low-med.xml\\n      mergeTestResults: true\\n      failTaskOnFailedTests: false\\n      testRunTitle: Trivy container scan - low/medium vulnerabilities\\n    condition: always()\\n    displayName: Publish result (Low/Med)', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\Recipes\\\\Trivy-ADO.md'}, {'chunkId': 'chunk113_4', 'chunkContent': 'condition: always()\\n    displayName: Publish result (Low/Med)\\n  # Publish the Trivy high and critical severity scan results as JUnit-style test results\\n  - task: PublishTestResults@2\\n    inputs:\\n      testResultsFormat: JUnit\\n      testResultsFiles: $(Agent.TempDirectory)/junit-report-high-crit.xml\\n      mergeTestResults: true\\n      failTaskOnFailedTests: true\\n      testRunTitle: Trivy container scan - high/critical vulnerabilities\\n    condition: always()\\n    displayName: Publish result (High/Crit)', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\Recipes\\\\Trivy-ADO.md'}, {'chunkId': 'chunk113_5', 'chunkContent': '```\\n\\nDescription of the above steps:\\n\\nThe first step pulls the Trivy image from Docker Hub into an agent local cache. This step can be skipped because the next step will pull the image if it is not available locally. However it is recommended to pull the image in a separate step to easily identify if the image pull failed.\\n\\nThe second step runs a container and maps the agent folder $(Agent.TempDirectory) into a folder /atd inside a container. The container will execute the Trivy scan with the low and medium severity level. The output of the scan will be written to the junit-report-low-med.xml file in JUnit format in the agent folder $(Agent.TempDirectory).\\n\\nThe third step runs the new container, but this time with the high and critical severity level. The output of the scan will be written to the junit-report-high-crit.xml file in JUnit format in the agent folder $(Agent.TempDirectory).\\n\\nThe fourth step publishes the Trivy low and medium severity scan results as JUnit-style test results. The failTaskOnFailedTests parameter is set to false to not fail the build in case of low and medium severity issues are found.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\Recipes\\\\Trivy-ADO.md'}, {'chunkId': 'chunk113_6', 'chunkContent': 'The fifth step publishes the Trivy high and critical severity scan results as JUnit-style test results. The failTaskOnFailedTests parameter is set to true to fail the build in case of high or critical severity issues are found.\\n\\nScan a container image from the private container registry\\n\\nIf you want to scan a container image from a private container registry, you need to log in to the registry first. Then you need to pull an image to an agent local cache from that registry. And then execute all the steps mentioned above.\\n\\nThe variable container_image should be set to the full name of the image in the registry (that contains a login server of the registry as well). Below is an example of the full name of the image myimage:latest stored in the Azure Container Registry myregistry:\\n\\nyaml\\nvariables:\\n  container_image: myregistry.azurecr.io/myimage:latest\\n\\nThe following example shows the steps that must be added before all of the steps mentioned above to sign in to the Azure Container Registry (ACR) and pull the private image to an agent. The example uses the Service Connection named myregistry-connection that needs to be created in Azure DevOps. The Service Connection uses the Docker Registry type and authenticates with the service principal to the registry:', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\Recipes\\\\Trivy-ADO.md'}, {'chunkId': 'chunk113_7', 'chunkContent': \"yaml\\n  # Login to the private container registry\\n  - task: Docker@2\\n    inputs:\\n      containerRegistry: 'myregistry-connection'\\n      command: 'login'\\n  # Pull an image from the private container registry\\n  - script: docker pull $(container_image)\\n    displayName: Pull $(container_image) image\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Container-Scanning\\\\Recipes\\\\Trivy-ADO.md'}, {'chunkId': 'chunk114_0', 'chunkContent': \"Dependency Scanning\\n\\nProblem statement\\n\\nSoftware engineering teams often rely on third-party libraries for accelerating development without recreating commonly used utilities and specialized tools for each application. Although these libraries are often a small part of a solution, they can present a pathway for the introduction of malicious code. In addition, these dependencies can add legal risk if the third-party library is not under a license with appropriate use conditions. These problems are complicated when organizations don't have a comprehensive inventory of their dependencies.\\n\\nEvaluating dependency risks\\n\\nDependency evaluation and risk analysis can be broken into a few areas of concern:\\n\\nThe development team does not always have insight into the details of the code included in the third-party software taken as a dependency. Even if the software is an Open Source Software (OSS) library, with public code repositories, part of the thought behind the use of OSS and other third-party libraries is to free up the engineering team to focus on developing features rather than common utilities.\\n\\nMaintainers of OSS change over time, lose support, and sometimes fail to maintain security updates. Even well-maintained libraries can have newly discovered vulnerabilities that have yet to be patched.\\n\\nMany OSS libraries have a host of contributors and a small number of maintainers. The maintainers take on the responsibility of considering the overall impact of contributions from the library's community. This places final security decisions on those few maintainers.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dependency-Scanning\\\\index.md'}, {'chunkId': 'chunk114_1', 'chunkContent': 'Some OSS libraries are passion projects without the resources necessary for holding themselves to a high standard of security.\\n\\nThere are documented cases of OSS libraries containing intentionally malicious code that can provide entry points for harmful actors into consuming applications. This can lead to hijacking resources, manipulating code, and other unwanted access.\\n\\nDependencies are chained. So, use of a third-party library automatically accepts each library used by that third-party (dependencies within dependencies).\\n\\nLicensing\\n\\nThere are many licenses that software can be released under. Licenses dictate how, and in some cases for what, a library can be used. This can be a complex landscape to navigate, and it is wise for an organization to set policy on what type of licenses are included and/or excluded from use. To enforce license policy, the previously mentioned dependency inventory must include the license under which each library is released.\\n\\nSolution\\n\\nDependency Scanning, Software Composition Analysis (SCA), and Open Source Security (OSS) Scanning solutions analyze applications and their dependencies to detect vulnerable open-source libraries and third-party software components. These scanners also identify licensing risks and components that may require security updates or patches.\\n\\nTools', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dependency-Scanning\\\\index.md'}, {'chunkId': 'chunk114_2', 'chunkContent': \"Tools\\n\\nWhen selecting a tool for dependency scanning, it should use more than one source for detecting vulnerabilities. Dependencies are usually represented by their Common Platform Enumeration (CPE) names and crosschecked with the Common Vulnerabilities and Exposure (CVE) database. NPM Audit, Sonatype's OSS Index, and Bundler Audit for Ruby are a few examples of other sources. Many paid scanning options rely on a combination of sources similar to these combined with databases developed by internal security researchers.\\n\\nNote:  This table is not ordered to imply tool relevance. It is only intended to offer a feature comparison of some available options. You may want to consider additional tools not mentioned\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dependency-Scanning\\\\index.md'}, {'chunkId': 'chunk114_3', 'chunkContent': \"Name Features Limitations Clair OSS, Static Analysis of Containers Works for images only OWASP Dependency-Check OSS, SCA, Check for Publicly Disclosed Vulnerabilities OSV-Scanner OSS, generates SBOM Trivy OSS, SCA, Covers many use-cases (Containers, Images, Code, Directories, IAC), Generates Software Bill of Materials (SBOM), Reports Licenses, Uses a combination of public vulnerabilities and internal research OSS Version may not include all options available with premium version Mend SCA, Public vulnerability databases and internal research, generates SBOM, Reports license, Enforcement Added Cost Black Duck SCA SCA, SBOM, Containers and Code, Binaries, Multiple sources used for database Added Cost Sonatype Nexus Lifecycle SCA, SBOM, Enforcement, AutoFix options Added Cost\\n\\nAnalyzing scan results\\n\\nDependency scanning is only one step in the process of resolving security vulnerabilities that can be revealed through the tools' usage. Once the tool has been selected and configured, remediation pathways must be identified. Common activities for remediation follow.\\n\\nTriage the output from the scans to identify false-positives and false-negatives.\\n\\nDocument and track the remaining issues following organization processes for engineering and security work as appropriate.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dependency-Scanning\\\\index.md'}, {'chunkId': 'chunk114_4', 'chunkContent': \"Document and track the remaining issues following organization processes for engineering and security work as appropriate.\\n\\nPrioritize work based on the Severity rating generated by the tool. Use the Severity rating on the CVE, or the CVSS Score if it's provided.\\n\\nProvide governance and training\\n\\nIn addition to the remediation steps, many dependency scanning tools include configuration options that can prevent specific types of licenses and levels of vulnerabilities from being introduced to the main repository. Taking advantage of these options can decrease the triage load and is widely considered a baseline component of shifting left. Additionally, DevSecOps best practice calls for codifying policy around dependencies.\\n\\nDocument the processes as a component of your secure software development lifecycle (SSDL).\\n\\nEnsure reference materials and training are available for interpreting the scan results. Include options for understanding both the reports from the selected tool and integrations into Continuous Integration (CI) systems. Depending on role and current use-case, both sources of understanding may be required. The organization must decide where this type of governance may be useful as it is not limited to engineering; it also provides an important picture of the overall security of application code.\\n\\nEnsure that appropriate personnel has permission to configure the selected tool. Follow the principle of least privilege\\n\\nEnsure that dependency scanning is included in periodic (not less frequent than annual) reviews to stay compliant with changing organizational policies.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dependency-Scanning\\\\index.md'}, {'chunkId': 'chunk114_5', 'chunkContent': \"Ensure that dependency scanning is included in periodic (not less frequent than annual) reviews to stay compliant with changing organizational policies.\\n\\nDependency scanning in the CI/CD pipeline via GitHub Actions\\n\\nIf you are not using Containers, it is possible to add software dependency vulnerability scanning using runners.\\n\\nDependency scanning can be added in the following way:\\n\\nAdd a scanner, such as OSV-Scanner, on the runners (if macOS: using Homebrew)\\n\\nUpdate the GitHub Actions ci-workflow to run the scanner on the machine\\n\\nUpload the OSV-Scanner Report artifact\\n\\nOnce scanning is added to the pipeline, the project's security plan should include recommendations around checking the vulnerability report in according with the governance and training guide (see above).\\n\\nResources\\n\\nOfficial Common Platform Enumeration (CPE) Dictionary\\n\\nCommon Vulnerabilities and Exposure (CVE)\\n\\nOpen Source Licenses\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dependency-Scanning\\\\index.md'}, {'chunkId': 'chunk115_0', 'chunkContent': \"Distroless Containers\\n\\nProblem Statement\\n\\nApplications built and deployed using container images almost always rely on components that are present in the underlying base images. These components can include libraries, executables, and plain text files. In the context of Linux-based images, the base image's distribution provides most of these components in the form of software packages that are created and maintained by the distribution owners.\\n\\nThe problem is that these images are created from the base Linux distributions, and therefore often carry packages that are not required by the deployed applications. Having these unnecessary packages in the base image creates the following problems:\\n\\nIt can be difficult to determine the exact contents of a base image and how it evolves over time with the distribution, which can lead to issues such as:\\n\\nThe application may stop working if the base image's dependency packages are updated in a new build.\\n\\nApplication's attack surface can be increased due to vulnerabilities found in packages it does not actually use.\\n\\nUnused binaries such as bash and sh, which are often present in base images for different distributions, can create a vulnerability if a remote code execution exploit is found in the deployed application.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Distroless-Containers\\\\index.md'}, {'chunkId': 'chunk115_1', 'chunkContent': 'A large number of packages in a base image can increase the size of the image, which in turn can increase the time it takes to build, scan, and deploy containers based on those images. More packages also often result in more false alarms during scanning, and more time spent evaluating risks and vulnerabilities in scanning reports.\\n\\nSolution\\n\\nA \"distroless\" container is a container image that does not include a Linux distribution or any other operating system components. Rather than using a traditional base image from Ubuntu or Debian, engineers can create a minimalist container that contains only the necessary libraries and runtime dependencies required for the application to run.\\n\\nTools\\n\\nThere are several tools available for working with distroless containers:\\n\\n\"Canonical Ubuntu Chisel project\" and \"dotnet Chisel container for .Net applications\": The Ubuntu Chisel toolset enables creating distroless containers from Ubuntu software packages. The .Net team has also created dotnet base images using the toolset. When building a .Net application, including ASP.Net, take a look at \"Chiselled .Net.\"\\n\\n\"Google Distroless Container Images\": Based on Debian software packages, this toolset makes it easier to create distroless containers to run C, C++, Java, Node.js, and Python applications.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Distroless-Containers\\\\index.md'}, {'chunkId': 'chunk115_2', 'chunkContent': '\"DockerSlim\": Rather than building the image from the bottom up and adding the application on top, this toolset profiles how an application is running in a container to understand what dependencies are used, and then slim down the container by stripping out everything the application is not using. The result is a container that has no unused system components and essentially makes it distroless. It can even slim down an application that is deployed on a distroless image to make it even smaller.\\n\\nCommon Pitfalls\\n\\nThinking the container is impenetrable after switching to distroless images. While using distroless images can reduce the attack surface of the application, it\\'s still important to apply other DevSecOps practices in the build and deployment pipeline and to keep monitoring the state of running containers. A reduced attack surface still has potential vulnerabilities, and attackers can still \"bring their own toolset\" if the application itself is vulnerable.\\n\\nDeveloping and testing the application using a normal container and then deploying it in a distroless container. Modern applications are very complex, and there could be undocumented libraries and dependencies that the engineers assume exist on the images. Always make sure to test the application thoroughly on a distroless container before deploying to production.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Distroless-Containers\\\\index.md'}, {'chunkId': 'chunk115_3', 'chunkContent': 'Not thinking about the build process and spending too much time building the distroless container. Many languages support static linking, which can come in handy when building distroless containers. Languages like C, C++, and Golang work very well with distroless containers when statically linked.\\n\\nResources\\n\\nTalk: How We Built Ubuntu Distroless Containers - Valentin Viennot, Canonical\\n\\nTalk: Using .NET with Chiseled Ubuntu Containers | .NET Conf 2022\\n\\nTalk: .NET in Ubuntu and Chiseled Containers - Canonical & Microsoft\\n\\nBlog: In Pursuit of Better Container Images', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Distroless-Containers\\\\index.md'}, {'chunkId': 'chunk116_0', 'chunkContent': \"Dynamic Application Security Testing (DAST)\\n\\nOverview\\n\\nThe DAST process is a form of closed-box test because it doesn't require visibility to the source code, and the techniques employed are similar to an attackers actions when hunting for weaknesses in a web-based application. DAST tools are designed to detect security vulnerabilities in a running web application.\\n\\nDAST tools look for a broad range of vulnerabilities, including broken object level authorizations, excessive data exposure, and web based attacks such as cross-site scripting (XSS) and SQL injection attacks (SQLi). Although other security tools, like static code analyzers that inspect source code, may detect the same vulnerabilities, a DAST tool will attempt to exploit the vulnerability. This is accomplished by accessing HTTP access points and performing random actions that produce unexpected results.\\n\\nThe example below shows how a DAST tool might send a HTTP request to a target. Given an API that returns a list of customers for a particular store:\\n\\nbash\\nGET /storecustomers?storeid=100\\n\\nAs expected, the HTTP response returns a list of customers.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dynamic-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk116_1', 'chunkContent': 'bash\\nGET /storecustomers?storeid=100\\n\\nAs expected, the HTTP response returns a list of customers.\\n\\njson\\n   {\\n    \"id\": 951,\\n    \"name\": \"Marco\",\\n    \"surname\": \"Polo\",\\n    \"creationDate\": \"2022-11-23T02:36:09-08:00\"\\n   },\\n   {\\n    \"id\": 952,\\n    \"name\": \"Jane\",\\n    \"surname\": \"Smith\",\\n    \"creationDate\": \"2022-11-14T18:30:09-07:00\"\\n   }\\n\\nA DAST tool can then be configured to update the store id to determine a potential break in object level authorizations. In this example, if store id 200 is not in scope, then the tool will flag this as an issue if the HTTP response contains a list of users.\\n\\nbash\\nGET /storecustomers?storeid=200', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dynamic-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk116_2', 'chunkContent': 'bash\\nGET /storecustomers?storeid=200\\n\\njson\\n   {\\n    \"id\": 100,\\n    \"name\": \"Elliot\",\\n    \"surname\": \"Alderson\",\\n    \"creationDate\": \"2015-06-24T18:00:09-09:00\"\\n   }\\n\\nKey Takeaways\\n\\nSince DAST tools work without knowing the internals of an application, running tests against the target application or API should be done both as an anonymous and authenticated user or service. Testing as an anonymous user allows the discovery of missing or misconfigured authentication and authorization mechanisms. Authenticated tests expose weak input validation, broken access control, and other security risks caused by bad design and coding.\\n\\nActions\\n\\n1. Finalize tools selection\\n\\nSelect a DAST solution that meet your requirements and budget.\\n\\nSteps:\\n\\nSelect a tool. OWASP publishes a number of free DAST tools for open source applications. OWASP Zap is good tool to try. Check out the sample provided in the next section.\\n\\nSelect a single tool to perform this function. Though multiple tools can certainly be used, remember that the outputs from these scanners need to be addressed. Verifying a large quantity of items may cause unnecessary overhead. Here are some things to consider when deciding which tool to use.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dynamic-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk116_3', 'chunkContent': \"Attribute Features to consider Mitigated risks Detection accuracy Low false-positive and false-negative rate Noise from scans cuts focus time needed to resolve real application risks Technology Agnostic The tool is not limited to the frameworks or server-side web technologies used by the target Technology lock-in Automation Ability to schedule ongoing and periodic scans of an application The attack window where a vulnerability is introduced into the application and the time it takes for mitigation Safe Testing DAST tools actively attack an application to produce undesirable outcomes so it's important to ensure that the exploits performed are safe Harm on the application and its data\\n\\n2. Analyze Scan Results\\n\\nReview and resolve the DAST scan results.\\n\\nSteps:\\n\\nTriage the output from the scans to identify false-positives and false-negatives.\\n\\nDocument and track the remaining issues.\\n\\nRate the severity of each issue to help with prioritization. Use the NIST Common Vulnerability Scoring System (CVSS) framework to assess the vulnerabilities characterized by each issue.\\n\\n3. Provide Governance and Training\\n\\nEnforce the proper use of the DAST process. Like any tool or process that manages security risks, verify compliance with any corporate policies and standards. Creating new or updating existing procedures may also be needed. Engage security stakeholders to determine any requirements that need to be met.\\n\\nSteps:\\n\\nDocument the process as a component of the software development lifecycle.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dynamic-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk116_4', 'chunkContent': 'Steps:\\n\\nDocument the process as a component of the software development lifecycle.\\n\\nCreate or update procedures on tool operation.\\n\\nTrain users on how to configure and operate the DAST tool.\\n\\nSample\\n\\nHere are useful resources and a sample implementation of OWASP Zap.\\n\\nSample implementation\\n\\nGetting started with OWASP ZAP\\n\\nOWASP ZAP automation and authentication workshop series\\n\\nUsing OWASP ZAP in DevOps\\n\\nResources\\n\\nFree Security Tools for Open Source Applications', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dynamic-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk117_0', 'chunkContent': \"Using OWASP ZAP\\n\\nIn this example, the containerized version of ZAP will be used for scanning a web application.\\n\\n1. Get the ZAP Docker Image\\n\\nGet the images from Docker.\\n\\nbash\\ndocker pull owasp/zap2docker-stable\\n\\n2. Run a baseline scan\\n\\nA ZAP baseline scan will passively crawl through the target web application and glean information captured from exposed endpoints without direct interaction. This type of scan can be safely executed on a CI/CD environment.\\n\\nbash\\ndocker run -t owasp/zap2docker-stable zap-baseline.py -t https://www.example.com\\n\\n3. Run a full scan\\n\\nA full or active scan will perform actual attacks on the target web application. This may take a long time to complete so it's not ideal to plug this into a CI/CD pipeline. Active scans will interact with the target endpoints by sending test payloads and examine the response to detect any weaknesses.\\n\\nbash\\ndocker run -t owasp/zap2docker-stable zap-full-scan.py -t https://www.example.com\\n\\n4. Run an API scan\\n\\nAn API scan is an active scan that targets an API endpoint with an OpenAPI, SOAP or GraphQL definition. Read more about scanning APIs here.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dynamic-Application-Security-Testing\\\\Recipes\\\\Using-OWASPZAP.md'}, {'chunkId': 'chunk117_1', 'chunkContent': 'bash\\ndocker run -t owasp/zap2docker-stable zap-api-scan.py -t https://www.example.com/openapi.json -f openapi\\n\\n5. Review the results\\n\\nThe scans will test for a number of vulnerabilities and show the following output.\\n\\nZAP will show the types of tests performed and their results. Passing tests are shown below.\\n\\nbash\\nUsing the Automation Framework\\nTotal of 3 URLs\\nPASS: Vulnerable JS Library (Powered by Retire.js) [10003]\\nPASS: In Page Banner Information Leak [10009]\\nPASS: Cookie No HttpOnly Flag [10010]\\nPASS: Cookie Without Secure Flag [10011]\\nPASS: Cross-Domain JavaScript Source File Inclusion [10017]\\n\\nMeanwhile, tests that failed are shown as warning (WARN) by default. This can be changed to FAIL or IGNORE using a configuration file. In this example, a header that prevents clickjacking is missing on the main page of example.com among other things.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dynamic-Application-Security-Testing\\\\Recipes\\\\Using-OWASPZAP.md'}, {'chunkId': 'chunk117_2', 'chunkContent': 'bash\\nWARN-NEW: Missing Anti-clickjacking Header [10020] x 1\\n        https://www.example.com (200 OK)\\nWARN-NEW: X-Content-Type-Options Header Missing [10021] x 1\\n        https://www.example.com (200 OK)\\nWARN-NEW: Strict-Transport-Security Header Not Set [10035] x 3\\n        https://www.example.com (200 OK)\\n        https://www.example.com/robots.txt (404 Not Found)\\n        https://www.example.com/sitemap.xml (404 Not Found)\\nWARN-NEW: Server Leaks Version Information via Server HTTP Response Header Field [10036] x 4\\n        https://www.example.com (200 OK)\\n        https://www.example.com (200 OK)\\n        https://www.example.com/robots.txt (404 Not Found)\\n        https://www.example.com/sitemap.xml (404 Not Found)\\n...', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Dynamic-Application-Security-Testing\\\\Recipes\\\\Using-OWASPZAP.md'}, {'chunkId': 'chunk118_0', 'chunkContent': 'Leveraging Ephemeral Environments for testing, development and disaster recovery\\n\\nHaving the ability to create and destroy a full environment for our solution in a repeatable and predictable way has several advantages on multiple fronts:\\n\\nValidates that the complete set of software and infrastructure pieces can be automatically re-deployed in a disaster-recovery situation.\\n\\nAllows for a full end-to-end testing of the solution before of after merging a software of infrastructure change.\\n\\nEnables the creation of full and independent development environments.\\n\\nWhile this is a very common practice in the DevOps world, it is not always easy to achieve because of the extra tasks and assets it needs that are not always contemplated as part of the technical requirements of the solution:\\n\\n100% of the infrastructure needs to be defined as IaC, and parameterized to allow for the creation of multiple environments.\\n\\nTeam time allocation to exercise the process from the start of the project, maintain it and validate that it works.\\n\\nScripts and pipelines to automate the process.\\n\\nThis article explores the challenges and benefits of using ephemeral environments and the benefits of putting them in practice early in the project.\\n\\nBusiness problems', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Ephemeral-Environments\\\\index.md'}, {'chunkId': 'chunk118_1', 'chunkContent': 'Business problems\\n\\nMost teams implement some sort of source control for both the software components being developed and the required IaC (Infrastructure as Code) to define the infrastructure components where the solution will be deployed. However, the full deployment and validation of the solution involves a manual process requiring a lot of time, effort and knowledge about the solution business and technical requirements. The same could be true for the creation of complex cloud development environments where existing IaC is leveraged but is not fully automated, hence manual configuration steps may still be required. This could ultimately be both, time consuming and error prone.\\n\\nThe aforementioned manual process is also prone to errors and inconsistencies, and it is not easily repeatable, which makes it difficult to validate the solution in a disaster-recovery situation.\\n\\nChallenges\\n\\nA first step is having the full code and infrastructure definition of a solution as part of the source control repository, but it is not enough to enable the full automation of the deployment and validation of the solution. The following challenges need to be addressed:\\n\\nDefinition of prerequisites to deploy a new version of the application on an existing environment.\\n\\nDefinition of prerequisites to deploy a new version of the application on a new environment.\\n\\nAutomated deployment and validation.\\n\\nAutomated environment destruction and cleanup.\\n\\nAutomated re-creation of the production environment for security and disaster recovery purposes.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Ephemeral-Environments\\\\index.md'}, {'chunkId': 'chunk118_2', 'chunkContent': \"Automated environment destruction and cleanup.\\n\\nAutomated re-creation of the production environment for security and disaster recovery purposes.\\n\\nTime and costs associated with the creation and destruction of each environment.\\n\\nSolution\\n\\nThe key piece of integrating the regular creation and destruction of full ephemeral environments in the lifecycle of a solution is to start doing it as early as possible. This means that the IaC should be designed to be able to create and destroy the full environment, including the infrastructure and the application components, in a repeatable and predictable way. This is not always easy to achieve, but it is a key requirement to enable the full automation of the process.\\n\\nIt also implies that, any external pre-requisite must be clearly defined and documented, since those components won't be part of the IaC or the software components of the solution, but equally important to be able to deploy it.\\n\\nIf done early into the project and integrated into a pipeline that validates the process (executed either on merge or on a regular basis, depending on feasibility), any subsequent updates to the environment can be validated in a fully automated way.\\n\\nThe same process can be used to create and destroy development environments, which can be leveraged by the development team to test new features and infrastructure changes to validate the solution in a production-like environment without affecting other people's work.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Ephemeral-Environments\\\\index.md'}, {'chunkId': 'chunk118_3', 'chunkContent': 'It is important to implement the environment deployment process as an idempotent, create-or-upgrade process, so that it can be used to deploy a new version of the solution on an existing environment, or to deploy a new version of the solution on a new environment. This is a key requirement to enable the full automation of the process, since it will allow to upgrade existing shared environments such as development, testing, staging or production without requiring any extra manual steps.\\n\\nDepending on the size and complexity of the environment, it may not be practical to create and destroy the environment on every merge. In those cases, it could be possible to schedule the pre-creation of environments based on the last valid state, and have them ready to be used by pipelines that would only need to apply the delta between that state and the newly implemented changes.\\n\\nThe same thing applies to the environment cleanup, since it could take a long time to fully destroy it. In that case, the deletion of the environment could be triggered after being used and a periodical process should validate that any leftover resources were properly deleted, ensuring no extra costs are generated by it.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Ephemeral-Environments\\\\index.md'}, {'chunkId': 'chunk118_4', 'chunkContent': 'One further step that can be taken is the full automation of re-creation of the production environment for security and disaster recovery purposes. This can be done by leveraging the same IaC that was used to create the environment in the first place, and it can be triggered on a regular basis, as long as the system is highly available and can be upgraded without downtime, or after a security incident or a disaster to ensure that a well-known version of the environment is available.\\n\\nKey Benefits\\n\\nIsolated environments for the development team\\n\\nFully automated deployment, validation and upgrade of the solution\\n\\nPeriodical re-creation of the production environment for security purposes\\n\\nDisaster recovery process is always validated and up-to-date\\n\\nChanges to both the environment and the solution can be validated in a production-like environment\\n\\nNo manual steps required to upgrade an existing environment, or to create a new one\\n\\nNo manual steps required to re-create the production environment', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Ephemeral-Environments\\\\index.md'}, {'chunkId': 'chunk119_0', 'chunkContent': \"Infrastructure as Code (IaC) Scanning\\n\\nProblem Statement\\n\\nOrganizations are increasingly provisioning their cloud solutions using an Infrastructure as Code (IaC) approach. IaC helps organizations manage and provision their infrastructure through machine-readable definition files. Therefore, security approaches for IaC solutions should focus on ensuring configuration definitions do not have misconfigurations or potential vulnerabilities, e.g., exposing sensitive data to logs.\\n\\nApplication and Infrastructure development teams should be aware of your organization's IaC security practices and proactively alerted to issues as part of their development and integration workflows. IaC Scanning practices can reduce solution risk, provide suggestions for fixing misconfigurations, and improve a solution's overall security posture.\\n\\nThe example below shows a portion of a Bicep configuration's output. The configuration's output prints a SQL connection string with a clear password. Every agent run of this pipeline will log this connection string data, presenting a potentially significant security risk.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\index.md'}, {'chunkId': 'chunk119_1', 'chunkContent': \"bicep\\noutput sqlDatabaseCatalogDbCS string = 'Server=tcp:${sqlServer.outputs.fqdn},1433;Initial Catalog=${sqlDatabaseCatalogDb.outputs.name};Persist Security Info=False;User ID=${sqlServerAdministratorLogin};Password=${sqlServerAdministratorPassword};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;'\\n\\nSolution\\n\\nIaC security scanning essentials are [^1] identifying misconfigurations, vulnerabilities, and security issues before they trigger incidents. IaC security scanning should target IaC templates, configuration files, and add-on modules, using industry standard policies and rules to detect potential issues.\\n\\nTools\\n\\nThe best way to scan your IaC is through automation (part of CI/CD workflow). Below is a list of widely used tools on the market that helps you with IaC Security Scanning.\\n\\nKICS (Keeping Infrastructure as Code Secure) helps find security vulnerabilities, compliance issues, and infrastructure misconfigurations early in the development cycle of your Infrastructure-as-Code. It can work well with Ansible, ARM Templates, CloudFormation, Bicep (after conversion to ARM template), GDM Templates, Kubernetes, and Terraform.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\index.md'}, {'chunkId': 'chunk119_2', 'chunkContent': \"Checkov prevents cloud misconfigurations and finds vulnerabilities during build-time in Infrastructure-as-Code, container images, and open-source. It can work well with Terraform, CloudFormation, Kubernetes, Dockerfile, Bicep, and ARM Templates.\\n\\nTemplate Best Practice Analyzer (BPA) scans for Bicep and ARM Templates' security misconfiguration and best practices.\\n\\nTerraScan detects compliance and security violations across Infrastructure as Code to mitigate risk before provisioning cloud-native infrastructure. It can work well with Terraform, Kubernetes, Helm, Kustomize, Dockerfiles, and CloudFormation.\\n\\nAzure Resource Manager Template Toolkit (ARM-TTK) [^2] checks ARM Templates and Bicep (after conversion to ARM template) for coding best practices.\\n\\nMicrosoft Security DevOps (MSDO) integrates static analysis tools like Bandit, BinSkim, ESlint, Template Analyzer, TerraScan, Trivy into the development cycle. It can work well with ARM Templates, Bicep, Terraform, Kubernetes, Helm, Kustomize, Dockerfiles, CloudFormation\\n\\nCode Examples and Recipes\\n\\nFor sample GitHub Workflows, Azure DevOps Pipelines, and general usage of selected tools please follow listed articles:\\n\\nIntegrate KICS with Azure Pipelines\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\index.md'}, {'chunkId': 'chunk119_3', 'chunkContent': 'Integrate KICS with Azure Pipelines\\n\\nKICS integration with GitHub Actions\\n\\nUse ARM Template Toolkit\\n\\nIntegrating ARM Template Toolkit (ARM-TTK) into your GitHub Workflow or Azure Pipelines\\n\\nTerraform scanning as part of GitHub Workflow or Azure Pipelines using Checkov\\n\\nIntegrating Template Best Practice Analyzer (BPA) into your GitHub Workflow or Azure Pipelines\\n\\nMore Reading\\n\\n[^1]: Discover misconfigurations in Infrastructure as Code (IaC)\\n[^2]: Validate Azure resources by using the ARM Template Test Toolkit', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\index.md'}, {'chunkId': 'chunk120_0', 'chunkContent': \"tags:\\n  - ARM-TTK\\n  - ARM\\n  - Bicep\\n  - IaC Scanning\\n\\nAzure Resource Manager Template Toolkit (ARM-TTK)\\n\\nOverview\\n\\nIn this article, you can find information on integrating Azure Resource Manager Template Toolkit (ARM-TTK) into your GitHub Workflow or Azure Pipelines as part of the DevOps process.\\n\\nWhat is ARM-TTK?\\n\\nAzure Resource Manager Template Toolkit aka ARM-TTK is an Open-Source tool released under MIT License. It's a SAST tool for analyzing and testing Azure Resource Manager Templates. The tests will check a template or set of templates for coding best practices (include some security areas e.g. Outputs don't contain secrets). There are some checks for simple syntactical errors but the intent is not to re-implement tests or checks that are provided by the platform (e.g. the /validate api).\\n\\nARM-TTK has been written in PowerShell. It's executable not only on Windows but also on Linux runners. For the Linux one, make sure PowerShell is part of the runner environment (Install PowerShell on Linux\\n) or setup it up on the fly with your workflow.\\n\\nGitHub Workflow\\n\\nThe below assumptions for the workflow are:\\n\\nIaC directory contains Bicep or/and ARM templates\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\ARM-TTK.md'}, {'chunkId': 'chunk120_1', 'chunkContent': 'The below assumptions for the workflow are:\\n\\nIaC directory contains Bicep or/and ARM templates\\n\\nARM-TTK can validate only ARM templates, so an extra step exists for compiling Bicep to ARM\\n\\n{% raw %}\\n\\n```yaml\\nname: ARM-TTK\\n\\non:\\n  push:\\n    branches: [\"main\"]\\n  pull_request:\\n    branches: [\"main\"]\\n  workflow_dispatch:\\n\\nenv:\\n  iac_files: ${{ github.workspace }}/IaC\\n\\njobs:\\n  arm-ttk:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v3', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\ARM-TTK.md'}, {'chunkId': 'chunk120_2', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nAzure Pipelines\\n\\nOption 1 - ARM-TTK natively\\n\\nThe below assumptions for the pipeline are:\\n\\n{% raw %}\\n\\n```yaml\\ntrigger:\\n  - main\\n\\npr:\\n  - main\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nvariables:\\n  - name: iac_files\\n    value: \"IaC\"\\n\\nsteps:\\n  - checkout: self\\n    displayName: Checkout repo', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\ARM-TTK.md'}, {'chunkId': 'chunk120_3', 'chunkContent': 'steps:\\n  - checkout: self\\n    displayName: Checkout repo\\n\\n# Setup script downloads the latest version of ARM-TTK directly from GitHub project site\\n  - script: |\\n      curl -o release.json https://api.github.com/repos/Azure/arm-ttk/releases/latest\\n      latest_version=$(cat release.json | jq .name -r -c)\\n      filename=\"arm-ttk.zip\"\\n      download_url=$(cat release.json | jq -c -r \".assets[] | select(.name == \\\\\"${filename/\\'v\\'/\\'\\'}\\\\\") | .browser_download_url\")\\n      curl -sSL -o \"${filename}\" \"${download_url}\"\\n      unzip \"${filename}\"\\n      rm -f \"${filename}\"\\n      chmod +x \"arm-ttk/Test-AzTemplate.sh\"\\n    displayName: Setup ARM-TTK', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\ARM-TTK.md'}, {'chunkId': 'chunk120_4', 'chunkContent': '# ARM-TTK does not work with Bicep files nativily, so compile Bicep to ARM templates\\n  - script: |\\n      find \"${IAC_FILES}\" -type f -name *.bicep -exec az bicep build --file {} \\\\;\\n    env:\\n      IAC_FILES: $(iac_files)\\n    displayName: Build ARM templates from Bicep\\n\\n# Run ARM-TTK on ARM templates generated from Bicep files from previous step\\n  - script: |\\n      ./arm-ttk/Test-AzTemplate.sh -TemplatePath \"${IAC_FILES}\"\\n    env:\\n      IAC_FILES: $(iac_files)\\n    displayName: Run ARM-TTK', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\ARM-TTK.md'}, {'chunkId': 'chunk120_5', 'chunkContent': \"```\\n\\n{% endraw %}\\n\\nOption 2 - ARM-TTK via extension\\n\\nRun ARM TTK Tests - It's an unofficial Azure DevOps Extension for running Azure Resource Manager Template Tool Kit (ARM-TTK) tests as part of Azure Pipelines.\\n\\nNOTE\\n\\nCurrently this extension can only be used on Windows build agents.\\n\\nARM-TTK natively supports only ARM templates (JSON files), and you must first compile Bicep templates. Using this extension, you do not have to do it by yourself. Instead, the extension automatically builds ARM templates from your Bicep files.\\n\\nOne of the helpful features of the extension is output in the NUnit format, which helps publish and visualize results in summary.\\n\\nNOTE\\n\\nVery often, unofficial Tasks are community-powered efforts and vary code maintenance and support approaches.\\n\\nThe below assumption for the pipeline is:\\n\\nIaC directory contains Bicep or/and ARM templates\\n\\n{% raw %}\\n\\n```yaml\\ntrigger:\\n  - main\\n\\npr:\\n  - main\\n\\npool:\\n  vmImage: windows-latest\\n\\nsteps:\\n  - checkout: self\\n    displayName: Checkout repo\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\ARM-TTK.md'}, {'chunkId': 'chunk120_6', 'chunkContent': 'steps:\\n  - checkout: self\\n    displayName: Checkout repo\\n\\n# Run ARM-TTK on Bicep or ARM templates files\\n  - task: RunARMTTKTestsXPlat@1\\n    displayName: Run ARM-TTK\\n    inputs:\\n      templateLocation: \\'$(System.DefaultWorkingDirectory)\\\\IaC\\'\\n      resultLocation: \\'$(System.DefaultWorkingDirectory)\\\\results\\'\\n      allTemplatesMain: false\\n      cliOutputResults: true\\n      recurse: true\\n\\n# RunARMTTKTestsXPlat task produces result NUnit, so let\\'s publish it\\n  - task: PublishTestResults@2\\n    displayName: Publish Test Results\\n    inputs:\\n      testResultsFormat: \"NUnit\"\\n      testResultsFiles: \\'$(System.DefaultWorkingDirectory)\\\\results*-armttk.xml\\'\\n      mergeTestResults: true\\n    condition: always()', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\ARM-TTK.md'}, {'chunkId': 'chunk120_7', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nTo adjust Run ARM TTK Tests extension to your needs, follow documentation: https://github.com/sam-cogan/arm-ttk-extension#parameters\\n\\nReferences\\n\\nAzure Resource Manager Template Toolkit', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\ARM-TTK.md'}, {'chunkId': 'chunk121_0', 'chunkContent': 'tags:\\n  - Checkov\\n  - Terraform\\n  - IaC Scanning\\n\\nTerraform scanning by Checkov\\n\\nOverview\\n\\nIn this article, you can find information on integrating Checkov into your GitHub Workflow or Azure Pipelines as part of the DevOps process.\\n\\nWhat is Checkov?\\n\\nCheckov is is a static code analysis tool for Infrastructure as Code (IaC) and also a software composition analysis (SCA) tool for images and open source packages.\\n\\nTerraform,\\n\\nTerraform plan,\\n\\nCloudformation,\\n\\nAWS SAM,\\n\\nKubernetes,\\n\\nHelm charts,\\n\\nKustomize,\\n\\nDockerfile,\\n\\nServerless,\\n\\nBicep,\\n\\nOpenAPI or\\n\\nARM Templates and detects security and compliance misconfigurations using graph-based scanning.\\n\\nCheckov is registered under Apache License 2.0. It also powers Bridgecrew, the developer-first platform that codifies and streamlines cloud security throughout the development lifecycle. Bridgecrew identifies, fixes, and prevents misconfigurations in cloud resources and infrastructure-as-code files.\\n\\nGitHub Workflow\\n\\nThe below assumptions for the workflow are:\\n\\nIaC directory contains terraform files', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Checkov-Terraform.md'}, {'chunkId': 'chunk121_1', 'chunkContent': 'GitHub Workflow\\n\\nThe below assumptions for the workflow are:\\n\\nIaC directory contains terraform files\\n\\nOne needs to create a service principal in azure, and the client_id, client_secret, tenant_id and subscription_id in the github actions secrets.\\n\\n{% raw %}\\n\\n```yaml\\nname: Checkov-Scan\\n\\non:\\n  push:\\n    branches: [\"main\"]\\n  pull_request:\\n    branches: [\"main\"]\\n\\nenv:\\n  iac_files: ${{ github.workspace }}/tf\\n\\njobs:\\n  checkov-job:\\n    runs-on: ubuntu-latest\\n    name: checkov-action\\n    steps:\\n      - name: Checkout repo\\n        uses: actions/checkout@v3', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Checkov-Terraform.md'}, {'chunkId': 'chunk121_2', 'chunkContent': '```', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Checkov-Terraform.md'}, {'chunkId': 'chunk121_3', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nAzure Pipelines\\n\\nCheckov Scan using Docker Container\\n\\nThe below assumptions for the workflow are:\\n\\ntf_folder directory contains terraform files.\\n\\n{% raw %}\\n\\n```yaml\\nname: Checkov-Scan\\n\\ntrigger:\\n  - main\\n\\npr:\\n  - main\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nvariables:\\n  - name: tf_folder\\n    value: $(System.DefaultWorkingDirectory)/platform/checkov/terraform\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n  - bash: |\\n            docker pull bridgecrew/checkov\\n    workingDirectory: $(System.DefaultWorkingDirectory)\\n    displayName: \"Docker Pull > bridgecrew/checkov\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Checkov-Terraform.md'}, {'chunkId': 'chunk121_4', 'chunkContent': 'bash: |\\n      docker run \\\\\\n        --volume $(tf_folder):/tf bridgecrew/checkov \\\\\\n        --directory /tf \\\\\\n        --output junitxml \\\\\\n        --soft-fail > $(pwd)/CheckovReport.xml\\n    workingDirectory: $(System.DefaultWorkingDirectory)\\n    displayName: \"Run > checkov\"\\n\\ntask: PublishTestResults@2\\n    inputs:\\n      testRunTitle: \"Checkov Results\"\\n      failTaskOnFailedTests: false    # Make it true if you want to fail the pipeline after scan\\n      testResultsFormat: \"JUnit\"\\n      testResultsFiles: \"CheckovReport.xml\"\\n      searchFolder: \"$(System.DefaultWorkingDirectory)\"\\n    displayName: \"Publish > Checkov scan results\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Checkov-Terraform.md'}, {'chunkId': 'chunk121_5', 'chunkContent': '```\\n\\n{% endraw %}', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Checkov-Terraform.md'}, {'chunkId': 'chunk122_0', 'chunkContent': 'summary: KICS usage examples with GitHub Workflows and Azure DevOps Pipelines\\nauthors:\\n  - Dariusz Porowski\\ndate: 2023-01-05\\ntags:\\n  - IaC\\n  - KICS\\n  - GitHub\\n  - Infrastructure as Code\\n  - Security\\n\\nKICS (Keeping Infrastructure as Code Secure)\\n\\nIn this article, you can find information on integrating KICS (Keeping Infrastructure as Code Secure) into your GitHub Workflow or Azure Pipelines as part of the DevOps process.\\n\\nWhat is KICS?\\n\\nKICS (Keeping Infrastructure as Code Secure) is an open-source solution for static code analysis of Infrastructure as Code maintained by Checkmark - a well-known company with an established position in the security testing space. KICS verifies the security posture using multiple knowledge bases, including Microsoft Cloud Security Benchmark (MCSB), which provides prescriptive best practices and recommendations to help improve the security of workloads, data, and services on Azure and your multi-cloud environment. This benchmark focuses on cloud-centric control areas with input from holistic Microsoft and industry security guidances.\\n\\nKICS produces a scan report as SARIF (Static Analysis Results Interchange Format) file, which is OASIS standard used to streamline how static analysis tools share their results.\\n\\nGitHub Workflow\\n\\nIn this section, you can find example recipe for GitHub Workflow.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\KICS.md'}, {'chunkId': 'chunk122_1', 'chunkContent': 'GitHub Workflow\\n\\nIn this section, you can find example recipe for GitHub Workflow.\\n\\nKICS officially released Gitleaks Action to perform IaC scanning, and it\\'s recommended and the easiest solution to use KICS in the GitHub Workflow.\\n\\nKICS result (SARIF file) may be uploaded to the GitHub Code Scanning service to see code scanning alerts from third-party tools.\\n\\nNOTE\\n\\nGitHub Code Scanning integration it\\'s only available for Organizations that use GitHub Enterprise Cloud and have a license for GitHub Advanced Security.\\n\\n{% raw %}\\n\\n```yaml\\nname: KICS example\\n\\nControls when the workflow will run\\n\\non:\\n  # Triggers the workflow on push or pull request events but only for the \"main\" branch\\n  push:\\n    branches: [\"main\"]\\n  pull_request:\\n    branches: [\"main\"]\\n\\n# Allows you to run this workflow manually from the Actions tab\\n  workflow_dispatch:\\n\\nenv:\\n  iac_files: ${{ github.workspace }}/IaC # your IaC files path', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\KICS.md'}, {'chunkId': 'chunk122_2', 'chunkContent': 'env:\\n  iac_files: ${{ github.workspace }}/IaC # your IaC files path\\n\\njobs:\\n  # This workflow contains a single job called \"build\"\\n  kics:\\n    # The type of runner that the job will run on\\n    runs-on: ubuntu-latest', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\KICS.md'}, {'chunkId': 'chunk122_3', 'chunkContent': \"```\\n\\n{% endraw %}\\n\\nRead more about GitHub integration options on KICS's documentation Integration with Github Actions\\n\\nAzure Pipelines\\n\\nIn this section, you can find example recipes for Azure Pipelines.\\n\\nNOTE\\n\\nOfficial Azure Pipelines task extension for KICS does not exist. Below example will uses official KICS container (checkmarx/kics:debian) as backend for IaC scanning.\\n\\nScanning result automatically uploads SARIF report to Pipeline Artifacts, and the result can be visualized using the SARIF SAST Scans Tab extension.\\n\\n{% raw %}\\n\\n```yaml\\ntrigger:\\n  - none\\n\\nname: KICS\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\ncontainer: checkmarx/kics:debian # use kics official container\\n\\nvariables:\\n  IAC_FILES: IaC # your IaC files path\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\KICS.md'}, {'chunkId': 'chunk122_4', 'chunkContent': 'variables:\\n  IAC_FILES: IaC # your IaC files path\\n\\nsteps:\\n  # run in CI mode, do not fail the pipeline on results found and export the report in both json and SARIF format.\\n  - script: |\\n      mkdir -p kics-results\\n      /app/bin/kics scan --ci -p ${PWD}/$(IAC_FILES) -o ${PWD}/kics-results --report-formats json,sarif --ignore-on-exit results\\n      cat kics-results/results.json\\n    displayName: Run KICS\\n\\n# scan results should be visible in the SARIF viewer tab of the build\\n  - task: PublishPipelineArtifact@1\\n    displayName: Upload SARIF report to Pipeline Artifacts\\n    inputs:\\n      targetPath: \"$(System.DefaultWorkingDirectory)/kics-results/results.sarif\"\\n      artifact: \"CodeAnalysisLogs\"\\n      publishLocation: \"pipeline\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\KICS.md'}, {'chunkId': 'chunk122_5', 'chunkContent': \"```\\n\\n{% endraw %}\\n\\nRead more about Azure Pipelines integration options on KICS's documentation Integrate KICS with Azure Pipelines\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\KICS.md'}, {'chunkId': 'chunk123_0', 'chunkContent': 'tags:\\n  - Template BPA\\n  - ARM\\n  - Bicep\\n  - IaC Scanning\\n\\nTemplate Best Practice Analyzer (BPA)\\n\\nOverview\\n\\nIn this article, you can find information on integrating Template Best Practice Analyzer (BPA) into your GitHub Workflow or Azure DevOps Pipeline as part of the DevOps process.\\n\\nNOTE\\n\\nThe Template BPA is currently in development. It is not yet recommended for production usage.\\n\\nWhat is Template Best Practice Analyzer?\\n\\nThe Template BPA scans ARM (Azure Resource Manager) and Bicep Infrastructure-as-Code (IaC) templates to ensure security and best practice checks are being followed before deployment of your Azure solutions.\\n\\nThe Template BPA provides a simple and extensible solution to improve the security of your Azure resources before deployment and ensures your templates follow best practices. The Template BPA is designed to be customizable - users can write their own checks and/or enforce only the checks that are relevant for them.\\n\\nTemplate BPA produces a scan report as SARIF (Static Analysis Results Interchange Format) file, which is OASIS standard used to streamline how static analysis tools share their results.\\n\\nGitHub Workflow\\n\\nThe below assumption for the workflow is:\\n\\nIaC directory contains Bicep or/and ARM templates\\n\\n{% raw %}', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Template-BPA.md'}, {'chunkId': 'chunk123_1', 'chunkContent': 'IaC directory contains Bicep or/and ARM templates\\n\\n{% raw %}\\n\\n```yaml\\nname: Template-BPA\\n\\non:\\n  push:\\n    branches: [\"main\"]\\n  pull_request:\\n    branches: [\"main\"]\\n  workflow_dispatch:\\n\\nenv:\\n  iac_files: ${{ github.workspace }}/IaC\\n\\njobs:\\n  template-bpa:\\n    runs-on: ubuntu-latest', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Template-BPA.md'}, {'chunkId': 'chunk123_2', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nTo adjust Template BPA parameters to your needs, follow the documentation at the Template Analyzer GitHub repo.\\n\\nAzure DevOps Pipelines\\n\\nThe below assumption for the workflow is:\\n\\nIaC directory contains Bicep or/and ARM templates\\n\\n{% raw %}\\n\\n```yaml\\ntrigger:\\n  - none\\n\\nname: Template-BPA\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nvariables:\\n  IAC_FILES: $(Pipeline.Workspace)/s/IaC\\n\\nsteps:\\n  - checkout: self\\n    displayName: Checkout repo', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Template-BPA.md'}, {'chunkId': 'chunk123_3', 'chunkContent': 'steps:\\n  - checkout: self\\n    displayName: Checkout repo\\n\\n# Setup script downloads the latest version of Template-BPA directly from GitHub project site\\n  - script: |\\n      curl -sSLO $(curl -sSL https://api.github.com/repos/Azure/template-analyzer/releases/latest | jq --compact-output --raw-output \\'.assets[] | select( .name | contains(\"linux-x64\") ) | select( .name | endswith(\".zip\") ).browser_download_url\\')\\n      filename=$(find . -type f -name \"TemplateAnalyzer*.zip\" -print0)\\n      unzip -o -u \"${filename}\" -d TemplateAnalyzer\\n      rm -f \"${filename}\"\\n      chmod +x \"TemplateAnalyzer/TemplateAnalyzer\"\\n      echo \"TemplateAnalyzer version: $(TemplateAnalyzer/TemplateAnalyzer --version)\"\\n    displayName: Setup Template-BPA', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Template-BPA.md'}, {'chunkId': 'chunk123_4', 'chunkContent': '# Run Template BPA on ARM templates / Bicep files\\n  - task: Bash@3\\n    displayName: Run Template-BPA\\n    inputs:\\n      targetType: \"inline\"\\n      script: |\\n        set +e\\n        report_name=\"TemplateAnalyzer-report\"\\n        report_format=\"sarif\"\\n\\n# Publish TemplateAnalyzer SARIF report to Pipeline Artifacts\\n  - task: PublishPipelineArtifact@1\\n    displayName: Publish Report\\n    condition: and(always(), eq(variables.report_exist, \\'1\\'))\\n    inputs:\\n      targetPath: \"TemplateAnalyzer/$(report)\"\\n      artifact: \"CodeAnalysisLogs\"\\n      publishLocation: \"pipeline\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Template-BPA.md'}, {'chunkId': 'chunk123_5', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nTo adjust Template BPA parameters to your needs, follow the documentation at the Template Analyzer GitHub repo.\\n\\nNOTE\\n\\nTo track/see the SARIF report from the Template BPA, make sure your Azure DevOps organization has the SARIF SAST Scans Tab extension.\\n\\nReferences\\n\\nTemplate Best Practice Analyzer (BPA)', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Infrastructure-as-Code-Scanning\\\\Recipes\\\\Template-BPA.md'}, {'chunkId': 'chunk124_0', 'chunkContent': \"Testing and Developing on Isolated Environments for Docker-based Azure Functions\\n\\nCreating an Azure Function as a Docker container is a great way to ensure that the function runtime will be the same regardless of where it is running. However, that is not enough to ensure that the function will run as expected when deployed. The function may depend on other services, such as databases, queues, and other Azure Functions, and it is important to ensure that the function will be able to properly interact with its external dependencies when running on the cloud. The ability to test that a function behaves consistently when interacting with those resources is essential to avoid issues like handling unexpected responses that weren't mocked; contemplating network latency while connecting to an external resource; and/or missing required configurations, firewall rules and permissions when deploying to a real environment.\\n\\nBusiness Problems and Challenges\\n\\nThe functionalities being developed in Docker-based Azure function usually have external dependencies, such as databases, queues, and other services. Though docker provides an isolated environment for the function, the external components on which the function depends on are still a key part of the software development and testing cycle.\\n\\nIn an ideal scenario each developer would have access to an isolated version of the full environment. However, this is not always possible. In some cases, the environment is too complex (or even impossible) to replicate on a local machine. The environment may also be too big to viably have a complete replica per developer.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Isolated-Docker-Function-Environments\\\\index.md'}, {'chunkId': 'chunk124_1', 'chunkContent': \"The ability to quickly test the function working with the external dependencies is also important. While it is possible to test the docker image without having it deployed to an Azure Function, it is not possible to test the function working with the external dependencies.\\n\\nDepending on how execution of the Azure Functions is secured, it could also present a challenge to trigger the function from a pipeline. For instance, if it's not being invoked from localhost, it could be necessary to authenticate the request.\\n\\nExecution of those functions when using queue triggers can also be challenging. This model requires a queue to be created and the function to be triggered by a message being added to the queue.\\n\\nSolution\\n\\nWhen a docker-based Azure Function has external Azure resources as dependencies, a good option is to have the ability to create and destroy those resources on demand as an isolated environment for test and development purposes. This provides a realistic environment without needing mocked dependencies while fully isolated from the rest of the team and project. If possible, creation of this environment should also leverage existing IaC modules on the solution that can be independently applied for each function that requires it for development and testing.\\n\\nAs a fallback, in cases where the creation of direct dependencies cannot be isolated from the general IaC code, it is possible to create minimal IaC version for each function. The downside of this approach is the requirement to maintain both IaC versions.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Isolated-Docker-Function-Environments\\\\index.md'}, {'chunkId': 'chunk124_2', 'chunkContent': 'For execution of a function in isolation, the docker container can be created and run either locally or in a pipeline, without having to deploy it to an Azure Function. This requires setting up the environment variables that are required by the function to point to the external dependencies created in the isolated environment.\\n\\nTo trigger the execution of the queue-based functions via HTTP, use the admin endpoint of the function. Since invoking this endpoint from an origin different than localhost requires authentication, a known key for the function should be generated and used it to authenticate the request. This key can be generated before the function is created, and then used to authenticate the request, as seen below:\\n\\n```bash\\n\\ngenerate some random key\\n\\nFAKE_MASTER_KEY=$(date | shasum | cut -d \\' \\' -f 1 )\\n\\ncreate a fake host.json file with the key\\n\\necho \"{ \\\\\"masterKey\\\\\": {\\\\\"name\\\\\": \\\\\"master\\\\\",\\\\\"value\\\\\": \\\\\"$FAKE_MASTER_KEY\\\\\",\\\\\"encrypted\\\\\": false},\\\\\"functionKeys\\\\\": [ ]}\" > host.json\\n\\nrun the container with the host.json file mounted as a volume', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Isolated-Docker-Function-Environments\\\\index.md'}, {'chunkId': 'chunk124_3', 'chunkContent': 'run the container with the host.json file mounted as a volume\\n\\ndocker run \\\\\\n    -e AzureWebJobsSecretStorageType=files \\\\\\n    -d \\\\\\n    -p 7073:80 \\\\\\n    -v $(pwd)/host.json:/azure-functions-host/Secrets/host.json \\\\\\n    my-function-docker-image-name\\n\\nfinally, invoke the function using the key\\n\\ncurl -X POST -H \"x-functions-key: $FAKE_MASTER_KEY\" -H \"Content-Type: application/json\" -d \\'{\"key\": \"value\"}\\' http://localhost:7073/admin/functions/my-function-name', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Isolated-Docker-Function-Environments\\\\index.md'}, {'chunkId': 'chunk124_4', 'chunkContent': '```\\n\\nKey Benefits and Drawbacks\\n\\nBenefits\\n\\nAbility to develop and test Azure Functions in isolation, without affecting other developers on a shared environment.\\n\\nLack of need to deploy the function to an Azure Function to test it.\\n\\nConfidence that the function will work as expected when interacting with real external resources.\\n\\nAutomation of the creation and destruction of the isolated environment for the function, to be used in a pipeline and as a reduced development environment.\\n\\nIndependence from queue triggers, allowing the function to be tested without the need to send messages to a queue.\\n\\nDrawbacks\\n\\nThe isolated environment is not a complete replica of the production environment, and may not be able to catch some issues.\\n\\nThe IaC modules must either be modularized to allow the creation of isolated environments, or a minimal IaC version must be created (and maintained) for each function.\\n\\nSome workaround are needed to be able to trigger queue-based functions via HTTP without deploying them to an Azure Function.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Isolated-Docker-Function-Environments\\\\index.md'}, {'chunkId': 'chunk125_0', 'chunkContent': 'Static Application Security Testing (SAST)\\n\\nOverview\\n\\nShifting left, a core concept in the DevSecOps methodology, means software quality and risk mitigation practices are done earlier in the development lifecycle. Security tasks, performed at the earlier phases, allow developers to address vulnerabilities when they are less costly to remediate.\\n\\nThe SAST process can be applied during the Development phase using tools integrated with the IDE, or at the Deployment phase when the scans are executed in the CI/CD pipeline. SAST tools analyze application source code, byte code and binaries for coding and design conditions that are indicative of security vulnerabilities. They analyze an application in a non-running state. Using SAST in DevSecOps mitigates security risks, such as SQL injections, cross-site scripting (XSS) and remote code execution (RCE) attacks, before a feature or application is complete.\\n\\nAn example of what a SAST tool would detect is SQL Injection, a common type of vulnerability found in source code. Check the code below for instance.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk125_1', 'chunkContent': 'c#\\nstring userName = ctx.getAuthenticatedUserName();\\nstring query = \"SELECT * FROM items WHERE owner = \"\\'\"\\n                + userName + \"\\' AND itemname = \\'\"\\n                + ItemName.Text + \"\\'\";\\nsda = new SqlDataAdapter(query, conn);\\nDataTable dt = new DataTable();\\nsda.Fill(dt);\\n\\nSince the query variable above stores a dynamically constructed string of a SQL query, an attacker can send \"name\\' OR 1=1 as the input for itemname. The following SQL string will then be executed, which essentially returns all the data in the items table.\\n\\nsql\\nSELECT * FROM items WHERE owner = \\'wiley\\' AND itemname = \\'name\\' OR 1=1;\\n\\nKey Takeaways\\n\\nThe SAST tool must be able to perform code reviews on the programming language used by the target application.\\n\\nThe tool should also be compatible with the scanning infrastructure in the DevOps pipeline.\\n\\nA good tool has the ability to detect well-known vulnerabilities such as code injection attacks. A great one also has a low false-positive and false-negative rate.\\n\\nActions\\n\\n1. Finalize tools selection', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk125_2', 'chunkContent': 'Actions\\n\\n1. Finalize tools selection\\n\\nSelect a SAST solution that supports the programming languages and frameworks used in the software project.\\n\\nSteps:\\n\\nSelect a tool based on the above criteria. OWASP publishes a number of SAST recommendations, from open source tools to commercial products. Sample implementation of SonarQube is shared below.\\n\\nSelect a single tool to perform this function. Though multiple tools can certainly be used, remember that the outputs from these scanners need to be addressed. Verifying a large quantity of items may cause unnecessary overhead. Here are some things to consider when deciding which tool to use.\\n\\n\\n\\nAttribute\\nFeatures to consider\\nMitigated risks\\n\\n\\n\\n\\nDetection accuracy\\nLow false-positive and false-negative rate\\nNoise from scans cuts focus time needed on resolving real application risks\\n\\n\\nLanguage versatility\\nCovers a wide range of programming languages\\nTechnology lock-in\\n\\n\\nSpeed\\nQuickly run parallel scans at scale\\nSlower delivery of scan results may delay code deployment to production\\n\\n\\nIntegration with other tools\\nAbility to easily send data to other tools like SIEM, SOAR, or CI/CD platforms\\nReduced visibility of scan results could cause delays in their resolution\\n\\n\\nDelivery model\\nOptions to deploy on-premise, the cloud, or a mix of both\\nOperational and infrastructure limitations', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk125_3', 'chunkContent': \"Delivery model\\nOptions to deploy on-premise, the cloud, or a mix of both\\nOperational and infrastructure limitations\\n\\nEnsure that organizational policies are considered during the selection process. Some tools require uploading the code to the vendor's cloud infrastructure for validation. If this poses as a risk, choose a tool that allows localized execution or self-hosting the required infrastructure.\\n\\n2. Integrate and Customize the Tool\\n\\nIntegrate the tool to the build environment and configure its scan settings, dashboards, and access controls.\\n\\nSteps:\\n\\nIntegrate the tool to the build environment used by the software project.\\n\\nConfigure access control and authorizations around the use of the tool and visibility to its output.\\n\\n3. Analyze Scan Results\\n\\nReview and resolve the SAST scan results.\\n\\nSteps:\\n\\nTriage the output from the scans to identify false-positives and false-negatives.\\n\\nDocument and track the remaining issues.\\n\\nRate the severity of each issue to help with prioritization. Use the NIST Common Vulnerability Scoring System (CVSS) framework to assess the vulnerabilities characterized by each issue.\\n\\n4. Provide Governance and Training\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk125_4', 'chunkContent': '4. Provide Governance and Training\\n\\nEnforce the proper use of the SAST process. Like any tool or process that manages security risks, verify compliance with any corporate policies and standards. Creating new or updating existing procedures may also be needed. Engage security stakeholders to determine any requirements that need to be met.\\n\\nSteps:\\n\\nDocument the process as a component of the software development lifecycle.\\n\\nCreate or update procedures on tool operation.\\n\\nTrain users on how to configure and operate the SAST tool.\\n\\nSample\\n\\nTo get an idea on the work needed to implement a SAST tool, here is a sample using SonarQube.\\n\\nUsing SonarQube in Azure DevOps and GitHub\\n\\nResources\\n\\nOWASP SAST Tools Recommendations\\n\\nNIST Common Vulnerability Scoring System (CVSS) Framework', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\index.md'}, {'chunkId': 'chunk126_0', 'chunkContent': 'Using SonarQube\\n\\nSetting up the server\\n\\nThe SonarQube instance consists of a server that handles code analysis, a database to store the results, and scanners running on the build pipeline.\\n\\nFor testing or evaluation purposes, install a local instance of the SonarQube server. A localized instance of SonarQube can be launched as a Docker container. The Community Edition of SonarQube can be found on Docker Hub. Start the server by running:\\n$ docker run -d --name sonarqube -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true -p 9000:9000 sonarqube:latest\\nLog in to http://localhost:9000 using System Administrator credentials admin/admin.\\n1. For production or pre-production environments, install a single-node SonarQube instance.\\n1. For larger workloads and where resiliency is required, install SonarQube as a cluster.\\n\\nDevOps platform integration\\n\\nAfter setting up the server, code repositories can now be integrated to SonarQube.\\n\\nAzure DevOps\\n\\nFollow these steps to integrate with AzDO.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\Recipes\\\\Using-SonarQube.md'}, {'chunkId': 'chunk126_1', 'chunkContent': \"Azure DevOps\\n\\nFollow these steps to integrate with AzDO.\\n\\nUse the SonarQube AzDO Plugin, found on the Visual Studio Marketplace, to incorporate tasks in the build definition YAML file.\\nSample YAML file for a .NET project:\\n{% raw %}\\n```yaml\\ntrigger:\\n- main # or the name of your main branch\\n- feature/*\\nsteps:\\nPrepare Analysis Configuration task\\n\\ntask: SonarQubePrepare@5\\ninputs:\\n    SonarQube: 'YourSonarqubeServerEndpoint'\\n    scannerMode: 'MSBuild'\\n    projectKey: 'YourProjectKey'\\n\\nRun Code Analysis task\\n\\ntask: SonarQubeAnalyze@5\\n\\nPublish Quality Gate Result task\\n\\ntask: SonarQubePublish@5\\ninputs:\\n    pollingTimeoutSec: '300'\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\Recipes\\\\Using-SonarQube.md'}, {'chunkId': 'chunk126_2', 'chunkContent': \"```\\n\\n{% endraw %}\\nSample YAML for a JavaScript, Go or Python project:\\n{% raw %}\\n```yaml\\ntrigger:\\n- main # or the name of your main branch\\n- feature/*\\nsteps:\\nPrepare Analysis Configuration task\\n\\ntask: SonarQubePrepare@5\\ninputs:\\n    SonarQube: 'YourSonarqubeServerEndpoint'\\n    scannerMode: 'CLI'\\n    configMode: 'manual'\\n    cliProjectKey: 'YourProjectKey'\\n\\nRun Code Analysis task\\n\\ntask: SonarQubeAnalyze@5\\n\\nPublish Quality Gate Result task\\n\\ntask: SonarQubePublish@5\\ninputs:\\n    pollingTimeoutSec: '300'\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\Recipes\\\\Using-SonarQube.md'}, {'chunkId': 'chunk126_3', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nGitHub\\n\\nFollow these steps to integrate with GitHub.\\n\\nA GitHub App is needed to connect to SonarQube. These are the steps to create one and to import the GitHub repository to SonarQube.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\Recipes\\\\Using-SonarQube.md'}, {'chunkId': 'chunk126_4', 'chunkContent': 'Configure the .github/workflows/build.yml file. For CLI, use the basic configuration using the SonarQube Scan GitHub Action from the GitHub Marketplace. For .NET, see the sample below.\\n{% raw %}\\nyaml\\nname: Build\\non:\\npush:\\n    branches:\\n    - main # or the name of your main branch\\npull_request:\\n    types: [opened, synchronize, reopened]\\njobs:\\nbuild:\\n    name: Build\\n    runs-on: windows-latest\\n    steps:\\n    - name: Set up JDK 11\\n        uses: actions/setup-java@v1\\n        with:\\n        java-version: 1.11\\n    - uses: actions/checkout@v2\\n        with:\\n        fetch-depth: 0  # Shallow clones should be disabled for a better relevancy of analysis\\n    - name: Cache SonarQube packages\\n        uses: actions/cache@v1\\n        with:\\n        path: ~\\\\sonar\\\\cache', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\Recipes\\\\Using-SonarQube.md'}, {'chunkId': 'chunk126_5', 'chunkContent': \"with:\\n        path: ~\\\\sonar\\\\cache\\n        key: ${{ runner.os }}-sonar\\n        restore-keys: ${{ runner.os }}-sonar\\n    - name: Cache SonarQube scanner\\n        id: cache-sonar-scanner\\n        uses: actions/cache@v1\\n        with:\\n        path: .\\\\.sonar\\\\scanner\\n        key: ${{ runner.os }}-sonar-scanner\\n        restore-keys: ${{ runner.os }}-sonar-scanner\\n    - name: Install SonarQube scanner\\n        if: steps.cache-sonar-scanner.outputs.cache-hit != 'true'\\n        shell: powershell\\n        run: |\\n        New-Item -Path .\\\\.sonar\\\\scanner -ItemType Directory\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\Recipes\\\\Using-SonarQube.md'}, {'chunkId': 'chunk126_6', 'chunkContent': 'New-Item -Path .\\\\.sonar\\\\scanner -ItemType Directory\\n        dotnet tool update dotnet-sonarscanner --tool-path .\\\\.sonar\\\\scanner\\n    - name: Build and analyze\\n        env:\\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Needed to get PR information, if any\\n        shell: powershell\\n        run: |\\n        .\\\\.sonar\\\\scanner\\\\dotnet-sonarscanner begin /k:\"example\" /d:sonar.login=\"${{ secrets.SONAR_TOKEN }}\" /d:sonar.host.url=\"${{ secrets.SONAR_HOST_URL }}\"\\n        dotnet build\\n        .\\\\.sonar\\\\scanner\\\\dotnet-sonarscanner end /d:sonar.login=\"${{ secrets.SONAR_TOKEN }}\"\\n{% endraw %}', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\Recipes\\\\Using-SonarQube.md'}, {'chunkId': 'chunk126_7', 'chunkContent': \"Analyzing the results\\n\\nSonarQube analyzes the files and stores the reports on the server. Here's a sample screen from the SonarQube server.\\n\\nBugs pertain to code quality issues found by SonarQube\\n\\nVulnerabilities are items that pose a security risk to application\\n\\nHotspots are blocks of code that may introduce a security risk\\n\\nCode Smells are potential deviations from good coding practices\\n\\nDuplication are repeating blocks of code\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Static-Application-Security-Testing\\\\Recipes\\\\Using-SonarQube.md'}, {'chunkId': 'chunk127_0', 'chunkContent': \"Implementing an Update Mechanism for Azure Managed Applications\\n\\nAzure Managed Applications provide a distribution mechanism for applications in Azure, as a single package, via the Service Catalog or the Azure Marketplace. Application developers must implement a mechanism to update an Azure Managed Application once they have been deployed; and this pattern describes how to how to do that.\\n\\nBusiness problems and challenges\\n\\nAzure Managed Applications allow Independent Software Vendors (ISVs) to distribute Azure-based solutions as a single package, via a Service Catalog (internally) and via the Azure Marketplace (externally). Application developer must implement a mechanism to update Azure Managed Applications once they have been deployed. One thing to be conscious of is that once a version of the solution is deployed, the only way to get an updated version is to remove it from the subscription and re-adding the updated version.\\n\\nSome of the challenges that ISVs face when deploying an Azure Managed Application are:\\n\\nHow to let their customers know that an update is available and how to update the application.\\n\\nHow to update the application without interrupting the customer's workloads.\\n\\nHow to roll back to a previous version of the application if there is an issue during the update process.\\n\\nHow to provide different versions of the application to different customers.\\n\\nFigure out a way to keep a communication channel between the deployed solution and the ISV for monitoring and observability purposes.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Updating-Azure-Managed-Apps\\\\index.md'}, {'chunkId': 'chunk127_1', 'chunkContent': 'Figure out a way to keep a communication channel between the deployed solution and the ISV for monitoring and observability purposes.\\n\\nThe realm of updates that could lead to deploying a new version of an existing solution is very broad and may require a changes of different complexity. For example:\\n\\nA simple configuration change, such as a single value being modified for an existing application configuration key\\n\\nA new version of the application binaries that, depending on the involved technologies, could just imply changing a docker image tag or a more complex process such as the deployment of a new binary package and/or swapping slots in an App Service\\n\\nAn urgent security update that potentially affects any of the already deployed components\\n\\nA core infrastructure change that could be as large as the initial deployment\\n\\nSolution\\n\\nA possible way to tackle both the application and infrastructure upgrading issue is to split the application deployment into two phases:\\n\\nThe initial deployment of the baseline resources to establish a communication channel between the deployed solution and the ISV.\\n\\nThe deployment of the application itself, which will be updated independently from the baseline resources.\\n\\nThis approach allows the ISV to update the application without affecting the baseline resources. Those resources are the ones that are required to establish a communication channel between the deployed solution and the ISV. This communication channel can be used to monitor the deployed solution and to provide the ISV with the means to apply updates to the application without requiring user intervention.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Updating-Azure-Managed-Apps\\\\index.md'}, {'chunkId': 'chunk127_2', 'chunkContent': 'The following diagram shows a high-level flow of the solution:\\n\\nmermaid\\ngraph TD\\n    AMA --> | Triggers | ID[Initial deployment]\\n    Customer --> | Deploys | AMA[Azure Managed App]\\n    ID --> | Creates | MRG[Managed Resource Group]\\n    ID --> | Creates | BR[Baseline resources]\\n    BR --> | Comm. channel | ISV\\n    ISV --> | App deployment | BR\\n    BR --> | App resources | MRG\\n    MRG --> | Makes available | App\\n\\nThere are many possible ways to implement this two-phase deployment pattern, depending on the solution itself:\\n\\nHTTP-based communication channel\\n\\nA relatively simple way to establish a communication channel between the deployed solution and the ISV is to use an HTTP endpoint. This endpoint could be an HTTP-triggered Azure Function that is deployed as part of the baseline resources and that is responsible for receiving and processing the application and infrastructure update requests, using an identity that has the necessary permissions to perform the required operations.\\n\\nIn a similar manner, the ISV could also use an HTTP-triggered Azure Function listening for requests coming from the deployed solution. This could be used to notify the ISV of any issues or to provide the ISV with any telemetry data.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Updating-Azure-Managed-Apps\\\\index.md'}, {'chunkId': 'chunk127_3', 'chunkContent': \"The following diagram shows an example of how this solution could be implemented:\\n\\nService Bus-based communication channel\\n\\nAnother way to establish a communication channel between the deployed solution and the ISV is to use Azure Service Bus. While conceptually similar to the HTTP-based communication channel, this approach provides a more robust and reliable communication channel, as it is based on message queues.\\n\\nDepending on costs, isolation and security requirements, the Service Bus namespace could be shared between multiple solutions or it could be dedicated to a single solution. In a similar manner, it would be possible to host a single Service Bus namespace in the ISV's subscription or to host a dedicated Service Bus namespace in the customer's subscription.\\n\\nThe following diagram shows an example of how this solution could be implemented:\\n\\nAzure Kubernetes Service + GitOps solution\\n\\nA different approach to implement the two-phase deployment pattern is to use Azure Kubernetes Service (AKS) and GitOps. In this case, the baseline resources would include an AKS cluster, and the application and its infrastructure requirements would be deployed as a set of Kubernetes resources, using a Git repository as the source of truth.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Updating-Azure-Managed-Apps\\\\index.md'}, {'chunkId': 'chunk127_4', 'chunkContent': 'The Git repository would be hosted in a private repository, such as Azure DevOps or GitHub and the credentials to access it would be provisioned to the Azure Managed Application as part of the initial deployment. The AKS cluster would be configured to use a GitOps operator such as Flux or Argo CD to monitor the Git repository and to apply any changes to the Kubernetes resources.\\n\\nWhile this solution has the overhead of having to manage an AKS cluster per Azure Managed Application deployment, it provides a very flexible and powerful and way to deploy the application and its infrastructure requirements without having to create a custom deployment solution. This could be especially useful for applications that are already containerized and planned to be deployed into an AKS instance.\\n\\nThe following diagram shows an example of how this solution could be implemented:\\n\\nKey Benefits\\n\\nRegardless of the approach used to implement the two-phase deployment pattern, the key benefits of implementing this pattern are:\\n\\nAllows the ISV to provide updates to their customers in a safe and transparent way, as the application is not removed from the subscription during the update process.\\n\\nSince the baseline resources are independent from the application itself, implementing the pattern allows the ISV to implement several Azure Managed Applications using the baseline.\\n\\nIt does not require the customer to take any action to update the application, or even to be aware that a new version was deployed.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Updating-Azure-Managed-Apps\\\\index.md'}, {'chunkId': 'chunk127_5', 'chunkContent': 'It does not require the customer to take any action to update the application, or even to be aware that a new version was deployed.\\n\\nIf an issue with the new version is detected during or after deployment, it is possible to roll back to the last working version by triggering a re-deployment of the previous application version, as long as the application changes can be applied in an idempotent way (i.e. using Terraform or the Azure Service Operator to deploy resources).\\n\\nThe creation of a two-way communication channel between the deployed solution and the ISV for monitoring and observability purposes.\\n\\nDescription and discussion of the related code\\n\\nAn implementation of the HTTP-based communication channel solution is available in the Azure Samples organization.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Updating-Azure-Managed-Apps\\\\index.md'}, {'chunkId': 'chunk128_0', 'chunkContent': 'Windows Binaries Scanning Tools\\n\\nOverview\\n\\nAs Windows-based software and applications become increasingly prevalent in the modern technological landscape for some industries, it is essential to ensure the security and stability of these systems. One critical aspect of this is the need for effective tools to identify and address potential security vulnerabilities in Windows binary files as early as possible.\\n\\nProblem Statement\\n\\nThe problem is that Windows binary files, which contain executable code and other program data, may be modified, or tampered with by malicious actors seeking to exploit vulnerabilities or launch attacks. As a result, there is a need for robust Windows binary scanners capable of quickly and accurately identifying potential threats, vulnerabilities, and other security risks in these files. To minimize the risk of attacks and ensure the continued security and stability of Windows-based systems and applications, it is essential to shift left and identify vulnerabilities as early as possible in the software development lifecycle. This requires effective integration of Windows binary scanners into the development process to detect and address potential vulnerabilities before they can be exploited.\\n\\nChallenges\\n\\nThere are several challenges associated with developing and implementing effective Windows binary scanners, including:\\n\\nComplexity: Windows binary files can be complex, making it challenging to identify potential security vulnerabilities or threats accurately.\\n\\nCompatibility: Windows binary scanners must be designed to work with a wide range of file types, including both commercial and open-source software, to be effective.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_1', 'chunkContent': 'Efficiency: Scanning large volumes of binary files can be time-consuming and resource-intensive, making it essential to develop efficient scanning methods that minimize the risk of attacks and ensure the continued security and stability of Windows-based systems and applications.\\n\\nFalse positives: Windows binary scanners must be capable of accurately identifying potential threats without generating an excessive number of false positives, which can be time-consuming and resource-intensive to investigate and address.\\n\\nSecurity: Windows binary scanners themselves must be secure and free from vulnerabilities that could be exploited by malicious actors seeking to circumvent the scanning process or launch attacks against the scanner or the systems it is designed to protect.\\n\\nShifting left: To effectively identify vulnerabilities early in the software development lifecycle, Windows binary scanners must be integrated into the development process and shifted left. However, this can be challenging due to the need to balance scanning efficiency with the risk of false positives and the need to minimize disruption to the development process.\\n\\nSolution', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_2', 'chunkContent': 'Solution\\n\\nOne solution to these challenges is to use Windows binary scanners that can be run as early as possible in the development process and do not require installation or pulling to a client environment. This approach can minimize disruption to the development process and allow for efficient and effective scanning of Windows binary files. By integrating these scanners into the development process and shifting left, potential security vulnerabilities can be identified and addressed early in the development process, reducing the risk of attacks and ensuring the continued security and stability of Windows-based systems and applications. Additionally, using scanners that can be run without installation or pulling to a client environment can minimize the risk of introducing new vulnerabilities or disruptions to the development process.\\n\\nTools\\n\\nWhen selecting a Windows binary scanner tool, it is important to consider factors such as ease of integration into the development pipeline, support for different operating systems, license, database updates, and offline mode. Some popular Windows binary scanner tools are ClamAV, Virus Total, Binskim and Binscope.\\n\\nClamAV', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_3', 'chunkContent': \"ClamAV\\n\\nClamAV includes a command-line scanner, automatic database updater, and a scalable multi-threaded daemon running on an anti-virus engine from a shared library. The application features a Milter interface for sent mail and on-demand scanning. It recognizes: ZIP, RAR, Tar, Gzip, Bzip2, OLE2, Cabinet, CHM, BinHex, and SIS formats1. ClamAV is designed to scan files quickly and provides real-time protection (Linux only). The ClamOnAcc client for the ClamD scanning daemon provides on-access scanning on modern versions of Linux. This includes an optional capability to block file access until a file has been scanned (on-access prevention).\\n\\nClamAV is licensed under the GNU General Public License (GPL). The source code is available on GitHub. The ClamAV project is a member of the Linux Foundation's Core Infrastructure Initiative (CII) and is a Linux Foundation Collaborative Project.\\n\\nFeatures\\n\\nCan run as a daemon (ClamDScan) or ad hoc (ClamScan).\\n\\nSignature updates for local databases can be performed ad hoc with FreshClam.\\n\\nValidates Authenticode signatures.\\n\\nClamAV detects millions of viruses, worms, trojans, and other malware, including Microsoft Office macro viruses, mobile malware, and other threats.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_4', 'chunkContent': 'Supports Windows executable file parsing, also known as Portable Executables (PE) both 32/64-bit, including PE files that are compressed or obfuscated with Petite, PeSpin, NsPack, wwpack32, MEW, Upack, Y0da Cryptor.\\n\\nEasy to plug/install in an development pipeline.\\n\\nScans binaries for Windows and Linux environments.\\n\\nClamScan can operate offline (ClamScan + FreshClam).\\n\\nNo special format for the output report.\\n\\nGeneral Flow\\n\\nFreshclam is a tool that updates the virus definitions used by ClamAV. It downloads the latest virus definitions from the ClamAV database and updates the local virus database on your system.\\n\\nOnce the virus definitions are updated, ClamScan can be executed to against files and directories. ClamScan checks each file against the virus definitions in the local database to determine if it is infected with malware.\\n\\nIf Clamscan detects malware in a file, it will report the name of the infected file and the type of malware detected.\\n\\nClamScan can be configured to take different actions when it detects malware, such as quarantining or deleting the infected file.\\n\\nVirus Total', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_5', 'chunkContent': \"Virus Total\\n\\nVirusTotal is a free online service that analyzes suspicious files using 40+ anti-virus applications. It helps detect viruses, worms, trojans, and all kinds of malware and provides reliable results preventing any false positive cases. VirusTotal’s aggregated data is the output of many different antivirus engines, website scanners, file and URL analysis tools, and user contributions. VirusTotal can analyze suspicious files, domains, IPs and URLs to detect malware and other breaches. VT Enterprise (premium services) provides additional context and threat landscape visibility. However, it's important to note that the file to be scanned must be uploaded online, which may not be acceptable for some industries due to security and privacy concerns.\\n\\nVirusTotal has a command-line interface (CLI) tool called vt-cli. This tool has most of the same functionality and features as the VirusTotal website. You can use vt-cli to automate your scanning process and integrate it with other tools.\\n\\nVirusTotal CLI itself is free and open-source under the Apache License 2.0, but its usage with the VirusTotal service is subject to the VirusTotal Terms of Service and licensing modes for its API.\\n\\nFeatures\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_6', 'chunkContent': 'Features\\n\\nVirusTotal is a free online service that analyzes files and URLs for viruses, worms, trojans, and other kinds of malicious content using over 70 antivirus scanners and URL/domain blocklisting services.\\n\\nSignature database is kept up to date but online.\\n\\nIt can extract suspicious signals. For example, OLE VBA code streams in Office document macros, invalid cross-reference tables in PDFs, packer details in Windows Executables, intrusion detection system alerts triggered in PCAPs, Exif metadata, or authenticode signatures.\\n\\nGeneral Flow\\n\\nUsers submit the binary file to VirusTotal for scanning.\\n\\nVirusTotal checks if the file has already been scanned before. If it has, it returns the existing scan results.\\n\\nIf the file has not been scanned before, VirusTotal sends the file to multiple antivirus engines for analysis.\\n\\nEach antivirus engine checks the file for known malware signatures and behaviors.\\n\\nEach antivirus engine returns a report to VirusTotal with its analysis results.\\n\\nVirusTotal aggregates the results from all the antivirus engines and generates a report with the overall analysis results.\\n\\nThe returned report includes information such as the number of antivirus engines that detected the file as malicious, the names of the antivirus engines that detected the file, and any additional information about the file.\\n\\nBinskim', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_7', 'chunkContent': 'Binskim\\n\\nBinskim is a binary code analysis tool developed by Microsoft that helps identify potential security vulnerabilities in Windows binaries. It uses a combination of static and dynamic analysis techniques to identify security issues such as buffer overflows, integer overflows, and other common vulnerabilities. Binskim works by analyzing the binary code of a Windows executable or DLL file and generating a report that highlights any potential security issues. The report includes information such as the location of the vulnerability in the code, the type of vulnerability, and recommendations for how to fix the issue. Binskim can be used as part of a software development lifecycle to identify and fix security issues in Windows binaries before they are deployed to production environments.\\n\\nBinskim is open-source binary code scanner developed by Microsoft released under the MIT License.\\n\\nFeatures\\n\\nIdentifies potential security vulnerabilities in Windows binaries using a combination of static and dynamic analysis techniques.\\n\\nGenerates a report that highlights any potential security issues, including the location of the vulnerability in the code, the type of vulnerability, and recommendations for how to fix the issue.\\n\\nSupports analyzing both 32-bit and 64-bit Windows binaries.\\n\\nCan be integrated into a software development lifecycle to identify and fix security issues in Windows binaries before they are deployed to production environments.\\n\\nProvides a command-line interface for easy integration into automated build and test systems.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_8', 'chunkContent': 'Provides a command-line interface for easy integration into automated build and test systems.\\n\\nSupports analyzing both standalone binaries and binaries that are part of an installer package.\\n\\nCan be used to analyze both native code and managed code binaries.\\n\\nSupports analyzing binaries compiled with a variety of compilers, including Visual C++, GCC, and Clang.\\n\\nCan be extended with custom rules to detect additional security issues.\\n\\nCan work offline. Binskim performs static analysis on the binary code of a file, which means that it does not require an internet connection to analyze the file.\\n\\nBinscope binary analyzer\\n\\nBinscope is a binary code analysis tool developed by Microsoft that helps identify potential security vulnerabilities in Windows binaries. It uses a combination of static and dynamic analysis techniques to identify security issues such as buffer overflows, integer overflows, and other common vulnerabilities. Binscope works by analyzing the binary code of a Windows executable or DLL file and generating a report that highlights any potential security issues. The report includes information such as the location of the vulnerability in the code, the type of vulnerability, and recommendations for how to fix the issue. Binscope can be used as part of a software development lifecycle to identify and fix security issues in Windows binaries before they are deployed to production environments.\\n\\nFeatures', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_9', 'chunkContent': 'Features\\n\\nIdentifies potential security vulnerabilities in Windows binaries using a combination of static and dynamic analysis techniques.\\n\\nGenerates a report that highlights any potential security issues, including the location of the vulnerability in the code, the type of vulnerability, and recommendations for how to fix the issue.\\n\\nSupports analyzing both 32-bit and 64-bit Windows binaries.\\n\\nCan be integrated into a software development lifecycle to identify and fix security issues in Windows binaries before they are deployed to production environments.\\n\\nProvides a graphical user interface (GUI) for easy analysis and reporting.\\n\\nSupports analyzing both standalone binaries and binaries that are part of an installer package.\\n\\nCan be used to analyze both native code and managed code binaries.\\n\\nSupports analyzing binaries compiled with a variety of compilers, including Visual C++, GCC, and Clang.\\n\\nCan be extended with custom rules to detect additional security issues.\\n\\nComparison\\n\\nTool Purpose License Operating System Command Line Interface Graphical User Interface ClamAV Detect malware in files and directories GNU GPL Windows, Linux, macOS Yes No VirusTotal Analyze suspicious files, domains, IPs, and URLs for malware Proprietary online (cli Linux) Yes Yes Binskim Identify potential security vulnerabilities in Windows binaries MIT License Windows Yes No Binscope Identify potential security vulnerabilities in Windows binaries MIT License Windows No Yes\\n\\nReferences\\n\\nClamAV', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk128_10', 'chunkContent': 'References\\n\\nClamAV\\n\\nVirus Total\\n\\nBinskim\\n\\nBinscope binary analyzer (user guide)', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\03-Deploy\\\\Windows-Binaries-Scanning\\\\index.md'}, {'chunkId': 'chunk129_0', 'chunkContent': 'Operate Phase\\n\\n```mermaid\\nflowchart LR\\n    plan{{PLAN}}==>dev{{DEVELOP}}==>deploy{{DEPLOY}}==>operate{{OPERATE}}\\n    style operate fill:lightgray, text:black', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\04-Operate\\\\index.md'}, {'chunkId': 'chunk129_1', 'chunkContent': \"```\\n\\nThe Operate Phase of DevSecOps focuses on maintaining and improving the security of applications that have been deployed to production environments. Even though an application may have passed rigorous testing during development, it is still vulnerable to attacks and its security certification can become stale over time. Therefore, additional security measures must be implemented in the live production environment to ensure that the software remains secure.\\n\\nTo support the Operate Phase, the team employs rich telemetry, an actionable alerting system, and full visibility into applications and their underlying subsystems. Security monitoring practices gather analytics to instrument and monitor security-related metrics. This involves monitoring the software for security issues, responding to security incidents, and implementing patches and updates to address vulnerabilities.\\n\\nThe team may also conduct regular security assessments to ensure that the software continues to meet security standards and identify potential vulnerabilities. They may also collect and analyze security-related data to inform decisions about how to improve the software's security posture.\\n\\nThe goal of the Operate Phase is to ensure that the software remains secure over time and continuously improves its security posture. By proactively monitoring and addressing security issues, the team can reduce the risk of security breaches and protect the software from potential threats.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\04-Operate\\\\index.md'}, {'chunkId': 'chunk129_2', 'chunkContent': \"Tool Supported Features Benefits Examples Intrusion detection and prevention systems These tools monitor network traffic for signs of malicious activity. Block or alert on potential threats. Snort and Suricata. Vulnerability management tools These tools help teams identify and prioritize vulnerabilities in their software and infrastructure. Helps track progress in addressing identified vulnerabilities. Rapid7, Nexpose and Qualys. Security information and event management (SIEM) systems These tools collect and analyze security-related data from various sources, such as logs and network traffic, to identify potential threats. Proactively monitor anomalies, identify trends, optimize performance and help troubleshoot issues. Splunk and LogRhythm. Access control systems These tools manage and enforce access to resources and systems. Helps to prevent unauthorized access. Active Directory, Okta and OneLogin. Data loss prevention (DLP) systems These tools help to protect sensitive data from being leaked or lost by monitoring and controlling the movement and storage of data. DLP systems can help organizations to better protect sensitive data, comply with regulations such as the EU's General Data Protection Regulation (GDPR) or the Payment Card Industry Data Security Standard (PCI DSS) , improve their security posture, and enhance collaboration. Symantec DLP and Forcepoint DLP.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\04-Operate\\\\index.md'}, {'chunkId': 'chunk130_0', 'chunkContent': 'Secrets Rotation\\n\\nOverview\\n\\nSecret rotation is the process of regularly changing the cryptographic secrets used to secure sensitive information and data within an organization. This is done to mitigate the risk of compromise due to potential exposure,  unauthorized access to sensitive information, data breaches, and reputational damage. The process involves generating a new set of secrets to replace the old ones and then securely updating the affected systems and applications with the new secrets.\\n\\nProblem Statement\\n\\nInadvertent sharing of secrets is a significant security risk that organizations must address. Among the most common ways in which secrets get exposed are embedding the secrets directly in code or leaking them through run-time processes, automation pipelines, and logs. One way to mitigate the risk of leaked secrets is to adopt a proactive approach to secrets management, including regular secrets rotation and proper storage.\\n\\nEffective Key Rotation Strategy\\n\\nEffective secrets management is essential for preventing unauthorized access to sensitive information, and one of the best practices is regular secret rotation. To implement this practice, developers should store secrets outside of code repositories using a secure Secrets Store, which can be accessed programmatically. It is crucial to use secret detection methods to scan for secrets that may be hard-coded in code repositories, as these could be easily exposed to unauthorized parties.\\n\\nSecret Rotation Goals', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\04-Operate\\\\Secrets-Rotation\\\\index.md'}, {'chunkId': 'chunk130_1', 'chunkContent': 'Secret Rotation Goals\\n\\nThe goal of an effective secret rotation strategy is to minimize disruption while ensuring that the new secret value is reliably and securely transmitted to all relevant systems. This requires careful planning and coordination to minimize downtime, avoid potential data loss or corruption, and ensure that all systems are updated with the new secret value in a secure and consistent manner. By prioritizing these considerations, organizations can implement secret rotation with confidence, knowing that they are protecting their sensitive data and assets against unauthorized access and exposure.\\n\\nIt is essential to align rotation plans with the secret inventory discussed in the Secrets Management section of this playbook. This ensures that the current rotation strategy for each secret is easily accessible to those responsible for DevOps processes. Any updates or additions to the rotation method for a given secret should be reflected in the inventory, making it simple for implementation. Maintaining an up-to-date inventory is vital in ensuring that secrets remain secure and protected against unauthorized access or exposure throughout their lifecycle.\\n\\nProposed Actions\\n\\nSecret rotation is a crucial aspect of a comprehensive secrets management strategy, and there are two primary actions to consider when implementing it.\\n\\nScheduled Rotation\\n\\nSecrets should be rotated on a predetermined schedule to reduce their lifetime, minimizing the risk of long-term exposure of assets that rely on a specific secret.\\n\\nOn-Demand Rotation', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\04-Operate\\\\Secrets-Rotation\\\\index.md'}, {'chunkId': 'chunk130_2', 'chunkContent': \"On-Demand Rotation\\n\\nIn the event that a secret is accidentally leaked, a rapid rotation strategy is required to help to proactively mitigate the exposure of resources protected by the compromised secret.\\n\\nSecret Rotation Recommendations\\n\\nChoose a reasonable time for rotation.This applies to both time of day and frequency.\\n\\nConsider using encryption key strength in relation to what is being protected. Longer keys provide stronger security, but may also result in slower key processing and decryption times.\\n\\nLeverage the benefits of highly-available infrastructure during key rotation to reduce the likelihood of downtime where possible.\\n\\nIn the course of Rotating keys/certificates ensure that the new keys or certificates have the same parameters and strength as the ones being replaced.\\n\\nAvoid creating secrets with empty expiration dates.\\n\\nWhen re-encryption of data is necessary, increase the time between scheduled rotations to optimize resource utilization. It is common to leverage (symmetric) Data Encryption Keys to protect data at rest and (asymmetric) Key Encryption Keys for data in transit. Schemes as described in this document, fit well into the Key Vault model.\\n\\nWhen the responsibility of application or system is transferred, it is recommended to rotate all the secrets.This mitigates the risk of continued access to the application and it's data by those who previously controlled the system.\\n\\nResources\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\04-Operate\\\\Secrets-Rotation\\\\index.md'}, {'chunkId': 'chunk130_3', 'chunkContent': 'Resources\\n\\nAutomated Key Rotation configuration in Azure Key Vault\\n\\nOn-Demand Manual Key Rotation\\n\\nConfigure Key Rotation using the Azure CLI\\n\\nTutorial: Automatic rotation of a single set of authentication credentials\\n\\nTutorial: Automatic rotation of two sets of authentication credentials\\n\\nSample: Using Terraform to store and rotate Azure Function API keys used by API Management in Key Vault', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\04-Operate\\\\Secrets-Rotation\\\\index.md'}, {'chunkId': 'chunk131_0', 'chunkContent': \"rings:\\n  - public\\nms.author: pabouwer\\n\\nPolicy\\n\\nPolicy defines an organization's business rules and governance around areas like compliance, security, cost, and consistency. Policy can be applied to entities at different levels of granularity. It can be used either to surface non-compliance for audit purposes and/or to enforce compliance through policy gates for control purposes.\\n\\nPolicy as code is an approach that treats policy artifacts as source code. This is conceptually similar to Infrastructure as Code (IaC) and provides similar benefits around repeatability and version control.\\n\\nCharacteristics\\n\\nDefinitions\\n\\nA policy definition is a policy artifact that at a minimum typically describes a collection of parameters, a set of compliance conditions, and the actions to take if the conditions aren't met.\\n\\nExamples of policy definitions include Azure Policy Definition in the Azure Policy ecosystem and Constraint Templates in Gatekeeper ecosystem.\\n\\nAssignment\\n\\nA policy assignment is a policy artifact that applies an instance of a policy definition to a collection of entities through parameter values and scopes.\\n\\nExamples of policy assignments include: Azure Policy Assignment and Gatekeeper Constraints.\\n\\nEnforcement\\n\\nA policy gate is used to enforce policy against entities being added to or modified within a system. The policy gate is typically built using a policy engine, the policy definitions, and the policy assignments.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\governance\\\\policy.md'}, {'chunkId': 'chunk131_1', 'chunkContent': 'Examples of services and tools that enforce policy include Azure Policy and Gatekeeper.\\n\\nAudit\\n\\nOften there is a requirement to report policy compliance through an audit across a collection of entities without taking any actions to enforce the policy. This may be due to policy updates over time and entities that no longer are compliant against updated policy. Manual intervention to correct entities may be preferred to automatic enforcement breaking the running system.\\n\\nExamples of audit capability include Azure Policy Compliance States, and Gatekeeper Audit.\\n\\nExamples\\n\\nEnforcement\\n\\nThe \"Improve release artifact and workload integrity in Kubernetes via a secure software supply chain\" solution demonstrates policy enforcement in an AKS cluster. The policy enforcement is achieved through the use of the Kubernetes admission controller mechanism which allows Kubernetes to intercept requests to the Kubernetes API server. In the solution, Gatekeeper is a policy controller that\\'s registered with the admission controller and enforces policy related to software integrity and governance.\\n\\nLearn More\\n\\nMS Learn - Azure Policy\\n\\nMS Learn - Design Azure Policy as Code workflows\\n\\nPalo Alto - What is Policy as Code?\\n\\nOpen Policy Agent - What is Policy?\\n\\nSolutions Playbook - Considerations when evaluating a policy framework', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\governance\\\\policy.md'}, {'chunkId': 'chunk132_0', 'chunkContent': \"rings:\\n  - public\\nms.author: pabouwer\\n\\nKnowledge Graph\\n\\nA knowledge graph uses a graph based data model to store details about entities, the relationships between those entities, and groupings or categorizations of those entities. Knowledge graphs are typically used when the relationships between entities, and the details or descriptions of those relationships, are a critical part of the data model.\\n\\nA well-defined ontology defines the entities and their properties, any grouping or categorization that can be applied to entities, and finally how these entities can be associated through relationships. Relationships are further defined through properties that elaborate on the details of how or why the entities are associated. An ontology provides a well defined model through which the entities, their categorizations, and their relationships can be queried.\\n\\nAn example ontology from a secure software supply chain ecosystem could involve the following entities:\\n\\nSoftware release\\n\\nPackage\\n\\nDeployment\\n\\nCluster\\n\\nVulnerability\\n\\nThe relationships across these entities might include:\\n\\nSoftware release is composed of many packages\\n\\nDeployment deploys a software release to a cluster\\n\\nVersion of Package has a vulnerability.\\n\\nThese relationships enable queries like:\\n\\nWhich clusters have a version of release x that's exposed to the critical zero-day vulnerability y?\\n\\nDoes release x have any critical severity vulnerabilities?\\n\\nWhich packages in release x are vulnerable to CVE-123?\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\observability\\\\knowledge-graph.md'}, {'chunkId': 'chunk132_1', 'chunkContent': 'Does release x have any critical severity vulnerabilities?\\n\\nWhich packages in release x are vulnerable to CVE-123?\\n\\nAn example ontology from the Microsoft Graph ecosystem could involve the following entities:\\n\\nEmployee\\n\\nFile\\n\\nThe relationships across these entities might include:\\n\\nEmployee a is the manager of employee b\\n\\nEmployee b has recently edited file x.\\n\\nThese relationships enable queries like:\\n\\nWhat files have the direct reports of manager a been working on recently?\\n\\nWho are the direct reports of manager a?\\n\\nCharacteristics\\n\\nEntity\\n\\nAn entity describes an object and its properties. An example could be an employee entity that has first name, last name, department, email address as properties.\\n\\nRelationship\\n\\nA relationship describes how and why two entities are associated. An example could be employee a is a manager for employee b.\\n\\nCategorization\\n\\nEntities can be grouped into categories. An example could be package, vulnerability, digital signature as entities that are grouped into the security artifacts category.\\n\\nOntology\\n\\nAn ontology defines the categories, properties, and relationships between the concepts in a specific domain. In the context of the knowledge graph, the ontology can be seen as the formal contract or schema of the data representing the domain in the graph.\\n\\nExamples\\n\\nOntology', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\observability\\\\knowledge-graph.md'}, {'chunkId': 'chunk132_2', 'chunkContent': 'Examples\\n\\nOntology\\n\\nThe \"Improve release artifact and workload integrity in Kubernetes via a secure software supply chain\" solution describes how the various processes and artifacts produced across the software supply chain can be associated, and their relationships queried to provide deep system insights.\\n\\nThe Microsoft Graph is an example of how the Microsoft 365 entities, services and the relationships between them can be queried to build an understanding of how things are connected, or to gain deeper insights across categories of entities.\\n\\nThe Azure Resource Graph is an example of how Azure resources and the relationships between them can be used to explore complex resource topologies, or understand policy compliance.\\n\\nGUAC (Graph for Understanding Artifact Composition) is an example of how software, software supply chain artifacts, and their relationships can be queried to improve governance and identify security risks.\\n\\nLearn More\\n\\nWikipedia - Knowledge Graph\\n\\nOntotext - What\\'s a Knowledge Graph\\n\\nWikipedia - Ontology', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\observability\\\\knowledge-graph.md'}, {'chunkId': 'chunk133_0', 'chunkContent': \"rings:\\n  - public\\nms.author: pabouwer\\n\\nAttestations\\n\\nAn attestation is a cryptographically signed collection of claims related to one or more software artifacts or events. Attestations can be cryptographically verified by any party with access to the public key that corresponds to the private key used to sign the attestation.\\n\\nCryptographic signatures applied to artifacts are a simple form of attestation. The signature is an implicit claim that agrees trust between two parties. Attestations build on this simple form and contain richer metadata about what's being attested.\\n\\nAn example of an attestation in the secure software supply chain ecosystem is attesting that a software release was built on a build runner that was backed by a VM in a specific Azure Subscription at a specific date/time.\\n\\nCharacteristics\\n\\nCryptographic Signature\\n\\nPublic-key cryptography is the foundation of ensuring integrity and non-tampering of the attestation. A securely held private key is leveraged to sign the attestation in a trusted environment. The corresponding public key from the key pair, is available in lower trust environments to cryptographically verify the attestation.\\n\\nClaim\\n\\nThe attestation has a collection of claims that reference an artifact or event, and also provide additional metadata about the claim.\\n\\nAn example in the secure software supply chain includes attestation that references vulnerability results and includes metadata about the scanning tool and scanning event.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\security\\\\attestations.md'}, {'chunkId': 'chunk133_1', 'chunkContent': 'An example in the secure software supply chain includes attestation that references vulnerability results and includes metadata about the scanning tool and scanning event.\\n\\nExamples\\n\\nThe \"Improve release artifact and workload integrity in Kubernetes via a secure software supply chain\" solution discusses attestations and provides an implementation that shows signing of security artifacts for simplified attestation.\\n\\nLearn More\\n\\nMS Learn - Real Life Attestation Examples\\n\\nSLSA - Software Attestations\\n\\nTestifySec - What is a Software Supply Chain Attestation - and why do I need it?\\n\\nin-toto - Attestation Predicates', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\security\\\\attestations.md'}, {'chunkId': 'chunk134_0', 'chunkContent': 'rings:\\n  - public\\nms-author: pabouwer\\n\\nScanning\\n\\nScanning identifies security and compliance issues by performing analysis on static artifacts. These issues can include aspects like software license compliance on third party dependencies, software component vulnerabilities, misconfigurations, and/or credentials baked into source code.\\n\\nScanning is a stand-alone capability but can also contribute to enabling the Software Composition Analysis capability.\\n\\nCharacteristics\\n\\nSecurity\\n\\nScanning can be used to identify the following types of security issues:\\n\\nVulnerabilities - identify known Common Vulnerabilities and Exposures (CVEs) in software components\\n\\nCredential - detect embedded secrets, passwords, and/or credentials in code, or secrets output to logs\\n\\nCode - identify susceptibility to attacks like SQL injection, cross-site scripting, and remote code execution\\n\\nCompliance\\n\\nScanning can be used to identify the following types of compliance issues:\\n\\nLicense - identify potential compliance issues or organization policy violations surrounding third-party and open-source software components\\n\\nExamples\\n\\nVulnerabilities\\n\\nThe \"Improve release artifact and workload integrity in Kubernetes via a secure software supply chain\" solution includes multiple examples of vulnerability scanning. The source code and base image are scanned within the build system. The final image is also scanned on a recurring basis within the artifact registry.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\security\\\\scanning.md'}, {'chunkId': 'chunk134_1', 'chunkContent': '{% if extra.ring == \\'internal\\' %}\\n\\nVulnerabilities, Credentials, Code\\n\\nThe \"Infrastructure as Code (IaC) Scanning\" solution includes implementations that demonstrate improving the security posture of IaC artifacts by identifying security, compliance, and best practice violations.\\n\\n{% endif %}\\n\\nLearn More\\n\\n{% if extra.ring == \\'internal\\' %}\\n\\nSolutions Playbook - Container Scanning\\n\\nSolutions Playbook - Dependency Scanning\\n\\nSolutions Playbook - Secrets Detection\\n\\nSolutions Playbook - Windows Binaries Scanning\\n{% endif %}\\n\\nGitHub - Code Scanning\\n\\nGitHub - Secret Scanning\\n\\nOWASP - Vulnerability Scanning Tools', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\security\\\\scanning.md'}, {'chunkId': 'chunk135_0', 'chunkContent': \"rings:\\n  - public\\nms.author: pabouwer\\n\\nSigning\\n\\nCryptographic signatures (also known as digital signatures) use public-key cryptography to validate the authenticity and integrity of digital artifacts, messages, or software. A valid signature verifies the identity of the author/owner and that the digital data hasn't been altered.\\n\\nThe signing process uses a hash algorithm to create a one-way hash of the digital data. The signer's private key is used to encrypt the hash. The signature is made up of the encrypted hash and additional information like the hashing algorithm. The digital data and the signature are stored for retrieval by the receiving party, or are transmitted to the receiving party. If the digital content is sensitive, it may be encrypted (typically with a symmetric key) as part of storage/transmission.\\n\\nThe receiver can verify that the digital content hasn't been altered by using the signer's public key to decrypt the signature and retrieve the hash. This hash is compared with a locally created hash using the hash algorithm documented in the signature. If the hashes match, then the digital content hasn't been altered in transit and the integrity of the digital content is confirmed. By decrypting the signature with the signer's public key, the receiver has verified that the signature was produced by the signer.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\security\\\\signing.md'}, {'chunkId': 'chunk135_1', 'chunkContent': \"Some examples of what cryptographic signatures can be used to sign are - code releases, container images and security artifacts, git commits, and authentication/authorization exchanges like JWT and mTLS.\\n\\nCharacteristics\\n\\nPublic-Key Cryptography\\n\\nPublic-key cryptography or asymmetric cryptography makes use of a pair of related keys. The private key must be kept secret, while the public key can be openly shared and distributed without compromising security. Asymmetric indicates that one key encrypts and the other key in the key pair decrypts. The reverse is also true.\\n\\nTo create a digital signature, the private key is used to encrypt the hash of the data. The hash can only be decrypted by the public key in the key pair. This ensures that only the trusted party can create the signature, but anyone can verify the signature.\\n\\nSignature\\n\\nA digital signature is used to verify the authenticity and integrity of some data. It's stored or transmitted with the data.\\n\\nCertificate Trust Chain\\n\\nPublic-key infrastructure is used to manage certificates which bind the public key to validity and identity information. A Certificate Authority (CA) creates the certificates and there may be a chain of intermediate certs to enable management at scale. The validity information like validity periods or usage types, and identity information can be used to provide additional validation of the entity that produced the signature.\\n\\nExamples\\n\\nArtifact Signing\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\security\\\\signing.md'}, {'chunkId': 'chunk135_2', 'chunkContent': 'Examples\\n\\nArtifact Signing\\n\\nThe \"Improve release artifact and workload integrity in Kubernetes via a secure software supply chain\" solution provides an implementation that shows how to sign and verify signatures for container images and associated security artifacts.\\n\\nLearn More\\n\\nWikipedia - Digital Signature\\n\\nWikipedia - Secure Hash Algorithms\\n\\nWikipedia - Public-Key Cryptography\\n\\nWikipedia - Public-Key Infrastructure\\n\\nMS Learn - Digital Signatures\\n\\nMS Learn - Cryptographic Signatures', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\security\\\\signing.md'}, {'chunkId': 'chunk136_0', 'chunkContent': \"rings:\\n  - public\\nms.author: pabouwer\\n\\nSoftware Composition Analysis\\n\\nSoftware Composition Analysis (SCA) is a process or set of tools that inspects software components and their dependencies to identify security and compliance concerns.\\n\\nCharacteristics\\n\\nLicenses\\n\\nThe licenses of third party dependencies are identified. This supports organizations enforcing compliance around license exposure on third party dependencies.\\n\\nDependencies\\n\\nDependencies often form complex graphs and the SCA process/tools build a complete picture of this graph. The dependency graph ensures that a complete picture around licence and vulnerability exposure is understood.\\n\\nVulnerabilities\\n\\nVulnerabilities for the software components and their dependencies are identified. The dependency graph and Common Vulnerabilities and Exposure (CVE) databases are used by the SCA process/tools to identify risks and provide mitigation options.\\n\\nSoftware Bill of Materials\\n\\nA Software Bill of Materials (SBOM) can be produced from the dependency graph to provide a detailed list of the dependencies and where they're used. The SBOM may be utilized to assess security vulnerability exposure on an ongoing basis.\\n\\nExamples\\n\\nLicenses, Dependencies, and Vulnerabilities\\n\\nThe GitHub Dependency Graph creates a dependency graph for code stored within GitHub. For each of the dependencies, license and vulnerability information is made available.\\n\\nSoftware Bill of Materials\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\security\\\\software-composition-analysis.md'}, {'chunkId': 'chunk136_1', 'chunkContent': 'Software Bill of Materials\\n\\nThe \"Improve release artifact and workload integrity in Kubernetes via a secure software supply chain\" solution includes an implementation that demonstrates how to create an SBOM when building a software release. The SBOM is used in conjunction with a policy gate to ensure workloads deployed to an AKS cluster meet security and compliance requirements.\\n\\nLearn More\\n\\nWikipedia - Software Composition Analysis', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\security\\\\software-composition-analysis.md'}, {'chunkId': 'chunk137_0', 'chunkContent': 'rings:\\n  - public\\nms.author: mrenard\\n\\nInfrastructure as Code Testing\\n\\nInfrastructure as Code (IaC) testing validates the quality of IaC scripts and the desired state of infrastructure deployments. IaC security tests are also used to identify known vulnerabilities and security misconfigurations in the IaC files. Automated testing of infrastructure code allows teams to efficiently validate IaC scripts before deploying to production.\\n\\nCharacteristics\\n\\nModule Tests\\n\\nModule tests validate individual components in IaC scripts. These tests are used to analyze module configurations and dependencies before execution to ensure that the module code creates the expected resources successfully. Module tests deploy the module resources; validate the deployed resources and configurations; and then tear down any deployed resources.\\n\\nEnd-to-End Tests\\n\\nEnd-to-end (E2E) tests are used to verify the interactions between components of the IaC scripts.\\n\\nE2E tests validate:\\n\\nThe desired state of the environment can be achieved\\n\\nThe full system of deployed resources works collectively as expected before starting to direct traffic to it\\n\\nIaC Template Validation and Linting\\n\\nWhen writing IaC, code can have valid syntax, but lack consistency with standards, conventions or organizational practices. Template validation ensures templates pass established standards. For example, the following uses illustrate when a template validation would not pass:', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\testing\\\\infrastructure-as-code-testing.md'}, {'chunkId': 'chunk137_1', 'chunkContent': 'Parameters that are not available during deployment or blending different code styles\\n\\nDifferent naming conventions, indentation and whitespace\\n\\nHard-coded values\\n\\nLinting and validation tools are used to perform static code analysis on IaC templates to identify these types of errors in the code.\\n\\nExamples\\n\\nAutomated Infrastructure as Code Testing\\n\\nThe \"Reduce errors, improve consistency, and deploy with confidence using codified IaC best practices\" solution provides implementations that enable automated IaC testing in Azure cloud deployments. Implementations include module testing, end-to-end testing, and IaC template validation.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Capabilities\\\\testing\\\\infrastructure-as-code-testing.md'}, {'chunkId': 'chunk138_0', 'chunkContent': \"Development in .NET with secrets\\n\\nCustomer Environment\\n\\nThe app settings (appsettings.json) file can be used to store configurations used by .NET application and can be accessed by injecting IConfiguration interface into services. However, it is also naturally used to store secrets such as connection strings which are stored in plaintext.\\n\\nCustomer Friction\\n\\nThis approach has several drawbacks:\\n\\nWe may inadvertently commit secrets in app settings.\\n\\nWe may inadvertently share the secrets during a presentation/troubleshooting session with another peer who may not need access.\\n\\nRecommendations\\n\\nInstead of storing secrets in app settings, we should use dotnet Secret Manager tool. This approach will mean secrets are configured via the tool and stored in a JSON file in the local machine's user profile folder. Secrets configuration can still be accessed the same way by injecting IConfiguration interface into services. This tool is project specific so we don't have to worry about global conflict of secrets with the same key. There is also direct support in the Visual Studio IDE.\\n\\nThis approach will provide the following benefits:\\n\\nWe will minimize the risk of checking the app settings which contains secret(s).\\n\\nWhen presenting (or video recording), we will minimize the risk of inadvertently sharing secrets with other team members who does not need to know the secret(s).\\n\\nResources\\n\\nSafe storage of app secrets in development in ASP.NET Core\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\development-in-dotnet-with-secrets.md'}, {'chunkId': 'chunk139_0', 'chunkContent': \"Azure Key Vault Access and Assignment\\n\\nCustomer Environment\\n\\nAzure Key Vault is used for storing secrets such as database connection strings used by the Application and Developers in lower environments. Developers do not have role assignments privileges and are granted the Contributor role. In order to facilitate access assignment, Azure Vault Key is configured with Vault access policy so Developers can manage access on a per-user basis. Any new Developer who joins the team will be assigned access permissions by reviewing the access permissions of an existing Developer.\\n\\nCustomer Friction\\n\\nBecause the assignment of access permissions is done on a per-user basis, this can result in mismatch of access permissions.\\n\\nThis may also result in uneven access permissions being assigned to each member (and thus not following the least privilege principal).\\n\\nThis practice will also result in additional troubleshooting time when a member is assumed to have the right permissions but unable to access.\\n\\nThis practice means Developers who have Contributor roles can negate other Developers' assigned access permissions to the Key Vault as they can always change access for those members.\\n\\nRecommendations\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\keyvault-access-with-rbac.md'}, {'chunkId': 'chunk139_1', 'chunkContent': 'Recommendations\\n\\nWe should use AAD group(s) to manage assignment of access to Azure Key Vault. Once the AAD group is assigned the right role(s) to Azure Key Vault, it and can be used for any new members assigned to the AAD group. This means we can have owners do the assignment without the need for role assignments privileges. Now, all members of the AAD group have consistent access.\\n\\nWe should use RBAC (VS Vault access policy). This practice is consistent with how Microsoft is applying permission access in other Azure Services. Microsoft has already created a list of Azure Key Vault specified roles based on careful review on common use cases which follows best practices. Here are some examples of the roles that would be useful for our scenario. See Resources section for more information.\\n\\nKey Vault Reader – \"List permissions\"\\nKey Vault Secrets Officer - \"CRUD operations\"\\nKey Vault Secrets User - \"Read secrets\"\\n\\nIn the case where an Azure Key Vault is shared, RBAC can be used to grant access to individual secrets. For example, if we have multiple applications with managed identities, we can allow access to a specified secrets used by individual application by assigned identity. However, Developers can still have access to all secrets.\\n\\nResources\\n\\nGrant permission to applications to access an Azure key vault using Azure RBAC', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\keyvault-access-with-rbac.md'}, {'chunkId': 'chunk140_0', 'chunkContent': 'Security Operations with Microsoft Sentinel\\n\\nOverview\\n\\nMicrosoft Sentinel is a security information and event management (SIEM) and security orchestration, automation, and response (SOAR) solution, and this fills the security role on the operations part of DevSecOps. The purpose of this tool is to provide another layer of defense for attack detection, threat visibility and threat response.\\n\\nKey Takeaways\\n\\nTo take full advantage of any SIEM and SOAR solution, connect it to all the resources that generate security events.\\n\\nAggregated data on SIEMs can help with compliance reporting for industry regulations.\\n\\nOn-board Microsoft Sentinel\\n\\nObjective\\n\\nEnable Microsoft Sentinel in your Azure environment.\\n\\nActions\\n\\nEnsure that prerequisites are met and data residency restrictions comply with your policies.\\n\\nEnable Microsoft Sentinel.\\n\\nConnect your data sources.\\n\\nInvestigate the Data\\n\\nObjective\\n\\nConfigure Microsoft Sentinel to generate incidents and send alerts so that threat mitigation activities can be executed.\\n\\nActions\\n\\nInvestigate cases.\\n\\nRelate alerts to incidents.\\n\\nCustomize entity activities.\\n\\nAutomate Response\\n\\nObjective\\n\\nSet up automation to manage and respond to security threats.\\n\\nActions\\n\\nCreate automation rules.\\n\\nAuthenticate playbooks to Microsoft Sentinel.\\n\\nUse triggers and actions in playbooks.\\n\\nCustomize playbooks from templates.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Security-Operations.md'}, {'chunkId': 'chunk140_1', 'chunkContent': 'Authenticate playbooks to Microsoft Sentinel.\\n\\nUse triggers and actions in playbooks.\\n\\nCustomize playbooks from templates.\\n\\nResources\\n\\nOverview of Microsoft Sentinel', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Security-Operations.md'}, {'chunkId': 'chunk141_0', 'chunkContent': \"Cloud Security Posture Management using Microsoft Defender for Cloud\\n\\nOverview\\n\\nCloud Security Posture Management (CSPM) is a critical component of a layered defense strategy for cloud assets. CSPM proactively and reactively identify security risks that are missed during the code scanning phases of the DevSecOps lifecycle. Microsoft Defender for Cloud continuously assesses security posture, harden cloud resources based on Azure and industry benchmarks, and defend against threats.\\n\\nKey Takeaways\\n\\nAs with any technology that manages security posture, start with a good baseline that meets your organization's internal policies, compliance requirements (HIPAA, PCI-DSS, FedRAMP, etc.), and best practices frameworks (ISO 27001, NIST, CSA, CIS, etc.).\\n\\nSet Up Microsoft Defender for Cloud\\n\\nObjective\\n\\nEnable Microsoft Defender for Cloud in your Azure environment.\\n\\nActions\\n\\nEnable Defender for Cloud on the Azure subscription.\\n\\nYou have the option of enabling enhanced security features in Defender for Cloud. This will extend its capability by providing additional management and threat protection features.\\n\\nConfigure auto provisioning.\\n\\nSet up email notifications.\\n\\nCreate auto-responses to alerts using an ARM template or Bicep.\\n\\nChoose standards for your compliance dashboard.\\n\\nManage Your Cloud Security Posture\\n\\nObjective\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Security-posture-Management.md'}, {'chunkId': 'chunk141_1', 'chunkContent': \"Choose standards for your compliance dashboard.\\n\\nManage Your Cloud Security Posture\\n\\nObjective\\n\\nUnderstand your organization's current security situation by continually assessing your cloud resources for security issues.\\n\\nActions\\n\\nUse the dashboard to get a unified view of your posture.\\n\\nTrack your Secure Score.\\n\\nUse Workbooks to visualize your data.\\n\\nIntegrate other security solutions.\\n\\nAddress Security Recommendations\\n\\nObjective\\n\\nRemediate security issues based on the recommendations provided by Microsoft Defender for Cloud.\\n\\nActions\\n\\nCreate custom policies.\\n\\nReview security recommendations.\\n\\nRemediate security issues.\\n\\nImplement governance of security recommendations.\\n\\nAutomate responses to recommendations.\\n\\nResources\\n\\nMicrosoft Defender for Cloud\\n\\nSecurity Posture for Microsoft Defender for Cloud\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Security-posture-Management.md'}, {'chunkId': 'chunk142_0', 'chunkContent': \"Vulnerability Scores Guidance\\n\\nOverview\\n\\nSecurity vulnerability scanning tools often employ a scoring system to qualify the severity of an issue found during execution. The resulting security scores aid software development teams to prioritize serious security issues so they can be addressed in the development cycle. Most tools use the Common Vulnerability Scoring System (CVSS), an open industry framework for assessing vulnerabilities, to represent the severity of their findings. CVSS assigns a numerical value to a metric that characterizes a vulnerability and then computes the score using these numbers. As an alternative to letting tools such as Snyk Code, Mend, or SonarQube do the scoring, the process can also be performed manually to account for individual observations and opinions. This can be useful during threat modeling exercises, work backlog grooming sessions, or any activity that requires the prioritization of security issues.\\n\\nMetrics\\n\\nThe metrics used to calculate the vulnerability score are categorized as Base, Temporal, and Environmental.\\n\\nBase metrics are intrinsic characteristics that remain consistent across time and environments.\\n\\nTemporal metrics measure the current state.\\n\\nEnvironmental metrics are dependent on the organization's policies, infrastructure and risk tolerance.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\About-Vulnerability-Scores\\\\index.md'}, {'chunkId': 'chunk142_1', 'chunkContent': 'Category Metric Description Use Case Base Attack Vector The path an attacker needs to take to exploit a vulnerability An attacker needs physical access to the device vs. attacking the target from the internet Base Attack Complexity Conditions that must exist for an attack to be successful An attacker needs to repeatedly exploit a system to possibly cause a race condition and trigger a DoS event vs. using tools to launch a DoS attack with just an IP address as input Base Privileges Required The authorizations required to execute an attack Administrator or root access is required for the exploit to succeed vs. exploit can be executed by a guest user Base User Interaction The requirement for a human user to exploit the system Attacks require manually running scripts in a command shell vs. using an attack framework to automate exploitation Base Scope The breadth of impact of a vulnerability being exploited An exploit impacts the target application only vs. an exploit on a vulnerable open-source library affects all the applications using that library Base Confidentiality Impact Impact on access limitations and unauthorized disclosures An attacker gains possession of an encrypted drive vs. a plaintext list of users and passwords Base Integrity Impact Impact on the trustworthiness of a system An attacker is able to modify certain HTML elements on a web app vs. full modifications on the web files Base Availability Impact Impact on the ability of a system to serve its purpose System performance is degraded but available vs. all users or services are denied access to the system Temporal Exploit Code Maturity The likelihood of an attack based on the availability', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\About-Vulnerability-Scores\\\\index.md'}, {'chunkId': 'chunk142_2', 'chunkContent': 'but available vs. all users or services are denied access to the system Temporal Exploit Code Maturity The likelihood of an attack based on the availability and state of existing exploits SQL injection, cross-site scripting, and other exploits that have been around for a while and proven to work will get a high value for this metric, whereas theoretical exploit code will be lower Temporal Remediation Level The availability of a solution to remediate the vulnerability A vendor patch is available vs. a zero-day vulnerability with no known solution Temporal Report Confidence The availability of information about the vulnerability Detailed reports are available vs. a vulnerability with an unclear nature or cause Environmental Confidentiality Requirements The importance of confidentiality on the asset being protected A hospital patient database would require a high level of confidentiality Environmental Integrity Requirements The importance of integrity on the asset being protected An API that handles bank transactions must maintain a high level of integrity Environmental Availability Requirements The importance of availability on the asset being protected A healthcare monitoring device needs to be highly available', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\About-Vulnerability-Scores\\\\index.md'}, {'chunkId': 'chunk142_3', 'chunkContent': \"Scoring Tool\\n\\nSecurity tools use the metrics above, or something similar, to determine the severity of a specific security issue. In most cases, this is done automatically. To perform this process manually, such as during threat modeling exercises or manual penetration testing activities, use the CVSS Calculator to compute the security score.\\n\\nConsiderations\\n\\nA security score gives software development teams a measure of severity or the degree of impact of a particular issue. This is valuable informations during work prioritization. However, it is also important to consider other factors like business impact, cost of remediation, and the organization's risk tolerance. The Environmental metrics on the CVSS score takes some of these factors into account but, deeper discussions with business stakeholders should be considered to make accurate priorities.\\n\\nResources\\n\\nCVSS Specification\\n\\nCVSS User Guide\\n\\nCVSS Calculator\\n\\nExamples of Vulnerability Scores\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\About-Vulnerability-Scores\\\\index.md'}, {'chunkId': 'chunk143_0', 'chunkContent': 'Cryptographic Key Management Recommended Practices\\n\\nOverview\\n\\nThis key management recommendation page provides general guidance and best practices for the management of cryptographic keying material. A key management plan is designed to provide the necessary protection for keys and metadata. Fully documenting and implementing all key management procedures is essential for a successful performance of a key management system.\\n\\nCreate a Key Management Plan\\n\\nEncryption Key Management is administering the full lifecycle of cryptographic keys. The proper management of cryptographic keys is essential to the effective use of cryptography for security. Keys are analogous to the combination of a safe. If a safe combination is known to an adversary, the strongest safe provides no security against penetration. Similarly, poor key management may easily compromise strong algorithms. Ultimately, the security of information protected by cryptography directly depends on the strength of the keys, the effectiveness of mechanisms and protocols associated with keys, and the protection afforded to the keys. All keys need to be protected against modification, and secret and private keys need to be protected against unauthorized disclosure. Key management provides the foundation for the secure generation, storage, distribution, use and destruction of keys. This section is all about Public Key Infrastructure. As stated in the previous section a secure boot process should be architected out to include the general key questions.\\n\\nCreate a key management plan and have discussions with your hardware, software, and firmware teams to ensure the base plan can work for everyone alongside securing the IoT device as a whole.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Cryptographic-Key-Management\\\\index.md'}, {'chunkId': 'chunk143_1', 'chunkContent': 'Include the following topics in your key management plan:\\nDefine the security services that may be provided and key types that may be employed in using cryptographic mechanisms.\\nProvide background information regarding the cryptographic algorithms that use cryptographic keying material.\\n\\nClassify the different types of keys and other cryptographic information according to their functions, specifies the protection that each type of information requires and identifies methods for providing this protection.\\n\\nIdentify the states in which a cryptographic key may exist during its lifetime.\\n\\nIdentify the multitude of functions involved in key management.\\n\\nDiscuss a variety of key management issues related to the keying material. Topics discussed include key usage, cryptoperiod length, domain-parameter validation, public key validation, accountability, audit, key management system survivability, and guidance for cryptographic algorithm and key size selection.\\n\\nIdentify Critical Program Information\\n\\nCritical Program Information (CPI) analysis is the means by which programs identify, protect, and monitor CPI. This analysis should be conducted early and throughout the life cycle of the program. Additionally, because CPI is critical to U.S. technological superiority, its value extends beyond any one program. As a result, CPI analysis should consider the broader impact of CPI identification and protection on national security', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Cryptographic-Key-Management\\\\index.md'}, {'chunkId': 'chunk143_2', 'chunkContent': 'The sensitivity of the information that will need to be protected by the system for the lifetime of the new algorithm(s) should be evaluated in order to determine the minimum security-requirement for the system. Care should be taken not to underestimate the lifetime of the system or the sensitivity of information that it may need to protect. In general encryption is computationally expensive, so it would be in an architects best interest to identify all of the critical program data that must be encrypted to safeguard this data, along with ensuring a high performance state can still be achieved.\\n\\nIdentify critical information that needs to be safeguarded from bad actors.\\n\\nGenerate data flow diagrams of this critical data to gain a holistic understanding of data travel throughout your system.\\n\\nAssign criticality to all the data in a system.\\n\\nIdentify Protection Mechanisms\\n\\nWhen initiating cryptographic protections for information, the strongest algorithm and key size that is appropriate for providing the protection should be used in order to minimize costly transitions. However, it should be noted that selecting some algorithms or key sizes that are unnecessarily large might have adverse performance effects\\n\\nCertain protective measures may be taken in order to minimize the likelihood or consequences of a key compromise. The following procedures are usually involved:\\n\\nIn general, a single key should be used for only one purpose, e.g., encryption, authentication, key wrapping, random number generation, or digital signatures.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Cryptographic-Key-Management\\\\index.md'}, {'chunkId': 'chunk143_3', 'chunkContent': 'Limit the amount of time a symmetric or private key is in plaintext form.\\n\\nPreventing humans from viewing plaintext symmetric and private keys in transit and at rest.\\n\\nRestricting plaintext symmetric and private keys to physically protected containers. This includes key generators, key-transport devices, key loaders, cryptographic modules, and key-storage devices.\\n\\nUsing integrity checks to ensure that the integrity of a key or its association with other data has not been compromised.\\n\\nProvide a cryptographic integrity check on the key (e.g., using a MAC or a digital signature).\\n\\nDestroy keys as soon as they are no longer needed.\\n\\nStore key information in some sort of Secure Processor or TPM that way if a reverse engineer tries to extract keying information it will make the process significantly more difficult to figure out your keys. If a reverse engineer gets their hands on your IoT device they can hook it up to a lab bench and start probing the system with differential power analysis alongside other methods to extract this information, but if its stored within a trusted device then this will also make their jobs very difficult.\\n\\nCreate Strong Key Materials', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Cryptographic-Key-Management\\\\index.md'}, {'chunkId': 'chunk143_4', 'chunkContent': \"Create Strong Key Materials\\n\\nThe following recommendations are offered regarding the disposition of this other keying material. In computer cryptography keys are integers. In some cases keys are randomly generated using a random number generator (RNG) or pseudorandom number generator (PRNG), the latter being a computer algorithm that produces data which appears random under analysis. Some types the PRNGs algorithms utilize system entropy to generate a seed data, such seeds produce better results, since this makes the initial conditions of the PRNG much more difficult for an attacker to guess. It is in the security engineers best interest to assure strong algorithms, random number generators, and PUFs are used when generating strong secure key materials.\\n\\nCreate Physical Unclonable Function (PUF) based keys. Keying information holds all of the secrets that we do not want to leak to any adversary. It is strongly recommended to keep the key information as complex as possible to prevent any kind of crack able pattern. This is why a PUF based key is very important because it can be created with software logic along with a hardware component to make it almost impossible to recreate.\\n\\nUtilize a non-deterministic random bit generator to create strong random numbers utilized for key materials.\\nEnsure each encryption algorithm's contains an Initialization Vector (IV) associated with the information that it helps to protect, and is needed until the information and its protection are no longer needed.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Cryptographic-Key-Management\\\\index.md'}, {'chunkId': 'chunk143_5', 'chunkContent': \"Shared secrets generated during the execution of key-agreement schemes shall be destroyed as soon as they are no longer needed to derive keying material.\\n\\nRandom number seeds shall be destroyed immediately after use.\\nEnsure key metadata is destroyed after use.\\n\\nDefine Key Rotation & Expiration\\n\\nLimiting the number of messages encrypted with the same key version helps prevent brute-force attacks enabled by cryptanalysis. Key lifetime recommendations depend on the key's algorithm, as well as either the number of messages produced or the total number of bytes encrypted with the same key version. For example, the recommended key lifetime for symmetric keys in Galois/Counter Mode (GCM) is based on the number of messages encrypted, as noted by NIST. In the event that a key is compromised, regular rotation limits the number of actual messages vulnerable to compromise.\\n\\nImplement key rotation/rolling into your architecture to prevent stale and predictable keys with your most critical data. A key rolling algorithm can complicate a reverse engineers life by ensuring the keys are always changing, this along with PUF based keys will lock down any leaked data.\\nRemove any static and developer keys before release.\\n\\nAll keys required to authenticate to external services should expire on a regular cadence so that any leaked credentials are not valid indefinitely.\\n\\nDesign in automated credential rotation from the beginning of a project so that the delivered solution can recover from a security breech quickly.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Cryptographic-Key-Management\\\\index.md'}, {'chunkId': 'chunk143_6', 'chunkContent': 'Design in automated credential rotation from the beginning of a project so that the delivered solution can recover from a security breech quickly.\\n\\nCreate integration checks to see if the key is compromised; if so, disable it and revoke access to it as soon as possible.\\n\\nWe recommend that you rotate keys automatically on a regular schedule.\\n\\nArchitect Key States and Key Transitions\\n\\nDuring the lifetime of cryptographic information, the information is either \"in transit\" (e.g., is in the process of being manually distributed or distributed using automated protocols to the authorized communications participants for use by those entities) or is \"at rest\" (e.g., the information is in storage). A key is used differently, depending upon its state in the key’s lifecycle. Key states are defined from a system point-of-view, as opposed to the point-of-view of a single cryptographic module.\\n\\nEstablish key protocols need to be carefully designed to not give secret information to a potential attacker. For example, a protocol that indicates abnormal conditions, such as a data integrity error, may permit an attacker to confirm or reject an assumption regarding secret data.\\n\\nEnsure each key state: pre-activation state, active state, deactivate state, destroyed state, and compromised state are accounted for.\\n\\nChoose Strong Encryption Algorithms', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Cryptographic-Key-Management\\\\index.md'}, {'chunkId': 'chunk143_7', 'chunkContent': 'Choose Strong Encryption Algorithms\\n\\nThe topic of which encryption algorithm to use all boils down a simple questions. How much time can you sacrifice for security? We all know the boot process can take a long time and certain encryption algorithms can almost double that time. National Institute of Standards and Technology (NIST) creates standards for encryption and hand selects the cream of the crop in regards to the strongest encryption used in the world, by reading their encryption standards it will be easy for you and your team to select a safe encryption algorithm that works best for your solution.\\n\\nIt is recommended to adopt Elliptic Curve Cryptography (ECC) which is a very secure algorithm with complex mathematics to ensure the strongest encryption of today.\\n\\nAsses the data that needs to be encrypted by identifying the critical program information, when you have all of the data in front of you, it can be easily determined to use certain algorithms depending on the criticality.\\n\\nEnsure you are using one of the approved cryptographic algorithms: hash functions, symmetric key algorithms and asymmetric key algorithms.\\n\\nAssess the encryption tools provided to you from the vendor and determine if they are strong enough, otherwise create tools to utilize outside crypto engines.\\n\\nNever roll your own crypto.\\n\\nAdditional Resources\\n\\nhttps://csrc.nist.gov/publications/detail/fips/140/2/final', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Cryptographic-Key-Management\\\\index.md'}, {'chunkId': 'chunk143_8', 'chunkContent': 'Additional Resources\\n\\nhttps://csrc.nist.gov/publications/detail/fips/140/2/final\\n\\nhttps://cheatsheetseries.owasp.org/cheatsheets/Key_Management_Cheat_Sheet.html\\n\\nhttps://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r3.pdf\\n\\nhttps://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r3.pdf\\n\\nhttps://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-38d.pdf\\n\\nhttps://en.wikipedia.org/wiki/Physical_unclonable_function\\n\\nhttps://www.nist.gov/', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Cryptographic-Key-Management\\\\index.md'}, {'chunkId': 'chunk144_0', 'chunkContent': 'Sample: Identifying assets, entry points and trust levels\\n\\nScenario\\n\\nAn application receives X-ray images from an external web service and determines if it contains any alphanumeric characters. The application will use an event grid to receive the images. The external web service will publish to the Azure Event Grid while an Azure Function, acting as an event handler, takes the image and sends it to the Azure Cognitive Services Computer Vision API to extract printed or handwritten texts from the image. If no texts are found, the image is saved to an Azure Blob Storage container.\\n\\nTable\\n\\nAsset Entry Point Trust Level Azure Event Grid HTTP endpoint Access Key or Share Access Token Azure Function HTTP endpoint AAD identity Azure Cognitive Services HTTP endpoint AAD identity Azure Blob Storage HTTP endpoint AAD identity Image files Azure Storage public endpoint AAD identity or Shared Key', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\Assets-Table.md'}, {'chunkId': 'chunk145_0', 'chunkContent': 'Creating a Data Flow Diagram (DFD) for the Threat Model\\n\\nIn order to provide a complete and thorough risk analysis, the data flows throughout the architecture need to be understood. The DFD gives us a better view of where the data comes from, where the data is heading, and every stop the data makes along the way.\\n\\nDFD Shapes\\n\\nExternal Entity\\n\\nExternal entities represent an outside user or system that send or receive data. These are also referred to as terminators or sinks.\\n\\nProcess\\n\\nAny component of the system that perform operations on data, and produces an output based on logic, is a process.\\n\\nData Store\\n\\nData Stores hold information for later use. Data inputs flow through a process and then through a data store while data outputs flow out of a data store and then through a process.\\n\\nData Flow\\n\\nData Flow arrows represent a data flow from one entity to another in the direction indicated by the arrow.\\n\\nInternet Boundary\\n\\nRepresents the border between a trusted network and the internet.\\n\\nTrust Boundary\\n\\nRepresents the border around logical groups of entities that share common attributes, such as a virtual network, subnet or cluster.\\n\\nFormat Guidelines\\n\\nIn order to preserve data, a processes may have at least one data flow in or one data flow out.\\n\\nEach data store should have at least one data flow in or data flow out.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\DFD-Guide.md'}, {'chunkId': 'chunk145_1', 'chunkContent': 'Each data store should have at least one data flow in or data flow out.\\n\\nProcesses in a DFD should link to another process, data store or an external entity.\\n\\nEach data flow arrow should have an arrow indicating the direction of the data.\\n\\nIf an element in your design is out of scope, highlight it or change the color.\\n\\nData flow use cases should be labeled with a number to represent the flow order throughout the use cases.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\DFD-Guide.md'}, {'chunkId': 'chunk146_0', 'chunkContent': \"Threat Modeling\\n\\nOverview\\n\\nApplications today face threats coming from both external and internal sources, with attacks causing damage to an organization's assets and brand reputation. Until recently, security activities were performed at the tail end of the software development process when all functionality and features of an application have been built. This approach has been proven to be expensive due to the fact that security products that mitigate attacks cost more than adding code to defend against them, and often provide inadequate protection. Threat modeling will address this problem by shifting security to the left or moving the security work to the plan phase.\\n\\nThe Engineering fundamentals(requires link) describes the process as a systematic approach that includes diagramming, risk identification, risk mitigation and validation. Diagramming gives a high-level view of how data flows between entities. The identification and mitigation steps provide the necessary information about the potential risks, and ways to avoid them or reduce their impact. Finally, continuously validating the threat model ensures the right security measures are enacted. The Engineering Fundamentals covers Threat Modeling at a high level. This playbook discusses the process in greater detail.\\n\\nKey Takeaways\\n\\nThreat model early in the lifecycle so its output can influence design, development and deployment decisions.\\n\\nAssemble a diverse team to get a variety of viewpoints.\\n\\nKey questions to ask: What are we working on? What can go wrong? What are we going to do? Did we do a good job?\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\index.md'}, {'chunkId': 'chunk146_1', 'chunkContent': \"Actions\\n\\n1. Create a model of the application\\n\\nCreate a Data Flow Diagram (DFD) to represent the whole, or specific parts, of the system. The DFD should display the flow of input and output data between entities. Modeling the system helps identify design-specific and implementation-specific threats. Consider the following during this phase:\\n\\nDefine Use Cases - define how the applications is supposed to be used so that misuse scenarios can be determined.\\n\\nIdentify Dependencies - collect information on application dependencies so they can be included on the model.\\n\\nSteps:\\n\\nUse a threat modeling tool such as the Microsoft TMT or Threat Manager Studio to create the DFD.\\n\\nDiagramming tools, such as Visio, can also be utilized if automated threat analysis is not needed. Here's a guide on creating DFD's without the aid of a threat modeling tool.\\n\\n2. Analyze threats\\n\\nBased on the information presented in the DFD, determine what could go wrong and analyze the impact of the threats.\\n\\nSteps:\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\index.md'}, {'chunkId': 'chunk146_2', 'chunkContent': \"Based on the information presented in the DFD, determine what could go wrong and analyze the impact of the threats.\\n\\nSteps:\\n\\nIdentify assets, entry points and trust levels. Assets are application resources that an adversary might manipulate or steal. Entry Points are locations an adversary can use to interface with the application assets. Trust Level are privileges assigned to authenticated entities. An adversary is driven to gain access to an application's assets, hindered by the finite number of entry points and the trust level required to access them. Use this information to determine what could go wrong. Here's a sample of what an asset table might look like.\\n\\nEnumerate application threats, describing the potential attacks that developers should mitigate against. Identifying threats involves determining how adversaries might exploit a vulnerability on an asset.\\n\\nUse the STRIDE model to categorize the threats and simplify the overall security conversation. Other methods can also be used for categorization like the OWASP Top 10 for web applications.\\n\\nDocument the threats on the threat model, a project management board, or any place where they can be tracked an acted upon. If a project tracking platform, such as Azure DevOps, is being used for project work items, it makes sense to also use this for the threats identified during modeling.\\n\\n3. Identify mitigations\\n\\nIdentify risk mitigation solutions that map to the threats discovered.\\n\\nSteps:\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\index.md'}, {'chunkId': 'chunk146_3', 'chunkContent': \"3. Identify mitigations\\n\\nIdentify risk mitigation solutions that map to the threats discovered.\\n\\nSteps:\\n\\nIdentify mitigation steps for each of the documented threats. The Web Application Security Framework can serve as a guide in this activity.\\n\\nDocument the mitigations and assign them to threats.\\n\\n4. Validate model\\n\\nValidate the model after changes are introduced to the systems design or code.\\n\\nSteps:\\n\\nUpdate the DFD to reflect the change.\\n\\nCertify that documented threats still apply.\\n\\nAnalyze new threats, if any are introduced.\\n\\nAdd mitigations for new threats identified.\\n\\nHere's an example of a Threat Model after going through all the steps.\\n\\nTemplates\\n\\nBelow you can find sample templates for Threat Model Report:\\n\\nSample Markdown template\\n\\nSample Microsoft Word template\\n\\nResources\\n\\nThe Threat Modeling Manifesto\\n\\nMicrosoft Threat Modeling Tool\\n\\nThreat Manager Studio\\n\\nThe STRIDE Model\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\index.md'}, {'chunkId': 'chunk147_0', 'chunkContent': 'Sample App Threat Model\\n\\nApplication Description\\n\\nAn application receives X-ray images from an external web service and determines if it contains any alphanumeric characters. The application will use an event grid to receive the images. The external web service will publish to the Azure Event Grid while an Azure Function, acting as an event handler, takes the image and sends it to the Azure Cognitive Services Computer Vision API to extract printed or handwritten texts from the image. If no texts are found, the image is saved to an Azure Blob Storage container.\\n\\nOverview\\n\\nPlease find the Threat Model for Sample App below. This document shows the threat model and data flow diagram of the application. These artifacts were constructed based on documentation and source code from the project itself and are subject to change as the architecture and codebase evolves. Each of the labeled entities in the figures below are accompanied by meta-information which describes the threats, describes the data in scope, and recommendations for security controls.\\n\\nAssets\\n\\nAsset Entry Point Trust Level Azure Event Grid HTTP endpoint Access Key or Shared Access Token Azure Function HTTP endpoint AAD identity Azure Cognitive Services HTTP endpoint AAD identity Azure Blob Storage HTTP endpoint AAD identity Image files Azure Storage public endpoint AAD identity or Shared Key\\n\\nDiagrams\\n\\nData Flow Diagram\\n\\nData Flow Diagram Attributes', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\TM-Sample.md'}, {'chunkId': 'chunk147_1', 'chunkContent': 'Diagrams\\n\\nData Flow Diagram\\n\\nData Flow Diagram Attributes\\n\\n# Transport Protocol Data Classification Authentication Notes 1 HTTPS/TLS 1.2 Confidential Access Key or Shared Access Token External Web service publishes an image to Event Grid 2 HTTPS/TLS 1.2 Confidential Azure AD Identity Event Grid triggers an Azure Function 3 HTTPS/TLS 1.2 Confidential Azure AD Identity Function sends the image to the Computer Vision API 4 HTTPS/TLS 1.2 Confidential Azure AD Identity or Shared Key Image is saved to a Blob Storage container\\n\\nThreat Properties\\n\\nThreat #: 1\\nPrinciple: Confidentiality and Integrity\\nThreat: As a result of the vulnerability of not encrypting data, plaintext data could be intercepted during transit via a man-in-the-middle (MitM) attack. Sensitive data could be exposed or tampered to allow further exploits.\\n\\nMitigation:\\n\\nAll products and services must encrypt data in transit using approved cryptographic protocols and algorithms.\\n\\nUse TLS to encrypt all HTTP-based network traffic. Use other mechanisms, such as IPSec, to encrypt non-HTTP network traffic that contains customer or confidential data.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\TM-Sample.md'}, {'chunkId': 'chunk147_2', 'chunkContent': 'Use only TLS 1.2 or TLS 1.3. Use ECDHE-based ciphers suites and NIST curves. Use strong keys. Enable HTTP Strict Transport Security (HSTS). Turn off TLS compression and do not use ticket-based session resumption.\\n\\nStatus:\\n\\nMitigated. Services set to only accept TLS 1.2 connections.\\n\\nThreat #: 2\\nPrinciple: Confidentiality\\nThreat: Data is a valuable target for most threat actors and attacking the data store directly, as opposed to stealing it during transit, allows data exfiltration at a much larger scale.\\n\\nMitigation:\\n\\nAll customer or confidential data must be encrypted before being written to non-volatile storage media (encrypted at-rest) per the following requirements.\\n\\nUse approved algorithms. This includes AES-256, AES-192, or AES-128.\\n\\nLeverage SQL TDE whenever available.\\n\\nEncryption must be enabled before writing data to storage.\\n\\nStatus:\\n\\nMitigated. Blobs in a Storage account are always encrypted at rest. This feature cannot be disabled.\\n\\nThreat #: 3\\nPrinciple: Confidentiality\\nThreat: Broken or non-existent authentication mechanisms may allow attackers to gain access to confidential information.\\n\\nMitigation:', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\TM-Sample.md'}, {'chunkId': 'chunk147_3', 'chunkContent': 'Mitigation:\\n\\nAll services within the Azure Trust Boundary must authenticate all incoming requests, including requests coming from the same network. Proper authorizations should also be applied to prevent unnecessary privileges.\\n\\nWhenever available, use Azure Managed Identities to authenticate services. Service Principals may be used if Managed Identities are not supported.\\n\\nExternal users or services may use Username + Passwords, Tokens, or Certificates to authenticate, provided these are stored on Key Vault or any other vaulting solution.\\n\\nFor authorization, use Azure RBAC to segregate duties and grant only the least amount of access to perform an action at a particular scope.\\n\\nStatus:\\n\\nMitigated. The External Web Service use certificate authentication, and the rest of the services use managed identities.\\n\\nThreat #: 4\\nPrinciple: Confidentiality and Integrity\\nThreat: A large attack surface, particularly those that are exposed on the internet, will increase the probability of a compromise.\\n\\nMitigation:\\n\\nMinimize the application attack surface by limiting publicly exposed services.\\n\\nUse strong network controls by using Azure Virtual Networks, Network Security Groups (NSG), or Private Endpoints to protect against unsolicited traffic.\\n\\nUse Azure Private Endpoints to block all internet connections to services that do not need to be publicly exposed.\\n\\nStatus:\\n\\nNot mitigated.\\n\\nAppendix\\n\\nSecurity Principles', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\TM-Sample.md'}, {'chunkId': 'chunk147_4', 'chunkContent': 'Status:\\n\\nNot mitigated.\\n\\nAppendix\\n\\nSecurity Principles\\n\\nConfidentiality\\n\\nConfidentiality refers to the objective of keeping data private or secret. In practice, it’s about controlling access to data to prevent unauthorized disclosure.\\n\\nIntegrity\\n\\nIntegrity is about ensuring that data has not been tampered with and, therefore, can be trusted. It is correct, authentic, and reliable.\\n\\nAvailability\\n\\nAvailability means that networks, systems, and applications are up and running. It ensures that authorized users have timely, reliable access to resources when they are needed.\\n\\nMicrosoft Zero Trust Principles\\n\\nVerify explicitly\\n\\nAlways authenticate and authorize based on all available data points, including user identity, location, device health, service or workload, data classification, and anomalies.\\n\\nUse least privileged access\\n\\nLimit user access with just-in-time and just-enough-access (JIT/JEA), risk-based adaptive policies, and data protection to help secure both data and productivity.\\n\\nAssume breach\\n\\nMinimize blast radius and segment access. Verify end-to-end encryption and use analytics to get visibility, drive threat detection, and improve defenses.\\n\\nCommercial Data Classification Reference', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\TM-Sample.md'}, {'chunkId': 'chunk147_5', 'chunkContent': \"Commercial Data Classification Reference\\n\\nClassification Guidelines for Classification Sensitive Data that is to have the most limited access and requires a high degree of integrity. This is typically data that will do the most damage to the organization should it be disclosed. Personal data (including PII) falls into this category and includes any identifier, such as name, an identification number, location data, online identifier. This also includes data related to one or more factors specific to the physical, psychological, genetic, mental, economic, cultural, or social identity of an individual. Confidential Data that might be less restrictive within the company but might cause damage if disclosed. Private Private data is usually compartmental data that might not do the company damage but must be kept private for other reasons. Human resources data is one example of data that can be classified as private. Proprietary Proprietary data is data that is disclosed outside the company on a limited basis or contains information that could reduce the company's competitive advantage, such as the technical specifications of a new product. Public Public data is the least sensitive data used by the company and would cause the least harm if disclosed. This could be anything from data used for marketing to the number of employees in the company.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\TM-Sample.md'}, {'chunkId': 'chunk148_0', 'chunkContent': 'Threat Model – [Project Name]\\n\\nPlease modify the template as necessary to add additional information and remove information that is out of scope.\\n\\nOverview\\n\\nPlease find the Threat Model for [Project Name] below. This document shows the threat model and data flow diagram of the application. These artifacts were constructed based on documentation and source code from the project itself and are subject to change as the architecture and codebase evolves. Each of the labeled entities in the figures below are accompanied by meta-information which describes the threats, describes the data in scope, and recommendations for security controls.\\n\\nDiagrams\\n\\nArchitecture Diagram\\n\\n<insert image here>\\n\\nData Flow Diagram\\n\\n<insert image here>\\n\\nData Flow Diagram Attributes\\n\\n# Transport Protocol Data Classification Authentication Notes 1 [Name of Protocol] [Data classification guidance can be found in the Appendix ] [Method of authenticating the service ] [Additional Notes]\\n\\nThreat Properties\\n\\nThreat #: 1\\nPrinciple: Confidentiality and Integrity\\nThreat: As a result of the vulnerability of not encrypting data, plaintext data could be intercepted during transit via a man-in-the-middle (MitM) attack. Sensitive data could be exposed or tampered to allow further exploits.\\n\\nMitigation:\\n\\nAll products and services must encrypt data in transit using approved cryptographic protocols and algorithms.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\Templates\\\\Threat-Model-Template.md'}, {'chunkId': 'chunk148_1', 'chunkContent': 'Mitigation:\\n\\nAll products and services must encrypt data in transit using approved cryptographic protocols and algorithms.\\n\\nUse TLS to encrypt all HTTP-based network traffic. Use other mechanisms, such as IPSec, to encrypt non-HTTP network traffic that contains customer or confidential data.\\n\\nUse only TLS 1.2 or TLS 1.3. Use ECDHE-based ciphers suites and NIST curves. Use strong keys. Enable HTTP Strict Transport Security (HSTS). Turn off TLS compression and do not use ticket-based session resumption.\\n\\nThreat #: 2\\nPrinciple: Confidentiality\\nThreat: Data is a valuable target for most threat actors and attacking the data store directly, as opposed to stealing it during transit, allows data exfiltration at a much larger scale.\\n\\nMitigation:\\n\\nAll customer or confidential data must be encrypted before being written to non-volatile storage media (encrypted at-rest) per the following requirements.\\n\\nUse approved algorithms. This includes AES-256, AES-192, or AES-128.\\n\\nLeverage SQL TDE whenever available.\\n\\nEncryption must be enabled before writing data to storage.\\n\\nThreat #: 3\\nPrinciple: Confidentiality\\nThreat: Broken or non-existent authentication mechanisms may allow attackers to gain access to confidential information.\\n\\nMitigation:', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\Templates\\\\Threat-Model-Template.md'}, {'chunkId': 'chunk148_2', 'chunkContent': 'Mitigation:\\n\\nAll services within the Azure Trust Boundary must authenticate all incoming requests, including requests coming from the same network. Proper authorizations should also be applied to prevent unnecessary privileges.\\n\\nWhenever available, use Azure Managed Identities to authenticate services. Service Principals may be used if Managed Identities are not supported.\\n\\nExternal users or services may use Username + Passwords, Tokens, or Certificates to authenticate, provided these are stored on Key Vault or any other vaulting solution.\\n\\nFor authorization, use Azure RBAC to segregate duties and grant only the least amount of access to perform an action at a particular scope.\\n\\nThreat #: 4\\nPrinciple: Confidentiality and Integrity\\nThreat: A large attack surface, particularly those that are exposed on the internet, will increase the probability of a compromise.\\n\\nMitigation:\\n\\nMinimize the application attack surface by limiting publicly exposed services.\\n\\nUse strong network controls by using Azure Virtual Networks, Network Security Groups (NSG), or Private Endpoints to protect against unsolicited traffic.\\n\\nUse Azure Private Endpoints to block all internet connections to services that do not need to be publicly exposed.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\Templates\\\\Threat-Model-Template.md'}, {'chunkId': 'chunk148_3', 'chunkContent': 'Use Azure Private Endpoints to block all internet connections to services that do not need to be publicly exposed.\\n\\nThreat #: 5\\nPrinciple: Integrity\\nThreat: Exploitation of insufficient logging and monitoring is the bedrock of nearly every major incident.\\nAttackers rely on the lack of monitoring and timely response to achieve their goals without being detected.\\n\\nMitigation:\\n\\nLogging of critical application events must be performed to ensure that, should a security incident occur, incident response and root-cause analysis may be done. Steps must also be taken to ensure that logs are available and cannot be overwritten or destroyed through malicious or accidental occurrences.\\n\\nAt a minimum, the following events should be logged:\\n\\nLogin/logout events.\\n\\nPassword change events (if not integrated into SSO).\\n\\nPrivilege delegation events.\\n\\nSecurity validation failures (e.g., input validation or authorization check failures).\\n\\nApplication errors and system events.\\n\\nApplication and system startups and shutdowns, as well as logging initialization.\\n\\nThreat #: 6\\nPrinciple: Confidentiality and Integrity\\nThreat: Secrets leaking into unsecured locations are an easy way for adversaries to gain access to a system. These secrets can be used to either spoof the owners of these secrets or, in the case of encryption keys, use them to decrypt data.\\n\\nMitigation:', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\Templates\\\\Threat-Model-Template.md'}, {'chunkId': 'chunk148_4', 'chunkContent': 'Mitigation:\\n\\nProper storage and management of secrets is critical in protecting systems from compromises, in most cases, with severe impact.\\n\\nNever store secrets in code or configuration files. Instead, use a vault or any secure container (such as encrypted variables) to store secrets.\\n\\nSeparate application secrets by environment.\\n\\nRotate all secrets before turning over the application to the customer.\\n\\nThreat #: 7\\nPrinciple: Confidentiality and Integrity\\nThreat: CI/CD Pipelines should always be cleansed of sensitive information being leaked.\\n\\nMitigation:\\n\\nAll secrets, key materials, and credentials stored in CI/CD Pipelines should always be cleansed of sensitive information that can be witnessed in plaintext.\\n\\nAdd static code analysis tools in CI/CD pipelines to gain security insights about the code being developed, gate insecure code from entering production, and generate an overall more secure and complete solution.\\n\\nAdd dependency scanning to CI/CD pipelines to ensure you are using the most current and secure libraries.\\n\\nAdd credential scanning to CI/CD pipelines to ensure secrets, key information, and passwords are not leaked into the open.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\Templates\\\\Threat-Model-Template.md'}, {'chunkId': 'chunk148_5', 'chunkContent': \"Add credential scanning to CI/CD pipelines to ensure secrets, key information, and passwords are not leaked into the open.\\n\\nThreat #: 8\\nPrinciple: Availability\\nThreat: Distributed denial of service (DDoS) attacks is some of the largest availability and security concerns facing customers that are moving their applications to the cloud. A DDoS attempts to exhaust an application's resources, making the application unavailable to legitimate users.\\n\\nMitigation:\\n\\nDDoS protection is important to ensure the availability of your cloud services. Azure provides DDoS solutions to you and your customers to provide extra safeguards to this popular attack.\\n\\nAppendix\\n\\nSecurity Principles\\n\\nConfidentiality\\n\\nConfidentiality refers to the objective of keeping data private or secret. In practice, it’s about controlling access to data to prevent unauthorized disclosure.\\n\\nIntegrity\\n\\nIntegrity is about ensuring that data has not been tampered with and, therefore, can be trusted. It is correct, authentic, and reliable.\\n\\nAvailability\\n\\nAvailability means that networks, systems, and applications are up and running. It ensures that authorized users have timely, reliable access to resources when they are needed.\\n\\nMicrosoft Zero Trust Principles\\n\\nVerify explicitly\\n\\nAlways authenticate and authorize based on all available data points, including user identity, location, device health, service or workload, data classification, and anomalies.\\n\\nUse least privileged access\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\Templates\\\\Threat-Model-Template.md'}, {'chunkId': 'chunk148_6', 'chunkContent': 'Use least privileged access\\n\\nLimit user access with just-in-time and just-enough-access (JIT/JEA), risk-based adaptive policies, and data protection to help secure both data and productivity.\\n\\nAssume breach\\n\\nMinimize blast radius and segment access. Verify end-to-end encryption and use analytics to get visibility, drive threat detection, and improve defenses.\\n\\nCommercial Data Classification Reference', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\Templates\\\\Threat-Model-Template.md'}, {'chunkId': 'chunk148_7', 'chunkContent': \"Commercial Data Classification Reference\\n\\nClassification Guidelines for Classification Sensitive Data that is to have the most limited access and requires a high degree of integrity. This is typically data that will do the most damage to the organization should it be disclosed. Personal data (including PII) falls into this category and includes any identifier, such as name, an identification number, location data, online identifier. This also includes data related to one or more factors specific to the physical, psychological, genetic, mental, economic, cultural, or social identity of an individual. Confidential Data that might be less restrictive within the company but might cause damage if disclosed. Private Private data is usually compartmental data that might not do the company damage but must be kept private for other reasons. Human resources data is one example of data that can be classified as private. Proprietary Proprietary data is data that is disclosed outside the company on a limited basis or contains information that could reduce the company's competitive advantage, such as the technical specifications of a new product. Public Public data is the least sensitive data used by the company and would cause the least harm if disclosed. This could be anything from data used for marketing to the number of employees in the company.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Contributions\\\\Threat-Modeling\\\\Templates\\\\Threat-Model-Template.md'}, {'chunkId': 'chunk149_0', 'chunkContent': \"Enterprise Solutions\\n\\nEnterprise Solutions are generalized, repeatable designs, bringing together a set of capabilities to solve real-word customer business problems. They include guidance, insights, and best practices you can use to architect and accelerate your solutions.\\n\\nHere are the list of available solutions:\\n\\nCloud Native Application Bundle(CNAB) for Azure Trusted Research Environment(TRE) - porter bundle case study to deploy to a secure environment on Azure.\\n\\nInfrastructure as Code(IaC) Orchestration & Testing for Enterprise Customers - codified best practices and guidance for developing, testing and deploying IaC to host large scale solutions.\\n\\nImprove release artifact and workload integrity in Kubernetes via a secure software supply chain - code samples and guidance on ways to enhance the software pipeline in a secure manner.\\n{% if extra.ring == 'internal' %}\\n\\nSecrets Management - recommended practices and guidance on handling secrets for an organization.\\n{% endif %}\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\index.md'}, {'chunkId': 'chunk150_0', 'chunkContent': 'Cloud Native Application Bundle (CNAB)\\n\\nProblem Statement\\n\\nDistributed cloud native applications deployment may become complex and require provisioning of many cloud services and deployment pipelines with dependencies in multiple and secured environments.\\n\\nKey Reasons\\n\\nReduces the app and technology stack-specific knowledge that is required to deploy applications.\\n\\nReduces the time spent on developing and maintaining provisioning pipelines, packaging and deployment.\\n\\nAllows you to more easily deploy to a highly constrained or air-gapped secured environment.\\n\\nSupports projects where multi-cloud requirements are present.\\n\\nSolution\\n\\nCloud Native Application Bundle (CNAB) is a package format specification that describes a technology for bundling, installing, and managing distributed applications, that are by design cloud agnostic.\\nThe specification was developed by a consortium of companies, including Microsoft, Docker, Pivotal and DataDog.\\n\\nArchitecture\\n\\nCNAB bundles get packaged as a docker image and shared with consumers using OCI Registries.\\nA CNAB bundle is composed of three components:\\n\\nApp images - Images, which contain the applications runtimes (for example world-press app and mysql database).\\n\\nInvocation image - installer for the application, an image that includes all scripts and tools necessary to run the installation.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_1', 'chunkContent': 'Invocation image - installer for the application, an image that includes all scripts and tools necessary to run the installation.\\n\\nBundle descriptor - metadata for the bundle, defines properties like the bundle name, version, well-known actions you can invoke for the bundle. Well-known actions such as install, upgrade and uninstall or other custom actions, credentials, used to deploy and dependencies of other bundles.\\n\\nInstead of creating a long readme with instructions for installing the app, or maintaining complex automation pipelines, you get a bundle format where you can package up your application along with everything necessary to install, upgrade or uninstall. In addition, you can implement a custom action like reduce-cost, which stops and downgrades SKUs of expensive cloud services for dev/test environments.\\n\\nCNAB Specification\\n\\nThe CNAB Specification is composed of multiple specifications:\\n\\nCNAB Core - Covers bundle execution.\\n\\nCNAB Registry - Covers storing bundles in OCI registries.\\n\\nCNAB Security - Covers supply-chain security for bundles.\\n\\nCNAB Claims - Covers data storage, such as what is installed and the status of the installation.\\n\\nCNAB Dependencies - Covers bundles depending upon other bundles, for example WordPress depending upon a MySQL bundle.\\n\\nCredential Sets - Covers managing credential sets on the client-side and passing them to a bundle.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_2', 'chunkContent': 'Credential Sets - Covers managing credential sets on the client-side and passing them to a bundle.\\n\\nParameter Sources - Covers how a bundle can connect its outputs to its own parameters for subsequent actions. For example, a bundle may output a connection string during install and then use that output as a parameter for all other actions in the bundle without the user having to specify it.\\n\\nAny CNAB-compliant tool must implement at least the CNAB Core Specification (other specifications are currently in final draft).\\n\\nTools\\n\\nPorter - Porter is an opinionated implementation of the CNAB specification. It creates installers, known as bundles that understand how to install not only your application but its infrastructure and configuration. A bundle helps you package the logic for installing your application, which can then be handed off to another team, a customer, or a co-worker who doesn’t know all the ins and outs of your application. The bundle provides them with a consistent installation experience that doesn’t require them to know what tools you use to deploy or how is it made.\\n\\nDocker App - Docker created \"Docker App\", which was an implementation of the CNAB spec and was used to distribute a bundle, using familiar tooling like compose. Docker has since deprecated Docker App and suggested using Porter to continue packaging their Docker Compose applications in bundles.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_3', 'chunkContent': 'Duffle - Originally while the CNAB specification was developed, Duffle (duffle.sh) was used as a reference implementation to assist other people when developing their own implementations. Now that the CNAB v1 spec has been stable for years, it is no longer needed and has been archived.\\n\\nPorter is now the only open-source implementation of CNAB and is constantly being improved and maintained.\\n\\nPorter is Microsoft recommended tool for customers to use. It is a part of the CNCF (cloud native computing foundation) and is maintained by multiple companies in addition to individual community members.\\n\\nCase Studies\\n\\nEY teams uses Porter to combine different Azure components infrastructure into a single application bundle. This reduces the complexity of dependency management and allows EY teams to reduce the operational overhead of managing its hundreds of applications.\\n\\nAzure Trusted Research Environment (Azure TRE) - An OSS solution developed and maintained by CSE, which uses Porter to bundle deployment of Azure Resources and third-party solutions on Azure. We will see an example later on from this project.\\n\\nBuilding a bundle\\n\\nA Porter bundle is defined by a Porter manifest file named porter.yaml. The manifest defines metadata about the bundle, such as its name or what parameters it accepts. In addition, it also defines actions that the bundle can execute, like install, upgrade and uninstall along with custom actions.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_4', 'chunkContent': \"We will show an example how to build a Porter bundle, and a real example from one of CSE projects.\\n\\nPorter manifest\\n\\nA Porter manifest is used to build a bundle, and contains:\\n\\nMetadata - name, version, description of the bundle.\\n\\nImages - images of the applications.\\n\\nCredentials - to use with the bundle.\\n\\nParameters - input parameters that are required by your bundle.\\n\\nOutputs - outputs of an action step can be passed to another step.\\n\\nMixins - Mixins are adapters between the Porter and an existing tool or system, each mixin is used for a specialized task, there are several built-in mixins including - Azure (AZ/ARM), AWS, GCP, Kubernetes, Helm, Terraform exec and docker-compose.\\n\\nActions and Steps - install / upgrade / uninstall or custom actions, each one can have multiple steps.\\n\\nHello world example\\n\\nOverview\\n\\nThe following example will demonstrate how to build a simple Porter bundle, which simply executes a bash script function, which echoes strings to the console.\\n\\nIn a real life scenario, you'll use the bundle actions to install/upgrade/uninstall your solution with the mixin described above as your solution deployment requires. We will see later on a real life example from Azure TRE.\\n\\nPrerequisites\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_5', 'chunkContent': 'Prerequisites\\n\\nInstall Porter: https://porter.sh/install/\\n\\nCreating a porter bundle\\n\\nLet\\'s use the  porter create command to generate a basic bundle with hello-world example:\\n\\nbash\\n$ mkdir hello-world\\n$ cd hello-world\\n$ porter create\\ncreating porter configuration in the current directory\\n$ tree\\n.\\n├── index.md\\n├── helpers.sh\\n├── porter.yaml\\n└── template.Dockerfile\\n\\nAs you can see above Porter created for us few files, which you can customize for your app.\\n\\nA bash script helpers.sh holds definition of three functions (install, upgrade and uninstall) which simply echoes strings to the console:\\n\\n```bash\\n\\n!/usr/bin/env bash\\n\\nset -euo pipefail\\n\\ninstall() {\\n  echo Hello World\\n}\\n\\nupgrade() {\\n  echo World 2.0\\n}\\n\\nuninstall() {\\n  echo Goodbye World\\n}\\n\\nCall the requested function and pass the arguments as-is\\n\\n\"$@\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_6', 'chunkContent': '```\\n\\nA Porter configuration file porter.yaml, which defines the three basic actions that use exec mixin to execute the corresponding functions of the helpers.sh bash script and metadata for our bundle:\\n\\n```yaml\\nschemaVersion: 1.0.0\\nname: porter-hello\\nversion: 0.1.0\\ndescription: \"An example Porter configuration\"\\nregistry: \"localhost:5000\"\\n\\nDeclare and optionally configure the mixins used by the bundle\\n\\nmixins:\\n  - exec\\n\\nDefine the steps that should execute when the bundle is installed\\n\\ninstall:\\n  - exec:\\n      description: \"Install Hello World\"\\n      command: ./helpers.sh\\n      arguments:\\n        - install\\n\\nDefine the steps that should execute when the bundle is upgraded\\n\\nupgrade:\\n  - exec:\\n      description: \"World 2.0\"\\n      command: ./helpers.sh\\n      arguments:\\n        - upgrade\\n\\nDefine the steps that should execute when the bundle is uninstalled', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_7', 'chunkContent': 'Define the steps that should execute when the bundle is uninstalled\\n\\nuninstall:\\n  - exec:\\n      description: \"Uninstall Hello World\"\\n      command: ./helpers.sh\\n      arguments:\\n        - uninstall', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_8', 'chunkContent': \"```\\n\\nAnd a customizable docker template file template.Dockerfile for the bundle's invocation image, which is built using the porter build command.\\n\\nAfter building the bundle you can run the actions using porter following the action name, for instance porter install or porter uninstall etc.\\n\\nFor more information and other examples, see Porter Quickstart.\\n\\nReferences\\n\\nAzure Trusted Research Environment\\n\\nThe Azure Trusted Research Environment (Azure TRE) project is an accelerator to assist Microsoft customers and partners who want to build out Trusted Research environments on Azure. This project enables authorized users to deploy and configure secure workspaces and researcher tooling without a dependency on IT teams.\\n\\nAzure TRE includes a resource processor component, which uses Porter to deploy components/applications to a secured environment on Azure:\\n\\nFor more information, see Azure TRE System Architecture page.\\n\\nIn the following Porter bundle example from Azure TRE, you can see a linux VM.\\n\\nNotice how custom actions implemented for stop/start VM and reset password, which uses Terraform, az, and exec mixins.\\nand how credentials and outputs are passed between actions and steps.\\n\\nThe porter.yaml was shortened for readability and to emphasize relevant parts, to view the complete bundle see guacamole-azure-linuxvm:\\n\\n```yaml\\n{% raw %}\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_9', 'chunkContent': '```yaml\\n{% raw %}\\n\\nname: tre-service-guacamole-linuxvm\\nversion: 0.5.2\\ndescription: \"An Azure TRE User Resource Template for Guacamole (Linux)\"\\ndockerfile: Dockerfile.tmpl\\nregistry: azuretre\\n\\n.\\n. content was cut\\n.\\n\\ncredentials:\\n  - name: azure_tenant_id\\n    env: ARM_TENANT_ID\\n  - name: azure_subscription_id\\n    env: ARM_SUBSCRIPTION_ID\\n  - name: azure_client_id\\n    env: ARM_CLIENT_ID\\n  - name: azure_client_secret\\n    env: ARM_CLIENT_SECRET\\n\\nparameters:\\n  - name: os_image\\n    type: string\\n    default: \"Ubuntu 18.04 Data Science VM\"\\n  - name: vm_size\\n    type: string\\n    default: \"2 CPU | 8GB RAM\"\\n.\\n. content was cut\\n.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_10', 'chunkContent': 'outputs:\\n  - name: ip\\n    type: string\\n    applyTo:\\n      - install\\n  - name: hostname\\n    type: string\\n    applyTo:\\n      - install\\n  - name: connection_uri\\n    type: string\\n    applyTo:\\n      - install\\n  - name: azure_resource_id\\n    type: string\\n    applyTo:\\n      - install\\n      - start\\n      - stop\\n      - reset_password\\n\\nmixins:\\n  - exec\\n  - terraform:\\n      clientVersion: 1.2.6\\n  - az', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_11', 'chunkContent': 'install:\\n  - terraform:\\n      description: \"Deploy Guacamole User Resource Service (Linux VM)\"\\n      vars:\\n        image: \"{{ bundle.parameters.os_image }}\"\\n        vm_size: \"{{ bundle.parameters.vm_size }}\"\\n.\\n. content was cut\\n.\\n      outputs:\\n        - name: ip\\n        - name: hostname\\n        - name: connection_uri\\n        - name: azure_resource_id\\n\\nupgrade:\\n  - terraform:\\n      description: \"Update Guacamole User Resource Service (Linux VM)\"\\n.\\n. content was cut\\n.\\n\\nuninstall:\\n.\\n. content was cut (start action is similar to stop action)\\n.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_12', 'chunkContent': 'uninstall:\\n.\\n. content was cut (start action is similar to stop action)\\n.\\n\\nstop:\\n  - terraform:\\n      arguments:\\n        - \"output\"\\n      description: \"Get VM resource_id from Terraform outputs\"\\n.\\n. content was cut\\n.\\n      outputs:\\n        - name: azure_resource_id\\n  - az:\\n      description: \"Login to Azure\"\\n      arguments:\\n        - login\\n      flags:\\n        identity:\\n        username: \"{{ bundle.credentials.azure_client_id }}\"\\n  - az:\\n      description: \"Stop the VM\"\\n      arguments:\\n        - vm\\n        - deallocate\\n      flags:\\n        ids: \"{{ bundle.outputs.azure_resource_id }}\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_13', 'chunkContent': 'reset_password:\\n  - terraform:\\n      description: \"Get VM details from Terraform outputs\"\\n.\\n. content was cut\\n.\\n      outputs:\\n        - name: azure_resource_id\\n        - name: vm_username\\n        - name: vm_password_secret_name\\n        - name: keyvault_name\\n  - az:\\n      description: \"Login to Azure\"\\n      arguments:\\n        - login\\n      flags:\\n        identity:\\n        username: \"{{ bundle.credentials.azure_client_id }}\"\\n  - exec:\\n      description: \"Reset password and persist to keyvault\"\\n      suppress-output: true\\n      command: ./reset_password.sh\\n      arguments:\\n        - \"{{ bundle.outputs.vm_password_secret_name }}\"\\n        - \"{{ bundle.outputs.keyvault_name }}\"', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk150_14', 'chunkContent': '- \"{{ bundle.outputs.keyvault_name }}\"\\n        - \"{{ bundle.outputs.vm_username }}\"\\n        - \"{{ bundle.outputs.azure_resource_id }}\"\\n{% endraw %}\\n```', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\\\index.md'}, {'chunkId': 'chunk151_0', 'chunkContent': 'rings:\\n- public\\nms.author: dturner\\n\\nSecrets Lifecycle Management\\n\\nBusiness Problem\\n\\nModern security and identity practices encourage replacing secrets throughout all technical implementations with the use of secure, managed identities. There are multiple reasons this is considered as a best practice but all come down to the understanding that the best way to protect the perimeter and prevent escalation within a system by malicious actors has moved from the network to the identity ecosystem.\\n\\nThe urgency of moving away from secrets has been highlighted by research and attack post analysis. This has been outlined by many governing and guiding bodies including the following.\\n\\nOWASP has relabeled secrets under \"Cryptographic Failures\" and moved this category up the top-10 list to number 2.\\n\\nOWASP separately discusses secrets in CI/CD use-cases because of the importance and impact across the software supply chain.\\n\\nNIST offers a host of guidance on secrets usage and reports on secrets as a source of entry for attackers.\\n\\nHowever, we are still faced with the reality that it\\'s not always possible to move completely away from using secrets. Even in the cases where it is possible to move away from secrets it isn\\'t something that can often be achieved overnight. This type of change takes planning and implementation time.\\n\\nSolution', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_1', 'chunkContent': \"Solution\\n\\nAlthough the need to move away from secrets is real, this document outlines the planning pathway for moving away from secrets while limiting impact on systems in production and offers guidance for the situations where the maturity level of some dependencies may not facilitate moving to a secret-free implementation across the technology stack adopted by a company.\\n\\nUnderstanding the complete lifecycle of a secret is key to interpreting the risk and properly planning mitigation of impact from a leaked secret. It also highlights the pathway toward ensuring secrets remain as secure as possible. Considering the lifecycle of secrets is simplified with a shared understanding and the main concerns of secrets can be categorized in the following list.\\n\\nCreate Securely: when a new secret is required complexity and usage of the secret must be considered.\\n\\nStore Securely: during and after the creation of a secret it must be located in a secure location with controlled access.\\n\\nOperationalization: secrets may be used in various places and we must consider how to securely access and use secrets for each case distinctly.\\n\\nRotation: we must consider several areas of concern when thinking about rotation of secrets. These include changes overtime to prevent secrets from becoming well-known and so-called break-glass scenarios where we have a high-level of confidence that a secret has been discovered.\\n\\nWeak Secrets: it's important to control the complexity of secrets above a minimum threshold and ensure that can be maintained over-time.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_2', 'chunkContent': \"Weak Secrets: it's important to control the complexity of secrets above a minimum threshold and ensure that can be maintained over-time.\\n\\nRevocation: just as with identities, secrets management must consider pathways for removing access granted to a secret.\\n\\nDeletion: when a secret is no longer needed there must be a pathway to ensure it is deleted and the previous access levels are removed across relying systems.\\n\\nExpiry of Secrets: secrets with a fixed lifetime improve other areas across the lifecycle as well as ensuring that if a secret is forgotten it does not provide overlong access lifetimes.\\n\\nIndependently, the various areas of the secret lifecycle show the unique concerns that together create the complete picture of what must be considered when managing the life of secrets. It also makes clear the complexity that is removed when we trade secrets off for managed identities.\\n\\nTo further visualize this lifecycle we can follow a secret through the various areas and look at mitigation. Gaining an understanding of the complexity along the way.\\n\\n```mermaid\\nflowchart TB\\n    sg[Secret Generation]\\n    ss((Secret Store))\\n    o[Operationalization]\\n    ws[Weak Secret]\\n    r{Rotation}\\n    rv{Revocation}\\n    d{delete}\\n    e{expiry}\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_3', 'chunkContent': \"```\\n\\nAs demonstrated in the chart above, there is much interlinking between the various concerns across secret management. Adequate secret management must account for all of these cross-cutting concerns to ensure that we are not leaving a secret unaccounted for and to make sure that as we manage secrets we are doing so in a way that doesn't inject failure points into a system in production.\\n\\nValue Proposition\\n\\nBy designing a plan for secret management that considers the entirety of the secrets lifespan we can have the best possible chance of tracking, using, securing, and managing secrets across the software ecosystem. We begin by treating all secrets with the same level of risk and viewing mitigation options across each of the discussed boundaries and the logical architecture of our systems.\\n\\nLogical Architecture\\n\\nHaving both viewed the secret lifecycle and demonstrated the interlinking across concerns we will take a look at how can we begin to bring some order by looking at how the secret management lifecycle fits into the software lifecycle.\\n\\n```mermaid\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_4', 'chunkContent': '```mermaid\\n\\nflowchart LR\\n  subgraph Plan\\n    direction TB\\n    SecretsGeneration([\"Secret Generation\"]) --- SecretStore([\"Store Secrets\"])\\n  end\\n  subgraph Develop\\n    direction TB\\n    Detection([\"Secret Detection\"]) --- Access([\"Use Secrets\"])\\n  end\\n  subgraph Build\\n    direction TB\\n    Utilize([\"Use Secrets\"])\\n  end\\n  subgraph Deploy\\n    direction TB\\n    EnvSecrets([\"Use Secrets\"])\\n  end\\n  subgraph Operate\\n    direction TB\\n    Rotation([\"Secret Rotation\"]) --- Observability([\"Observability\"])\\n  end\\n  Plan o==o Develop\\n  Develop o==o Build\\n  Build o==o Deploy\\n  Deploy o==o Operate\\n  Operate o==o Plan', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_5', 'chunkContent': '```\\n\\nPlan\\n\\nWhen planning for secrets management there are many considerations. Each area of the logical architecture of the SDLC has unique areas of concerns, tooling, and pathways to properly use secrets. In addition to the considerations we cover here you should also consider the specific software and data-flow pathways in your environment for additional mitigation efforts. Avoid logging secrets at any point through your environment. If a secret is logged at any point for troubleshooting, bug-fix, etc. the log should be immediate scrubbed and that secret should be rotated!\\n\\nWhere possible, Use Managed Identities or Service Principals in Lieu of Application Secrets\\n\\nManaged identities offer a host of capabilities over secrets, including the following.\\n\\nEliminates the need to manage credentials for supported resources.\\n\\nSupports any Azure service supporting Azure AD authentication.\\n\\nControl of the managed identity is maintained by the team protecting the data or service.\\n\\nBetter audit trails without additional overhead.\\n\\nAbstract away operational requirements.\\n\\nThese benefits, combined with the pitfalls and complexity related to managing and using secrets described in this document offer a sound argument for using managed identities rather than secrets in all possible cases.\\n\\nDefine Data Classifications and Map To Access Control Requirements\\n\\nHaving a well-understood set of data classifications is important to understand how to implement the levels of protection with your RBAC (or other control) system.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_6', 'chunkContent': 'Early in the engagement, brainstorm what data classification levels will be used by your application. For each of those levels, decide what level of protection is appropriate for that data; key strength, time to respond to revocation, and period of validity are all important considerations.\\nHigher sensitivity data may require a stronger (larger) encryption key than lower sensitivity data.\\n\\nMore information on managing data classification can be found in the Data Security and Encryption Best Practices.\\n\\nInventory Secrets\\n\\nManaging secrets begins with the ability to track them. Track the secrets and include relevant information for the ongoing tracking and usage of the secrets. Throughout the various parts of the secrets lifecycle discussed later, this inventory will be a primary source of driving and tracking change.\\n\\nThis should include:\\n\\nName of the secret.\\n\\nDescription of the secret.\\n\\nWhere the secret originates from.\\n\\nWhere the secret is stored (see Secrets-Store for some common options).\\n\\nWhere the secret is used.\\n\\nHow the secret is generated, rotated, and distributed (see Secrets-Store and Secrets-Rotation for additional guidance on these areas.\\n\\nConsiderations', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_7', 'chunkContent': \"Considerations\\n\\nDecide what the format of the secrets inventory will be. This could be a markdown document, spreadsheet, or any other medium that fits well with other project documentation. It can be as complex or simple as required to easily communicate the details required. Note: Because this inventory does not include the value of the secret, but the name it should contain information on all secrets involved with a project's end-to-end lifecycle.\\n\\nDefine the location for storing and accessing the Secrets Inventory. Ideally this should live in (or be linked from) a(n) area central to code and documentation for the project and be accessible to key stakeholders who would be involved with development and handling of secrets. Note: that not all of these stakeholders need access to the value of secrets at each environment, but they will require the knowledge of how to provide each secret to the applications in a safe and reliable way.\\n\\nEnsure that each team member has the access required to update the secrets inventory and is encouraged to continue updating as secrets and usage are changed. Note: it may be wise to keep previously used secrets (that are no longer used) in this inventory for historic purposes. If this path is taken it may be advisable to add an additional bit of information to each entry that contains the status of the secret (i.e. Status may be New, In-Use, Deprecated, etc. as appropriate for the project).\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_8', 'chunkContent': 'Separate Secrets by Application and Environment\\n\\nIsolating secrets helps to minimize the blast radius of a potential compromise.\\n\\nFor a single application, create and use different secrets for different environments.\\n\\nEnsure as much separation exists between the environments as possible.\\n\\nWhen using Key Vault, create a separate vault per application per environment.\\n\\nChoose Key Strengths Based on Sensitivity of Data\\n\\nNot all things require a high degree of security; algorithmic complexity and size are both meaningful tradeoffs to higher key strengths.\\n\\nUnderstand the sensitivity of each type of data and how that maps to any cryptographic requirements (including regulatory, if they exist).\\n\\nKey strength and cryptographic algorithm selection go hand-in-hand; ensure you understand how they influence each other. NIST Key Management has excellent recommendations about how to balance these two things.\\n\\nNEVER Store Application Secrets in Code or Configuration Files\\n\\nRemember that once things are checked into public source control, you can never claw them back; they must be rotated or discarded. Keeping secrets out of even commit histories is critical to keeping software secure.\\n\\nYou can use tools like GitSecrets, Yelp Detect-Secrets, CredScan, TruffleHog, or GitRob to scan your repositories for secrets.\\n\\nKeep application secrets in a secrets vault such as Azure Key Vault.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_9', 'chunkContent': 'Keep application secrets in a secrets vault such as Azure Key Vault.\\n\\nFind an approach that works for allowing secure secret access by developers in their local development environment.\\n\\nOrganize Access to Resources to Minimize Impact of Compromise\\n\\nIdentify the least amount of access required for each application/role to complete its actions. If this role is compromised, an attacker is limited in the scope of their subsequent attacks.\\n\\nAccess to secrets should be segregated by at least application, if not role.\\n\\nOnly grant the identity access to the resources it manages/uses\\nWhere appropriate, use virtual networks or other network segregation approach.\\n\\nGoal is to reduce the ability for an attacker to use a single vulnerability as a foothold to compromise other parts of the system.\\n\\nHave a Disaster Recovery Plan in Place for Cryptographic Key Problems\\n\\nIf a key is lost or needs to be revoked in an emergency, make sure you have a plan on how to recover any data which is protected with that key. A schedule for testing the recovery plan should be created and followed to keep the plan applicable, up-to-date, and understood.\\n\\nSpecial consideration must be given if you are using a non-public certificate authority, which delegates operational responsibilities to you, the administrator. If you are using a public CA, the points below largely do not apply.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_10', 'chunkContent': \"Consider how to recover data if a key is lost (emergency key, second master key held by another person).Rule of thumb: If all encryption keys for data are lost, the data itself is lost.\\n\\nConsider how to revoke keys if a key is compromised (certificate revocation lists, secret/key versioning).\\n\\nConsider how to securely backup/restore keys, if appropriate.\\n\\nUse soft delete and purge protection features in Key Vault to mitigate accidental/malicious deletion.\\n\\nSlicing the secrets lifecycle\\n\\nAs a general rule of thumb security related concerns are boundary crossing. While some areas of DevSecOps fit cleanly into a specific concern within the SDLC that isn't true of secrets. One aspect of using secrets is creep and scrawl. As we look closer at each area of the software lifecycle, keep that sprawl in mind, and understand that many of the concerns are cross-cutting.\\n\\nDevelop\\n\\nDevelopment secrets, as a general rule, should be treated with the same respect and under similar constraints as production secrets.\\n\\nSecrets used for development work should be stored in a secure store separate from any production stores. This means if you are using Key Vault for production then development secrets should also be stored in Key Vault, but in a different vault from the store used for production secrets.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_11', 'chunkContent': \"Include an expiration date when generated. It's tempting to create this expiration date at a time in the future when work on the core features will be complete, but this should be avoided. Instead, it is better to create a shorter life-span for development secrets. In this way you can both ensure that the secrets do not outlive the development and test the rotation strategies outside of production. The latter becomes vital as a system becomes more complex and secrets are required for more parts of the code. Failing to set an expiry on development secrets is a common pathway to breach and lateral movement in software!\\n\\nIt is helpful to have a dedicated inventory that lives in the repository or other location near the development. This ensures that engineers have appropriate content and context when considering secrets. Further, this inventory should be reflected in or derived from a broader inventory for the org or company already discussed. The primary difference between the two should be scope.\\n\\nEnsure that a pathway for accessing secrets in the local development environment is a component of the engineering guidance for teams and that the guidance is followed.\\n\\nEnforce secret scanning as part of the Pull Request process for every requests. This should be a pre-commit hook to ensure that no secrets can be merged into the source control history! Preventing secrets from being pushed to a branch under source control is vital as this prevents a breach of source control from becoming a source of historical data on secrets that may still be active.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_12', 'chunkContent': \"Encourage consideration for how an application underdevelopment should access any required secrets and as possible use appropriate SDKs to enable the application to access secrets when needed. This isn't always possible, but gives the highest level of control over pathways for accessing secrets. Avoid storing secrets as environment variables to the highest extent possible with consideration for language and tooling.\\n\\nBuild\\n\\nThe build system is generally responsible for several operations around building your software and preparing it to be packaged. Some of the primary responsibilities for the build system revolve around testing of the software. This may include topics such as code and image scanning as well as quality tests. Your build system should also include scanning the source code for secrets as a part of final preparation. If you have a well configured code repository this likely seems redundant, but it's a good bit of extra protection to ensure that secrets are not being leaked into the build system.\\n\\nEnsure that your build system is using appropriately configured secrets stores to access any credentials required for the build process. As possible you should be following the best practice of using managed identities here (as well as other areas), but as we have discussed that isn't always possible. When you must use secrets as part of the build steps there are a few rules that must be followed.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_13', 'chunkContent': 'Logging across the build system is an important component of continuous build processes, but is also an area for potential secret leaks. Ensure that secrets used in the build system are used in a manner that does not log them in plain text at any point in the build system.\\n\\nEnsure that any rotation is accounted for in the build system. As you move the needle on continuous builds it is paramount that the refreshed secrets are available to the build system. This is generally not as complex as keeping code required secrets up-to-date it may be more difficult in complex build pipelines.\\n\\nEnsure that access to secret stores is time-boxed. The secret should be available to the build system with the tightest possible limitations that will still enable the build processes to run. If the secret must be access multiple times during the build process this is preferable to access for the duration of all build steps.\\n\\nUse one of the recipes suggested in secrets store for properly configuring access to secrets.\\n\\nWhen using multi-stage build pipelines consider which stage needs access and limit as closely as possible to the operations within that stage of the layered pipelines.\\n\\nAncillary scans for secrets to ensure that no secrets are hard-coded into the source or build environments. The secret detection guidance has useful resources for detecting secrets.\\n\\nDeploy', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_14', 'chunkContent': 'Deploy\\n\\nMuch of the guidance for the deploy layer of the SDLC also applies to deploying the software with additional considerations for new tools. When planning for deployment strategy you may potentially deal with different types of secrets:\\n\\nLogging secrets during deployment should be avoided. Deployment often includes an augmentation or completely separate set of tools. Ensure that the passwords used for these tools are provisioned only the permissions required for deployment.\\n\\nBe mindful of logging and network traversal required to access secrets during deployment.\\n\\nIf possible, use different secrets for deployment rather than reusing build secrets.\\n\\nNever use development secrets to deploy software to staging and production environments.\\n\\nConfigure deployment pipelines using the suggestions in secrets store.\\n\\nAccount for deployment in secret rotation strategies.\\n\\nOperate\\n\\nSoftware running in production deserves prudent consideration. Although we are testing for secrets at other layers of the SDLC we should still be diligent about ensuring that operational minded folks have a high level of confidence in a secret free environment and the ability to appropriately update secrets as required.\\n\\nOperations must be aware of automated secret rotation strategy as well as access to guidance on break-glass rotation scenarios.\\n\\nSecrets across the software supply chain should include operations insights into secret fidelity.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk151_15', 'chunkContent': 'Secrets across the software supply chain should include operations insights into secret fidelity.\\n\\nProduction systems include many logging operations. Secret detection across logging should be applied as a final stop-gap to ensure both that secrets are not log in the normal course of operation in addition to the assurance that no nefarious actor has been able to trigger logging as a result of vulnerability to any tools in the supply chain or applications the operations team manage and monitor.\\n\\nAny software that requires manual updates to secrets for rotation should be known and the update process documented. This should be documented in the secrets inventory for ease of access and should include steps for ensuring that the secret is updated in any secrets store along with triggering rotation within reliant applications.\\n\\nOne final reminder that the use of managed identities is always preferred over secrets and this guidance is meant to be implemented for those cases and situations where the use of managed identity is not possible.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secrets-lifecycle-management.md'}, {'chunkId': 'chunk152_0', 'chunkContent': \"rings:\\n  - public\\nms.author: pabouwer\\n\\nImprove release artifact and workload integrity in Kubernetes via a secure software supply chain\\n\\nBusiness Problem\\n\\nA software supply chain typically refers to all the components and processes required to successfully build, distribute, and deploy a product. This is made up of everything from the source code, to the code repos and artifact registries, to the build servers, and to the deployment and operating systems/tools.\\n\\nAttacks against the supply chain come in a variety of forms, from a direct attack on a company's software build system to the compromise of a third-party dependency. In an infamous attack, hackers infiltrated SolarWinds' build system to inject malicious code into their widely used enterprise management products, enabling severe attacks against SolarWinds' customers. In an attack against Log4j, the ubiquitous open-source Java logging framework, malicious code was added to the Log4Shell tool. This enabled attacks against Log4j users, leading to exfiltrated data, injection of malicious content, and/or takeover of targeted systems.\\n\\nThere is an urgent need to mitigate these risks across the software supply chain by improving security controls. This has been widely acknowledged by authoritative organizations:\\n\\nUS Government issued US Executive Order 14028 to improve the nation's cybersecurity\\n\\nCNCF released a Software Supply Chain Best Practices paper and Framework for Supply Chain Evaluation\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_1', 'chunkContent': \"CNCF released a Software Supply Chain Best Practices paper and Framework for Supply Chain Evaluation\\n\\nGoogle published SLSA (Supply chain Levels for Software Artifacts) which is a set of incrementally adoptable security guidelines\\n\\nMicrosoft donated the Secure Supply Chain Consumption Framework (S2C2F) to the OpenSSF to provide guidance for securing the OSS dependencies consumed in the developer's workflow.\\n\\nSolution\\n\\nThis document describes a general approach to a Secure Software Supply Chain (SSSC) solution. The solution focuses on improving software supply chain security for containerized workloads deployed to, and operated in, Kubernetes environments. This is achieved through defense-in-depth and trust-but-verify approaches.\\n\\nOur team has worked with many of Microsoft's largest customers, and collaborated with Azure product engineering, to understand the business and technical requirements for building a secure software supply chain that delivers against real customer requirements.\\n\\nEnsuring integrity in the software release begins when developers start writing the code. All code commits must be cryptographically signed in trusted development environments. Additionally, known vulnerabilities for included dependencies should be surfaced in the development environment to ensure developers do not commit those vulnerabilities to the codebase. Policy enforcement within the developer environment can be used to ensure that developers sign their commits and dependencies within specific vulnerability categories cannot be committed. This ensures that code authors can be cryptographically verified and that the vulnerability risk is well understood.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_2', 'chunkContent': \"To ensure that there is a complete understanding of all the components that make up a software release, a component inventory is built as part of the build/release process. This includes all release dependencies, including third party libraries and OS packages. A list of all known component vulnerabilities is added to the inventory, providing a comprehensive understanding of the bits and vulnerabilities that make up the software release. The security team uses this information to design and implement a risk mitigation strategy.\\n\\nTo ensure that the software release, component inventory, and vulnerability collection can't be manipulated by untrusted actors, these software supply chain security artifacts are cryptographically signed in a trusted environment. This ensures that the integrity of all of these artifacts can be cryptographically verified later, even in low-trust environments.\\n\\nIn addition, attestations are used to provide cryptographically verifiable traceability. For example, an attestation can confirm that a component was built by an authorized build runner, backed by a VM running a known Azure subscription, and signed at a specific date-time. Signatures provide confidence in the integrity of all released artifacts, and attestations provide traceability.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_3', 'chunkContent': 'The software supply chain security artifacts contain useful information, but need to be combined with policy enforcement to ensure that all components are properly signed and can run with acceptable risk within the software development and deployment lifecycle. Examples include not allowing a software release to be built when a dependency has a major vulnerability and only allowing signed container images to be deployed to Kubernetes clusters.\\n\\nIt is important to provide developers the observability tools they need to understand the relationships between the software supply chain artifacts and the systems and processes that touch them. An emerging best practice is to enable developers to observe these components and relationships via a knowledge graph. For example, if a developer learns about a new vulnerability identified in a particular release of log4j, she might ask questions like the following: \"Which of my recent releases have log4j version 2.12.3 as a dependency?\" Another question might be, \"Which software releases deployed to which clusters have exposure to CVE-2021-44228?\"\\n\\nValue Proposition\\n\\nBy employing the secure software supply chain concepts and components, an organization can ensure that the integrity of their software releases are verifiable from code through to operations. Policies and software supply chain artifacts provide centralized control over risk mitigation, and the knowledge graph provides increased risk assessment insights across the entire software supply chain.\\n\\nLogical Architecture', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_4', 'chunkContent': \"Logical Architecture\\n\\nThere are many capabilities across the phases of the software supply chain lifecycle, which are required to deliver a solution for the business problem described. This is illustrated in the logical architecture diagram below.\\n\\nDevelop\\n\\nWithin the develop phase, developers must be supported to ensure the security and integrity of all code, binaries, and configuration that is expected to be included in the software release.\\n\\nThe development environment encompasses all the tools that the developer uses to write, build and test code. Examples include VS Code, Devcontainers and/or GitHub Codespaces. The development environment uses a code repository to store the code that's written. Additional components, such as libraries, frameworks, container base images, are retrieved from a component registry.\\n\\nRepeatable and deterministic builds are an important aspect of a secure software supply chain. These ensure that the contents that make up a software release are well known, and any attempts to tamper with the artifacts can be detected. The development ecosystem used within the develop phase must ensure a record of all the dependencies is captured. Examples of this component inventory include application package managers (npm, NuGet, go.mod), OS package managers (winget, apt get), and container manifests (dockerfile). Version pinning of components from the component registry is encouraged to ensure that builds remain deterministic.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_5', 'chunkContent': 'To improve integrity and security, developers use the Signing capability to ensure that all code commits are cryptographically signed in their trusted environment. Signing commits contributes to the trust chain for the code. Signing keys are managed via a certificate store.\\n\\nIn addition, developers use the Scanning capability to lookup vulnerabilities for the current version of each dependent component. For example, developers might learn that the current version of a component has an unacceptable vulnerability, and they can immediately mitigate that risk switching to a version without the vulnerability.\\n\\nThe Policy capability is used to enforce policies available via a policy store. Example policies might include:\\n\\nno code can be committed without signing\\n\\nno code with a non-zero number of critical severity vulnerabilities can be committed\\n\\ncomponent versions must be pinned\\n\\nThe Knowledge Graph capability provides deeper insights into the software supply chain to ensure that risks can be identified and mitigated in new code releases.\\n\\nBuild\\n\\nWithin the build phase, the processes that create a software release must be supported to ensure the security and integrity of the software release.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_6', 'chunkContent': 'Within the build phase, the processes that create a software release must be supported to ensure the security and integrity of the software release.\\n\\nThe build agent retrieves code from a code repository and pulls down dependencies from their associated component registries. Examples of component registries include the registries for package managers and container registries for container base images. Security and compliance teams may deny access to public component registries and enforce the use of internal/private registries instead. This is to ensure integrity across the entire software supply chain used to build the software release.\\n\\nThe build agent uses the code and component to build a software release. In containerized workloads, the release artifact is typically a container image.\\n\\nThe knowledge graph is used to store traceability details about the software build process and subsequent release.\\n\\nBuild-time policies are enforced through an initial policy gate to ensure that the software release has been built in a secure and compliant manner. An example is a policy that enforces pinning dependency versions for all components.\\n\\nSecurity artifacts are built to improve the integrity and security of the software release:\\n\\nThe Software Composition Analysis capability is used to build a Software Bill of Materials (SBOM) which is a collection of all the components used to build the software release. This can also include information like version and license.\\n\\nThe Attestations capability is used to create any required attestations relevant to the building of the software release.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_7', 'chunkContent': 'The Attestations capability is used to create any required attestations relevant to the building of the software release.\\n\\nThe Scanning capability is used to produce a collection of vulnerabilities for all the associated components.\\n\\nAll the security artifacts (SBOM, Attestations, Vulnerability Scans) produced are cryptographically signed using the Signing capability to ensure that any tampering of these artifacts can be detected.\\n\\nSigning keys are managed via a certificate store.\\n\\nThe knowledge graph is used to associate each security artifact with the software release.\\n\\nBuild-time policies are enforced through a second policy gate, using the additional information from the security artifacts to ensure that the software release is still secure and compliant. An example of policies here could include no release without signed security artifacts, no release if any critical vulnerabilities detected in components, no release if any components use non-compliant license.\\n\\nThe software release is published to an artifact repository, such as a container registry. The software release is associated with additional release details with the knowledge graph. Examples of additional release details include service (GitHub, Azure DevOps), repository (repo url), and release commit (git commit). Some implementations may bundle the security artifacts with the software release in the artifact repository, treating the whole as a \"package.\" Other implementations may rely on the security artifacts being referenced via the knowledge graph, with the artifact repository simply storing the software release.\\n\\nDeploy', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_8', 'chunkContent': 'Deploy\\n\\nWithin the deploy phase, a workload that references the software release is deployed to a Kubernetes cluster. Policy enforcement verifies the integrity and checks the security risks for the software release before allowing the deployment to be scheduled.\\n\\nThe workload may be provisioned via either push-based (pipelines, control planes) or pull-based (GitOps) deployments, since the policy enforcement runs within the Kubernetes control plane boundary.\\n\\nKubernetes enforces a policy gate with policies from a policy store (see the policy capability). Policy enforcement verifies component signatures (see the signing capability) on container images and security artifacts. Policy enforcement may also use information from the artifact repository and/or Knowledge Graph to enforce rules around integrity and security of the workload being deployed.\\n\\nKubernetes pulls the container images required by the workload from the artifact repository, and then schedules them to run on a node. A mechanism that can be local to the Kubernetes cluster or part of the knowledge graph system uses the knowledge graph functionality to associate the deployed software release with operational details. An example of the relationships stored in the knowledge graph includes - \"which cluster, in which region, is running the software release, which is composed of a workload definition and associated container images\".\\n\\nOperate', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_9', 'chunkContent': \"Operate\\n\\nWithin the operate phase, a regular policy audit process ensures that any changes in the integrity and security of deployed software releases are noted and acted upon.\\n\\nSecurity artifacts like SBOMs and signatures are static and don't change over time. These artifacts ensure that the integrity of a software release can be verified. Artifacts like vulnerability scans are more dynamic and the risks they represent may change over time. For example, new vulnerabilities may be detected in components that were considered safe when first built or used in a software release.\\n\\nSince new vulnerabilities are constantly emerging, it's critical to regularly monitor existing software releases to identify components for which new vulnerabilities have been identified. This is done with a scheduled job that scans software releases and audits their policy compliance to identify new security risks. New risks are associated with the software release and the identified components within the knowledge graph. These new risk associations in the knowledge graph can be used by the security team to proactively mitigate security risks, or by the deployment policy gates to automatically ensure that no new deployments of the software release will be accepted by the Kubernetes cluster.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_10', 'chunkContent': 'The Kubernetes cluster may also use the knowledge graph capability to associate runtime violations with the cluster, deployment, and software release. An example is a security component in Kubernetes identifying an attempt to escalate access privileges in a workload. This could indicate a security risk due to mis-configuration of the workload, or a vulnerability in the container image used by the workload.\\n\\nObservability\\n\\nThe knowledge graph is used across all lifecycle phases to provide deep insights into the relationships between the software supply chain artifacts, and the systems and processes that touch them. This ensures traceability across the system and allows for a quick and efficient mechanism for identifying risks so that they can be mitigated.\\n\\nImplementations\\n\\nYou will need to consider the following when using/building implementations for the solution:\\n\\nTooling Ecosystem - There are two major tooling ecosystem approaches - notation and sigstore/cosign. Each of these approaches makes decisions that should be understood. Those decisions impact how security artifacts are stored and how the security artifact information can be accessed. Also how security artifacts are signed and verified.\\n\\nSigning Infrastructure - A good understanding of certificates, trust chains, and management of certificates is required since one of the core aspects of a secure software supply chain is signing.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_11', 'chunkContent': 'Component Registry and Inventory - Package managers and sbom tools are typically aligned well with a set of coding languages and/or ecosystems. A basic understanding of how each of these tools supports the software environment being used is important.\\n\\nVulnerability Infrastructure - Vulnerability databases are heavily skewed towards support for Linux only in the OSS world. The vulnerability database support for Windows is typically available in commercial offerings. Ensure a basic understanding of the software release os requirements and how these may impact how and where you can do vulnerability scanning.\\n\\nAttestations - Attestations are non-trivial to work with in the ecosystem at this time. Sigstore has made some progress in simplifying their use for simple cases. Consider what additional information within the secure software supply chain needs to be attested and consider using the knowledge graph to persist the attestation and its relationship to other artifacts.\\n\\nArtifact Repository - Typically in containerized workloads, the container registry is the artifact repository. Depending on notation or cosign alignment, there are other considerations as to where the security artifacts and signatures will be present. Depending on implementation, the software release, and security artifacts, could be located in different systems. This is something to be aware of when building out the secure supply chain infrastructure.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_12', 'chunkContent': \"Policy Infrastructure - A basic understanding of the various policy frameworks, ecosystems and infrastructure is required. Each has different approaches to policy language and integrations into various stages of the software supply chain. Consider also if there is a requirement around centralized policy management and the projection of those policies into the various stages of the software supply chain.\\n\\nKnowledge Graph - This capability is in the early stages of being solved in the broader ecosystem. This is not mature yet.\\n\\nNotation-based secure software supply chain in Azure Kubernetes Service (AKS)\\n\\nThis implementation delivers a notation-based secure software supply chain in Azure built on Azure Kubernetes Service (AKS) and Azure Container Registry (ACR).\\n\\nEither Azure Pipelines or GitHub Actions can be used in the build phase to create the software release and security artifacts. Microsoft's sbom-tool is used to generate an SBOM from the source code, dependencies, and the container image packages. A vulnerability report is generated using Aquasec's Trivy tool (only supports Linux workloads). Notation is used to sign the container image and security artifacts with a signing cert stored in Azure Key Vault. The ORAS tool is used to bundle the signatures, security artifacts, and container image (software release), and then to publish the release bundle to Azure Container Registry.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk152_13', 'chunkContent': \"Policy enforcement is enabled in the deploy phase by using Open Policy Agent's Gatekeeper and the Ratify verification tool. Ratify acts as an external data provider for Gatekeeper and facilitates exposing the appropriate information from the release bundle to the policy engine. Ratify verifies component signatures using a root CA certificate stored in its configuration.\\n\\nThe following policies are enforced:\\n\\nAll images must be signed to ensure workload integrity\\n\\nAll images must have an attached and signed SBOM and vulnerability scan result\\n\\nAll images may only be retrieved from allowed list of container registries\\n\\n:fontawesome-brands-github: View GitHub repo{ .md-button .md-button--primary }\\n\\nLearn more\\n\\nNotation - Standards-based spec and tooling for securing software supply chains\\n\\nSigstore and Cosign - sign. verify. protect. Making sure your software is what it claims to be.\\n\\nLinux Foundation - What is a Software Bill of Materials (SBOM) ?\\n\\nCloud Native Computing Foundation (CNCF) - Attestations\\n\\nMS Learn - Build, sign, and verify container images using Notary and Azure Key Vault\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\governance\\\\secure-software-supply-chain-for-containerized-workloads.md'}, {'chunkId': 'chunk153_0', 'chunkContent': \"rings:\\n  - public\\nms.author: mrenard\\n\\nReduce errors, improve consistency, and deploy with confidence using codified IaC best practices\\n\\nBusiness Problem\\n\\nEnterprises with complex information technology infrastructures struggle to reproduce infrastructure consistently for applications and services. Among the common challenges are manual processes for deploying and configuring infrastructure and overly simplified automated processes that lack required validation steps. Available guidance is fragmented, and a lack of applied best practices can result in infrastructure downtime, security vulnerabilities, and inefficient resource utilization.\\n\\nKey DevOps practices and engineering fundamentals such as infrastructure as code (IaC), continuous integration/continuous deployment (CI/CD), and testing ensure infrastructure resources deploy consistently across all environments. Using tools to automate the application of these practices minimizes the use of error prone manual tasks in the deployment phase, leading to increased reliability and security.\\n\\nSolution\\n\\nTo overcome common infrastructure provisioning and deployment challenges, enterprises use components from a repository, Symphony, to manage aspects of their IaC deployment lifecycle within their cloud environments. The Symphony repository encompasses codified IaC best practices and workflows that developers can apply to IaC projects to achieve process automation and end to end validation before deploying to production. Symphony's purpose is to help organizations improve availability and reduce remediation time across on-premises and multi-cloud environments.\\n\\nSymphony includes templates and workflows to help with:\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\infrastructure\\\\provisioning-infrastructure-using-pipelines.md'}, {'chunkId': 'chunk153_1', 'chunkContent': 'Symphony includes templates and workflows to help with:\\n\\nAutomating deployment of resources using IaC\\n\\nIncorporating development lifecycle practices for IaC\\n\\nResource validation\\n\\nDependency management\\n\\nIaC test automation and execution\\n\\nSecurity scanning\\n\\nDepending on the use case, Symphony can be used to manage the full deployment lifecycle or improve the reliability of existing IaC deployments.\\n\\nChallenges Addressed by Symphony\\n\\nManual provisioning and deployment tasks - Organizations that provision and deploy cloud resources manually run the risk of accidentally deleting cloud resources during mission-critical production deployments. Adopting IaC with CI/CD best practices ensures correctness and consistency across production deployments. Using Symphony to automate best practice application reduces the need for error prone manual steps and shortens time to re-deploy a cloud infrastructure.\\n\\nSerial cloud deployment - Serial cloud deployments require an entire infrastructure deployment for even the most minor change. When setting up IaC environment structure, using Symphony to modularize deployments enables parallelized deployments. A modular IaC deployment will reduce deployment time as teams would be able to independently deploy modules instead of the entire infrastructure at once.', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\infrastructure\\\\provisioning-infrastructure-using-pipelines.md'}, {'chunkId': 'chunk153_2', 'chunkContent': 'Lack of infrastructure testing - Testing is a fundamental software practice. Automated testing of infrastructure code enables IaC scripts validation before deploying to production. Lack of testing and best practices results in inefficient deployments, redundancies, and a complex deployment environment. Implementing an automated IaC testing strategy with Symphony simplifies the testing process and accelerates deployment.\\n\\nValue Proposition\\n\\nManaging IaC deployment lifecycle with Symphony -\\n\\nImproved consistency and repeatability of infrastructure provisioning\\n\\nIncreased confidence in the reliability of each IaC module, and the solution as a whole\\n\\nEnforced security best practices\\n\\nImproved readability and maintainability of IaC\\n\\nImproved collaboration and onboarding of new team members\\n\\nApplying IaC and infrastructure deployment test automation with Symphony -\\n\\nReduced risk of errors and inconsistencies in IaC scripts\\n\\nEarly internal consistency errors detection\\n\\nEnforced coding standards and organizational practices\\n\\nReduced time spent on code reviews and detection of issues\\n\\nLogical Architecture', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\infrastructure\\\\provisioning-infrastructure-using-pipelines.md'}, {'chunkId': 'chunk153_3', 'chunkContent': 'Enforced coding standards and organizational practices\\n\\nReduced time spent on code reviews and detection of issues\\n\\nLogical Architecture\\n\\nThe following logical architecture diagram illustrates how the Symphony Pull Request (PR) workflow is used to provision and orchestrate infrastructure deployments. This workflow ensures best practices are applied in IaC repos in the development phase. The PR workflow provides a set of validations in the code review process. Validations will vet the PR branch code changes. Changes to the IaC are validated at the code quality level. Changes are also tested on a real deployed environment to determine impact of resource changes.\\n\\nValidate\\n\\nThis stage ensures code readiness of the pull request changes. It executes pre-validated events, runs validations and linting tools, scans code for possible cred leaks, and executes any unit tests. Stage steps are executed in the following sequential order.\\n\\nPreview & Deploy\\n\\nThis stage plans the execution of the IaC code and estimates the scope of the changes. The steps include:\\n\\ninitializes the IaC tool selected,\\n\\nruns plan/what-if commands to detect the changing scope,\\n\\nexecutes pre_deploy events,\\n\\nruns deploy commands to update the resources,\\n\\nexecutes post_deploy events, and\\n\\nensures successful resource updates.\\n\\nTest', 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\infrastructure\\\\provisioning-infrastructure-using-pipelines.md'}, {'chunkId': 'chunk153_4', 'chunkContent': \"executes post_deploy events, and\\n\\nensures successful resource updates.\\n\\nTest\\n\\nThis stage executes the integration or end-to-end tests against the recently deployed/updated resources. Successful tests ensure the configurations/changes are reflected and resources are working as expected. It then publishes the results of the tests and drops them as artifacts for future reference.\\n\\nDestroy\\n\\nThis stage destroys the deployed IaC resources of an environment and report any failures for easier cost and resource management. Details of stage execution may vary based on features available on the orchestrator's IaC tool.\\n\\nIn this workflow, a set of input variables can be updated at execution time. While the inputs could vary based on the selected IaC tool, one common input variable across all is the environment name, which defines what environment configurations to be used.\\n\\nReport\\n\\nThis stage generates the needed scripts to repro the deployments if available, publish the created reports, and backup state files if necessary.\\n\\nImplementations\\n\\nYou will need to consider the following when using/building implementations for the solution:\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\infrastructure\\\\provisioning-infrastructure-using-pipelines.md'}, {'chunkId': 'chunk153_5', 'chunkContent': \"Implementations\\n\\nYou will need to consider the following when using/building implementations for the solution:\\n\\nChoice of IaC tool: Terraform is multi-cloud and platform-agnostic. Bicep, however, is Azure-specific. If you are working exclusively with Azure, Bicep might provide a more streamlined and simplified experience. But if you're working with multiple clouds or planning to in the future, Terraform might be a better choice due to its broader compatibility.\\n\\nExpertise: Terraform has its own DSL, HCL, which might require some learning if you or your team are not familiar with it. Bicep, on the other hand, is designed to be simpler and more readable than ARM templates.\\n\\nIntegration with CI/CD tools: Both GitHub Actions and Azure DevOps have strong integration with Terraform and Bicep. Consider the rest of your tech stack, as well as your team's familiarity and comfort with these tools.\\n\\nSecurity: IaC can inadvertently lead to security issues if not managed carefully. Sensitive data might accidentally get exposed, or insecure configurations might get deployed.\\n\\nVersion control: IaC goes hand-in-hand with version control. Make sure all infrastructure changes are tracked in a version control system like Git. This allows you to see who made changes, what the changes were, and when they were made.\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\infrastructure\\\\provisioning-infrastructure-using-pipelines.md'}, {'chunkId': 'chunk153_6', 'chunkContent': \"State management: With Terraform, consider where and how you will store your state files. Remote state storage with locking is usually a good idea for team environments. Bicep doesn't have state files, as the state is managed by Azure Resource Manager.\\n\\nTerraform Deployment with Symphony\\n\\nThis Symphony implementation features GitHub Actions as an orchestrator and Terraform as the IaC tool. Symphony configures the IaC repository with pipelines and settings needed to achieve CI/CD for IaC.\\n\\nTo provision your new repository with Symphony using GitHub Actions and Terraform, the following command is executed:\\n\\nbash\\nsymphony pipeline config github terraform\\n\\nThis reference implementation contains the output of what to expect in your environment:\\n\\n:fontawesome-brands-github: View GitHub repo{ .md-button .md-button--primary }\\n\\nBicep Deployment with Symphony\\n\\nThis Symphony implementation features GitHub Actions as an orchestrator and Bicep as the IaC tool. Symphony configures the IaC repository with pipelines and settings needed to achieve CI/CD for IaC.\\n\\nTo provision your new repository with Symphony using GitHub Actions and Bicep, the following command is executed:\\n\\nbash\\nsymphony pipeline config github bicep\\n\\nThis reference implementation contains the output of what to expect in your environment:\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\infrastructure\\\\provisioning-infrastructure-using-pipelines.md'}, {'chunkId': 'chunk153_7', 'chunkContent': \"bash\\nsymphony pipeline config github bicep\\n\\nThis reference implementation contains the output of what to expect in your environment:\\n\\n:fontawesome-brands-github: View GitHub repo{ .md-button .md-button--primary }\\n\\nUsing Symphony with other well-known Azure deployment services\\n\\nWhile Symphony can be used in isolation, a service like Azure Deployment Environments (ADE) can be used to provide isolated, secure, and repeatable environments to test and validate Azure infrastructure before it's deployed to production. Symphony and ADE can be used together to create a robust deployment pipeline.\\n\\nIn a typical use case, you use Symphony to define and orchestrate your infrastructure as code. These definitions are then deployed to an ADE for testing and validation. Once validated, the same definitions are used to deploy the infrastructure to your production environment. This way, ADE and Symphony complement each other, combining to deliver a powerful, automated, and reliable infrastructure deployment pipeline.\\n\\nLearn more\\n\\nWhat is Infrastructure as code (IaC)?\\n\\nCreate your own Symphony Repository and workflows\\n\\nGetting Started with Symphony and self hosted build agents on a Virtual Machine\", 'source': '..\\\\data\\\\docs\\\\code-with-devsecops\\\\Enterprise-Solutions\\\\infrastructure\\\\provisioning-infrastructure-using-pipelines.md'}, {'chunkId': 'chunk154_0', 'chunkContent': 'Engineering Fundamentals Checklist\\n\\nThis checklist helps to ensure that our projects meet our Engineering Fundamentals.\\n\\nSource Control\\n\\n[ ] The default target branch is locked.\\n\\n[ ] Merges are done through PRs.\\n\\n[ ] PRs reference related work items.\\n\\n[ ] Commit history is consistent and commit messages are informative (what, why).\\n\\n[ ] Consistent branch naming conventions.\\n\\n[ ] Clear documentation of repository structure.\\n\\n[ ] Secrets are not part of the commit history or made public. (see Credential scanning)\\n\\n[ ] Public repositories follow the OSS guidelines, see Required files in default branch for public repositories.\\n\\nMore details on source control\\n\\nWork Item Tracking\\n\\n[ ] All items are tracked in AzDevOps (or similar).\\n\\n[ ] The board is organized (swim lanes, feature tags, technology tags).\\n\\nMore details on backlog management\\n\\nTesting\\n\\n[ ] Unit tests cover the majority of all components (>90% if possible).\\n\\n[ ] Integration tests run to test the solution e2e.\\n\\nMore details on automated testing\\n\\nCI/CD\\n\\n[ ] Project runs CI with automated build and test on each PR.\\n\\n[ ] Project uses CD to manage deployments to a replica environment before PRs are merged.\\n\\n[ ] Main branch is always shippable.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\ENG-FUNDAMENTALS-CHECKLIST.md'}, {'chunkId': 'chunk154_1', 'chunkContent': '[ ] Main branch is always shippable.\\n\\nMore details on continuous integration and continuous delivery\\n\\nSecurity\\n\\n[ ] Access is only granted on an as-needed basis\\n\\n[ ] Secrets are stored in secured locations and not checked in to code\\n\\n[ ] Data is encrypted in transit (and if necessary at rest) and passwords are hashed\\n\\n[ ] Is the system split into logical segments with separation of concerns? This helps limiting security vulnerabilities.\\n\\nMore details on security\\n\\nObservability\\n\\n[ ] Significant business and functional events are tracked and related metrics collected.\\n\\n[ ] Application faults and errors are logged.\\n\\n[ ] Health of the system is monitored.\\n\\n[ ] The client and server side observability data can be differentiated.\\n\\n[ ] Logging configuration can be modified without code changes (eg: verbose mode).\\n\\n[ ] Incoming tracing context is propagated to allow for production issue debugging purposes.\\n\\n[ ] GDPR compliance is ensured regarding PII (Personally Identifiable Information).\\n\\nMore details on observability\\n\\nAgile/Scrum\\n\\n[ ] Process Lead (fixed/rotating) runs the daily standup\\n\\n[ ] The agile process is clearly defined within team.\\n\\n[ ] The Dev Lead (+ PO/Others) are responsible for backlog management and refinement.\\n\\n[ ] A working agreement is established between team members and customer.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\ENG-FUNDAMENTALS-CHECKLIST.md'}, {'chunkId': 'chunk154_2', 'chunkContent': \"[ ] A working agreement is established between team members and customer.\\n\\nMore details on agile development\\n\\nDesign Reviews\\n\\n[ ] Process for conducting design reviews is included in the Working Agreement.\\n\\n[ ] Design reviews for each major component of the solution are carried out and documented, including alternatives.\\n\\n[ ] Stories and/or PRs link to the design document.\\n\\n[ ] Each user story includes a task for design review by default, which is assigned or removed during sprint planning.\\n\\n[ ] Project advisors are invited to design reviews or asked to give feedback to the design decisions captured in documentation.\\n\\n[ ] Discover all the reviews that the customer's processes require and plan for them.\\n\\n[ ] Clear non-functional requirements captured (see Non-Functional Requirements Guidance)\\n\\n[ ] Risks and opportunities captured (see Risk/Opportunity Management)\\n\\nMore details on design reviews\\n\\nCode Reviews\\n\\n[ ] There is a clear agreement in the team as to function of code reviews.\\n\\n[ ] The team has a code review checklist or established process.\\n\\n[ ] A minimum number of reviewers (usually 2) for a PR merge is enforced by policy.\\n\\n[ ] Linters/Code Analyzers, unit tests and successful builds for PR merges are set up.\\n\\n[ ] There is a process to enforce a quick review turnaround.\\n\\nMore details on code reviews\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\ENG-FUNDAMENTALS-CHECKLIST.md'}, {'chunkId': 'chunk154_3', 'chunkContent': '[ ] There is a process to enforce a quick review turnaround.\\n\\nMore details on code reviews\\n\\nRetrospectives\\n\\n[ ] Retrospectives are conducted each week/at the end of each sprint.\\n\\n[ ] The team identifies 1-3 proposed experiments to try each week/sprint to improve the process.\\n\\n[ ] Experiments have owners and are added to project backlog.\\n\\n[ ] The team conducts longer retrospective for Milestones and project completion.\\n\\nMore details on retrospectives\\n\\nEngineering Feedback\\n\\n[ ] The team submits feedback on business and technical blockers that prevent project success\\n\\n[ ] Suggestions for improvements are incorporated in the solution\\n\\n[ ] Feedback is detailed and repeatable\\n\\nMore details on engineering feedback\\n\\nDeveloper Experience (DevEx)\\n\\nDevelopers on the team can:\\n\\n[ ] Build/Compile source to verify it is free of syntax errors and compiles.\\n\\n[ ] Execute all automated tests (unit, e2e, etc).\\n\\n[ ] Start/Launch end-to-end to simulate execution in a deployed environment.\\n\\n[ ] Attach a debugger to started solution or running automated tests, set breakpoints, step through code, and inspect variables.\\n\\n[ ] Automatically install dependencies by pressing F5 (or equivalent) in their IDE.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\ENG-FUNDAMENTALS-CHECKLIST.md'}, {'chunkId': 'chunk154_4', 'chunkContent': '[ ] Automatically install dependencies by pressing F5 (or equivalent) in their IDE.\\n\\n[ ] Use local dev configuration values (i.e. .env, appsettings.development.json).\\n\\nMore details on developer experience', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\ENG-FUNDAMENTALS-CHECKLIST.md'}, {'chunkId': 'chunk155_0', 'chunkContent': 'ISE Code-With Engineering Playbook\\n\\nAn engineer working for a ISE project...\\n\\nHas responsibilities to their team – mentor, coach, and lead.\\n\\nKnows their playbook. Follows their playbook. Fixes their playbook if it is broken. If they find a better playbook, they copy it. If somebody could use their playbook, they share it.\\n\\nLeads by example. Models the behaviors we desire both interpersonally and technically.\\n\\nStrives to understand how their work fits into a broader context and ensures the outcome.\\n\\nThis is our playbook. All contributions are welcome! Please feel free to submit a pull request to get involved.\\n\\nWhy Have A Playbook\\n\\nTo increase overall efficiency for team members and the whole team in general.\\n\\nTo reduce the number of mistakes and avoid common pitfalls.\\n\\nTo strive to be better engineers and learn from other people\\'s shared experience.\\n\\n\"The\" Checklist\\n\\nIf you do nothing else follow the Engineering Fundamentals Checklist!\\n\\nStructure of a Sprint\\n\\nThe structure of a sprint is a breakdown of the sections of the playbook according to the structure of an Agile sprint.\\n\\nGeneral Guidance\\n\\nKeep the code quality bar high.\\n\\nValue quality and precision over ‘getting things done’.\\n\\nWork diligently on the one important thing.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\index.md'}, {'chunkId': 'chunk155_1', 'chunkContent': 'Value quality and precision over ‘getting things done’.\\n\\nWork diligently on the one important thing.\\n\\nAs a distributed team take time to share context via wiki, teams and backlog items.\\n\\nMake the simple thing work now. Build fewer features today, but ensure they work amazingly. Then add more features tomorrow.\\n\\nAvoid adding scope to a backlog item, instead add a new backlog item.\\n\\nOur goal is to ship incremental customer value.\\n\\nKeep backlog item details up to date to communicate the state of things with the rest of your team.\\n\\nReport product issues found and provide clear and repeatable engineering feedback!\\n\\nWe all own our code and each one of us has an obligation to make all parts of the solution great.\\n\\nQuickLinks\\n\\nEngineering Fundamentals Checklist\\n\\nStructure of a Sprint\\n\\nEngineering Fundamentals\\n\\nAccessibility\\n\\nAgile Development\\n\\nAutomated Testing\\n\\nCode Reviews\\n\\nContinuous Delivery (CD)\\n\\nContinuous Integration (CI)\\n\\nDesign\\n\\nDeveloper Experience\\n\\nDocumentation\\n\\nEngineering Feedback\\n\\nObservability\\n\\nSecurity\\n\\nPrivacy\\n\\nSource Control\\n\\nReliability\\n\\nFundamentals for Specific Technology Areas\\n\\nMachine Learning Fundamentals\\n\\nUser-Interface Engineering\\n\\nContributing\\n\\nSee CONTRIBUTING.md for contribution guidelines.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\index.md'}, {'chunkId': 'chunk156_0', 'chunkContent': 'Who We Are\\n\\nOur team, ISE (Industry Solutions Engineering), works side by side with customers to help them tackle their toughest technical problems both in the cloud and on the edge. We meet customers where they are, work in the languages they use, with the open source frameworks they use, on the operating systems they use. We work with enterprises and start-ups across many industries from financial services to manufacturing. Our work covers a broad spectrum of domains including IoT, machine learning, and high scale compute. Our \"superpower\" is that we work closely with both our customers’ engineering teams and Microsoft’s product engineering teams, developing real-world expertise that we can use to help our customers grow their business and help Microsoft improve our products and services.\\n\\nWe are very community focused in our work, with one foot in Microsoft and one foot in the open source communities that we help. We make pull requests on open source projects to add support for Microsoft platforms and/or improve existing implementations. We build frameworks and other tools to make it easier for developers to use Microsoft platforms. We source all the ideas for this work by maintaining very deep connections with these communities and the customers and partners that use them.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\ISE.md'}, {'chunkId': 'chunk156_1', 'chunkContent': 'If you like variety, coding in many languages, using any available tech across our industry, digging in with our customers, hack fests, occasional travel, and telling the story of what you’ve done in blog posts and at conferences, then come talk to us.\\n\\nYou can check out some of our work on our Developer Blog', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\ISE.md'}, {'chunkId': 'chunk157_0', 'chunkContent': 'Structure of a Sprint\\n\\nThe purpose of this document is to:\\n\\nOrganize content in the playbook for quick reference and discoverability\\n\\nProvide content in a logical structure which reflects the engineering process\\n\\nExtensible hierarchy to allow teams to share deep subject-matter expertise\\n\\nThe first week of an ISE Project\\n\\nBefore starting the project\\n\\n[ ] Discuss and start writing the Team Agreements. Update these documents with any process decisions made throughout the project\\n\\nWorking Agreement\\n\\nDefinition of Ready\\n\\nDefinition of Done\\n\\nEstimation\\n\\n[ ] Set up the repository/repositories\\n\\nDecide on repository structure/s\\n\\nAdd README.md, LICENSE, CONTRIBUTING.md, .gitignore, etc\\n\\n[ ] Build a Product Backlog\\n\\nSet up a project in your chosen project management tool (ex. Azure DevOps)\\n\\nINVEST in good User Stories and Acceptance Criteria\\n\\nNon-Functional Requirements Guidance\\n\\nDay 1\\n\\n[ ] Plan the first sprint\\n\\nAgree on a sprint goal, and how to measure the sprint progress\\n\\nDetermine team capacity\\n\\nAssign user stories to the sprint and split user stories into tasks\\n\\nSet up Work in Progress (WIP) limits\\n\\n[ ] Decide on test frameworks and discuss test strategies', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\SPRINT-STRUCTURE.md'}, {'chunkId': 'chunk157_1', 'chunkContent': 'Set up Work in Progress (WIP) limits\\n\\n[ ] Decide on test frameworks and discuss test strategies\\n\\nDiscuss the purpose and goals of tests and how to measure test coverage\\n\\nAgree on how to separate unit tests from integration, load and smoke tests\\n\\nDesign the first test cases\\n\\n[ ] Decide on branch naming\\n\\n[ ] Discuss security needs and verify that secrets are kept out of source control\\n\\nDay 2\\n\\n[ ] Set up Source Control\\n\\nAgree on best practices for commits\\n\\n[ ] Set up basic Continuous Integration with linters and automated tests\\n\\n[ ] Set up meetings for Daily Stand-ups and decide on a Process Lead\\n\\nDiscuss purpose, goals, participants and facilitation guidance\\n\\nDiscuss timing, and how to run an efficient stand-up\\n\\n[ ] If the project has sub-teams, set up a Scrum of Scrums\\n\\nDay 3\\n\\n[ ] Agree on code style and on how to assign Pull Requests\\n\\n[ ] Set up Build Validation for Pull Requests (2 reviewers, linters, automated tests) and agree on Definition of Done\\n\\n[ ] Agree on a Code Merging strategy and update the CONTRIBUTING.md\\n\\n[ ] Agree on logging and observability frameworks and strategies\\n\\nDay 4\\n\\n[ ] Set up Continuous Deployment', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\SPRINT-STRUCTURE.md'}, {'chunkId': 'chunk157_2', 'chunkContent': '[ ] Agree on logging and observability frameworks and strategies\\n\\nDay 4\\n\\n[ ] Set up Continuous Deployment\\n\\nDetermine what environments are appropriate for this solution\\n\\nFor each environment discuss purpose, when deployment should trigger, pre-deployment approvers, sing-off for promotion.\\n\\n[ ] Decide on a versioning strategy\\n\\n[ ] Agree on how to Design a feature and conduct a Design Review\\n\\nDay 5\\n\\n[ ] Conduct a Sprint Demo\\n\\n[ ] Conduct a Retrospective\\n\\nDetermine required participants, how to capture input (tools) and outcome\\n\\nSet a timeline, and discuss facilitation, meeting structure etc.\\n\\n[ ] Refine the Backlog\\n\\nDetermine required participants\\n\\nUpdate the Definition of Ready\\n\\nUpdate estimates, and the Estimation document\\n\\n[ ] Submit Engineering Feedback for issues encountered', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\SPRINT-STRUCTURE.md'}, {'chunkId': 'chunk158_0', 'chunkContent': 'Accessibility\\n\\nAccessibility is a critical component of any successful project and ensures the solutions we build are usable and enjoyed by as many people as possible. While meeting accessibility compliance standards is required, accessibility is much broader than compliance alone. Accessibility is about using techniques like inclusive design to infuse different perspectives and the full range of human diversity into the products we build. By incorporating accessibility into your project from the initial envisioning through MVP and beyond, you are promoting a more inclusive environment for your team and helping close the \"Disability Divide\" that exists for many people living with disabilities.\\n\\nGetting Started\\n\\nIf you are new to accessibility or are looking for an overview of accessibility fundamentals, Microsoft Learn offers a great training course that covers a broad range of topics from creating accessible content in Office to designing accessibility features in your own apps. You can learn more about the course or get started at Microsoft Learn: Accessibility Fundamentals.\\n\\nInclusive Design\\n\\nInclusive design is a methodology that embraces the full range of human diversity as a resource to help build better products and services. Inclusive design compliments accessibility going beyond accessibility compliance standards to ensure products are usable and enjoyed by all people. By leveraging the inclusive design methodology early in a project, you can expect a more inclusive and better solution for everyone. The Microsoft Inclusive Design website offers a variety of resources for incorporating inclusive design in your projects including inclusive design activities that can be used in envisioning and architecture design sessions.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\accessibility\\\\README.md'}, {'chunkId': 'chunk158_1', 'chunkContent': 'The Microsoft Inclusive Design methodology includes the following principles:\\n\\nRecognize exclusion\\n\\nDesigning for inclusivity not only opens up our products and services to more people, it also reflects how people really are. All humans grow and adapt to the world around them and we want our designs to reflect that.\\n\\nSolve for one, extend to many\\n\\nEveryone has abilities, and limits to those abilities. Designing for people with permanent disabilities actually results in designs that benefit people universally. Constraints are a beautiful thing.\\n\\nLearn from diversity\\n\\nHuman beings are the real experts in adapting to diversity. Inclusive design puts people in the center from the very start of the process, and those fresh, diverse perspectives are the key to true insight.\\n\\nTools\\n\\nAccessibility Insights\\n\\nAccessibility Insights is a free, open-source solution for identifying accessibility issues in Windows, Android, and web applications. Accessibility Insights can identify a broad range of accessibility issues including problems with missing image alt tags, heading organization, tab order, color contrast, and many more. In addition, you can use Accessibility Insights to simulate color blindness to ensure your user interface is accessible to those that experience some form of color blindness. You can download Accessibility Insights here: https://accessibilityinsights.io/downloads/\\n\\nAccessibility Linter', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\accessibility\\\\README.md'}, {'chunkId': 'chunk158_2', 'chunkContent': \"Accessibility Linter\\n\\nDeque Systems are web accessibility experts that provide accessibility training and tools to many organizations including Microsoft. One of the many tools offered by Deque is the axe Accessibility Linter for VS Code. This VS Code extension use the axe-core rules engine to identify accessibility issues in HTML, Angular, React, Markdown, and Vue. Using an accessibility linter can help ensure accessibility issues get addressed early in the development lifecycle.\\n\\nPractices\\n\\nAccessibility Testing\\n\\nAccessibility testing is a specialized subset of software testing and includes automated tools and manual testing processes that vary from project to project. In addition to tools like Accessibility Insights discussed earlier, there are many other solutions for accessibility testing. The W3C provides a comprehensive list of evaluation and testing tools on their website at https://www.w3.org/WAI/ER/tools/.\\n\\nIf you are looking to add automated testing to your Azure Pipelines, you may want to consider the Accessibility Testing extension built by Drew Lewis, a former Microsoft employee.\\n\\nIt's important to keep in mind that automated tooling alone is not enough - make sure to augment your automated tests with manual ones. Accessibility Insights (linked above) can guide users through some manual testing steps.\\n\\nCode and Documentation Basics\\n\\nBefore you get to testing, you can make some small changes in how you write code and documentation.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\accessibility\\\\README.md'}, {'chunkId': 'chunk158_3', 'chunkContent': 'Code and Documentation Basics\\n\\nBefore you get to testing, you can make some small changes in how you write code and documentation.\\n\\nDocument! Beyond text documentation, this also means code comments, clear variable and file naming, and pipeline or script outputs that clearly report success or failure and give details.\\n\\nAvoid small case for variable and file names, hashtags, neologisms, etc. Use camelCase, snake_case, or other methods of creating separation between words.\\n\\nIntroduce abbreviations by spelling the full term out, then the abbreviation in parentheses.\\n\\nUse headers effectively to break up content by topic. Don\\'t use more than one h1 per page, and don\\'t skip levels (e.g. use an h3 directly under an h1). Avoid using formatting to make something look like a header when it\\'s not.\\n\\nUse descriptive link text. Avoid attaching a link to phrases like \"Read more\" and ensure that the text directly states what it links to. Link text should be able to stand on its own.\\n\\nWhen including images or diagrams, add alt text. This should never just be \"Image\" or \"Diagram\" (or similar). In your description, highlight the purpose of the image or diagram in the page and what it is intended to convey.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\accessibility\\\\README.md'}, {'chunkId': 'chunk158_4', 'chunkContent': 'Prefer tabs to spaces when possible. This allows users to default to their preferred tab width, so users with a range of vision can all take in code easily.\\n\\nAdditional Resources\\n\\nMicrosoft Accessibility Technology & Tools\\n\\nWeb Content Accessibility Guidelines (WCAG)\\n\\nAccessibility Guidelines and Requirements | Microsoft Style Guide\\n\\nGoogle Developer Style Guide: Write Accessible Documentation', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\accessibility\\\\README.md'}, {'chunkId': 'chunk159_0', 'chunkContent': 'Agile documentation\\n\\nAgile Basics: Learn or refresh your basic agile knowledge.\\n\\nAgile Core Expectations: What are our core expectations from an Agile team.\\n\\nAgile Advanced Topics: Go beyond the basics.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\README.md'}, {'chunkId': 'chunk160_0', 'chunkContent': 'Agile Development advanced topics\\n\\nDocumentation that help you going beyond the basics and core expectations.\\n\\nBacklog Management\\n\\nCollaboration\\n\\nEffective Organization\\n\\nTeam Agreements', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\README.md'}, {'chunkId': 'chunk161_0', 'chunkContent': 'External Feedback\\n\\nVarious stakeholders can provide feedback to the working product during a project, beyond any formal\\nreview and feedback sessions required by the organization. The frequency and method of collecting\\nfeedback through reviews varies depending on the case, but a couple of good practices are:\\n\\nCapture each review in the backlog as a separate user story.\\n\\nStandardize the tasks that implement this user story.\\n\\nPlan for a review user story per Epic / Feature in your backlog proactively.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\backlog-management\\\\external-feedback.md'}, {'chunkId': 'chunk162_0', 'chunkContent': 'Minimalism Slices\\n\\nAlways deliver your work using minimal valuable slices\\n\\nSplit your work item into small chunks that are contributed in incremental commits.\\n\\nContribute your chunks frequently. Follow an iterative approach by regularly providing updates and changes to the team. This allows for instant feedback and early issue discovery and ensures you are developing in the right direction, both technically and functionally.\\n\\nDo NOT work independently on your task without providing any updates to your team.\\n\\nExample\\n\\nImagine you are working on adding UWP (Universal Windows Platform) application building functionality for existing continuous integration service which already has Android/iOS support.\\n\\nBad approach\\n\\nAfter six weeks of work you created PR with all required functionality, including portal UI (build settings), backend REST API (UWP build functionality), telemetry, unit and integration tests, etc.\\n\\nGood approach\\n\\nYou divided your feature into smaller user stories (which in turn were divided into multiple tasks) and started working on them one by one:\\n\\nAs a user I can successfully build UWP apps using current service\\n\\nAs a user I can see telemetry when building the apps\\n\\nAs a user I have the ability to select build configuration (debug, release)\\n\\nAs a user I have the ability to select target platform (arm, x86, x64)\\n\\n...', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\backlog-management\\\\minimal-slices.md'}, {'chunkId': 'chunk162_1', 'chunkContent': 'As a user I have the ability to select target platform (arm, x86, x64)\\n\\n...\\n\\nYou also divided your stories into smaller tasks and sent PRs based on those tasks.\\nE.g. you have the following tasks for the first user story above:\\n\\nEnable UWP platform on backend\\n\\nAdd build button to the UI (build first solution file found)\\n\\nAdd select solution file dropdown to the UI\\n\\nImplement unit tests\\n\\nImplement integration tests to verify build succeeded\\n\\nUpdate documentation\\n\\n...\\n\\nResources\\n\\nMinimalism Rules', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\backlog-management\\\\minimal-slices.md'}, {'chunkId': 'chunk163_0', 'chunkContent': 'Advanced recommendations for Backlog Management\\n\\nExternal Feedback\\n\\nMinimal slices\\n\\nRisk Management', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\backlog-management\\\\README.md'}, {'chunkId': 'chunk164_0', 'chunkContent': 'Risk Management\\n\\nAgile methodologies are conceived to be driven by risk management principles, but no methodology can eliminate all risks.\\n\\nGoal\\n\\nAnticipation is a key aspect of software project management, involving the proactive identification and assessment of potential risks and challenges to enable effective planning and mitigation strategies.\\n\\nThe following guidance aims to provide decision-makers with the information needed to make informed choices, understanding trade-offs, costs, and project timelines throughout the project.\\n\\nGeneral Guidance\\n\\nIdentify risks in every activity such as a planning meetings, design and code reviews, or daily standups. All team members are responsible for identifying relevant risks.\\n\\nAssess risks in terms of their likelihood and potential impact on the project. Use the issues to report and track risks. Issues represent unplanned activities.\\n\\nPrioritize them based on their severity and likelihood, focusing on addressing the most critical ones first.\\n\\nMitigate or reduce the impact and likelihood of the risks.\\n\\nMonitor continuously to ensure the effectiveness of the mitigation strategies.\\n\\nPrepare contingency plans for high-impact risks that may still materialize.\\n\\nCommunicate and report risks to keep all stakeholders informed.\\n\\nOpportunity Management\\n\\nThe same process can be applied to opportunities, but while risk management involves applying mitigation actions to decrease the likelihood of a risk, in opportunity management, you enhance actions to increase the likelihood of a positive outcome.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\backlog-management\\\\risk-management.md'}, {'chunkId': 'chunk165_0', 'chunkContent': 'How to add a Pairing Custom Field in Azure DevOps User Stories\\n\\nThis document outlines the benefits of adding a custom field of type Identity in Azure DevOps user stories, prerequisites, and a step-by-step guide.\\n\\nBenefits of adding a custom field\\n\\nHaving the names of both individuals pairing on a story visible on the Azure DevOps cards can be helpful during sprint ceremonies and lead to greater accountability by the pairing assignee. For example, it is easier to keep track of the individuals assigned stories as part of a pair during sprint planning by using the \"pairing names\" field. During stand-up it can also help the Process Lead filter stories assigned to the individual (both as an owner or as a pairing assignee) and show these on the board. Furthermore, the pairing field can provide an additional data point for reports and burndown rates.\\n\\nPrerequisites\\n\\nPrior to customizing Azure DevOps, review Configure and customize Azure Boards.\\n\\nIn order to add a custom field to user stories in Azure DevOps changes must be made as an Organizational setting. This document therefore assumes use of an existing Organization in Azure DevOps and that the user account used to make these changes is a member of the Project Collection Administrators Group.\\n\\nChange the organization settings\\n\\nDuplicate the process currently in use.\\nNavigate to the Organization Settings, within the Boards / Process tab.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\add-pairing-field-azure-devops-cards.md'}, {'chunkId': 'chunk165_1', 'chunkContent': \"Duplicate the process currently in use.\\nNavigate to the Organization Settings, within the Boards / Process tab.\\n\\nSelect the Process type, click on the icon with three dots ... and click Create inherited process.\\n\\nClick on the newly created inherited process.\\nAs you can see in the example below, we called it 'Pairing'.\\n\\nClick on the work item type User Story.\\n\\nClick New Field.\\n\\nGive it a Name and select Identity in Type. Click on Add Field.\\n\\nThis completes the change in Organization settings. The rest of the instructions must be completed under Project Settings.\\n\\nChange the project settings\\n\\nGo to the Project that is to be modified, select Project Settings.\\n\\nSelect Project configuration.\\n\\nClick on process customization page.\\n\\nClick on Projects then click on Change process.\\n\\nChange the target process to Pairing then click Save.\\n\\nGo to Boards.\\n\\nClick on the Gear icon to open Settings.\\n\\nAdd field to card.\\nClick on the green + icon to add select the Pairing field. Check the box to display fields, even when they are empty. Save and close.\\n\\nView the modified the card.\\nNotice the new Pairing field. The Story can now be assigned an Owner and a Pairing assignee!\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\add-pairing-field-azure-devops-cards.md'}, {'chunkId': 'chunk166_0', 'chunkContent': 'Effortless Pair Programming with GitHub Codespaces and VSCode\\n\\nPair programming used to be a software development technique in which two programmers work together on a single computer, sharing one keyboard and mouse, to jointly design, code, test, and debug software. It is one of the patterns explored in the section why collaboration? of this playbook, however with teams that work mostly remotely, sharing a physical computer became a challenge, but opened the door to a more efficient approach of pair programming.\\n\\nThrough the effective utilization of a range of tools and techniques, we have successfully implemented both pair and swarm programming methodologies. As such, we are eager to share some of the valuable insights and knowledge gained from this experience.\\n\\nHow to make pair programming a painless experience?\\n\\nWorking Sessions\\n\\nIn order to enhance pair programming capabilities, you can create regular working sessions that are open to all team members. This facilitates smooth and efficient collaboration as everyone can simply join in and work together before branching off into smaller groups. This approach has proven particularly beneficial for new team members who may otherwise feel overwhelmed by a large codebase. It emulates the concept of the \"humble water cooler,\" which fosters a sense of connectedness among team members through their shared work.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\pair-programming-tools.md'}, {'chunkId': 'chunk166_1', 'chunkContent': 'Additionally, scheduling these working sessions in advance ensures intentional collaboration and provides clarity on user story responsibilities. To this end, assign a single person to each user story to ensure clear ownership and eliminate ambiguity. By doing so, this could eliminate the common problem of engineers being hesitant to modify code outside of their assigned tasks due to the sentiment of lack of ownership. These working sessions are instrumental in promoting a cohesive team dynamic, allowing for effective knowledge sharing and collective problem-solving.\\n\\nGitHub Codespaces\\n\\nGitHub Codespaces is a vital component in an efficient development environment, particularly in the context of pair programming. Prioritize setting up a Codespace as the initial step of the project, preceding tasks such as local machine project compilation or VSCode plugin installation. To this end, make sure to update the Codespace documentation before incorporating any quick start instructions for local environments. Additionally, consistently demonstrate demos in codespaces environment to ensure its prominent integration into our workflow.\\n\\nWith its cloud-based infrastructure, GitHub Codespaces presents a highly efficient and simplified approach to real-time collaborative coding. As a result, new team members can easily access the GitHub project and begin coding within seconds, without requiring installation on their local machines. This seamless, integrated solution for pair programming offers a streamlined workflow, allowing you to direct your attention towards producing exemplary code, free from the distractions of cumbersome setup processes.\\n\\nVSCode Live Share', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\pair-programming-tools.md'}, {'chunkId': 'chunk166_2', 'chunkContent': \"VSCode Live Share\\n\\nVSCode Live Share is specifically designed for pair programming and enables you to work on the same codebase, in real-time, with your team members. The arduous process of configuring complex setups, grappling with confusing configurations, straining one's eyes to work on small screens, or physically switching keyboards is not a problem with LiveShare. This innovative solution enables seamless sharing of your development environment with your team members, facilitating smooth collaborative coding experiences.\\n\\nFully integrated into Visual Studio Code and Visual Studio, LiveShare offers the added benefit of terminal sharing, debug session collaboration, and host machine control. When paired with GitHub Codespaces, it presents a potent tool set for effective pair programming.\\n\\nTip: Share VSCode extensions (including Live Share) using a base devcontainer.json. This ensure all team members have available the same set of extensions, and allow them to focus in solving the business needs from day one.\\n\\nResources\\n\\nGitHub Codespaces.\\n\\nVSCode Live Share.\\n\\nCreate a Dev Container.\\n\\nHow companies have optimized the humble office water cooler.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\pair-programming-tools.md'}, {'chunkId': 'chunk167_0', 'chunkContent': 'Advanced recommendations for collaboration\\n\\nWhy Collaboration\\n\\nHow to use the \"Social Question of the Day\"\\n\\nEngagement Team Development\\n\\nPair and Swarm programming\\n\\nVirtual Collaboration and Pair Programming\\n\\nHow to add a Pairing Custom Field in Azure DevOps User Stories\\n\\nEffortless Pair Programming with GitHub Codespaces and VSCode', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\README.md'}, {'chunkId': 'chunk168_0', 'chunkContent': \"Social Question of the Day\\n\\nThe social question of the day is an optional short question to follow the three project questions in the daily stand-up. It develops team cohesion and interpersonal trust over the course of an engagement by facilitating the sharing of personal preferences, lifestyle, or other context.\\n\\nThe social question should be chosen before the stand-up. The facilitator should select the question either independently or from the team's asynchronous suggestions. This minimizes delays at the start of the stand-up.\\n\\nTip: having the stand-up facilitator role rotate each sprint lets the facilitator choose the social question independently without burdening any one team member.\\n\\nProperties of a good question\\n\\nA good question has a brief answer with small optional elaboration. A yes or no answer doesn't tell you very much about someone, while knowing that their favorite fruit is a durian is informative.\\n\\nGood questions are low in consequence but allow controversy. Watching someone strongly exclaim that salmon and lox on cinnamon-raisin is the best bagel order is endearing. As a corollary, a good question is one someone is likely to be passionate about. You know a little more about a team member's personality if their eyes light up when describing their favorite karaoke song.\\n\\nStarter list of questions\\n\\nPotentially good questions include:\\n\\nWhat's your Starbucks order?\\n\\nWhat's your favorite operating system?\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\social-question.md'}, {'chunkId': 'chunk168_1', 'chunkContent': \"Potentially good questions include:\\n\\nWhat's your Starbucks order?\\n\\nWhat's your favorite operating system?\\n\\nWhat's your favorite version of Windows?\\n\\nWhat's your favorite plant, houseplant or otherwise?\\n\\nWhat's your favorite fruit?\\n\\nWhat's your favorite fast food?\\n\\nWhat's your favorite noodle?\\n\\nWhat's your favorite text editor?\\n\\nMountains or beach?\\n\\nDC or Marvel?\\n\\nCoffee with one person from history: who?\\n\\nWhat's your silliest online purchase?\\n\\nWhat's your alternate career?\\n\\nWhat's the best bagel topping?\\n\\nWhat's your guilty TV pleasure?\\n\\nWhat's your go-to karaoke song?\\n\\nWould you rather see the past or the future?\\n\\nWould you rather be able to teleport or to fly?\\n\\nWould you rather live underwater or in space for a year?\\n\\nWhat's your favorite phone app?\\n\\nWhat's your favorite fish, to eat or otherwise?\\n\\nWhat was your best costume?\\n\\nWho is someone you admire (from history, from your personal life, etc.)? Give one reason why.\\n\\nWhat's the best compliment you've ever received?\\n\\nWhat's your favorite or most used emoji right now?\\n\\nWhat was your biggest DIY project?\\n\\nWhat's a spice that you use on everything?\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\social-question.md'}, {'chunkId': 'chunk168_2', 'chunkContent': \"What was your biggest DIY project?\\n\\nWhat's a spice that you use on everything?\\n\\nWhat's your top Spotify (or just your favorite) genre/artist for this year?\\n\\nWhat was your first computer?\\n\\nWhat's your favorite kind of taco?\\n\\nWhat's your favorite decade?\\n\\nWhat's the best way to eat potatoes?\\n\\nWhat was your best vacation (stay-cations acceptable)?\\n\\nFavorite cartoon?\\n\\nPick someone in your family and tell us something awesome about them.\\n\\nWhat was your longest road trip?\\n\\nWhat thing do you remember learning when you were young that is taught differently now?\\n\\nWhat was your favorite toy as a child?\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\social-question.md'}, {'chunkId': 'chunk169_0', 'chunkContent': 'Engagement Team Development\\n\\nIn every ISE engagement, dynamics are different so are the team requirements. Based on transfer learning among teams, we aim to build right \"code-with\" environments in every team.\\n\\nThis documentation gives a high-level template with some suggestions by aiming to accelerate team swarming phase to achieve a high speed agility however it has no intention to provide a list of \"must-do\" items.\\n\\nIdentification\\n\\nAs it\\'s stated in Tuckman\\'s team phases, traditional team development has several stages.\\nHowever those phases can be extremely fast or sometimes mismatched in teams due to external factors, what applies to ISE engagements.\\n\\nIn order to minimize the risk and set the expectations on the right way for all parties, an identification phase is important to understand each other.\\nSome potential steps in this phase may be as following (not limited):\\n\\nWorking agreement\\n\\nIdentification of styles/preferences in communication, sharing, learning, decision making of each team member\\n\\nTalking about necessity of pair programming\\n\\nDecisions on backlog management & refinement meetings, weekly design sessions, social time sessions...etc.\\n\\nSync/Async communication methods, work hours/flexible times\\n\\nDecisions and identifications of charts that will be helpful to provide transparent and true information to everyone', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\teaming-up.md'}, {'chunkId': 'chunk169_1', 'chunkContent': 'Decisions and identifications of charts that will be helpful to provide transparent and true information to everyone\\n\\nIdentification of \"Software Craftspersonship\" areas which means the tools and methods will be widely used during the engagement and taking the required actions on team upskilling side if necessary.\\n\\nGitHub, VSCode LiveShare, AzDevOps, necessary development tools & libraries ... more.\\n\\nIf upskilling on certain topic(s) is needed, identifying the areas and arranging code spikes for increasing the team knowledge on the regarding topic(s).\\n\\nIdentification of communication channels, feedback loops and recurrent team call slots out of regular sprint meetings\\n\\nIntroduction to Technical Agility Team Manifesto and planning the technical delivery by aiming to keep\\ntechnical debt risk minimum.\\n\\nFollowing the Plan and Agile Debugging\\n\\nIdentification phase accelerates the process of building a safe environment for every individual in the team, later on team has the required assets to follow the plan.\\nAnd it is team\\'s itself responsibility (engineers,PO,Process Lead) to debug their Agility level.\\n\\nIn every team stabilization takes time and pro-active agile debugging is the best accelerator to decrease the distraction away from sprint/engagement goal.\\nTeam is also responsible to keep the plan up-to-date based on team changes/needs and debugging results.\\n\\nJust as an example, agility debugging activities may include:', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\teaming-up.md'}, {'chunkId': 'chunk169_2', 'chunkContent': 'Just as an example, agility debugging activities may include:\\n\\nDashboards related with \"Goal\" such as burndown/burnout, Item/PR Aging, Mood Chart ..etc. are accessible to the team and team is always up-to-date\\n\\nBacklog Refinement meetings\\n\\nSize of stories (Too big? Too small?)\\n\\nAre \"User Stories\" and \"Tasks\" clear ?\\n\\nAre Acceptance Criteria enough and right?\\n\\nIs everyone ready-to-go after taking the User Story/Task?\\n\\nRunning Efficient Retrospectives\\n\\nIs the Sprint Goal clear in every iteration ?\\n\\nIs the estimation process in the team improving over time or does it meet the delivery/workload prediction?\\n\\nKindly check Scrum Values to have a better understanding to improve team commitment.\\n\\nFollowing that, above suggestions aim to remove agile/team disfunctionalities and provide a broader team understanding, potential time savings and full transparency.\\n\\nResources\\n\\nTuckman\\'s Stages of Group Development\\n\\nScrum Values', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\teaming-up.md'}, {'chunkId': 'chunk170_0', 'chunkContent': 'Virtual Collaboration and Pair Programming\\n\\nPair programming is the de facto work method that most large engineering organizations use for “hands on keyboard” coding. Two developers, working synchronously, looking at the same screen and attempting to code and design together, which often results in better and clearer code than either could produce individually.\\n\\nPair programming works well under the correct circumstances, but it loses some of its charm when executed in a completely virtual setting. The virtual setup still involves two developers looking at the same screen and talking out their designs, but there are often logistical issues to deal with, including lag, microphone set up issues, workspace and personal considerations, and many other small, individually trivial problems that worsen the experience.\\n\\nVirtual work patterns are different from the in-person patterns we are accustomed to. Pair programming at its core is based on the following principles:\\n\\nGenerating clarity through communication\\n\\nProducing higher quality through collaboration\\n\\nCreating ownership through equal contribution\\n\\nPair programming is one way to achieve these results. Red Team Testing (RTT) is an alternate programming method that uses the same principles but with some of the advantages that virtual work methods provide.\\n\\nRed Team Testing', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\virtual-collaboration.md'}, {'chunkId': 'chunk170_1', 'chunkContent': 'Red Team Testing\\n\\nRed Team Testing borrows its name from the “Red Team” and “Blue Team” paradigm of penetration testing, and is a collaborative, parallel way of working virtually. In Red Team Testing, two developers jointly decide on the interface, architecture, and design of the program, and then separate for the implementation phase. One developer writes tests using the public interface, attempting to perform edge case testing, input validation, and otherwise stress testing the interface. The second developer is simultaneously writing the implementation which will eventually be tested.\\n\\nRed Team Testing has the same philosophy as any other Test-Driven Development lifecycle: All implementation is separated from the interface, and the interface can be tested with no knowledge of the implementation.\\n\\nSteps\\n\\nDesign Phase: Both developers design the interface together. This includes:\\n\\nMethod signatures and names\\nWriting documentation or docstrings for what the methods are intended to do.\\nArchitecture decisions that would influence testing (Factory patterns, etc.)\\n\\nImplementation Phase: The developers separate and parallelize work, while continuing to communicate.\\n\\nDeveloper A will design the implementation of the methods, adhering to the previously decided design.\\nDeveloper B will concurrently write tests for the same method signatures, without knowing details of the implementation.\\n\\nIntegration & Testing Phase: Both developers commit their code and run the tests.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\virtual-collaboration.md'}, {'chunkId': 'chunk170_2', 'chunkContent': 'Integration & Testing Phase: Both developers commit their code and run the tests.\\n\\nUtopian Scenario: All tests run and pass correctly.\\nRealistic Scenario: The tests have either broken or failed due to flaws in testing. This leads to further clarification of the design and a discussion of why the tests failed.\\n\\nThe developers will repeat the three phases until the code is functional and tested.\\n\\nWhen to follow the RTT strategy\\n\\nRTT works well under specific circumstances. If collaboration needs to happen virtually, and all communication is virtual, RTT reduces the need for constant communication while maintaining the benefits of a joint design session. This considers the human element: Virtual communication is more exhausting than in person communication.\\n\\nRTT also works well when there is complete consensus, or no consensus at all, on what purpose the code serves. Since creating the design jointly and agreeing to implement and test against it are part of the RTT method, RTT forcibly creates clarity through iteration and communication.\\n\\nBenefits\\n\\nRTT has many of the same benefits as Pair Programming and Test-Driven development but tries to update them for a virtual setting.\\n\\nCode implementation and testing can be done in parallel, over long distances or across time zones, which reduces the overall time taken to finish writing the code.\\n\\nRTT maintains the pair programming paradigm, while reducing the need for video communication or constant communication between developers.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\virtual-collaboration.md'}, {'chunkId': 'chunk170_3', 'chunkContent': 'RTT maintains the pair programming paradigm, while reducing the need for video communication or constant communication between developers.\\n\\nRTT allows detailed focus on design and engineering alignment before implementing any code, leading to cleaner and simpler interfaces.\\n\\nRTT encourages testing to be prioritized alongside implementation, instead of having testing follow or be influenced by the implementation of the code.\\n\\nDocumentation is inherently a part of RTT, since both the implementer and the tester need correct, up to date documentation, in the implementation phase.\\n\\nWhat you need for RTT to work well\\n\\nDemand for constant communication and good teamwork may pose a challenge; daily updates amongst team members are essential to maintain alignment on varying code requirements.\\n\\nClarity of the code design and testing strategy must be established beforehand and documented as reference. Lack of an established design will cause misalignment between the two major pieces of work and a need for time-consuming refactoring.\\n\\nRTT does not work well if only one developer has knowledge of the overall design. Team communication is critical to ensuring that every developer involved in RTT is on the same page.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\virtual-collaboration.md'}, {'chunkId': 'chunk171_0', 'chunkContent': 'Why Collaboration\\n\\nWhy collaboration is important\\n\\nIn engagements, we aim to be highly collaborative because when we code together, we perform better, have a higher sprint velocity, and have a greater degree of knowledge sharing across the team.\\n\\nThere are two common patterns we use for collaboration: Pairing and swarming.\\n\\nPair programming (“pairing”) - two software engineers assigned to, and working on, one shared story at a time during the sprint. The Dev Lead assigns a user story to two engineers -- one primary engineer (story owner) and one secondary engineer (pairing assignee).\\n\\nSwarm programming (“swarming”) - three or more software engineers collaborating on a high-priority item to bring it to completion.\\n\\nHow to pair program\\n\\nAs mentioned, every story is intentionally assigned to a pair. The pairing assignee may be in the process of upskilling, nevertheless, they are equal partners in the development effort.\\nBelow are some general guidelines for pairing:\\n\\nUpon assignment of the story/product backlog item (PBI), the pair needs to be deliberate about defining how to work together and have a firm definition of the work to be completed. This information should be expressed clearly in the story’s description and acceptance criteria. The expectations about this need to be communicated and agreed upon by both engineers and should be done prior to any actual working sessions.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\why-collaboration.md'}, {'chunkId': 'chunk171_1', 'chunkContent': 'The story owner and pairing assignee do not merely split the work up and sync regularly – they actively work together on the same tasks, and might share their screens via a Teams online session. Collaborative tools like VS Live Share can be preferable to sharing screens. Not all collaboration needs to be screen-share based.\\n\\nDuring the collaborative sessions, one engineer provides the development environment while the other actively views and comments verbally.\\n\\nEngineers trade places often from one session to the next so that everyone has time in control of the keyboard.\\n\\nEngineers leverage feature branches for the collaboration during the development of each story to have small Pull Requests (PRs) (as opposed to a single giant PR) at the end of the sprint.\\n\\nCode is committed to the repository by both members of the assigned pair where and when it makes sense as tasks were completed.\\n\\nThe pairing assignee is the voice representing the pair during the daily standup while being supported by the story owner.\\n\\nHaving the names of both individuals (owner and pair assignee) visible on the PBI can be helpful during sprint ceremonies and lead to greater accountability by the pairing assignee. An example of this using Azure DevOps cards can be found here.\\n\\nWhy pair programming helps collaboration', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\why-collaboration.md'}, {'chunkId': 'chunk171_2', 'chunkContent': 'Why pair programming helps collaboration\\n\\nPair programming helps collaboration because both engineers share equal responsibility for bringing the story to completion. This is a mutually beneficial exercise because, while the story owner often has more experience to lean on, the pairing assignee brings a fresh view that is unclouded by repetition.\\n\\nSome other benefits include:\\n\\nFewer defects and increased accountability. Having two sets of eyes allows the engineers more opportunity to catch errors and to remember often-overlooked tasks such as writing unit and integration tests.\\n\\nPairing allows engineers with different experience and expertise to learn from one another by collaborating and receiving feedback in real-time. Instead of having an engineer work alone on a task for long hours and hit an isolation breaking point, pairing allows the pair to check in with one another.\\n\\nEven something as simple as describing the problem out loud can help uncover issues or bugs in the code.\\n\\nPairing can help brainstorming as well as validating details such as making the variable names consistent.\\n\\nWhen to swarm program\\n\\nIt is important to know that not every PBI needs to use swarming. Some sprints may not even warrant swarming at all.\\nSwarm when:\\n\\nThe work is complex enough to have collective minds collaborating (not because the quantity of work is more than what would be completed in one sprint).', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\why-collaboration.md'}, {'chunkId': 'chunk171_3', 'chunkContent': 'The work is complex enough to have collective minds collaborating (not because the quantity of work is more than what would be completed in one sprint).\\n\\nThe task that the swarm works on has become (or is in imminent danger of becoming) a blocker to other stories.\\n\\nAn unknown is discovered that needs a collaborative effort to form a decision on how to move forward. The collective knowledge and expertise help move the story forward more quickly and ultimately produced better quality code.\\n\\nA conflict or unresolved difference of opinion arises during a pairing session. Promote the work to become a swarming session to help resolve the conflict.\\n\\nHow to swarm program\\n\\nAs soon the pair finds out that the PBI will warrant swarming, the pair brings it up to the rest of the team (via parking lot during stand-up or asynchronously). Members of the team agree or volunteer to assist.\\n\\nThe story owner (or pairing assignee) sends Teams call invite to the interested parties. This allows the swarm to have dedicated focus time by blocking time in calendars.\\n\\nDuring a swarming session, an engineer can branch out if there is something that needs to be handled while the swarm tackles the main problem at hand, then reconnects and reports back. This allows the swarm to focus on a core aspect and to be all on the same page.\\n\\nThe Teams call is repeated until resolution is found or alternative path forward is formulated.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\why-collaboration.md'}, {'chunkId': 'chunk171_4', 'chunkContent': 'The Teams call is repeated until resolution is found or alternative path forward is formulated.\\n\\nWhy swarm programming helps collaboration\\n\\nSwarming allows the collective knowledge and expertise of the team to come together in a focused and unified way.\\n\\nNot only does swarming help close out the item faster, but it also helps the team understand each other’s strengths and weaknesses.\\n\\nAllows the team to build a higher level of trust and work as a cohesive unit.\\n\\nWhen to decide to swarm, pair, and/or split\\n\\nWhile a lot of time can be spent on pair programming, it does make sense to split the work when folks understand how the work will be carried out, and the work to be done is largely prescriptive.\\n\\nOnce the story has been jointly tasked out by both engineers, the engineers may choose to tackle some tasks separately and then combine the work together at the end.\\n\\nPair programming is more helpful when the engineers do not have perfect clarity about what is needed to be done or how it can be done.\\n\\nSwarming is done when the two engineers assigned to the story need an additional sounding board or need expertise that other team members could provide.\\n\\nBenefits of increased collaboration', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\why-collaboration.md'}, {'chunkId': 'chunk171_5', 'chunkContent': 'Benefits of increased collaboration\\n\\nKnowledge sharing and bringing ISE and customer engineers together in a ‘code-with’ manner is an important aspect of ISE engagements. This grows both our customers’ and our ISE team’s capability to build on Azure. We are responsible for demonstrating engineering fundamentals and leaving the customer in a better place after we disengage. This can only happen if we collaborate and engage together as a team. In addition to improved software quality, this also adds a beneficial social aspect to the engagements.\\n\\nResources\\n\\nHow to add a pairing custom field in Azure DevOps User Stories - adding a custom field of type Identity in Azure DevOps for pairing\\n\\nOn Pair Programming - Martin Fowler\\n\\nPair Programming hands-on lessons - these can be used (and adapted) to support bringing pair programming into your team (MS internal or including customers)\\n\\nEffortless Pair Programming with GitHub Codespaces and VSCode', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\why-collaboration.md'}, {'chunkId': 'chunk172_0', 'chunkContent': 'Delivery Plan\\n\\nGoals\\n\\nWhile Scrum does not require and discourages planning more than one sprint at a time. Most of us work in enterprises where we are dependent outside teams (for example: marketing, sales, support).\\n\\nA rough assessment of the planned project scope is achievable within a reasonable time frame and resources. The goal is to have a rough plan and estimate as a starting point, not to implement \"Agilefall.\"\\n\\nNote that this is just a starting point to enable planning discussions. We expect the actual schedule to evolve and shift over time and that you will update the scope and timeline as you progress.\\n\\nDelivery Plans ensure your teams are aligning with your organizational goals.\\n\\nBenefits\\n\\nAs you complete the assessment, you can push back on the scope, time frame or ask for more resources.\\n\\nAs you progress in your project/product delivery, you can highlight risks to the scope, time frame, and resources.\\n\\nApproach\\n\\nOne approach you can take to accomplish is with stickies and a spreadsheet.\\n\\nStep 1: Stack rank the features for everything in your backlog\\n\\nFunctional Features\\n\\n[Non-functional Features] (docs/TECH-LEADS-CHECKLIST.md)\\n\\nUser Research and Design\\n\\nTesting\\n\\nDocumentation\\n\\nKnowledge Transfer/Support Processes', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\effective-organization\\\\delivery-plan.md'}, {'chunkId': 'chunk172_1', 'chunkContent': \"User Research and Design\\n\\nTesting\\n\\nDocumentation\\n\\nKnowledge Transfer/Support Processes\\n\\nStep 2: T-Shirt Features in terms of working weeks per person. In some scenarios, you have no idea how complex the work. In this situation, you can ask for time to conduct a spike (timebox the effort so you can get back on time).\\n\\nStep 3: Calculate the capacity for the team based on the number of weeks person with his/her start and end date and minus holidays, vacation, conferences, training, and onboarding days. Also, minus time if the person is also working on defects and support.\\n\\nStep 4: Based on your capacity, you know have the options\\n\\nAsk for more resources. Caution: onboarding new resources take time.\\n\\nReduce the scope to the most MVP.  Caution: as you trim more of the scope, it might not be valuable anymore to the customer. Consider a cupcake which is everything you need. You don't want to skim off the frosting.\\n\\nAsk for more time. Usually, this is the most flexible, but if there is a marketing date that you need to hit, this might be as flexible.\\n\\nTools\\n\\nYou can also leverage one of these tools by creating your epics and features and add the weeks estimates.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\effective-organization\\\\delivery-plan.md'}, {'chunkId': 'chunk172_2', 'chunkContent': 'Tools\\n\\nYou can also leverage one of these tools by creating your epics and features and add the weeks estimates.\\n\\nThe Plans (Preview) feature on Azure DevOps will help you make a plan. Delivery Plans provide a schedule of stories or features your team plan to deliver. Delivery Plans show the scheduled work items by a sprint (iteration path) of selected teams against a calendar view.\\n\\nConfluence JIRA, Trello, Rally, Asana, Basecamp, and GitHub Issues are other similar tools in the market (some are free, others you pay a monthly fee, or you can install on-prem) that you can leverage.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\effective-organization\\\\delivery-plan.md'}, {'chunkId': 'chunk173_0', 'chunkContent': 'Advanced recommendations for a more effective organization\\n\\nDelivery/Release plan\\n\\nScrum of Scrum', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\effective-organization\\\\README.md'}, {'chunkId': 'chunk174_0', 'chunkContent': 'Scrum of Scrums\\n\\nScrum of scrums is a technique used to scale Scrum to a larger group working towards the same project goal. In Scrum, we consider a team being too big when going over 10-12 individuals. This should be decided on a case by case basis. If the project is set up in multiple work streams that contain a fixed group of people and a common stand-up meeting is slowing down productivity: scrum of scrums should be considered. The team would identify the different subgroups that would act as a separate scrum teams with their own backlog, board and stand-up.\\n\\nGoals\\n\\nThe goal of the scrum of scrums ceremony is to give sub-teams the agility they need while not loosing visibility and coordination. It also helps to ensure that the sub-teams are achieving their sprint goals, and they are going in the right direction to achieve the overall project goal.\\n\\nThe scrum of scrums ceremony happens every day and can be seen as a regular stand-up:\\n\\nWhat was done the day before by the sub-team.\\n\\nWhat will be done today by the sub-team.\\n\\nWhat are blockers or other issues for the sub-team.\\n\\nWhat are the blockers or issues that may impact other sub-teams.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\effective-organization\\\\scrum-of-scrums.md'}, {'chunkId': 'chunk174_1', 'chunkContent': 'What are the blockers or issues that may impact other sub-teams.\\n\\nThe outcome of the meeting will result in a list of impediments related to coordination of the whole project. Solutions could be: agreeing on interfaces between teams, discussing architecture changes, evolving responsibility boundaries, etc.\\n\\nThis list of impediments is usually managed in a separate backlog but does not have to.\\n\\nParticipation\\n\\nThe common guideline is to have on average one person per sub-team to participate in the scrum of scrums. Ideally, the Process Lead of each sub-team would represent them in this ceremony. In some instances, the representative for the day is selected at the end of each sub-team daily stand-up and could change every day. In practice, having a fixed representative tends to be more efficient in the long term.\\n\\nImpact\\n\\nThis practice is helpful in cases of longer projects and with a larger scope, requiring more people. When having more people, it is usually easier to divide the project in sub-teams. Having a daily scrum of scrums improves communication, lowers the risk of integration issues and increases the project chances of success.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\effective-organization\\\\scrum-of-scrums.md'}, {'chunkId': 'chunk174_2', 'chunkContent': 'When choosing to implement Scrum of Scrums, you need to keep in mind that some team members will have additional meetings to coordinate and participate in. Also: all team members for each sub-team need to be updated on the decisions at a later point to ensure a good flow of information.\\n\\nMeasures\\n\\nThe easiest way to measure the impact is by tracking the time to resolve issues in the scrum of scrums backlog. You can also track issues reported during the retrospective related to global coordination (is it well done? can it be improved?).\\n\\nFacilitation Guidance\\n\\nThis should be facilitated like a regular stand-up.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\effective-organization\\\\scrum-of-scrums.md'}, {'chunkId': 'chunk175_0', 'chunkContent': 'Definition of Done\\n\\nTo close a user story, a sprint, or a milestone it is important to verify that the tasks are complete.\\n\\nThe development team should decide together what their Definition of Done is and document this in the project. Below are some examples of checks to verify that the user story, sprint, task is completed.\\n\\nFeature/User Story\\n\\n[ ] Acceptance criteria are met\\n\\n[ ] Refactoring is complete\\n\\n[ ] Code builds with no error\\n\\n[ ] Unit tests are written and pass\\n\\n[ ] Existing Unit Tests pass\\n\\n[ ] Sufficient diagnostics/telemetry are logged\\n\\n[ ] Code review is complete\\n\\n[ ] UX review is complete (if applicable)\\n\\n[ ] Documentation is updated\\n\\n[ ] The feature is merged into the develop branch\\n\\n[ ] The feature is signed off by the product owner\\n\\nSprint Goal\\n\\n[ ] Definition of Done for all user stories included in the sprint are met\\n\\n[ ] Product backlog is updated\\n\\n[ ] Functional and Integration tests pass\\n\\n[ ] Performance tests pass\\n\\n[ ] End 2 End tests pass\\n\\n[ ] All bugs are fixed\\n\\n[ ] The sprint is signed off from developers, software architects, project manager, product owner etc.\\n\\nRelease/Milestone\\n\\n[ ] Code Complete (goals of sprints are met)', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\definition-of-done.md'}, {'chunkId': 'chunk175_1', 'chunkContent': 'Release/Milestone\\n\\n[ ] Code Complete (goals of sprints are met)\\n\\n[ ] Release is marked as ready for production deployment by product owner', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\definition-of-done.md'}, {'chunkId': 'chunk176_0', 'chunkContent': 'Definition of Ready\\n\\nWhen the development team picks a user story from the top of the backlog, the user story needs to have enough detail to estimate the work needed to complete the story within the sprint. If it has enough detail to estimate, it is Ready to be developed.\\n\\nIf a user story is not Ready in the beginning of the Sprint it increases the chance that the story will not be done at the end of this sprint.\\n\\nWhat it is\\n\\nDefinition of Ready is the agreement made by the scrum team around how complete a user story should be in order to be selected as candidate for estimation in the sprint planning. These can be codified as a checklist in user stories using GitHub Issue Templates or Azure DevOps Work Item Templates.\\n\\nIt can be understood as a checklist that helps the Product Owner to ensure that the user story they wrote contains all the necessary details for the scrum team to understand the work to be done.\\n\\nExamples of ready checklist items\\n\\n[ ] Does the description have the details including any input values required to implement the user story?\\n\\n[ ] Does the user story have clear and complete acceptance criteria?\\n\\n[ ] Does the user story address the business need?\\n\\n[ ] Can we measure the acceptance criteria?\\n\\n[ ] Is the user story small enough to be implemented in a short amount of time, but large enough to provide value to the customer?', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\definition-of-ready.md'}, {'chunkId': 'chunk176_1', 'chunkContent': \"[ ] Is the user story small enough to be implemented in a short amount of time, but large enough to provide value to the customer?\\n\\n[ ] Is the user story blocked? For example, does it depend on any of the following:\\n\\nThe completion of unfinished work\\n\\nA deliverable provided by another team (code artifact, data, etc...)\\n\\nWho writes it\\n\\nThe ready checklist can be written by a Product Owner in agreement with the development team and the Process Lead.\\n\\nWhen should a Definition of Ready be updated\\n\\nUpdate or change the definition of ready anytime the scrum team observes that there are missing information in the user stories that recurrently impacts the planning.\\n\\nWhat should be avoided\\n\\nThe ready checklist should contain items that apply broadly. Don't include items or details that only apply to one or two user stories. This may become an overhead when writing the user stories.\\n\\nHow to get stories ready\\n\\nIn the case that the highest priority work is not yet ready, it still may be possible to make forward progress. Here are some strategies that may help:\\n\\nBacklog Refinement sessions are a good time to validate that high priority user stories are verified to have a clear description, acceptance criteria and demonstrable business value. It is also a good time to breakdown large stories will likely not be completable in a single sprint.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\definition-of-ready.md'}, {'chunkId': 'chunk176_2', 'chunkContent': 'Prioritization sessions are a good time to prioritize user stories that unblock other blocked high priority work.\\n\\nBlocked user stories can often be broken down in a way that unblocks a portion of the original stories scope. This is a good way to make forward progress even when some work is blocked.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\definition-of-ready.md'}, {'chunkId': 'chunk177_0', 'chunkContent': 'Team Agreements\\n\\nDefinition of Done\\n\\nDefinition of Ready\\n\\nWorking Agreements\\n\\nTeam Manifesto\\n\\nGoals\\n\\nTeam agreements help clarify expectations for all team members, whether they are expectations around how the team works together (Working Agreements) or how to judge if a story is complete (Definition of Done).', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\README.md'}, {'chunkId': 'chunk178_0', 'chunkContent': 'Team Manifesto\\n\\nIntroduction\\n\\nISE teams work with a new development team in each customer engagement which requires a phase of introduction & knowledge transfer before starting an engagement.\\n\\nCompletion of this phase of ice-breakers and discussions about the standards takes time, but is required to start increasing the learning curve of the new team.\\n\\nA team manifesto is a light-weight one page agile document among team members which summarizes the basic principles and values of the team and aiming to provide a consensus about technical expectations from each team member in order to deliver high quality output at the end of each engagement.\\n\\nIt aims to reduce the time on setting the right expectations without arranging longer \"team document reading\" meetings and provide a consensus among team members to answer the question - \"How does the new team develop the software?\" - by covering all engineering fundamentals and excellence topics such as release process, clean coding, testing.\\n\\nAnother main goal of writing the manifesto is to start a conversation during the \"manifesto building session\" to detect any differences of opinion around how the team should work.\\n\\nIt also serves in the same way when a new team member joins to the team. New joiners can quickly get up to speed on the agreed standards.\\n\\nHow to Build a Team Manifesto\\n\\nIt can be said that the best time to start building it is at the very early phase of the engagement when teams meet with each other for swarming or during the preparation phase.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\team-manifesto.md'}, {'chunkId': 'chunk178_1', 'chunkContent': \"It is recommended to keep team manifesto as simple as possible, so preferably, one-page simple document which doesn't include any references or links is a nice format for it.\\nIf there is a need for providing knowledge on certain topics, the way to do is delivering brown-bag sessions, technical katas, team practices, documentations and others later on.\\n\\nA few important points about the team manifesto\\n\\nThe team manifesto is built by the development team itself\\n\\nIt should cover all required technical engineering points for the excellence as well as behavioral agility mindset items that the team finds relevant\\n\\nIt aims to give a common understanding about the desired expertise, practices and/or mindset within the team\\n\\nBased on the needs of the team and retrospective results, it can be modified during the engagement.\\n\\nIn ISE, we aim for quality over quantity, and well-crafted software as well as to a comfortable/transparent environment where each team member can reach their highest potential.\\n\\nThe difference between the team manifesto and other team documents is that it is used to give a short summary of expectations around the technical way of working and supported mindset in the team, before code-with sprints starts.\\n\\nBelow, you can find some including, but not limited, topics many teams touch during engagements,\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\team-manifesto.md'}, {'chunkId': 'chunk178_2', 'chunkContent': 'Topic What is it about ? Collective Ownership Does team own the code rather than individuals? What is the expectation? Respect Any preferred statement about it\\'s a \"must-have\" team value Collaboration Any preferred statement about how does team want to collaborate ? Transparency A simple statement about it\\'s a \"must-have\" team value and if preferred, how does this being provided by the team ? meetings, retrospective, feedback mechanisms etc. Craftspersonship Which tools such as Git, VS Code LiveShare, etc. are being used ? What is the definition of expected best usage of them? PR sizing What does team prefer in PRs ? Branching Team\\'s branching strategy and standards Commit standards Preferred format in commit messages, rules and more Clean Code Does team follow clean code principles ? Pair/Mob Programming Will team apply pair/mob programming ? If yes, what programming styles are suitable for the team ? Release Process Principles around release process such as quality gates, reviewing process ...etc. Code Review Any rule for code reviewing such as min number of reviewers, team rules ...etc. Action Readiness How the backlog will be refined? How do we ensure clear Definition of Done and Acceptance Criteria ? TDD Will the team follow TDD ? Test Coverage Is there any expected number, percentage or measurement ? Dimensions in Testing Required tests for high quality software, eg : unit, integration, functional, performance, regression, acceptance Build process build for all? or not; The clear statement of where', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\team-manifesto.md'}, {'chunkId': 'chunk178_3', 'chunkContent': 'quality software, eg : unit, integration, functional, performance, regression, acceptance Build process build for all? or not; The clear statement of where code and under what conditions code should work ? eg : OS, DevOps, tool dependency Bug fix The rules of bug fixing in the team ? eg: contact people, attaching PR to the issue etc. Technical debt How does team manage/follow it? Refactoring How does team manage/follow it? Agile Documentation Does team want to use diagrams and tables more rather than detailed KB articles ? Efficient Documentation When is it necessary ? Is it a prerequisite to complete tasks/PRs etc.? Definition of Fun How will we have fun for relaxing/enjoying the team spirit during the engagement?', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\team-manifesto.md'}, {'chunkId': 'chunk178_4', 'chunkContent': 'Tools\\n\\nGenerally team sessions are enough for building a manifesto and having a consensus around it, and if there is a need for improving it in a structured way, there are many blogs and tools online, any retrospective tool can be used.\\n\\nResources\\n\\nTechnical Agility*', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\team-manifesto.md'}, {'chunkId': 'chunk179_0', 'chunkContent': \"Sections of a Working Agreement\\n\\nA working agreement is a document, or a set of documents that describe how we work together as a team and what our\\nexpectations and principles are.\\n\\nThe working agreement created by the team at the beginning of the project, and is stored in the repository so that it is\\nreadily available for everyone working on the project.\\n\\nThe following are examples of sections and points that can be part of a working agreement but each team should compose\\ntheir own, and adjust times, communication channels, branch naming policies etc. to fit their team needs.\\n\\nGeneral\\n\\nWe work as one team towards a common goal and clear scope\\n\\nWe make sure everyone's voice is heard, listened to\\n\\nWe show all team members equal respect\\n\\nWe work as a team to have common expectations for technical delivery that are documented in a Team Manifesto.\\n\\nWe make sure to spread our expertise and skills in the team, so no single person is relied on for one skill\\n\\nAll times below are listed in CET\\n\\nCommunication\\n\\nWe communicate all information relevant to the team through the Project Teams channel\\n\\nWe add all technical spikes, trade studies, and other technical documentation to the project repository through async design reviews in PRs\\n\\nWork-life Balance\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\working-agreements.md'}, {'chunkId': 'chunk179_1', 'chunkContent': \"Work-life Balance\\n\\nOur office hours, when we can expect to collaborate via Microsoft Teams, phone or face-to-face are Monday to Friday 10AM - 5PM\\n\\nWe are not expected to answer emails past 6PM, on weekends or when we are on holidays or vacation.\\n\\nWe work in different time zones and respect this, especially when setting up recurring meetings.\\n\\nWe record meetings when possible, so that team members who could not attend live can listen later.\\n\\nQuality and not Quantity\\n\\nWe agree on a Definition of Done for our user story's and sprints and live by it.\\n\\nWe follow engineering best practices like the Code With Engineering Playbook\\n\\nScrum Rhythm\\n\\nActivity When Duration Who Accountable Goal Project Standup Tue-Fri 9AM 15 min Everyone Process Lead What has been accomplished, next steps, blockers Sprint Demo Monday 9AM 1 hour Everyone Dev Lead Present work done and sign off on user story completion Sprint Retro Monday 10AM 1 hour Everyone Process Lead Dev Teams shares learnings and what can be improved Sprint Planning Monday 11AM 1 hour Everyone PO Size and plan user stories for the sprint Task Creation After Sprint Planning - Dev Team Dev Lead Create tasks to clarify and determine velocity Backlog refinement Wednesday 2PM 1 hour Dev Lead, PO PO Prepare for next sprint and ensure that stories are ready for next sprint.\\n\\nProcess Lead\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\working-agreements.md'}, {'chunkId': 'chunk179_2', 'chunkContent': 'Process Lead\\n\\nThe Process Lead is responsible for leading any scrum or agile practices to enable the project to move forward.\\n\\nFacilitate standup meetings and hold team accountable for attendance and participation.\\n\\nKeep the meeting moving as described in the Project Standup page.\\n\\nMake sure all action items are documented and ensure each has an owner and a due date and tracks the open issues.\\n\\nNotes as needed after planning / stand-ups.\\n\\nMake sure that items are moved to the parking lot and ensure follow-up afterwards.\\n\\nMaintain a location showing team’s work and status and removing impediments that are blocking the team.\\n\\nHold the team accountable for results in a supportive fashion.\\n\\nMake sure that project and program documentation are up-to-date.\\n\\nGuarantee the tracking/following up on action items from retrospectives (iteration and release planning) and from daily standup meetings.\\n\\nFacilitate the sprint retrospective.\\n\\nCoach Product Owner and the team in the process, as needed.\\n\\nBacklog Management\\n\\nWe work together on a Definition of Ready and all user stories assigned to a sprint need to follow this\\n\\nWe communicate what we are working on through the board\\n\\nWe assign ourselves a task when we are ready to work on it (not before) and move it to active', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\working-agreements.md'}, {'chunkId': 'chunk179_3', 'chunkContent': \"We assign ourselves a task when we are ready to work on it (not before) and move it to active\\n\\nWe capture any work we do related to the project in a user story/task\\n\\nWe close our tasks/user stories only when they are done (as described in the Definition of Done)\\n\\nWe work with the PM if we want to add a new user story to the sprint\\n\\nIf we add new tasks to the board, we make sure it matches the acceptance criteria of the user story (to avoid scope creep).\\n  If it doesn't match the acceptance criteria we should discuss with the PM to see if we need a new user story for the task or if we should adjust the acceptance criteria.\\n\\nCode Management\\n\\nWe follow the git flow branch naming convention for branches and identify the task number e.g. feature/123-add-working-agreement\\n\\nWe merge all code into main branches through PRs\\n\\nAll PRs are reviewed by one person from [Customer/Partner Name] and one from Microsoft (for knowledge transfer and to ensure code and security standards are met)\\n\\nWe always review existing PRs before starting work on a new task\\n\\nWe look through open PRs at the end of stand-up to make sure all PRs have reviewers.\\n\\nWe treat documentation as code and apply the same standards to Markdown as code\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\team-agreements\\\\working-agreements.md'}, {'chunkId': 'chunk180_0', 'chunkContent': 'Agile Development Basics\\n\\nIf you are new to Agile development or if you are looking for a refresher, this section will provides links to information that provide best pracices for Backlog Management, Agile Ceremonies, Roles within Agile and Agile Sprints.\\n\\nWhat is Agile\\n\\nWhat is Agile Development\\n\\nBacklog Management\\n\\nCeremonies\\n\\nRoles\\n\\nSprints', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\basics\\\\README.md'}, {'chunkId': 'chunk181_0', 'chunkContent': 'Backlog Management basics for the Product and Sprint backlog\\n\\nThis section has links directing you to best practices for managing Product and Sprint backlogs.  After reading through the best practices you should have a basic understanding for managing both product and sprint backlogs, how to create acceptance criteria for user stories, creating a definition of done and definition of ready for user stories and the basics around estimating user stories.\\n\\nProduct Backlog\\n\\nSprint Backlog\\n\\nAcceptance Criteria\\n\\nDefinition of Done\\n\\nDefinition of Ready\\n\\nEstimation Basics in Agile', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\basics\\\\Backlog Management\\\\README.md'}, {'chunkId': 'chunk182_0', 'chunkContent': 'Agile Ceremonies basics\\n\\nThis section has links directing you to best practices for conducting the Agile ceremonies.  After reading through the best practices you should have a basic understanding of the key Agile ceremonies in terms of purpose, value and best practices around conducting and facilitating these ceremonies.\\n\\nPlanning\\n\\nRefinement\\n\\nRetrospective\\n\\nSprint Review/Demo\\n\\nStand-Up/Daily Scrum', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\basics\\\\Ceremonies\\\\README.md'}, {'chunkId': 'chunk183_0', 'chunkContent': 'Agile/Scrum Roles\\n\\nThis section has links directing you to definitions for the traditional roles within Agile/Scrum.  After reading through the best practices you should have a basic understanding of the key Agile roles in terms of what they are and the expectations for the role.\\n\\nProduct Owner\\n\\nScrum Master\\n\\nDevelopment Team', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\basics\\\\Roles\\\\README.md'}, {'chunkId': 'chunk184_0', 'chunkContent': 'The Sprint\\n\\nThis section has links directing you to best practices in regards to what a sprint is within agile and the practices around the sprint.  After reading through the best practices you should have a basic understanding of Sprint Planning and the Sprint Backlog, Sprint Execution and the Daily Standup, Sprint Review and Sprint Retrospective and the key output of the sprint which is called the Increment.\\n\\nSprint Planning and the Sprint Backlog\\n\\nSprint Execution and the Daily Standup\\n\\nSprint Review and Sprint Retrospective\\n\\nIncrement', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\basics\\\\Sprints\\\\README.md'}, {'chunkId': 'chunk185_0', 'chunkContent': 'Agile core expectations\\n\\nThis section contains core expectations for agile practices in ISE:\\n\\nIt should stay concise and link to external documentation for details.\\n\\nEach section contains a list of core expectations and suggestions:\\n\\nCore expectations are what each dev team is expected to aim for.\\n\\nSuggestions are not expectations. They are our learned experience for meeting those expectations, and can be adopted and modified to suit the project.\\n\\nNotes:\\n\\nWe prefer the usage of \"process lead\" over \"scrum master\". It describes the same role.\\n\\n\"Crew\", in this document, refers to the entire team working on an project (dev team, dev lead, PM, etc.).\\n\\nWe follow Agile principles and usually Scrum\\n\\nOverall expectations for a project\\n\\nThe crew is predictable in their delivery.\\n\\nThe crew makes relevant adjustments and shares these transparently.\\n\\nRoles and Responsibilities are clarified and agreed before the project starts.\\n\\nThe crew is driving continuous improvement of their own process to meet the core expectations and increase project success.\\n\\nCore expectations and suggestions\\n\\nSprints\\n\\nExpectations:\\n\\nSprint structure gives frequent opportunities for feedback and adjustment in the context of relatively small projects.\\n\\nSprint ceremonies should be planned to accommodate working schedules of the team and take into consideration hard and soft time constraints.\\n\\nSuggestions:', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_1', 'chunkContent': 'Sprint ceremonies should be planned to accommodate working schedules of the team and take into consideration hard and soft time constraints.\\n\\nSuggestions:\\n\\nSprinting starts day 1: Game plan creation, game plan review and sharing are included in sprints and should be reflected in the backlog.\\n\\nDefine a sprint goal that will be used to determine the success of the sprint.\\n\\nNote: Sprints are usually 1 week long to increase the number of opportunities for adjustments. And minimize the risk of missing the sprint goal.\\n\\nEstimation\\n\\nExpectations:\\n\\nEstimation supports the predictability of the team work and delivery.\\n\\nEstimation re-enforces the value of accountability to the team.\\n\\nThe estimation process is improved over time and discussed on a regular basis.\\n\\nEstimation is inclusive of the different individuals in the team.\\n\\nSuggestions:\\nRough estimation is usually done for a generic SE 2 dev.\\n\\nExample 1\\n\\nThe team use t-shirt sizes (S, M, L, XL) and agrees in advance which size fits a sprint.\\n\\nIn this example: S, M fits a sprint, L, XL too big for a sprint and need to be split / refined\\n\\nThe dev lead with support of the team roughly estimates how much S and M stories can be done in the first sprints', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_2', 'chunkContent': 'The dev lead with support of the team roughly estimates how much S and M stories can be done in the first sprints\\n\\nThis rough estimation is refined over time and used to as an input for future sprint planning and to adjust project end date forecasting\\n\\nExample 2\\n\\nThe team uses a single indicator: \"does this story fits in one sprint?\", if not, the story needs to be split\\n\\nThe dev lead with support of the team roughly estimates how many stories can be done in the first sprints\\n\\nHow many stories are done in each sprint on average is used as an input for future sprint planning and as an indicator to adjust project end date forecasting\\n\\nExample 3\\n\\nThe team does planning poker and estimates in story points\\n\\nStory points are roughly used to estimate how much can be done in next sprint\\n\\nThe dev lead and the TPM uses the past sprints and observed velocity to adjust project end date forecasting\\n\\nOther considerations\\n\\nEstimating stories using story points in smaller project does not always provide the value it would in bigger ones.\\n\\nAvoid converting story points or t-shirt sizes to days.\\n\\nMeasure estimation accuracy:\\nCollect data to monitor estimation accuracy and sprint completion over time to drive improvements.\\nUse the sprint goal to understand if the estimation was correct. If the sprint goal is met: does anything else matter?', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_3', 'chunkContent': \"Scrum Practices: While Scrum does not prescribe how to size work, Professional Scrum is biased away from absolute estimation (hours, function points, ideal-days, etc.) and towards relative sizing.\\nPlanning Poker: is a collaborative technique to assign relative size. Developers may choose whatever units they want - story points and t-shirt sizes are examples of units.\\n'Same-Size' PBIs is a relative estimation approach that involves breaking items down small enough that they are roughly the same size. Velocity can be understood as a count of PBIs; this is sometimes used by teams doing continuously delivery.\\n'Right-Size' PBIs is a relative estimation approach that involves breaking things down small enough to deliver value in a certain time period (i.e. get to Done by the end of a Sprint). This is sometimes associated with teams utilizing flow for forecasting. Teams use historical data to determine if they think they can get the PBI done within the confidence level that their historical data says they typically get a PBI done.\\n\\nLinks:\\n\\nThe Most Important Thing You Are Missing about Estimation\\n\\nSprint planning\\n\\nExpectations:\\n\\nThe planning supports Diversity and Inclusion principles and provides equal opportunities.\\n\\nThe Planning defines how the work is going to be completed in the sprint.\\n\\nStories fit in a sprint and are designed and ready before the planning.\\n\\nSuggestions:\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_4', 'chunkContent': 'Stories fit in a sprint and are designed and ready before the planning.\\n\\nSuggestions:\\n\\nSprint goal:\\n\\nConsider defining a sprint goal, or list of goals for each sprint. Effective sprint goals are a concise bullet point list of items. A Sprint goal can be created first and used as an input to choose the Stories for the sprint. A sprint goal could also be created from the list of stories that were picked for the Sprint.\\n\\nThe sprint goal can be used :\\n\\nAt the end of each stand up meeting, to remember the north star for the Sprint and help everyone taking a step back\\n\\n*During the sprint review (\"was the goal achieved?\", \"If not, why?\")\\n\\nNote: A simple way to define a sprint goal, is to create a User Story in each sprint backlog and name it \"Sprint XX goal\". You can add the bullet points in the description.\\n\\nStories:\\n\\nExample 1 - Preparing in advance:\\n\\nThe dev lead and product owner plan time to prepare the sprint backlog ahead of sprint planning.\\n\\nThe dev lead uses their experience (past and on the current project) and the estimation made for these stories to gauge how many should be in the sprint.\\n\\nThe dev lead asks the entire team to look at the tentative sprint backlog in advance of the sprint planning.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_5', 'chunkContent': \"The dev lead asks the entire team to look at the tentative sprint backlog in advance of the sprint planning.\\n\\nThe dev lead assigns stories to specific developers after confirming with them that it makes sense\\n\\nDuring the sprint planning meeting, the team reviews the sprint goal and the stories. Everyone confirm they understand the plan and feel it's reasonable.\\n\\nExample 2 - Building during the planning meeting:\\n\\nThe product owner ensures that the highest priority items of the product backlog is refined and estimated following the team estimation process.\\n\\nDuring the Sprint planning meeting, the product owner describe each stories, one by one, starting by highest priority.\\n\\nFor each story, the dev lead and the team confirm they understand what needs to be done and add the story to the sprint backlog.\\n\\nThe team keeps considering more stories up to a point where they agree the sprint backlog is full. This should be informed by the estimation, past developer experience and past experience in this specific project.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_6', 'chunkContent': 'Stories are assigned during the planning meeting:\\nOption 1: The dev lead makes suggestion on who could work on each stories. Each engineer agrees or discuss if required.\\nOption 2: The team review each story and engineer volunteer select the one they want to be assigned to. (Note: this option might cause issues with the first core expectations. Who gets to work on what? Ultimately, it is the dev lead responsibility to ensure each engineer gets the opportunity to work on what makes sense for their growth.)\\n\\nTasks:\\n\\nExamples of approaches for task creation and assignment:\\n\\nStories are split into tasks ahead of time by dev lead and assigned before/during sprint planning to engineers.\\n\\nStories are assigned to more senior engineers who are responsible for splitting into tasks.\\n\\nStories are split into tasks during the Sprint planning meeting by the entire team.\\n\\nNote: Depending on the seniority of the team, consider splitting into tasks before sprint planning. This can help getting out of sprint planning with all work assigned. It also increase clarity for junior engineers.\\n\\nLinks:\\n\\nDefinition of Ready\\n\\nSprint Goal Template', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_7', 'chunkContent': 'Links:\\n\\nDefinition of Ready\\n\\nSprint Goal Template\\n\\nNotes: Self assignment by team members can give a feeling of fairness in how work is split in the team. Sometime, this ends up not being the case as it can give an advantage to the loudest or more experienced voices in the team. Individuals also tend to stay in their comfort zone, which might not be the right approach for their own growth.\\n\\nBacklog\\n\\nExpectations:\\n\\nUser stories have a clear acceptance criteria and definition of done.\\n\\nDesign activities are planned as part of the backlog (a design for a story that needs it should be done before it is added in a Sprint).\\n\\nSuggestions:\\n\\nConsider the backlog refinement as an ongoing activity, that expands outside of the typical \"Refinement meeting\".\\n\\nTechnical debt is mostly due to shortcuts made in the implementation as well as the future maintenance cost as the natural result of continuous improvement. Shortcuts should generally be avoided. In some rare instances where they happen, prioritizing and planning improvement activities to reduce this debt at a later time is the recommended approach.\\n\\nRetrospectives\\n\\nExpectations:\\n\\nRetrospectives lead to actionable items that help grow the team\\'s engineering practices. These items are in the backlog, assigned, and prioritized to be fixed by a date agreed upon (default being next retrospective).', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_8', 'chunkContent': 'Is used to ask the hard questions (\"we usually don\\'t finish what we plan, let\\'s talk about this\") when necessary.\\n\\nSuggestions:\\n\\nConsider other retro formats available outside of Mad Sad Glad.\\n\\nGather Data: Triple Nickels, Timeline, Mad Sad Glad, Team Radar\\n\\nGenerate Insights: 5 Whys, Fishbone, Patterns and Shifts\\n\\nConsider setting a retro focus area.\\n\\nSchedule enough time to ensure that you can have the conversation you need to get the correct plan an action and improve how you work.\\n\\nBring in a neutral facilitator for project retros or retros that introspect after a difficult period.\\n\\nUse the following retrospectives techniques to address specific trends that might be  emerging on an engagement:\\n\\n5 whys:\\n\\nIf a team is confronting a problem and is unsure of the exact root cause, the 5 whys exercise taken from the business analysis sector can help get to the bottom of it.\\xa0For example, if a team cannot get to Done each Sprint, that would go at the top of the whiteboard. The team then asks why that problem exists, writing that answer in the box below.\\xa0 Next, the team asks why again, but this time in response to the why they just identified. Continue this process until the team identifies an actual root cause, which usually becomes apparent within five steps.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_9', 'chunkContent': 'Processes, tools, individuals, interactions and the Definition of Done:\\n\\nThis approach encourages team members to think more broadly.\\xa0 Ask team members to identify what is going well and ideas for improvement within the categories of processes, tools, individuals/interactions, and the Definition of Done.\\xa0 Then, ask team members to vote on which improvement ideas to focus on during the upcoming Sprint.\\n\\nFocus:\\n\\nThis retrospective technique incorporates the concept of visioning. Using this technique, you ask team members where they would like to go?\\xa0 Decide what the team should look like in 4 weeks, and then ask what is holding them back from that and how they can resolve the impediment.\\xa0 If you are focusing on specific improvements, you can use this technique for one or two Retrospectives in a row so that the team can see progress over time.\\n\\nSprint Demo\\n\\nExpectations:\\n\\nEach sprint has demos that illustrate the sprint goal and how it fits in the engagement goal.\\n\\nSuggestions:\\n\\nConsider not pre-recording sprint demos in advance. You can record the demo meeting and archive them.\\n\\nA demo does not have to be about running code. It can be showing documentation that was written.\\n\\nStand-up\\n\\nExpectations:\\n\\nThe stand-up is run efficiently.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk185_10', 'chunkContent': 'Stand-up\\n\\nExpectations:\\n\\nThe stand-up is run efficiently.\\n\\nThe stand-up helps the team understand what was done, what will be done and what are the blockers.\\n\\nThe stand-up helps the team understand if they will meet the sprint goal or not.\\n\\nSuggestions:\\n\\nKeep stand up short and efficient. Table the longer conversations for a parking lot section, or for a conversation that will be planned later.\\n\\nRun daily stand ups: 15 minutes of stand up and 15 minutes of parking lot.\\n\\nIf someone cannot make the stand-up exceptionally: Ask them to do a written stand up in advance.\\n\\nStand ups should include everyone involved in the project, including the customer.\\n\\nProjects with widely divergent time zones should be avoided if possible, but if you are on one, you should adapt the standups to meet the needs and time constraints of all team members.\\n\\nDocumentation\\n\\nWhat Is Scrum?\\n\\nAgile Retrospective: Making Good Teams Great\\n\\nUser Stories Applied: For Software Development\\n\\nEssential Scrum: A Practical Guide to The Most Popular Agile Process', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\core-expectations\\\\README.md'}, {'chunkId': 'chunk186_0', 'chunkContent': 'Testing\\n\\nMap of Outcomes to Testing Techniques\\n\\nThe table below maps outcomes -- the results that you may want to achieve in your validation efforts -- to one or more techniques that can be used to accomplish that outcome.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\README.md'}, {'chunkId': 'chunk186_1', 'chunkContent': 'When I am working on... I want to get this outcome... ...so I should consider Development Prove backward compatibility with existing callers and clients Shadow testing Development Ensure program logic is correct for a variety of expected, mainline, edge and unexpected inputs Unit testing ; Functional tests; Consumer-driven Contract Testing ; Integration testing Development Prevent regressions in logical correctness; earlier is better Unit testing ; Functional tests; Consumer-driven Contract Testing ; Integration testing ; Rings (each of these are expanding scopes of coverage) Development Validate interactions between components in isolation, ensuring that consumer and provider components are compatible and conform to a shared understanding documented in a contract Consumer-driven Contract Testing Development; Integration testing Validate that multiple components function together across multiple interfaces in a call chain, incl network hops Integration testing ; End-to-end ( End-to-End testing ) tests; Segmented end-to-end ( End-to-End testing ) Development Prove disaster recoverability – recover from corruption of data DR drills Development Find vulnerabilities in service Authentication or Authorization Scenario (security) Development Prove implementation correctness in advance of a dependency or absent a dependency Unit testing (with mocks); Unit testing (with emulators); Consumer-driven Contract Testing Development Ensure that the user interface is accessible Accessibility Development Ensure that users can operate the interface UI testing (automated) (human usability observation) Development Prevent regression in user experience UI automation; End-to-End testing Development Detect and prevent', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\README.md'}, {'chunkId': 'chunk186_2', 'chunkContent': 'testing (automated) (human usability observation) Development Prevent regression in user experience UI automation; End-to-End testing Development Detect and prevent \\'noisy neighbor\\' phenomena Load testing Development Detect availability drops Synthetic Transaction testing ; Outside-in probes Development Prevent regression in \\'composite\\' scenario use cases / workflows (e.g. an e-commerce system might have many APIs that used together in a sequence perform a \"shop-and-buy\" scenario) End-to-End testing ; Scenario Development; Operations Prevent regressions in runtime performance metrics e.g. latency / cost / resource consumption; earlier is better Rings; Synthetic Transaction testing / Transaction; Rollback Watchdogs Development; Optimization Compare any given metric between 2 candidate implementations or variations in functionality Flighting; A/B testing Development; Staging Prove production system of provisioned capacity meets goals for reliability, availability, resource consumption, performance Load testing (stress) ; Spike; Soak; Performance testing Development; Staging Understand key user experience performance characteristics – latency, chattiness, resiliency to network errors Load; Performance testing ; Scenario (network partitioning) Development; Staging; Operation Discover melt points (the loads at which failure or maximum tolerable resource consumption occurs) for each individual component in the stack Squeeze; Load testing (stress) Development; Staging; Operation Discover overall system melt point (the loads at which the end-to-end system', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\README.md'}, {'chunkId': 'chunk186_3', 'chunkContent': 'Load testing (stress) Development; Staging; Operation Discover overall system melt point (the loads at which the end-to-end system fails) and which component is the weakest link in the whole stack Squeeze; Load testing (stress) Development; Staging; Operation Measure capacity limits for given provisioning to predict or satisfy future provisioning needs Squeeze; Load testing (stress) Development; Staging; Operation Create / exercise failover runbook Failover drills Development; Staging; Operation Prove disaster recoverability – loss of data center (the meteor scenario); measure MTTR DR drills Development; Staging; Operation Understand whether observability dashboards are correct, and telemetry is complete; flowing Trace Validation; Load testing (stress) ; Scenario; End-to-End testing Development; Staging; Operation Measure impact of seasonality of traffic Load testing Development; Staging; Operation Prove Transaction and alerts correctly notify / take action Synthetic Transaction testing (negative cases); Load testing Development; Staging; Operation; Optimizing Understand scalability curve, i.e. how the system consumes resources with load Load testing (stress) ; Performance testing Operation; Optimizing Discover system behavior over long-haul time Soak Optimizing Find cost savings opportunities Squeeze Staging; Operation Measure impact of failover / scale-out (repartitioning, increasing provisioning) / scale-down Failover drills; Scale drills Staging; Operation', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\README.md'}, {'chunkId': 'chunk186_4', 'chunkContent': 'failover / scale-out (repartitioning, increasing provisioning) / scale-down Failover drills; Scale drills Staging; Operation Create/Exercise runbook for increasing/reducing provisioning Scale drills Staging; Operation Measure behavior under rapid changes in traffic Spike Staging; Optimizing Discover cost metrics per unit load volume (what factors influence cost at what load points, e.g. cost per million concurrent users) Load (stress) Development; Operation Discover points where a system is not resilient to unpredictable yet inevitable failures (network outage, hardware failure, VM host servicing, rack/switch failures, random acts of the Malevolent Divine, solar flares, sharks that eat undersea cable relays, cosmic radiation, power outages, renegade backhoe operators, wolves chewing on junction boxes, …) Chaos Development Perform unit testing on Power platform custom connectors Custom Connector Testing', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\README.md'}, {'chunkId': 'chunk186_5', 'chunkContent': 'Sections within Testing\\n\\nConsumer-driven contract (CDC) testing\\n\\nEnd-to-End testing\\n\\nFault Injection testing\\n\\nIntegration testing\\n\\nPerformance testing\\n\\nShadow testing\\n\\nSmoke testing\\n\\nSynthetic Transaction testing\\n\\nUI testing\\n\\nUnit testing\\n\\nTechnology Specific Testing\\n\\nUsing DevTest Pattern for building containers with AzDO\\n\\nUsing Azurite to run blob storage tests in pipeline\\n\\nBuild for Testing\\n\\nTesting is a critical part of the development process.  It is important to build your application with testing in mind.  Here are some tips to help you build for testing:\\n\\nParameterize everything. Rather than hard-code any variables, consider making everything a configurable parameter with a reasonable default. This will allow you to easily change the behavior of your application during testing. Particularly during performance testing, it is common to test different values to see what impact that has on performance. If a range of defaults need to change together, consider one or more parameters which set \"modes\", changing the defaults of a group of parameters together.\\n\\nDocument at startup. When your application starts up, it should log all parameters. This ensures the person reviewing the logs and application behavior know exactly how the application is configured.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\README.md'}, {'chunkId': 'chunk186_6', 'chunkContent': 'Log to console. Logging to external systems like Azure Monitor is desirable for traceability across services. This requires logs to be dispatched from the local system to the external system and that is a dependency that can fail. It is important that someone be able to console logs directly on the local system.\\n\\nLog to external system. In addition to console logs, logging to an external system like Azure Monitor is desirable for traceability across services and durability of logs.\\n\\nLog all activity. If the system is performing some activity (reading data from a database, calling an external service, etc.), it should log that activity. Ideally, there should be a log message saying the activity is starting and another log message saying the activity is complete. This allows someone reviewing the logs to understand what the application is doing and how long it is taking. Depending on how noisy this is, different messages can be associated with different log levels, but it is important to have the information available when it comes to debugging a deployed system.\\n\\nCorrelate distributed activities. If the system is performing some activity that is distributed across multiple systems, it is important to correlate the activity across those systems. This can be done using a Correlation ID that is passed from system to system. This allows someone reviewing the logs to understand the entire flow of activity. For more information, please see Observability in Microservices.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\README.md'}, {'chunkId': 'chunk186_7', 'chunkContent': 'Log metadata. When logging, it is important to include metadata that is relevant to the activity. For example, a Tenant ID, Customer ID, or Order ID. This allows someone reviewing the logs to understand the context of the activity and filter to a manageable set of logs.\\n\\nLog performance metrics. Even if you are using App Insights to capture how long dependency calls are taking, it is often useful to know long certain functions of your application took. It then becomes possible to evaluate the performance characteristics of your application as it is deployed on different compute platforms with different limitations on CPU, memory, and network bandwidth. For more information, please see Metrics.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\README.md'}, {'chunkId': 'chunk187_0', 'chunkContent': 'Consumer-driven Contract Testing (CDC)\\n\\nConsumer-driven Contract Testing (or CDC for short) is a software testing methodology used to test components of a system in isolation while ensuring that provider components are compatible with the expectations that consumer components have of them.\\n\\nWhy Consumer-driven Contract Testing\\n\\nCDC tries to overcome the several painful drawbacks of automated E2E tests with components interacting together:\\n\\nE2E tests are slow\\n\\nE2E tests break easily\\n\\nE2E tests are expensive and hard to maintain\\n\\nE2E tests of larger systems may be hard or impossible to run outside a dedicated testing environment\\n\\nAlthough testing best practices suggest to write just a few E2E tests compared to the cheaper, faster and more stable integration and unit tests as pictured in the testing pyramid below, experience shows many teams end up writing too many E2E tests. A reason for this is that E2E tests give developers the highest confidence to release as they are testing the \"real\" system.\\n\\nCDC addresses these issues by testing interactions between components in isolation using mocks that conform to a shared understanding documented in a \"contract\". Contracts are agreed between consumer and provider, and are regularly verified against a real instance of the provider component. This effectively partitions a larger system into smaller pieces that can be tested individually in isolation of each other, leading to simpler, fast and stable tests that also give confidence to release.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\cdc-testing\\\\README.md'}, {'chunkId': 'chunk187_1', 'chunkContent': \"Some E2E tests are still required to verify the system as a whole when deployed in the real environment, but most functional interactions between components can be covered with CDC tests.\\n\\nCDC testing was initially developed for testing RESTful API's, but the pattern scales to all consumer-provider systems and tooling for other messaging protocols besides HTTP does exist.\\n\\nConsumer-driven Contract Testing Design Blocks\\n\\nIn a consumer-driven approach the consumer drives changes to contracts between a consumer (the client) and a provider (the server). This may sound counterintuitive, but it helps providers create APIs that fit the real requirements of the consumers rather than trying to guess these in advance. Next we describe the CDC building blocks ordered by their occurrence in the development cycle.\\n\\nConsumer Tests with Provider Mock\\n\\nThe consumers start by creating integration tests against a provider mock and running them as part of their CI pipeline. Expected responses are defined in the provider mock for requests fired from the tests. Through this, the consumer essentially defines the contract they expect the provider to fulfill.\\n\\nContract\\n\\nContracts are generated from the expectations defined in the provider mock as a result of a successful test run. CDC frameworks like Pact provide a specification for contracts in json format consisting of the list of request/responses generated from the consumer tests plus some additional metadata.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\cdc-testing\\\\README.md'}, {'chunkId': 'chunk187_2', 'chunkContent': 'Contracts are not a replacement for a discussion between the consumer and provider team. This is the moment where this discussion should take place (if not already done before). The consumer tests and generated contract are refined with the feedback and cooperation of the provider team. Lastly the finalized contract is versioned and stored in a central place accessible by both consumer and provider.\\n\\nContracts are complementary to API specification documents like OpenAPI. API specifications describe the structure and the format of the API. A contract instead specifies that for a given request, a given response is expected. An API specifications document is helpful in writing an API contract and can be used to validate that the contract conforms to the API specification.\\n\\nProvider Contract Verification\\n\\nOn the provider side tests are also executed as part of a separate pipeline which verifies contracts against real responses of the provider. Contract verification fails if real responses differ from the expected responses as specified in the contract. The cause of this can be:\\n\\nInvalid expectations on the consumer side leading to incompatibility with the current provider implementation\\n\\nBroken provider implementation due to some missing functionality or a regression\\n\\nEither way, thanks to CDC it is easy to pinpoint integration issues down to the consumer/provider of the affected interaction. This is a big advantage compared to the debugging pain this could have been with an E2E test approach.\\n\\nCDC Testing Frameworks and Tools', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\cdc-testing\\\\README.md'}, {'chunkId': 'chunk187_3', 'chunkContent': 'CDC Testing Frameworks and Tools\\n\\nPact is an implementation of CDC testing that allows mocking of responses in the consumer codebase, and verification of the interactions in the provider codebase, while defining a specification for contracts. It was originally written in Ruby but has available wrappers for multiple languages. Pact is the de-facto standard to use when working with CDC.\\n\\nSpring Cloud Contract is an implementation of CDC testing from Spring, and offers easy integration in the Spring ecosystem. Support for non-Spring and non-JVM providers and consumers also exists.\\n\\nConclusion\\n\\nCDC has several benefits that make it an approach worth considering when dealing with systems composed of multiple components interacting together.\\n\\nMaintenance efforts can be reduced by testing consumer-provider interactions in isolation without the need of a complex integrated environment, specially as the interactions between components grow in number and become more complex.\\n\\nAdditionally, a close collaboration between consumer and provider teams is strongly encouraged through the CDC development process, which can bring many other benefits. Contracts offer a formal way to document the shared understanding how components interact with each other, and serve as a base for the communication between teams. In a way, the contract repository serves as a live documentation of all consumer-provider interactions of a system.\\n\\nCDC has some drawbacks as well. An extra layer of testing is added requiring a proper investment in education for team members to understand and use CDC correctly.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\cdc-testing\\\\README.md'}, {'chunkId': 'chunk187_4', 'chunkContent': \"Additionally, the CDC test scope should be considered carefully to prevent blurring CDC with other higher level functional testing layers. Contract tests are not the place to verify internal business logic and correctness of the consumer.\\n\\nResources\\n\\nTesting pyramid from Kent C. Dodd's blog\\n\\nPact, a code-first consumer-driven contract testing tool with support for several different programming languages\\n\\nConsumer-driven contracts from Ian Robinson\\n\\nContract test from Martin Fowler\\n\\nA simple example of using Pact consumer-driven contract testing in a Java client-server application\\n\\nPact dotnet workshop\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\cdc-testing\\\\README.md'}, {'chunkId': 'chunk188_0', 'chunkContent': \"E2E Testing\\n\\nEnd-to-end (E2E) testing is a Software testing methodology to test a functional and data application flow consisting of several sub-systems working together from  start to end.\\n\\nAt times, these systems are developed in different technologies by different teams or organizations. Finally, they come together to form a functional business application.  Hence, testing a single system would not suffice. Therefore, end-to-end testing verifies the application from start to end putting all its components together.\\n\\nWhy E2E Testing [The Why]\\n\\nIn many commercial software application scenarios, a modern software system consists of its interconnection with multiple sub-systems. These sub-systems can be within the same organization or can be components of different organizations. Also, these sub-systems can have somewhat similar or different lifetime release cycle from the current system. As a result, if there is any failure or fault in any sub-system, it can adversely affect the whole software system leading to its collapse.\\n\\nThe above illustration is a testing pyramid from Kent C. Dodd's blog which is a combination of the pyramids from Martin Fowler’s blog and the Google Testing Blog.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\README.md'}, {'chunkId': 'chunk188_1', 'chunkContent': \"The majority of your tests are at the bottom of the pyramid. As you move up the pyramid, the number of tests gets smaller. Also, going up the pyramid, tests get slower and more expensive to write, run, and maintain. Each type of testing vary for its purpose, application and the areas it's supposed to cover. For more information on comparison analysis of different testing types, please see this ## Unit vs Integration vs System vs E2E Testing document.\\n\\nE2E Testing Design Blocks [The What]\\n\\nWe will look into all the 3 categories one by one:\\n\\nUser Functions\\n\\nFollowing actions should be performed as a part of building user functions:\\n\\nList user initiated functions of the software systems, and their interconnected sub-systems.\\n\\nFor any function, keep track of the actions performed as well as Input and Output data.\\n\\nFind the relations, if any between different Users functions.\\n\\nFind out the nature of different user functions i.e. if they are independent or are reusable.\\n\\nConditions\\n\\nFollowing activities should be performed as a part of building conditions based on user functions:\\n\\nFor each and every user functions, a set of conditions should be prepared.\\n\\nTiming, data conditions and other factors that affect user functions can be considered as parameters.\\n\\nTest Cases\\n\\nFollowing factors should be considered for building test cases:\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\README.md'}, {'chunkId': 'chunk188_2', 'chunkContent': 'Test Cases\\n\\nFollowing factors should be considered for building test cases:\\n\\nFor every scenario, one or more test cases should be created to test each and every functionality of the user functions. If possible, these test cases should be automated through the standard CI/CD build pipeline processes with the track of each successful and failed build in AzDO.\\n\\nEvery single condition should be enlisted as a separate test case.\\n\\nApplying the E2E testing [The How]\\n\\nLike any other testing, E2E testing also goes through formal planning, test execution, and closure phases.\\n\\nE2E testing is done with the following steps:\\n\\nPlanning\\n\\nBusiness and Functional Requirement analysis\\n\\nTest plan development\\n\\nTest case development\\n\\nProduction like Environment setup for the testing\\n\\nTest data setup\\n\\nDecide exit criteria\\n\\nChoose the testing methods that most applicable to your system. For the definition of the various testing methods, please see Testing Methods document.\\n\\nPre-requisite\\n\\nSystem Testing should be complete for all the participating systems.\\n\\nAll subsystems should be combined to work as a complete application.\\n\\nProduction like test environment should be ready.\\n\\nTest Execution\\n\\nExecute the test cases\\n\\nRegister the test results and decide on pass and failure\\n\\nReport the Bugs in the bug reporting tool\\n\\nRe-verify the bug fixes\\n\\nTest closure\\n\\nTest report preparation', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\README.md'}, {'chunkId': 'chunk188_3', 'chunkContent': 'Report the Bugs in the bug reporting tool\\n\\nRe-verify the bug fixes\\n\\nTest closure\\n\\nTest report preparation\\n\\nEvaluation of exit criteria\\n\\nTest phase closure\\n\\nTest Metrics\\n\\nThe tracing the quality metrics gives insight about the current status of testing. Some common metrics of E2E testing are:\\n\\nTest case preparation status: Number of test cases ready versus the total number of test cases.\\n\\nFrequent Test progress: Number of test cases executed in the consistent frequent manner, e.g. weekly, versus a target number of the test cases in the same time period.\\n\\nDefects Status: This metric represents the status of the defects found during testing. Defects should be logged into defect tracking tool (e.g. AzDO backlog) and resolved as per their severity and priority. Therefore, the percentage of open and closed defects as per their severity and priority should be calculated to track this metric. The AzDO Dashboard Query can be used to track this metric.\\n\\nTest environment availability: This metric tracks the duration of the test environment used for end-to-end testing versus its scheduled allocation duration.\\n\\nE2E Testing Frameworks and Tools\\n\\n1. Gauge Framework\\n\\nGauge is a free and open source framework for writing and running E2E tests. Some key features of Gauge that makes it unique include:', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\README.md'}, {'chunkId': 'chunk188_4', 'chunkContent': 'Simple, flexible and rich syntax based on Markdown.\\n\\nConsistent cross-platform/language support for writing test code.\\n\\nA modular architecture with plugins support.\\n\\nSupports data driven execution and external data sources.\\n\\nHelps you create maintainable test suites.\\n\\nSupports Visual Studio Code, Intellij IDEA, IDE Support.\\n\\nSupports html, json and XML reporting.\\n\\nGauge Framework Website\\n\\n2. Robot Framework\\n\\nRobot Framework is a generic open source automation framework. The framework has easy syntax, utilizing human-readable keywords. Its capabilities can be extended by libraries implemented with Python or Java.\\n\\nRobot shares a lot of the same \"pros\" as Gauge, except the developer tooling and the syntax. In our usage, we found the VS Code Intellisense offered with Gauge to be much more stable than the offerings for Robot. We also found the syntax to be less readable than what Gauge offered. While both frameworks allow for markup based test case definitions, the Gauge syntax reads much more like an English sentence than Robot. Finally, Intellisense is baked into the markup files for Gauge test cases, which will create a function stub for the actual test definition if the developer allows it. The same cannot be said of the Robot Framework.\\n\\nRobot Framework Website\\n\\n3. TestCraft', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\README.md'}, {'chunkId': 'chunk188_5', 'chunkContent': 'Robot Framework Website\\n\\n3. TestCraft\\n\\nTestCraft is a codeless Selenium test automation platform. Its revolutionary AI technology and unique visual modeling allow for faster test creation and execution while eliminating test maintenance overhead.\\n\\nThe testers create fully automated test scenarios without coding. Customers find bugs faster, release more frequently, integrate with the CI/CD approach and improve the overall quality of their digital products. This all creates a complete end-to-end testing experience.\\n\\nPerfecto (TestCraft) Website or get it  from the Visual Studio Marketplace\\n\\n4. Ranorex Studio\\n\\nRanorex Studio is a complete end-to-end test automation tool for desktop, web, and mobile applications. Create reliable tests fast without any coding at all, or using the full IDE. Use external CSV or Excel files, or a SQL database as inputs to your tests.\\n\\nRun tests in parallel or on a Selenium Grid with built-in Selenium WebDriver. Ranorex Studio integrates with your CI/CD process to shorten your release cycles without sacrificing quality.\\n\\nRanorex Studio tests also integrate with Azure DevOps (AzDO), which can be run as part of a build pipeline in AzDO.\\n\\nRanorex Studio Website\\n\\n5. Katalon Studio', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\README.md'}, {'chunkId': 'chunk188_6', 'chunkContent': 'Ranorex Studio Website\\n\\n5. Katalon Studio\\n\\nKatalon Studio is an excellent end-to-end automation solution for web, API, mobile, and desktop testing with DevOps support.\\n\\nWith Katalon Studio, automated testing can be easily integrated into any CI/CD pipeline to release products faster while guaranteeing high quality. Katalon Studio customizes for users from beginners to experts. Robust functions such as Spying, Recording, Dual-editor interface and Custom Keywords make setting up, creating and maintaining tests possible for users.\\n\\nBuilt on top of Selenium and Appium, Katalon Studio helps standardize your end-to-end tests standardized. It also complies with the most popular frameworks to work seamlessly with other tools in the automated testing ecosystem.\\n\\nKatalon is endorsed by Gartner, IT professionals, and a large testing community.\\n\\nNote: At the time of this writing, Katalon Studio extension for AzDO was NOT available for Linux.\\n\\nKatalon Studio Website or read about its integration with AzDO\\n\\n6. BugBug.io', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\README.md'}, {'chunkId': 'chunk188_7', 'chunkContent': \"Katalon Studio Website or read about its integration with AzDO\\n\\n6. BugBug.io\\n\\nBugBug is an easy way to automate tests for web applications. The tool focuses on simplicity, yet allows you to cover all essential test cases without coding. It's an all-in-one solution - you can easily create tests and use the built-in cloud to run them on schedule or from your CI/CD, without changes to your own infrastructure.\\n\\nBugBug is an interesting alternative to Selenium because it's actually a completely different technology. It is based on a Chrome extension that allows BugBug to record and run tests faster than old-school frameworks.\\n\\nThe biggest advantage of BugBug is its user-friendliness. Most tests created with BugBug simply work out of the box. This makes it easier for non-technical people to maintain tests - with BugBug you can save money on hiring a QA engineer.\\n\\nBugBug Website\\n\\nConclusion\\n\\nHope you learned various aspects of E2E testing like its processes, metrics, the difference between Unit, Integration  and E2E testing, and the various recommended E2E test frameworks and tools.\\n\\nFor any commercial release of the software, E2E test verification plays an important role as it tests the entire application in an environment that exactly imitates real-world users like network communication, middleware and backend services interaction, etc.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\README.md'}, {'chunkId': 'chunk188_8', 'chunkContent': 'Finally, the E2E test is often performed manually as the cost of automating such test cases is too high to be afforded by any organization. Having said that, the ultimate goal of each organization is to make the e2e testing as streamlined as possible adding full and semi-automation testing components into the process. Hence, the various E2E testing frameworks and tools listed in this article come to the rescue.\\n\\nResources\\n\\nWikipedia: Software testing\\n\\nWikipedia: Unit testing\\n\\nWikipedia: Integration testing\\n\\nWikipedia: System testing', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\README.md'}, {'chunkId': 'chunk189_0', 'chunkContent': 'Unit vs Integration vs System vs E2E Testing\\n\\nThe table below illustrates the most critical characteristics and differences among Unit, Integration, System, and End-to-End Testing, and when to apply each methodology in a project.\\n\\nUnit Test Integration Test System Testing E2E Test Scope Modules, APIs Modules, interfaces Application, system All sub-systems, network dependencies, services and databases Size Tiny Small to medium Large X-Large Environment Development Integration test QA test Production like Data Mock data Test data Test data Copy of real production data System Under Test Isolated unit test Interfaces and flow data between the modules Particular system as a whole Application flow from start to end Scenarios Developer perspectives Developers and IT Pro tester perspectives Developer and QA tester perspectives End-user perspectives When After each build After Unit testing Before E2E testing and after Unit and Integration testing After System testing Automated or Manual Automated Manual or automated Manual or automated Manual', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\testing-comparison.md'}, {'chunkId': 'chunk190_0', 'chunkContent': 'E2E Testing Methods\\n\\nHorizontal Test\\n\\nThis method is used very commonly. It occurs horizontally across the context of multiple applications. Take an example of a data ingest management system.\\n\\nThe inbound data may be  injected from various sources, but it then \"flatten\" into a horizontal processing pipeline that may include various components, such as a gateway API, data transformation, data validation, storage, etc... Throughout the entire Extract-Transform-Load (ETL) processing, the data flow can be tracked and monitored under the horizontal spectrum with little sprinkles of optional, and thus not important for the overall E2E test case, services, like logging, auditing, authentication.\\n\\nVertical Test\\n\\nIn this method, all most critical transactions of any application are verified and evaluated right from the start to finish. Each individual layer of the application is tested starting from top to bottom. Take an example of a web-based application that uses middleware services for reaching back-end resources.\\n\\nIn such case, each layer (tier) is required to be fully tested in conjunction with the \"connected\" layers above and beneath, in which services \"talk\" to each other during the end to end data flow. All these complex testing scenarios will require proper validation and dedicated automated testing. Thus, this method is much more difficult.\\n\\nE2E Test Cases Design Guidelines', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\testing-methods.md'}, {'chunkId': 'chunk190_1', 'chunkContent': 'E2E Test Cases Design Guidelines\\n\\nBelow enlisted are few guidelines that should be kept in mind while designing the test cases for performing E2E testing:\\n\\nTest cases should be designed from the end user’s perspective.\\n\\nShould focus on testing some existing features of the system.\\n\\nMultiple scenarios should be considered for creating multiple test cases.\\n\\nDifferent sets of test cases should be created to focus on multiple scenarios of the system.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\testing-methods.md'}, {'chunkId': 'chunk191_0', 'chunkContent': 'Gauge Framework\\n\\nGauge is a free and open source framework for writing and running E2E tests. Some key features of Gauge that makes it unique include:\\n\\nSimple, flexible and rich syntax based on Markdown.\\n\\nConsistent cross-platform/language support for writing test code.\\n\\nA modular architecture with plugins support\\n\\nExtensible through plugins and hackable.\\n\\nSupports data driven execution and external data sources\\n\\nHelps you create maintainable test suites\\n\\nSupports Visual Studio Code, Intellij IDEA, IDE Support\\n\\nWhat is a Specification\\n\\nGauge specifications are written using a Markdown syntax. For example\\n\\n{% raw %}\\n\\n```bash\\n\\nSearch for the data blob\\n\\nLook for file\\n\\nGoto Azure blob', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk191_1', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nIn this specification Search for the data blob is the specification heading, Look for file is a scenario with a step Goto Azure blob\\n\\nWhat is an Implementation\\n\\nYou can implement the steps in a specification using a programming language, for example:\\n\\n{% raw %}\\n\\nbash\\nfrom getgauge.python import step\\nimport os\\nfrom step_impl.utils.driver import Driver\\n@step(\"Goto Azure blob\")\\ndef gotoAzureStorage():\\n  URL = os.getenv(\\'STORAGE_ENDPOINT\\')\\n  Driver.driver.get(URL)\\n\\n{% endraw %}\\n\\nThe Gauge runner reads and runs steps and its implementation for every scenario in the specification and generates a report of passing or failing scenarios.\\n\\n{% raw %}\\n\\n```bash\\n\\nSearch for the data blob\\n\\nLook for file  ✔\\n\\nSuccessfully generated html-report to => reports/html-report/index.html\\nSpecifications:       1 executed      1 passed        0 failed        0 skipped\\nScenarios:    1 executed      1 passed        0 failed        0 skipped', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk191_2', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nRe-using Steps\\n\\nGauge helps you focus on testing the flow of an application. Gauge does this by making steps as re-usable as possible. With Gauge, you don’t need to build custom frameworks using a programming language.\\n\\nFor example, Gauge steps can pass parameters to an implementation by using a text with quotes.\\n\\n{% raw %}\\n\\n```bash\\n\\nSearch for the data blob\\n\\nLook for file\\n\\nGoto Azure blob\\n\\nSearch for \"store_data.csv\"\\n```\\n\\n{% endraw %}\\n\\nThe implementation can now use “store_data.csv” as follows\\n\\n{% raw %}\\n\\n```bash\\nfrom getgauge.python import step\\nimport os\\n@step(\"Search for \")\\ndef searchForQuery(query):\\n  write(query)\\n  press(\"Enter\")\\n\\nstep(\"Search for \", (query) => {\\n  write(query);\\n  press(\"Enter\");', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk191_3', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nYou can then re-use this step within or across scenarios with different parameters:\\n\\n{% raw %}\\n\\n```bash\\n\\nSearch for the data blob\\n\\nLook for Store data #1\\n\\nGoto Azure blob\\n\\nSearch for \"store_1.csv\"\\n\\nLook for Store data #2\\n\\nGoto Azure blob\\n\\nSearch for \"store_2.csv\"\\n```\\n\\n{% endraw %}\\n\\nOr combine more than one step into concepts\\n\\n{% raw %}\\n\\n```bash\\n\\nSearch Azure Storage for\\n\\nGoto Azure blob\\n\\nSearch for \"store_1.csv\"\\n```\\n\\n{% endraw %}\\n\\nThe concept, Search Azure Storage for <query> can be used like a step in a specification\\n\\n{% raw %}\\n\\n```bash\\n\\nSearch for the data blob\\n\\nLook for Store data #1\\n\\nSearch Azure Storage for \"store_1.csv\"\\n\\nLook for Store data #2\\n\\nSearch Azure Storage for \"store_2.csv\"', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk191_4', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nData-Driven Testing\\n\\nGauge also supports data driven testing using Markdown tables as well as external csv files for example\\n\\n{% raw %}\\n\\n```bash\\n\\nSearch for the data blob\\n\\nquery store_1 store_2 store_3\\n\\nLook for stores data\\n\\nSearch Azure Storage for', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk191_5', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nThis will execute the scenario for all rows in the table.\\n\\nIn the examples above, we refactored a specification to be concise and flexible without changing the implementation.\\n\\nOther Features\\n\\nThis is brief introduction to a few Gauge features. Please refer to the Gauge documentation for additional features such as:\\n\\nReports\\n\\nTags\\n\\nParallel execution\\n\\nEnvironments\\n\\nScreenshots\\n\\nPlugins\\n\\nAnd much more\\n\\nInstalling Gauge\\n\\nThis getting started guide takes you through the core features of Gauge. By the end of this guide, you’ll be able to install Gauge and learn how to create your first Gauge test automation project.\\n\\nInstallation Instructions for Windows OS\\n\\nStep 1: Installing Gauge on Windows\\n\\nThis section gives specific instructions on setting up Gauge in a Microsoft Windows environment.\\nDownload the following installation bundle to get the latest stable release of Gauge.\\n\\nStep 2: Installing Gauge extension for Visual Studio Code\\n\\nFollow the steps to add the Gauge Visual Studio Code plugin from the IDE\\n\\nInstall the following Gauge extension for Visual Studio Code.\\n\\nTroubleshooting Installation\\n\\nIf, when you run your first gauge spec you receive the error of missing python packages, open the command line terminal window and run this command:\\n\\n{% raw %}', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk191_6', 'chunkContent': '{% raw %}\\n\\nbash\\npython.exe -m pip install getgauge==0.3.7 --user\\n\\n{% endraw %}\\n\\nInstallation Instructions for macOS\\n\\nStep 1: Installing Gauge on macOS\\n\\nThis section gives specific instructions on setting up Gauge in a macOS environment.\\n\\nInstall brew if you haven’t already: Go to the brew website, and follow the directions there.\\n\\nRun the brew command to install Gauge\\n\\n{% raw %}\\n\\n```bash\\n\\nbrew install gauge', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk191_7', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nif HomeBrew is working properly, you should see something similar to the following:\\n\\n{% raw %}\\n\\n```bash\\n==> Fetching gauge\\n==> Downloading https://ghcr.io/v2/homebrew/core/gauge/manifests/1.4.3\\n\\n################################################################## 100.0%\\n\\n==> Downloading https://ghcr.io/v2/homebrew/core/gauge/blobs/sha256:05117bb3c0b2efeafe41e817cd3ad86307c1d2ea7e0e835655c4b51ab2472893\\n==> Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sha256:05117bb3c0b2efeafe41e817cd3ad86307c1d2ea7e0e835655c4b51ab2472893?se=2022-12-13T12%3A35%3A00Z&sig=I78SuuwNgSMFoBTT\\n\\n################################################################## 100.0%', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk191_8', 'chunkContent': '################################################################## 100.0%\\n\\n==> Pouring gauge--1.4.3.ventura.bottle.tar.gz\\n    /usr/local/Cellar/gauge/1.4.3: 6 files, 18.9MB', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk191_9', 'chunkContent': '```\\n\\n{% endraw %}\\n\\nStep 2 : Installing Gauge extension for Visual Studio Code\\n\\nFollow the steps to add the Gauge Visual Studio Code plugin from the IDE\\n\\nInstall the following Gauge extension for Visual Studio Code.\\n\\nPost-Installation Troubleshooting\\n\\nIf, when you run your first gauge spec you receive the error of missing python packages, open the command line terminal window and run this command:\\n\\n{% raw %}\\n\\nbash\\npython.exe -m pip install getgauge==0.3.7 --user\\n\\n{% endraw %}', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\gauge-framework.md'}, {'chunkId': 'chunk192_0', 'chunkContent': \"Postman Testing\\n\\nThis purpose of this document is to provide guidance on how to use Newman in your CI/CD pipeline to run End-to-end (E2E) tests defined in Postman Collections while following security best practices.\\n\\nFirst, we'll introduce Postman and Newman and then outline several Postman testing use cases that answer why you may want to go beyond local testing with Postman Collections.\\n\\nIn the final use case, we are looking to use a shell script that references the Postman Collection file path and Environment file path as inputs to Newman. Below is a flow diagram representing the outcome of the final use case:\\n\\nPostman and Newman\\n\\nPostman is a free API platform for testing APIs. Key features highlighted in this guidance include:\\n\\nPostman Collections\\n\\nPostman Environment Files\\n\\nPostman Scripts\\n\\nNewman is a command-line Collection Runner for Postman. It enables you to run and test a Postman Collection directly from the command line. Key features highlighted in this guidance include:\\n\\nNewman Run Command\\n\\nWhat is a Collection\\n\\nA Postman Collection is a group of executable saved requests. A collection can be exported as a json file.\\n\\nWhat is an Environment File\\n\\nA Postman Environment file holds environment variables that can be referenced by a valid Postman Collection.\\n\\nWhat is a Postman Script\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_1', 'chunkContent': 'A Postman Environment file holds environment variables that can be referenced by a valid Postman Collection.\\n\\nWhat is a Postman Script\\n\\nA Postman Script is Javascript hosted within a Postman Collection that can be written to execute against your Postman Collection and Environment File.\\n\\nWhat is the Newman Run Command\\n\\nA Newman CLI command that allows you to specify a Postman Collection to be run.\\n\\nInstalling Postman and Newman\\n\\nFor specific instruction on installing Postman, visit the Downloads Postman page.\\n\\nFor specific instruction on installing Newman, visit the NPMJS Newman package page.\\n\\nImplementing Automated End-to-end (E2E) Tests With Postman Collections\\n\\nIn order to provide guidance on implementing automated E2E tests with Postman, the section below begins with a use case that explains the trade-offs a dev or QA analyst might face when intending to use Postman for early testing. Each use case represents scenarios that facilitate the end goal of automated E2E tests.\\n\\nUse Case - Hands-on Functional Testing Of Endpoints', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_2', 'chunkContent': \"Use Case - Hands-on Functional Testing Of Endpoints\\n\\nA developer or QA analyst would like to locally test input data against API services all sharing a common oauth2 token. As a result, they use Postman to craft an API test suite of Postman Collections that can be locally executed against individual endpoints across environments. After validating that their Postman Collection works, they share it with their team.\\n\\nSteps may look like the following:\\n\\nFor each of your existing API services, use the Postman IDE's import feature to import its OpenAPI Spec (Swagger) as a Postman Collection.\\nIf a service is not already using Swagger, look for language specific guidance on how to use Swagger to generate an OpenAPI Spec for your service. Finally, if your service only has a few endpoints, read Postman docs for guidance on how to manually build a Postman Collection.\\n\\nProvide extra clarity about a request in a Postman Collection by using Postman's Example feature to save its responses as examples. You can also simply add an example manually. Please read Postman docs for guidance on how to specify examples.\\n\\nCombine each Postman Collection into a centralized Postman Collection.\\n\\nBuild Postman Environment files (local, Dev and/or QA) and parameterize all saved requests of the Postman Collection in a way that references the Postman Environment files.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_3', 'chunkContent': \"Use the Postman Script feature to create a shared prefetch script that automatically refreshes expired auth tokens per saved request. This would require referencing secrets from a Postman Environment file.\\n{% raw %}\\n```javascript\\n// Please treat this as pseudocode, and adjust as necessary.\\n/ The request to an oauth2 authorization endpoint that will issue a token \\nbased on provided credentials./\\nconst oauth2Request = POST {...};\\nvar getToken = true;\\nif (pm.environment.get('ACCESS_TOKEN_EXPIRY') <= (new Date()).getTime()) {\\n    console.log('Token is expired')\\n} else {\\n    getToken = false;\\n    console.log('Token and expiry date are all good');\\n}\\nif (getToken === true) {\\n    pm.sendRequest(oauth2Request, function (_, res) {\\n            console.log('Save the token')\\n            var responseJson = res.json();\\n            pm.environment.set('token', responseJson.access_token)\\n            console.log('Save the expiry date')\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_4', 'chunkContent': \"console.log('Save the expiry date')\\n            var expiryDate = new Date();\\n            expiryDate.setSeconds(expiryDate.getSeconds() + responseJson.expires_in);\\n            pm.environment.set('ACCESS_TOKEN_EXPIRY', expiryDate.getTime());\\n    });\\n}\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_5', 'chunkContent': \"```\\n{% endraw %}\\n\\nUse Postman IDE to exercise endpoints.\\n\\nExport collection and environment files then remove any secrets before committing to your repo.\\n\\nStarting with this approach has the following upsides:\\n\\nYou've set yourself up for the beginning stages of an E2E postman collection by aggregating the collections into a single file and using environment files to make it easier to switch environments.\\n\\nToken is refreshed automatically on every call in the collection. This saves you time normally lost from manually having to request a token that expired.\\n\\nGrants QA/Dev granular control of submitting combinations of input data per endpoint.\\n\\nGrants developers a common experience via Postman IDE features.\\n\\nEnding with this approach has the following downsides:\\n\\nPromotes unsafe sharing of secrets. Credentials needed to request JWT token in the prefetch script are being manually shared.\\n\\nSecrets may happen to get exposed in the git commit history for various reasons (ex. Sharing the exported Postman Environment files).\\n\\nCollections can only be used locally to hit APIs (local or deployed). Not CI based.\\n\\nEach developer has to keep both their Postman Collection and Postman environment file(s) updated in order to keep up with latest changes to deployed services.\\n\\nUse Case - Hands-on Functional Testing Of Endpoints with Azure Key Vault and Azure App Config\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_6', 'chunkContent': 'Use Case - Hands-on Functional Testing Of Endpoints with Azure Key Vault and Azure App Config\\n\\nA developer or QA analyst may have an existing API test suite of Postman Collections, however, they now want to discourage unsafe sharing of secrets. As a result, they build a script that connects to both Key Vault and Azure App Config in order to automatically generate Postman Environment files instead of checking them into a shared repository.\\n\\nSteps may look like the following:\\n\\nCreate an Azure Key Vault and store authentication secrets per environment:\\n\"Key:value\" (ex. \"dev-auth-password:12345\")\\n\"Key:value\" (ex. \"qa-auth-password:12345\")\\n\\nCreate a shared Azure App Configuration instance and save all your Postman environment variables. This instance will be dedicated to holding all your Postman environment variables:\\n    > NOTE: Use the Label feature to delineate between environments.\\n\"Key:value\" -> \"apiRoute:url\" (ex. \"servicename:https://servicename.net\" & Label = \"QA\")\\n\"Key:value\" -> \"Header:value\"(ex. \"token: \" & Label = \"QA\")\\n\"Key:value\" -> \"KeyVaultKey:KeyVaultSecret\" (ex. \"authpassword:qa-auth-password\" & Label = \"QA\")', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_7', 'chunkContent': 'Install Powershell or Bash. Powershell works for both Azure Powershell and Azure CLI.\\n\\nDownload Azure CLI, login to the appropriate subscription and ensure you have access to the appropriate resources. Some helpful commands are below:\\n{% raw %}\\n```powershell\\nlogin to the appropriate subscription\\naz login\\nvalidate login\\naz account show\\nvalidate access to Key Vault\\naz keyvault secret list --vault-name \"$KeyvaultName\"\\nvalidate access to App Configuration\\naz appconfig kv list --name \"$AppConfigName\"', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_8', 'chunkContent': \"```\\n{% endraw %}\\n\\nBuild a script that automatically generates your environment files.\\n    > NOTE: App Configuration references Key Vault, however, your script is responsible for authenticating properly to both App Configuration and Key Vault. The two services don't communicate directly.\\n{% raw %}\\n```powershell (CreatePostmanEnvironmentFiles.ps1)\\nPlease treat this as pseudocode, and adjust as necessary.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_9', 'chunkContent': 'env = $arg1\\n1. list app config vars for an environment\\nenvVars = az appconfig kv list --name PostmanAppConfig --label $env | ConvertFrom-Json\\n2. step through envVars array to get Key Vault uris\\nkeyvaultURI = \"\"\\n$envVars | % {if($.key -eq \\'password\\'){keyvaultURI = $.value}} \\n3. parse uris for Key Vault name and secret names\\n4. get secret from Key Vault\\nkvsecret = az keyvault secret show --name $secretName --vault-name $keyvaultName --query \"value\"\\n5. set password value to returned Key Vault secret\\n$envVars | % {if($.key -eq \\'password\\'){$.value=$kvsecret}}  \\n6. create environment file\\nenvFile = @{ \"_postman_variable_scope\" = \"environment\", \"name\" = $env, values = @() }\\nforeach($var in $envVars){\\n        $envFile.values += @{ key = $var.key; value = $var.value; }\\n}\\n$envFile | ConvertTo-Json -depth 50 | Out-File -encoding ASCII -FilePath .\\\\$env.postman_environment.json', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_10', 'chunkContent': \"```\\n{% endraw %}\\n\\nUse Postman IDE to import the Postman Environment files to be referenced by your collection.\\n\\nThis approach has the following upsides:\\n\\nInherits all the upsides of the previous case.\\n\\nDiscourages unsafe sharing of secrets. Secrets are now pulled from Key Vault via Azure CLI. Key Vault Uri also no longer needs to be shared for access to auth tokens.\\n\\nSingle source of truth for Postman Environment files. There's no longer a need to share them via repo.\\n\\nDeveloper only has to manage a single Postman Collection.\\n\\nEnding with this approach has the following downsides:\\n\\nSecrets may happen to get exposed in the git commit history if .gitIgnore is not updated to ignore Postman Environment files.\\n\\nCollections can only be used locally to hit APIs (local or deployed). Not CI based.\\n\\nUse Case - E2E testing With Continuous Integration and Newman\\n\\nA developer or QA analyst may have an existing API test suite of local Postman Collections that follow security best practices for development, however, they now want E2E tests to run as part of automated CI pipeline. With the advent of Newman, you can now more readily use Postman to craft an API test suite executable in your CI.\\n\\nSteps may look like the following:\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_11', 'chunkContent': 'Steps may look like the following:\\n\\nUpdate your Postman Collection to use the Postman Test feature in order to craft test assertions that will cover all saved requests E2E. Read Postman docs for guidance on how to use the Postman Test feature.\\n\\nLocally use Newman to validate tests are working as intended\\n{% raw %}\\npowershell\\nnewman run tests\\\\e2e_Postman_collection.json -e qa.postman_environment.json\\n{% endraw %}\\n\\nBuild a script that automatically executes Postman Test assertions via Newman and Azure CLI.\\n    > NOTE: An Azure Service Principal must be setup to continue using azure cli in this CI pipeline example.\\n{% raw %}\\n```powershell (RunPostmanE2eTests.ps1)\\nPlease treat this as pseudocode, and adjust as necessary.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_12', 'chunkContent': '1. login to Azure using a Service Principal\\naz login --service-principal -u $APP_ID -p $AZURE_SECRET --tenant $AZURE_TENANT\\n2. list app config vars for an environment\\nenvVars = az appconfig kv list --name PostmanAppConfig --label $env | ConvertFrom-Json\\n3. step through envVars array to get Key Vault uris\\nkeyvaultURI = \"\"\\n@envVars | % {if($.key -eq \\'password\\'){keyvaultURI = $.value}} \\n4. parse uris for Key Vault name and secret names\\n5. get secret from Key Vault\\nkvsecret = az keyvault secret show --name $secretName --vault-name $keyvaultName --query \"value\"\\n6. set password value to returned Key Vault secret\\n$envVars | % {if($.key -eq \\'password\\'){$.value=$kvsecret}}  \\n7. create environment file\\nenvFile = @{ \"_postman_variable_scope\" = \"environment\", \"name\" = $env, values = @() }\\nforeach($var in $envVars){\\n        $envFile.values += @{ key = $var.key; value = $var.value; }', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_13', 'chunkContent': '}\\n$envFile | ConvertTo-Json -depth 50 | Out-File -encoding ASCII $env.postman_environment.json\\n8. install Newman\\nnpm install --save-dev newman\\n9. run automated E2E tests via Newman\\nnode_modules.bin\\\\newman run tests\\\\e2e_Postman_collection.json -e $env.postman_environment.json', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk192_14', 'chunkContent': \"```\\n{% endraw %}\\n\\nCreate a yaml file and define a step that will run your test script. (ex. A yaml file targeting Azure Devops that runs a Powershell script.)\\n{% raw %}\\n```yaml\\nPlease treat this as pseudocode, and adjust as necessary.\\n\\ndisplayName: 'Run Postman E2E tests'\\ninputs:\\n    targetType: 'filePath'\\n    filePath: RunPostmanE2eTests.ps1\\nenv:\\n    APP_ID: $(environment.appId) # credentials for az cli\\n    AZURE_SECRET: $(environment.secret)\\n    AZURE_TENANT: $(environment.tenant)\\n```\\n{% endraw %}\\n\\nThis approach has the following upside:\\n\\nE2E tests can now be run automatically as part of a CI pipeline.\\n\\nEnding with this approach has the following downside:\\n\\nPostman Environment files are no longer being output to a local environment for hands-on manual testing. However, this can be solved by managing 2 scripts.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\postman-testing.md'}, {'chunkId': 'chunk193_0', 'chunkContent': 'Templates\\n\\nGauge Framework\\n\\nPostman', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\e2e-testing\\\\recipes\\\\README.md'}, {'chunkId': 'chunk194_0', 'chunkContent': 'Fault Injection Testing\\n\\nFault injection testing is the deliberate introduction of errors and faults to a system to validate and harden its stability and reliability. The goal is to improve the system\\'s design for resiliency and performance under intermittent failure conditions over time.\\n\\nWhen To Use\\n\\nProblem Addressed\\n\\nSystems need to be resilient to the conditions that caused inevitable production disruptions. Modern applications are built with an increasing number of dependencies; on infrastructure, platform, network, 3rd party software or APIs, etc. Such systems increase the risk of impact from dependency disruptions. Each dependent component may fail. Furthermore, its interactions with other components may propagate the failure.\\n\\nFault injection methods are a way to increase coverage and validate software robustness and error handling, either at build-time or at run-time, with the intention of \"embracing failure\" as part of the development lifecycle. These methods assist engineering teams in designing and continuously validating for failure, accounting for known and unknown failure conditions, architect for redundancy, employ retry and back-off mechanisms, etc.\\n\\nApplicable to\\n\\nSoftware - Error handling code paths, in-process memory management.\\n\\nExample tests: Edge-case unit/integration tests and/or load tests (i.e. stress and soak).\\n\\nProtocol - Vulnerabilities in communication interfaces such as command line parameters or APIs.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\fault-injection-testing\\\\README.md'}, {'chunkId': 'chunk194_1', 'chunkContent': 'Protocol - Vulnerabilities in communication interfaces such as command line parameters or APIs.\\n\\nExample tests: Fuzzing provides invalid, unexpected, or random data as input we can assess the level of protocol stability of a component.\\n\\nInfrastructure - Outages, networking issues, hardware failures.\\n\\nExample tests: Using different methods to cause fault in the underlying infrastructure such as Shut down virtual machine (VM) instances, crash processes, expire certificates, introduce network latency, etc. This level of testing relies on statistical metrics observations over time and measuring the deviations of its observed behavior during fault, or its recovery time.\\n\\nHow to Use\\n\\nArchitecture\\n\\nTerminology\\n\\nFault - The adjudged or hypothesized cause of an error.\\n\\nError - That part of the system state that may cause a subsequent failure.\\n\\nFailure - An event that occurs when the delivered service deviates from correct state.\\n\\nFault-Error-Failure cycle - A key mechanism in dependability: A fault may cause an error. An error may cause further errors within the system boundary; therefore each new error acts as a fault. When error states are observed at the system boundary, they are termed failures.\\n\\nFault Injection Testing Basics', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\fault-injection-testing\\\\README.md'}, {'chunkId': 'chunk194_2', 'chunkContent': 'Fault Injection Testing Basics\\n\\nFault injection is an advanced form of testing where the system is subjected to different failure modes, and where the testing engineer may know in advance what is the expected outcome, as in the case of release validation tests, or in an exploration to find potential issues in the product, which should be mitigated.\\n\\nFault Injection and Chaos Engineering\\n\\nFault injection testing is a specific approach to testing one condition. It introduces a failure into a system to validate its robustness. Chaos engineering, coined by Netflix, is a practice for generating new information. There is an overlap in concerns and often in tooling between the terms, and many times chaos engineering uses fault injection to introduce the required effects to the system.\\n\\nHigh-level Step-by-step\\n\\nFault injection testing in the development cycle\\n\\nFault injection is an effective way to find security bugs in software, so much so that the Microsoft Security Development Lifecycle requires fuzzing at every untrusted interface of every product and penetration testing which includes introducing faults to the system, to uncover potential vulnerabilities resulting from coding errors, system configuration faults, or other operational deployment weaknesses.\\n\\nAutomated fault injection coverage in a CI pipeline promotes a Shift-Left approach of testing earlier in the lifecycle for potential issues.\\nExamples of performing fault injection during the development lifecycle:\\n\\nUsing fuzzing tools in CI.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\fault-injection-testing\\\\README.md'}, {'chunkId': 'chunk194_3', 'chunkContent': \"Using fuzzing tools in CI.\\n\\nExecute existing end-to-end scenario tests (such as integration or stress tests), which are augmented with fault injection.\\n\\nWrite regression and acceptance tests based on issues that were found and fixed or based on resolved service incidents.\\n\\nAd-hoc (manual) validations of fault in the dev environment for new features.\\n\\nFault injection testing in the release cycle\\n\\nMuch like Synthetic Monitoring Tests, fault injection testing in the release cycle is a part of Shift-Right testing approach, which uses safe methods to perform tests in a production or pre-production environment. Given the nature of distributed, cloud-based applications, it is very difficult to simulate the real behavior of services outside their production environment. Testers are encouraged to run tests where it really matters, on a live system with customer traffic.\\n\\nFault injection tests rely on metrics observability and are usually statistical; The following high-level steps provide a sample of practicing fault injection and chaos engineering:\\n\\nMeasure and define a steady (healthy) state for the system's interoperability.\\n\\nCreate hypotheses based on predicted behavior when a fault is introduced.\\n\\nIntroduce real-world fault-events to the system.\\n\\nMeasure the state and compare it to the baseline state.\\n\\nDocument the process and the observations.\\n\\nIdentify and act on the result.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\fault-injection-testing\\\\README.md'}, {'chunkId': 'chunk194_4', 'chunkContent': 'Document the process and the observations.\\n\\nIdentify and act on the result.\\n\\nFault injection testing in kubernetes\\n\\nWith the advancement of kubernetes (k8s) as the infrastructure platform, fault injection testing in kubernetes has become inevitable to ensure that system behaves in a reliable manner in the event of a fault or failure. There could be different type of workloads running within a k8s cluster which are written in different languages. For eg. within a K8s cluster, you can run a micro service, a web app and/or a scheduled job. Hence you need to have mechanism to inject fault into any kind of workloads running within the cluster. In addition, kubernetes clusters are managed differently from traditional infrastructure. The tools used for fault injection testing within kubernetes should have compatibility with k8s infrastructure. These are the main characteristics which are required:\\n\\nEase of injecting fault into kubernetes pods.\\n\\nSupport for faster tool installation within the cluster.\\n\\nSupport for YAML based configurations which works well with kubernetes.\\n\\nEase of customization to add custom resources.\\n\\nSupport for workflows to deploy various workloads and faults.\\n\\nEase of maintainability of the tool\\n\\nEase of integration with telemetry\\n\\nBest Practices and Advice', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\fault-injection-testing\\\\README.md'}, {'chunkId': 'chunk194_5', 'chunkContent': 'Ease of maintainability of the tool\\n\\nEase of integration with telemetry\\n\\nBest Practices and Advice\\n\\nExperimenting in production has the benefit of running tests against a live system with real user traffic, ensuring its health, or building confidence in its ability to handle errors gracefully. However, it has the potential to cause unnecessary customer pain.\\nA test can either succeed or fail. In the event of failure, there will likely be some impact on the production environment. Thinking about the Blast Radius of the effect, should the test fail, is a crucial step to conduct beforehand. The following practices may help minimize such risk:\\n\\nRun tests in a non-production environment first. Understand how the system behaves in a safe environment, using synthetic workload, before introducing potential risk to customer traffic.\\n\\nUse fault injection as gates in different stages through the CD pipeline.\\n\\nDeploy and test on Blue/Green and Canary deployments. Use methods such as traffic shadowing (a.k.a. Dark Traffic) to get customer traffic to the staging slot.\\n\\nStrive to achieve a balance between collecting actual result data while affecting as few production users as possible.\\n\\nUse defensive design principles such as circuit breaking and the bulkhead patterns.\\n\\nAgreed on a budget (in terms of Service Level Objective (SLO)) as an investment in chaos and fault injection.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\fault-injection-testing\\\\README.md'}, {'chunkId': 'chunk194_6', 'chunkContent': \"Agreed on a budget (in terms of Service Level Objective (SLO)) as an investment in chaos and fault injection.\\n\\nGrow the risk incrementally - Start with hardening the core and expand out in layers. At each point, progress should be locked in with automated regression tests.\\n\\nFault Injection Testing Frameworks and Tools\\n\\nFuzzing\\n\\nOneFuzz - is a Microsoft open-source self-hosted fuzzing-as-a-service platform which is easy to integrate into CI pipelines.\\n\\nAFL and WinAFL - Popular fuzz tools by Google's project zero team which is used locally to target binaries on Linux or Windows.\\n\\nWebScarab - A web-focused fuzzer owned by OWASP which can be found in Kali linux distributions.\\n\\nChaos\\n\\nAzure Chaos Studio - An in-preview tool for orchestrating controlled fault injection experiments on Azure resources.\\n\\nChaos toolkit - A declarative, modular chaos platform with many extensions, including the Azure actions and probes kit.\\n\\nKraken - An Openshift-specific chaos tool, maintained by Redhat.\\n\\nChaos Monkey - The Netflix platform which popularized chaos engineering (doesn't support Azure OOTB).\\n\\nSimmy - A .NET library for chaos testing and fault injection integrated with the Polly library for resilience engineering.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\fault-injection-testing\\\\README.md'}, {'chunkId': 'chunk194_7', 'chunkContent': 'Simmy - A .NET library for chaos testing and fault injection integrated with the Polly library for resilience engineering.\\n\\nLitmus - A CNCF open source tool for chaos testing and fault injection for kubernetes cluster.\\n\\nThis ISE dev blog post provides code snippets as an example of how to use Polly and Simmy to implement a hypothesis-driven approach to resilience and chaos testing.\\n\\nConclusion\\n\\nFrom the principals of chaos: \"The harder it is to disrupt the steady-state, the more confidence we have in the behavior of the system. If a weakness is uncovered, we now have a target for improvement before that behavior manifests in the system at large\".\\n\\nFault injection techniques increase resilience and confidence in the products we ship. They are used across the industry to validate applications and platforms before and while they are delivered to customers.\\nFault injection is a powerful tool and should be used with caution. Cases such as the Cloudflare 30 minute global outage, which was caused due to a deployment of code that was meant to be “dark launched”, entail the importance of curtailing the blast radius in the system during experiments.\\n\\nResources\\n\\nMark Russinovich\\'s fault injection and chaos engineering blog post\\n\\nCindy Sridharan\\'s Testing in production blog post\\n\\nCindy Sridharan\\'s Testing in production blog post cont.\\n\\nFault injection in Azure Search', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\fault-injection-testing\\\\README.md'}, {'chunkId': 'chunk194_8', 'chunkContent': \"Cindy Sridharan's Testing in production blog post cont.\\n\\nFault injection in Azure Search\\n\\nAzure Architecture Framework - Chaos engineering\\n\\nAzure Architecture Framework - Testing resilience\\n\\nLandscape of Software Failure Cause Models\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\fault-injection-testing\\\\README.md'}, {'chunkId': 'chunk195_0', 'chunkContent': 'Integration Testing\\n\\nIntegration testing is a software testing methodology used to determine how well individually developed components, or modules of a system communicate with each other. This method of testing confirms that an aggregate of a system, or sub-system, works together correctly or otherwise exposes erroneous behavior between two or more units of code.\\n\\nWhy Integration Testing\\n\\nBecause one component of a system may be developed independently or in isolation of another it is important to verify the interaction of some or all components. A complex system may be composed of databases, APIs, interfaces, and more, that all interact with each other or additional external systems. Integration tests expose system-level issues such as broken database schemas or faulty third-party API integration. It ensures higher test coverage and serves as an important feedback loop throughout development.\\n\\nIntegration Testing Design Blocks\\n\\nConsider a banking application with three modules: login, transfers, and current balance, all developed independently. An integration test may verify when a user logs in they are re-directed to their current balance with the correct amount for the specific mock user. Another integration test may perform a transfer of a specified amount of money. The test may confirm there are sufficient funds in the account to perform the transfer, and after the transfer the current balance is updated appropriately for the mock user. The login page may be mocked with a test user and mock credentials if this module is not completed when testing the transfers module.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\integration-testing\\\\README.md'}, {'chunkId': 'chunk195_1', 'chunkContent': 'Integration testing is done by the developer or QA tester. In the past, integration testing always happened after unit and before system and E2E testing. Compared to unit-tests, integration tests are fewer in quantity, usually run slower, and are more expensive to set up and develop. Now, if a team is following agile principles, integration tests can be performed before or after unit tests, early and often, as there is no need to wait for sequential processes. Additionally, integration tests can utilize mock data in order to simulate a complete system. There is an abundance of language-specific testing frameworks that can be used throughout the entire development lifecycle.\\n\\n** It is important to note the difference between integration and acceptance testing. Integration testing confirms a group of components work together as intended from a technical perspective, while acceptance testing confirms a group of components work together as intended from a business scenario.\\n\\nApplying Integration Testing\\n\\nPrior to writing integration tests, the engineers must identify the different components of the system, and their intended behaviors and inputs and outputs. The architecture of the project must be fully documented or specified somewhere that can be readily referenced (e.g., the architecture diagram).\\n\\nThere are two main techniques for integration testing.\\n\\nBig Bang', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\integration-testing\\\\README.md'}, {'chunkId': 'chunk195_2', 'chunkContent': 'There are two main techniques for integration testing.\\n\\nBig Bang\\n\\nBig Bang integration testing is when all components are tested as a single unit. This is best for small system as a system too large may be difficult to localize for potential errors from failed tests. This approach also requires all components in the system under test to be completed which may delay when testing begins.\\n\\nIncremental Testing\\n\\nIncremental testing is when two or more components that are logically related are tested as a unit. After testing the unit, additional components are combined and tested all together. This process repeats until all necessary components are tested.\\n\\nTop Down\\n\\nTop down testing is when higher level components are tested following the control flow of a software system. In the scenario, what is commonly referred to as stubs are used to emulate the behavior of lower level modules not yet complete or merged in the integration test.\\n\\nBottom Up\\n\\nBottom up testing is when lower level modules are tested together. In the scenario, what is commonly referred to as drivers are used to emulate the behavior of higher level modules not yet complete or included in the integration test.\\n\\nA third approach known as the sandwich or hybrid model combines the bottom up and town down approaches to test lower and higher level components at the same time.\\n\\nThings to Avoid', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\integration-testing\\\\README.md'}, {'chunkId': 'chunk195_3', 'chunkContent': 'Things to Avoid\\n\\nThere is a tradeoff a developer must make between integration test code coverage and engineering cycles. With mock dependencies, test data, and multiple environments at test, too many integration tests are infeasible to maintain and become increasingly less meaningful. Too much mocking will slow down the test suite, make scaling difficult, and may be a sign the developer should consider other tests for the scenario such as acceptance or E2E.\\n\\nIntegration tests of complex systems require high maintenance. Avoid testing business logic in integration tests by keeping test suites separate. Do not test beyond the acceptance criteria of the task and be sure to clean up any resources created for a given test. Additionally, avoid writing tests in a production environment. Instead, write them in a scaled-down copy environment.\\n\\nIntegration Testing Frameworks and Tools\\n\\nMany tools and frameworks can be used to write both unit and integration tests. The following tools are for automating integration tests.\\n\\nJUnit\\n\\nRobot Framework\\n\\nmoq\\n\\nCucumber\\n\\nSelenium\\n\\nBehave (Python)\\n\\nConclusion', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\integration-testing\\\\README.md'}, {'chunkId': 'chunk195_4', 'chunkContent': 'Robot Framework\\n\\nmoq\\n\\nCucumber\\n\\nSelenium\\n\\nBehave (Python)\\n\\nConclusion\\n\\nIntegration testing demonstrates how one module of a system, or external system, interfaces with another. This can be a test of two components, a sub-system, a whole system, or a collection of systems. Tests should be written frequently and throughout the entire development lifecycle using an appropriate amount of mocked dependencies and test data. Because integration tests prove that independently developed modules interface as technically designed, it increases confidence in the development cycle providing a path for a system that deploys and scales.\\n\\nResources\\n\\nIntegration testing approaches\\n\\nIntegration testing pros and cons\\n\\nIntegration tests mocks and stubs\\n\\nSoftware Testing: Principles and Practices\\n\\nIntegration testing Behave test quick start', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\integration-testing\\\\README.md'}, {'chunkId': 'chunk196_0', 'chunkContent': 'Performance Test Iteration Template\\n\\nThis document provides template for capturing results of performance tests. Performance tests are done in iterations and each iteration should have a clear goal. The results of any iteration is immutable regardless whether the goal was achieved or not. If the iteration failed or the goal is not achieved then a new iteration of testing is carried out with appropriate fixes. It is recommended to keep track of the recorded iterations to maintain a timeline of how system evolved and which changes affected the performance in what way. Feel free to modify this template as needed.\\n\\nIteration Template\\n\\nGoal\\n\\nMention in bullet points the goal for this iteration of test. The goal should be small and measurable within this iteration.\\n\\nTest Details\\n\\nDate: Date and time when this iteration started and ended\\n\\nDuration: Time it took to complete this iteration.\\n\\nApplication Code: Commit id and link to the commit for the code(s) which are being tested in this iteration\\n\\nBenchmarking Configuration:\\n\\nApplication Configuration: In bullet points mention the configuration for application that should be recorded\\n\\nSystem Configuration: In bullet points mention the configuration of the infrastructure\\n\\nRecord different types of configurations. Usually application specific configuration changes between iterations whereas system or infrastructure configurations rarely change\\n\\nWork Items\\n\\nList of links to relevant work items (task, story, bug) being tested in this iteration.\\n\\nResults\\n\\n{% raw %}', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\iterative-perf-test-template.md'}, {'chunkId': 'chunk196_1', 'chunkContent': 'Results\\n\\n{% raw %}\\n\\nmd\\nIn bullet points document the results from the test.  \\n- Attach any documents supporting the test results.\\n- Add links to the dashboard for metrics and logs such as Application Insights.\\n- Capture screenshots for metrics and include it in the results. Good candidate for this is CPU/Memory/Disk usage.\\n\\n{% endraw %}\\n\\nObservations\\n\\nObservations are insights derived from test results. Keep the observations brief and as bullet points. Mention outcomes supporting the goal of the iteration. If any of the observation results in a work item (task, story, bug) then add the link to the work item together with the observation.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\iterative-perf-test-template.md'}, {'chunkId': 'chunk197_0', 'chunkContent': 'Load Testing\\n\\n\"Load testing is performed to determine a system\\'s behavior under both normal and anticipated peak load conditions.\" - Load testing - Wikipedia\\n\\nA load test is designed to determine how a system behaves under expected normal and peak workloads. Specifically its main purpose is to confirm if a system can handle the expected load level. Depending on the target system this could be concurrent users, requests per second or data size.\\n\\nWhy Load Testing\\n\\nThe main objective is to prove the system can behave normally under the expected normal load before releasing it to production. The criteria that define \"behave normally\" will depend on your target, this may be as simple as \"the system remains available\", but it could also include meeting a response time SLA or error rate.\\n\\nAdditionally, the results of a load test can also be used as data to help with capacity planning and calculating scalability.\\n\\nLoad Testing Design Blocks\\n\\nThere are a number of basic components that are required to carry out a load test.\\n\\nIn order to have meaningful results the system needs to be tested in a production-like environment with a network and hardware which closely resembles the expected deployment environment.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk197_1', 'chunkContent': 'The load test will consist of a module which simulates user activity. Of course the composition of this \"user activity\" will vary based on the type of application being tested. For example, an e-commerce website might simulate user browsing and purchasing items, but an IoT data ingestion pipeline would simulate a stream of device readings. Please ensure the simulation is as close to real activity as possible, and consider not just volume but also patterns and variability. For example, if the simulator data is too uniform or predictable, then cache/hit ratios may impact your results.\\n\\nThe load test will be initiated from a component external to the target system which can control the amount of load applied. This can be a single agent, but may need to scaled to multiple agents in order to achieve higher levels of activity.\\n\\nAlthough not required to run a load test, it is advisable to have monitoring and/or logging in place to be able to measure the impact of the test and discover potential bottlenecks.\\n\\nApplying the Load Testing\\n\\nPlanning\\n\\nIdentify key scenarios to measure - Gather these scenarios from Product Owner, they should provide a representative sample of real world traffic. The key activity of this phase is to agree on and define the load test cases.\\n\\nDetermine expected normal and peak load for the scenarios - Determine a load level such as concurrent users or requests per second to find the size of the load test you will run.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk197_2', 'chunkContent': 'Identify success criteria metrics - These may be on testing side such as response time and error rate, or they may be on the system side such as CPU and memory usage.\\n\\nAgree on test matrix - Which load test cases should be run for which combinations of input parameters.\\n\\nSelect the right tool - Many frameworks exist for load testing so consider if features and limitations are suitable for your needs (Some popular tools are listed below). This may also include development of a custom load test client, see Preparation phase below.\\n\\nObservability - Determine which metrics need to gathered to gain insight into throughput, latency, resource utilization, etc.\\n\\nScalability - Determine the amount of scale needed by load generator, workload application, CPU, Memory, and network components needed to achieve testing goals. The use of kubernetes on the cloud can be used to make testing infinitely scalable.\\n\\nPreparation\\n\\nThe key activity is to replace the end user client with a test bench that simulates one or more instances of the original client. For standard 3rd party tools it may suffice to configure the existing test UI before initiating the load tests.\\n\\nIf a custom client is used, code development will be required:', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk197_3', 'chunkContent': 'If a custom client is used, code development will be required:\\n\\nCustom development - Design for minimal impact/overhead. Be sure to capture only those features of the production client that are relevant from a load perspective. Does it matter if the same test is duplicated, or must the workload be unique for each test? Can all tests be run under the same user context?\\n\\nTest environment - Create test environment that resembles production environment. This includes the platform as well as external systems, e.g., data sources.\\n\\nSecurity contexts - Be sure to have all requisite security contexts for the test environment. Automation like pipelines may require special setup, e.g., OAuth2 client credential flow instead of auth code flow, because interactive login is replaced by non-interactive. Allow planning leeway in case admin approval is required for new security contexts.\\n\\nTest data strategy - Make sure that output data format (ascii/binary/...) is compatible with whatever analysis tool is used in the analysis phase. This also includes storage areas (local/cloud/...), which may trigger new security contexts. Bear in mind that it may be necessary to collect data from sources external to the application to correlate potential performance issues with the application behavior. This includes platform and network metrics. Make sure to collect data that covers analysis needs (statistical measures, distributions, graphs, etc.).', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk197_4', 'chunkContent': 'Automation - Repeatability is critical. It must be possible to re-run a given test multiple times to verify consistency and resilience of the application itself and the underlying platform.  Pipelines are recommended whenever possible.\\nEvaluate whether load tests should be run as part of the PR strategy.\\n\\nTest client debugging - All test modules should be carefully debugged to ensure that the execution phase progresses smoothly.\\n\\nTest client validation - All test modules should be validated for extreme values of the input parameters. This reduces the risk of running into unexpected difficulties when stepping through the full test matrix during the execution phase.\\n\\nExecution\\n\\nIt is recommended to use an existing testing framework (see below). These tools will provide a method of both specifying the user activity scenarios and how to execute those at load. Depending on the situation, it may be advisable to coordinate testing activities with the platform operations team.\\n\\nIt is common to slowly ramp up to your desired load to better replicate real world behavior. Once you have reached your defined workload, maintain this level long enough to see if your system stabilizes. To finish up the test you should also ramp to see record how the system slows down as well.\\n\\nYou should also consider the origin of your load test traffic. Depending on the scope of the target system you may want to initiate from a different location to better replicate real world traffic such as from a different region.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk197_5', 'chunkContent': 'Note: Before starting please be aware of any restrictions on your network such as DDOS protection where you may need to notify a network administrator or apply for an exemption.\\n\\nNote: In general, the preferred approach to load testing would be the usage of a standard test framework such as the ones discussed below.  There are cases, however, where a custom test client may be advantageous. Examples include batch oriented workloads that can be run under a single security context and the same test data can be re-used for multiple load tests.  In such a scenario it may be beneficial to develop a custom script that can be used interactively as well as non-interactively.\\n\\nAnalysis\\n\\nThe analysis phase represents the work that brings all previous activities together:\\n\\nSet aside time to allow for collection of new test data based on the analysis of the load tests.\\n\\nCorrelate application metrics and platform metrics to identify potential pitfalls and bottlenecks.\\n\\nInclude business stakeholders early in the analysis phase to validate application findings. Include platform operations to validate platform findings.\\n\\nReport writing\\n\\nSummarize your findings from the analysis phase. Be sure to include application and platform enhancement suggestions, if any.\\n\\nFurther Testing\\n\\nAfter completing your load test you should be set up to continue on to additional related testing such as;', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk197_6', 'chunkContent': 'Further Testing\\n\\nAfter completing your load test you should be set up to continue on to additional related testing such as;\\n\\nSoak Testing - Also known as Endurance Testing. Performing a load test over an extended period of time to ensure long term stability.\\n\\nStress Testing - Gradually increasing the load to find the limits of the system and identify the maximum capacity.\\n\\nSpike Testing - Introduce a sharp short-term increase into the load scenarios.\\n\\nScalability Testing - Re-testing of a system as your expand horizontally or vertically to measure how it scales.\\n\\nDistributed Testing - Distributed testing allows you to leverage the power of multiple machines to perform larger or more in-depth tests faster. Is necessary when a fully optimized node cannot produce the load required by your extremely large test.\\n\\nLoad Generation Testing Frameworks and Tools\\n\\nHere are a few popular load testing frameworks you may consider, and the languages used to define your scenarios.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk197_7', 'chunkContent': 'Here are a few popular load testing frameworks you may consider, and the languages used to define your scenarios.\\n\\nAzure Load Testing (https://learn.microsoft.com/en-us/azure/load-testing/) - Managed platform for running load tests on Azure. It allows to run and monitor tests automatically, source secrets from the KeyVault, generate traffic at scale, and load test Azure private endpoints. In the simple case, it executes load tests with HTTP GET traffic to a given endpoint. For the more complex cases, you can upload your own JMeter scenarios.\\n\\nJMeter (https://github.com/apache/jmeter) - Has built in patterns to test without coding, but can be extended with Java.\\n\\nArtillery (https://artillery.io/) - Write your scenarios in Javascript, executes a node application.\\n\\nGatling (https://gatling.io/) -  Write your scenarios in Scala with their DSL.\\n\\nLocust (https://locust.io/) - Write your scenarios in Python using the concept of concurrent user activity.\\n\\nK6 (https://k6.io/) - Write your test scenarios in Javascript, available as open source kubernetes operator, open source Docker image, or as SaaS. Particularly useful for distributed load testing. Integrates easily with prometheus.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk197_8', 'chunkContent': 'NBomber (https://nbomber.com/) - Write your test scenarios in C# or F#, available integration with test runners (NUnit/xUnit).\\n\\nWebValidate (https://github.com/microsoft/webvalidate) - Web request validation tool used to run end-to-end tests and long-running performance and availability tests.\\n\\nSample Workload Applications\\n\\nIn the case where a specific workload application is not being provided and the focus is instead on the system, here are a few popular sample workload applications you may consider.\\n\\nHttpBin (Python, GoLang) - Supports variety of endpoint types and language implementations. Can echo data used in request.\\n\\nNGSA (Java, C#) - Intended for Kubernetes Platform and Monitoring Testing. Built on top of IMDB data store with many CRUD endpoints available. Does not need to have a live database connection.\\n\\nMockBin (https://github.com/Kong/mockbin) - Allows you to generate custom endpoints to test, mock, and track HTTP requests & responses between libraries, sockets and APIs.\\n\\nConclusion\\n\\nA load test is critical step to understand if a target system will be reliable under the expected real world traffic.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk197_9', 'chunkContent': \"Conclusion\\n\\nA load test is critical step to understand if a target system will be reliable under the expected real world traffic.\\n\\nOf course, it's only as good as your ability to predict the expected load, so it's important to follow up with other further testing to truly understand how your system behaves in different situations.\\n\\nResources\\n\\nList additional readings about this test type for those that would like to dive deeper.\\n\\nMicrosoft Azure Well-Architected Framework > Load Testing\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\load-testing.md'}, {'chunkId': 'chunk198_0', 'chunkContent': \"Performance Testing\\n\\nPerformance Testing is an overloaded term that is used to refer to several\\nsubcategories of performance related testing, each of which has different purpose.\\n\\nA good description of overall performance testing is as follows:\\n\\nPerformance testing is a type of testing intended to determine the\\nresponsiveness, throughput, reliability, and/or scalability of a system under a\\ngiven workload. Performance Testing Guidance for Web\\nApplications.\\n\\nBefore getting into the different subcategories of performance tests let us\\nunderstand why performance testing is typically done.\\n\\nWhy Performance Testing\\n\\nPerformance testing is commonly conducted to accomplish one or more the\\nfollowing:\\n\\nTune the system's performance\\n\\nIdentifying bottlenecks and issues with the system at different load\\n    levels.\\n\\nComparing performance characteristics of the system for different system\\n    configurations.\\n\\nCome up with a scaling strategy for the system.\\n\\nAssist in capacity planning\\n\\nCapacity planning is the process of determining what type of hardware and\\n    software resources are required to run an application to support pre-defined performance goals.\\n\\nCapacity planning involves identifying business\\n    expectations, the periodic fluctuations of application usage, considering\\n    the cost of running the hardware and software infrastructure.\\n\\nAssess the system's readiness for release:\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_1', 'chunkContent': \"Assess the system's readiness for release:\\n\\nEvaluating the system's performance characteristics (response time, throughput)\\n  in a production-like environment.\\n  The goal is to ensure that performance goals can be achieved upon release.\\n\\nEvaluate the performance impact of application changes\\n\\nComparing the performance characteristics of an application after a change\\n    to the values of performance characteristics during previous runs (or\\n    baseline values), can provide an indication of performance issues (performance regression) or\\n    enhancements introduced due to a change\\n\\nKey Performance Testing categories\\n\\nPerformance testing is a broad topic. There are many areas where you can perform\\ntests. In broad strokes you can perform tests on the backend and on the front\\nend. You can test the performance of individual components as well as testing\\nthe end-to-end functionality.\\n\\nThere are several categories of tests as well:\\n\\nLoad Testing\\n\\nThis is the subcategory of performance testing that focuses on validating the\\nperformance characteristics of a system, when the system faces the load volumes\\nwhich are expected during production operation. An Endurance Test or a Soak Test\\nis a load test carried over a long duration ranging from several hours to\\ndays.\\n\\nStress Testing\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_2', 'chunkContent': \"Stress Testing\\n\\nThis is the subcategory of performance testing that focuses on validating the\\nperformance characteristics of a system when the system faces extreme load. The\\ngoal is to evaluate how does the system handles being pressured to its limits,\\ndoes it recover (i.e., scale-out) or does it just break and fail?\\n\\nEndurance Testing\\n\\nThe goal of endurance testing is to make sure that the system can maintain\\ngood performance under extended periods of load.\\n\\nSpike testing\\n\\nThe goal of Spike testing is to validate that a software system can respond well\\nto large and sudden spikes.\\n\\nChaos testing\\n\\nChaos testing or Chaos engineering is the practice of experimenting on a system\\nto build confidence that the system can withstand turbulent conditions in\\nproduction. Its goal is to identify weaknesses before they manifest system wide.\\nDevelopers often implement fallback procedures for service failure. Chaos\\ntesting arbitrarily shuts down different parts of the system to validate that\\nfallback procedures function correctly.\\n\\nBest practices\\n\\nConsider the following best practices for performance testing:\\n\\nMake one change at a time. Don't make multiple changes to the system\\n  between tests. If you do, you won't know which change caused the performance\\n  to improve or degrade.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_3', 'chunkContent': \"Automate testing. Strive to automate the setup and teardown of resources\\n  for a performance run as much as possible. Manual execution can lead to\\n  misconfigurations.\\n\\nUse different IP addresses. Some systems will throttle requests from a\\n  single IP address. If you are testing a system that has this type of\\n  restriction, you can use different IP addresses to simulate multiple users.\\n\\nPerformance monitor metrics\\n\\nWhen executing the various types of testing approaches, whether it is stress,\\nendurance, spike, or chaos testing, it is important to capture various\\nmetrics to see how the system performs.\\n\\nAt the basic hardware level, there are four areas to consider.\\n\\nPhysical disk\\n\\nMemory\\n\\nProcessor\\n\\nNetwork\\n\\nThese four areas are inextricably linked, meaning that poor performance in one\\narea will lead to poor performance in another area. Engineers concerned with\\nunderstanding application performance, should focus on these four core areas.\\n\\nThe classic example of how performance in one area can affect performance in\\nanother area is memory pressure.\\n\\nIf an application's available memory is running low, the operating system will\\ntry to compensate for shortages in memory by transferring pages of data from\\nmemory to disk, thus freeing up memory. But this work requires help from the CPU\\nand the physical disk.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_4', 'chunkContent': 'This means that when you look at performance when there are low amounts of\\nmemory, you will also notice spikes in disk activity as well as CPU.\\n\\nPhysical Disk\\n\\nAlmost all software systems are dependent on the performance of the physical\\ndisk. This is especially true for the performance of databases. More modern\\napproaches to using SSDs for physical disk storage can dramatically improve the\\nperformance of applications. Here are some of the metrics that you can capture\\nand analyze:', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_5', 'chunkContent': \"Counter Description Avg. Disk Queue Length This value is derived using the (Disk Transfers/sec)*(Disk sec/Transfer) counters. This metric describes the disk queue over time, smoothing out any quick spikes. Having any physical disk with an average queue length over 2 for prolonged periods of time can be an indication that your disk is a bottleneck. % Idle Time This is a measure of the percentage of time that the disk was idle. ie. there are no pending disk requests from the operating system waiting to be completed. A low number here is a positive sign that disk has excess capacity to service or write requests from the operating system. Avg. Disk sec/Read and Avg. Disk sec/Write These both measure the latency of your disks. Latency is defined as the average time it takes for a disk transfer to complete. You obviously want is low numbers as possible but need to be careful to account for inherent speed differences between SSD and traditional spinning disks. For this counter is important to define a baseline after the hardware is installed. Then use this value going forward to determine if you are experiencing any latency issues related to the hardware. Disk Reads/sec and Disk Writes/sec These counters each measure the total number of IO requests completed per second. Similar to the latency counters, good and bad values for these counters depend on your disk hardware but values higher than your initial baseline don't normally point to a hardware issue in this case. This counter can be useful to\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_6', 'chunkContent': \"depend on your disk hardware but values higher than your initial baseline don't normally point to a hardware issue in this case. This counter can be useful to identify spikes in disk I/O.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_7', 'chunkContent': 'Processor\\n\\nIt is important to understand the amount of time spent in kernel or privileged\\nmode. In general, if code is spending too much time executing operating system\\ncalls, that could be an area of concern because it will not allow you to run\\nyour user mode applications, such as your databases, Web servers/services, etc.\\n\\nThe guideline is that the CPU should only spend about 20% of the total processor\\ntime running in kernel mode.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_8', 'chunkContent': 'Counter Description % Processor time This is the percentage of total elapsed time that the processor was busy executing. This counter can either be too high or too low. If your processor time is consistently below 40%, then there is a question as to whether you have over provisioned your CPU. 70% is generally considered a good target number and if you start going higher than 70%, you may want to explore why there is high CPU pressure. % Privileged (Kernel Mode) time This measures the percentage of elapsed time the processor spent executing in kernel mode. Since this counter takes into account only kernel operations a high percentage of privileged time (greater than 25%) may indicate driver or hardware issue that should be investigated. % User time The percentage of elapsed time the processor spent executing in user mode (your application code). A good guideline is to be consistently below 65% as you want to have some buffer for both the kernel operations mentioned above as well as any other bursts of CPU required by other applications. Queue Length This is the number of threads that are ready to execute but waiting for a core to become available. On single core machines a sustained value greater than 2-3 can mean that you have some CPU pressure. Similarly, for a multicore machine divide the queue length by the number of cores and if that is continuously greater than 2-3 there might be CPU pressure.\\n\\nNetwork Adapter', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_9', 'chunkContent': 'Network Adapter\\n\\nNetwork speed is often a hidden culprit of poor performance. Finding the root\\ncause to poor network performance is often difficult. The source of issues can\\noriginate from bandwidth hogs such as videoconferencing, transaction data,\\nnetwork backups, recreational videos.\\n\\nIn fact, the three most common reasons for a network slow down are:\\n\\nCongestion\\n\\nData corruption\\n\\nCollisions\\n\\nSome of the tools that can help include:\\n\\nifconfig\\n\\nnetstat\\n\\niperf\\n\\ntcpretrans\\n\\ntcpdump\\n\\nWireShark\\n\\nTroubleshooting network performance usually begins with checking the hardware.\\nTypical things to explore is whether there are any loose wires or checking that\\nall routers are powered up. It is not always possible to do so, but sometimes a\\nsimple case of power recycling of the modem or router can solve many problems.\\n\\nNetwork specialists often perform the following sequence of troubleshooting steps:\\n\\nCheck the hardware\\n\\nUse IP config\\n\\nUse ping and tracert\\n\\nPerform DNS Check\\n\\nMore advanced approaches often involve looking at some of the networking\\nperformance counters, as explained below.\\n\\nNetwork Counters', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_10', 'chunkContent': 'More advanced approaches often involve looking at some of the networking\\nperformance counters, as explained below.\\n\\nNetwork Counters\\n\\nThe table above gives you some reference points to better understand what you\\ncan expect out of your network. Here are some counters that can help you\\nunderstand where the bottlenecks might exist:\\n\\nCounter Description Bytes Received/sec The rate at which bytes are received over each network adapter. Bytes Sent/sec The rate at which bytes are sent over each network adapter. Bytes Total/sec The number of bytes sent and received over the network. Segments Received/sec The rate at which segments are received for the protocol Segments Sent/sec The rate at which segments are sent. % Interrupt Time The percentage of time the processor spends receiving and servicing hardware interrupts. This value is an indirect indicator of the activity of devices that generate interrupts, such as network adapters.\\n\\nThere is an important distinction between latency and throughput.\\nLatency measures the time it takes for a packet to be transferred across the\\nnetwork, either in terms of a one-way transmission or a round-trip\\ntransmission. Throughput is different and attempts to measure the quantity\\nof data being sent and received within a unit of time.\\n\\nMemory', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_11', 'chunkContent': 'Memory\\n\\nCounter Description Available MBs This counter represents the amount of memory that is available to applications that are executing. Low memory can trigger Page Faults, whereby additional pressure is put on the CPU to swap memory to and from the disk. if the amount of available memory dips below 10%, more memory should be obtained. Pages/sec This is actually the sum of \"Pages Input/sec\" and \"Pages Output/sec\" counters which is the rate at which pages are being read and written as a result of pages faults. Small spikes with this value do not mean there is an issue but sustained values of greater than 50 can mean that system memory is a bottleneck. Paging File(_Total)\\\\% Usage The percentage of the system page file that is currently in use. This is not directly related to performance, but you can run into serious application issues if the page file does become completely full and additional memory is still being requested by applications.\\n\\nKey Performance testing activities\\n\\nPerformance testing activities vary depending on the subcategory of performance\\ntesting and the system\\'s requirements and constraints. For specific guidance you can\\nfollow the link to the subcategory of performance tests listed above.\\nThe following activities might be included depending on the performance test subcategory:\\n\\nIdentify the Acceptance criteria for the tests\\n\\nThis will generally include identifying the goals and constraints\\nfor the performance characteristics of the system\\n\\nPlan and design the tests', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk198_12', 'chunkContent': 'This will generally include identifying the goals and constraints\\nfor the performance characteristics of the system\\n\\nPlan and design the tests\\n\\nIn general we need to consider the following points:\\n\\nDefining the load the application should be tested with\\n\\nEstablishing the metrics to be collected\\n\\nEstablish what tools will be used for the tests\\n\\nEstablish the performance test frequency: whether the performance tests be\\n  done as a part of the feature development sprints, or only prior to release to\\n  a major environment?\\n\\nImplementation\\n\\nImplement the performance tests according to the designed approach.\\n\\nInstrument the system and ensure that is emitting the needed performance metrics.\\n\\nTest Execution\\n\\nExecute the tests and collect performance metrics.\\n\\nResult analysis and re-testing\\n\\nAnalyze the results/performance metrics from the tests.\\n\\nIdentify needed changes to tweak the system (i.e., code, infrastructure) to better accommodate the test objectives.\\n\\nThen test again. This cycle continues until the test objective is achieved.\\n\\nThe Iterative Performance Test Template can be used to capture details about the test result for every iterations.\\n\\nResources\\n\\nPatters and Practices: Performance Testing Guidance for Web\\n  Applications', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\performance-testing\\\\README.md'}, {'chunkId': 'chunk199_0', 'chunkContent': 'Shadow Testing\\n\\nShadow testing is one approach to reduce risks before going to production. Shadow testing is also known as \"Shadow Deployment\" or \"Shadowing Traffic\" and similarities with \"Dark launching\".\\n\\nWhen to use\\n\\nShadow Testing reduces risks when you consider replacing the current environment (V-Current) with candidate environment with new feature (V-Next). This approach is monitoring and capturing differences between two environments then compare and reduces all risks before you introduce a new feature/release.\\n\\nIn our test cases, code coverage is very important however sometimes providing code coverage can be tricky to replicate real-life combinations and possibilities. In this approach, to test V-Next environment we have side by side deployment, we\\'re replicating the same traffic with V-Current environment and directing same traffic to V-Next environment, the only difference is we don\\'t return any response from V-Next environment to users, but we collect those responses to compare with V-Current responses.\\n\\nReferencing back to one of the Principles of Chaos Engineering, mentions importance of sampling real traffic like below:\\n\\nSystems behave differently depending on environment and traffic patterns. Since the behavior of utilization can change at any time, sampling real traffic is the only way to reliably capture the request path. To guarantee both authenticity of the way in which the system is exercised and relevance to the current deployed system, Chaos strongly prefers to experiment directly on production traffic.', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\shadow-testing\\\\README.md'}, {'chunkId': 'chunk199_1', 'chunkContent': \"With this Shadow Testing approach we're leveraging real customer behavior in V-Next environment with sampling real traffic and mitigating the risks which users may face on production. At the same time we're testing V-Next environment infrastructure for scaling with real sampled traffic. V-Next should scale with the same way V-Current does. We're testing actual behavior of the product and this cause zero impact to production to test new features since traffic is replicated to V-next environment.\\n\\nThere are some similarities with Dark Launching, Dark Launching proposes to integrate new feature into production code, but users can't use the feature. On the backend you can test your feature and improve the performance until it's acceptable. It is also similar to Feature Toggles which provides you with an ability to enable/disable your new feature in production on a UI level. With this approach your new feature will be visible to users, and you can collect feedback. Using Dark Launching with Feature Toggles can be very useful for introducing a new feature.\\n\\nApplicable to\\n\\nProduction deployments: V-Next in Shadow testing always working separately and not effecting production. Users are not effected with this test.\\n\\nInfrastructure: Shadow testing replicating the same traffic, in test environment you can have the same traffic on the production. It helps to produce real life test scenarios\\n\\nHandling Scale: All traffic is replicated, and you have a chance to see how your system scaling.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\shadow-testing\\\\README.md'}, {'chunkId': 'chunk199_2', 'chunkContent': 'Handling Scale: All traffic is replicated, and you have a chance to see how your system scaling.\\n\\nShadow Testing Frameworks and Tools\\n\\nThere are some tools to implement shadow testing. The main purpose of these tools is to compare responses of V-Current and V-Next then find the differences.\\n\\nDiffy\\n\\nEnvoy\\n\\nMcRouter\\n\\nScientist\\n\\nOne of the most popular tools is Diffy. It was created and used at Twitter. Now the original author and a former Twitter employee maintains their own version of this project, called Opendiffy. Twitter announced this tool on their engineering blog as \"Testing services without writing tests\".\\n\\nAs of today Diffy is used in production by Twitter, Airbnb, Baidu and Bytedance companies. Diffy explains the shadow testing feature like this:\\n\\nDiffy finds potential bugs in your service using running instances of your new code, and your old code side by side. Diffy behaves as a proxy and multicasts whatever requests it receives to each of the running instances. It then compares the responses, and reports any regressions that may surface from those comparisons. The premise for Diffy is that if two implementations of the service return “similar” responses for a sufficiently large and diverse set of requests, then the two implementations can be treated as equivalent, and the newer implementation is regression-free.\\n\\nDiffy architecture\\n\\nConclusion', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\shadow-testing\\\\README.md'}, {'chunkId': 'chunk199_3', 'chunkContent': 'Diffy architecture\\n\\nConclusion\\n\\nShadow Testing is a useful approach to reduce risks when you consider replacing the current environment with candidate environment using new feature(s). Shadow testing replicates traffic of the production to candidate environment for testing, so you get same production use case scenarios in the test environment. You can compare differences on both environments and validate your candidate environment to be ready for releasing.\\n\\nSome advantages of shadow testing are:\\n\\nZero impact to production environment\\n\\nNo need to generate test scenarios and test data\\n\\nWe can test real-life scenarios with real-life data.\\n\\nWe can simulate scale with replicated production traffic.\\n\\nReferences\\n\\nMartin Fowler - Dark Launching\\n\\nMartin Fowler - Feature Toggle\\n\\nTraffic Shadowing/Mirroring', 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\shadow-testing\\\\README.md'}, {'chunkId': 'chunk200_0', 'chunkContent': \"Smoke Testing\\n\\nSmoke tests, sometimes named Sanity, Acceptance, or Build/Release Verification tests, are a sub-type of system/functional tests that are usually used as gates that verify the application's readiness as a preliminary step. If an application passes the smoke tests, it is acceptable, or in a stable-enough state, for the next stages of testing or deployment.\\n\\nWhen To Use\\n\\nProblem Addressed\\n\\nSmoke tests are meant to find, as early as possible, if an application is working or not. The goal of smoke tests is to save time; if the current version of the application does not pass smoke tests, then the rest of the integration or deployment chain for it can be abandoned. Smoke tests do not aim to provide full functionality coverage but instead focus on a few quick acceptance invocations for which the application should, at all times, respond correctly to.\\n\\nROI Tipping Point\\n\\nSmoke tests cover only the most critical application path, and should not be used to actually test the application's behavior, keeping execution time and complexity to minimum. The tests can be formed of a subset of the application's integration or e2e tests, and they cover as much of the functionality with as little depth as required.\\n\\nThe golden rule of a good smoke test is that it saves time on validating that the application is acceptable to a stage where better, more thorough testing will begin.\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\smoke-testing\\\\README.md'}, {'chunkId': 'chunk200_1', 'chunkContent': \"Applicable to\\n\\n[x] Local dev desktop - Example: Applying manual smoke testing to verify that the application is OK.\\n\\n[x] Build pipelines - Example: Running a small set of the integration test suite before running the full coverage of tests, which may take a long time.\\n\\n[x] Non-production and Production deployments - Example: Running a curl command to the product's API and asserting the response is 200 before running load test which consume resources.\\n\\n[x] PR Validation - Example: - Deploying the application chart to a test namespace and validating the release is successful and no immediate regressions are merged.\\n\\nConclusion\\n\\nSmoke testing is a low-effort, high-impact step to ship more reliable software. It should be considered amongst the first stages to implement when planning continuously integrated and delivered systems.\\n\\nResources\\n\\nWikipedia - Smoke Testing\\n\\nGoogle SRE Book - System Tests\", 'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\automated-testing\\\\smoke-testing\\\\README.md'}]\n",
      "Error: Expecting property name enclosed in double quotes: line 5 column 5 (char 158)\n",
      "Error: Expecting property name enclosed in double quotes: line 5 column 5 (char 112)\n",
      "Error: Expecting property name enclosed in double quotes: line 5 column 5 (char 139)\n"
     ]
    }
   ],
   "source": [
    "qa_paris = generate_qa_pairs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
