{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Workshop Setup\n",
    "\n",
    "## Pre-requisites:\n",
    "\n",
    "- [Request Access to Azure OpenAI Service](https://aka.ms/oai/access)\n",
    "- Azure Search Service (which can host one or more search indexes) with Semantic Ranker enabled. Note: it is not supported in sweden central https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/?products=search\n",
    "- Azure OpenAI Service and text-embedding-ada-002 model deployed\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this part, we will build the building blocks of a RAG solution.\n",
    "\n",
    "- We will create a Search Index\n",
    "- We will create a prompt\n",
    "  ...\n",
    "\n",
    "<!-- To create the index we need the following objects:\n",
    "\n",
    "- Data Source - a `link` to some data storage\n",
    "- Azure Index - defines the data structure over which to search\n",
    "  - Create an empty index based on an index schema\n",
    "  - Fill in the data using the Search Indexer (below\\_)\n",
    "- Azure Search Indexer - which acts as a crawler that retrieves data from external sources, can also trigger skillsets (Optical Character Recognition) -->\n",
    "\n",
    "### Setup\n",
    "\n",
    "First, we install the necessary dependencies.\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-search-documents==11.4.0 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (11.4.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from azure-search-documents==11.4.0) (1.29.7)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from azure-search-documents==11.4.0) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from azure-search-documents==11.4.0) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-search-documents==11.4.0) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-search-documents==11.4.0) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-search-documents==11.4.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-search-documents==11.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-search-documents==11.4.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-search-documents==11.4.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-search-documents==11.4.0) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install azure-search-documents==11.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we'll use `dotenv`. To connect with Azure OpenAI and the Search index, the following variables should be added to a .env file in KEY=VALUE format:\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "# %reload_ext dotenv\n",
    "# %dotenv\n",
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery,\n",
    "    VectorFilterMode,\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType,\n",
    ")\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    ScoringProfile,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearch,\n",
    "    HnswParameters,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    ")\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "subscription_id = os.environ[\"subscription_id\"]\n",
    "resource_group_name = os.environ[\"resource_group_name\"]\n",
    "workspace_name = os.environ[\"workspace_name\"]\n",
    "service_endpoint = os.environ[\n",
    "    \"service_endpoint\"\n",
    "]  # the endpoint of your Azure Cognitive Search service\n",
    "key = os.environ[\"search_key\"]\n",
    "\n",
    "# aoai_connection_name = os.environ['aoai_connection_name']\n",
    "aoi_api_key = os.environ[\"aoi_api_key\"]\n",
    "aoai_endpoint = os.environ[\"aoai_endpoint\"]\n",
    "embedding_model_name = os.environ[\"embeddingModelName\"]\n",
    "\n",
    "search_index_name = \"my_index_2\"\n",
    "search_index_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "credential = AzureKeyCredential(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Search Index\n",
    "\n",
    "<!-- https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_index_crud_operations.py\n",
    "\n",
    "https://github.com/microsoft/rag-experiment-accelerator/blob/development/rag_experiment_accelerator/init_Index/create_index.py\n",
    "\n",
    "Used for overall Fields and Semantic Settings inspiration - https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/azure-search-vector-python-huggingface-model-sample.ipynb\n",
    "\n",
    "Used for SearchField inspiration - https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_vector_search.py -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(search_index_name):\n",
    "    client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))\n",
    "\n",
    "    # 1. Define the fields\n",
    "    fields = [\n",
    "        SimpleField(\n",
    "            name=\"id\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            key=True,\n",
    "            sortable=True,\n",
    "            filterable=True,\n",
    "            # facetable=True,\n",
    "        ),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SearchableField(\n",
    "            name=\"category\", type=SearchFieldDataType.String, filterable=True\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"titleVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=384,\n",
    "            # Assign a vector profile to the field to specify the algorithm\n",
    "            # to use when searching the vector field.\n",
    "            vector_search_profile_name=\"my-vector-config\",\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=384,\n",
    "            vector_search_profile_name=\"my-vector-config\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # 2. Define the semantic Settings\n",
    "    # Note: It requires semantic ranker enabled on your search service\n",
    "    # https://learn.microsoft.com/en-us/azure/search/semantic-search-overview\n",
    "    # https://learn.microsoft.com/en-us/azure/search/semantic-how-to-query-request?tabs=portal%2Cportal-query\n",
    "    # https://learn.microsoft.com/en-us/azure/search/semantic-how-to-query-request?tabs=sdk%2Cportal-query\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"my-semantic-config\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "            content_fields=[SemanticField(field_name=\"content\")],\n",
    "        ),\n",
    "    )\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # 3. Configure the vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"my-vector-config\",\n",
    "                algorithm_configuration_name=\"my-algorithms-config\",\n",
    "                # Configuring a vectorizer in a search index is currently in public preview and available through API and beta SDK.\n",
    "                # A vectorizer is a component of a search index that specifies a vectorization agent, such as a deployed embedding model on Azure OpenAI that converts text to vectors. You can define a vectorizer once, and then reference it in the vector profile assigned to a vector field.\n",
    "                # A vectorizer is used for queries. It allows the search service to vectorize a text query on your behalf.\n",
    "                # https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-vectorizer\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[\n",
    "            # Contains configuration options specific to the hnsw approximate nearest neighbors  algorithm used during indexing and querying\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"my-algorithms-config\",\n",
    "                kind=\"hnsw\",\n",
    "                # https://learn.microsoft.com/en-us/python/api/azure-search-documents/azure.search.documents.indexes.models.hnswparameters?view=azure-python-preview#variables\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    # The size of the dynamic list containing the nearest neighbors, which is used during index time.\n",
    "                    # Increasing this parameter may improve index quality, at the expense of increased indexing time.\n",
    "                    ef_construction=400,\n",
    "                    # The size of the dynamic list containing the nearest neighbors, which is used during search time.\n",
    "                    # Increasing this parameter may improve search results, at the expense of slower search.\n",
    "                    ef_search=500,\n",
    "                    # The similarity metric to use for vector comparisons.\n",
    "                    # Known values are: \"cosine\", \"euclidean\", and \"dotProduct\"\n",
    "                    metric=\"cosine\",\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        \n",
    "    )\n",
    "\n",
    "    # CORS is used for apps that issues requests from different domains.\n",
    "    # cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n",
    "\n",
    "    # 4. Add scoring profiles when the default ranking behavior doesn't go far enough in meeting your business objectives.\n",
    "    # https://learn.microsoft.com/en-us/azure/search/index-add-scoring-profiles\n",
    "    scoring_profiles: List[ScoringProfile] = []\n",
    "    index = SearchIndex(\n",
    "        name=search_index_name,\n",
    "        fields=fields,\n",
    "        scoring_profiles=scoring_profiles,\n",
    "        # cors_options=cors_options,\n",
    "        # tokenizers=[], # TOOD: Add tokenizers,\n",
    "        semantic_search=semantic_search,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    result = client.create_or_update_index(index)\n",
    "    print(f\"{result.name} created or updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_index_2 created or updated\n"
     ]
    }
   ],
   "source": [
    "create_index(search_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Embeddings\n",
    "\n",
    "<!-- #### Which Embeddings Model to use?\n",
    "\n",
    "There are several embedding options:\n",
    "\n",
    "- OpenAI models, such as: [`text-embedding-ada-002`](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings), `text-embedding-3-small`, `text-embedding-3-large`\n",
    "- HuggingFace models, which offers a wide range of models. The [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) ranks the performance of embeddings models on a few axis, though not all models can be run locally. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### a) Embed a query using an embedding model from OpenAI -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# def get_query_embedding(\n",
    "#     query,\n",
    "#     endpoint=aoai_endpoint,\n",
    "#     api_key=aoi_api_key,\n",
    "#     api_version=\"2023-07-01-preview\",\n",
    "#     embedding_model_deployment=embedding_model_name,\n",
    "# ):\n",
    "#     request_url = f\"{endpoint}/openai/deployments/{embedding_model_deployment}/embeddings?api-version={api_version}\"\n",
    "#     headers = {\"Content-Type\": \"application/json\", \"api-key\": api_key}\n",
    "#     request_payload = {\"input\": query}\n",
    "#     embedding_response = requests.post(\n",
    "#         request_url, json=request_payload, headers=headers, timeout=None\n",
    "#     )\n",
    "#     if embedding_response.status_code == 200:\n",
    "#         data_values = embedding_response.json()[\"data\"]\n",
    "#         embeddings_vectors = [data_value[\"embedding\"] for data_value in data_values]\n",
    "#         return embeddings_vectors\n",
    "#     else:\n",
    "#         raise Exception(f\"failed to get embedding: {embedding_response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Hello\"\n",
    "\n",
    "# query_vectors = get_query_embedding(\n",
    "#     query, aoai_endpoint, aoi_api_key, \"2023-07-01-preview\", embedding_model_name\n",
    "# )\n",
    "\n",
    "# print(f\"The embedded vector is: {query_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Create embeddings using OpenAI\n",
    "\n",
    "Read your data, generate embeddings using OpenAI model -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/text-sample.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     input_data = json.load(file)\n",
    "\n",
    "# for item in input_data:\n",
    "#     title = item[\"title\"]\n",
    "#     content = item[\"content\"]\n",
    "#     title_embeddings = get_query_embedding(title)\n",
    "#     content_embeddings = get_query_embedding(content)\n",
    "#     item[\"titleVector\"] = title_embeddings\n",
    "#     item[\"contentVector\"] = content_embeddings\n",
    "\n",
    "# with open(\"./output/docVectors-openai.json\", \"w\") as f:\n",
    "#     json.dump(input_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed a query using an embedding model from Hugging Face\n",
    "\n",
    "We will use [`infloat/e5-small-v2`](https://huggingface.co/intfloat/e5-small-v2) from Hugging Face, which is of size 0.13 GB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\Workshop2024\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
    "query = \"Hello\"\n",
    "\n",
    "embedded_query = model.encode(query, normalize_embeddings=True)\n",
    "print(len(embedded_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embeddings using Hugging Face model\n",
    "\n",
    "Read your data, generate embeddings using HuggingFace model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
    "\n",
    "with open(\"./data/text-sample.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "for item in input_data:\n",
    "    title = item[\"title\"]\n",
    "    content = item[\"content\"]\n",
    "    title_embeddings = model.encode(title, normalize_embeddings=True)\n",
    "    content_embeddings = model.encode(content, normalize_embeddings=True)\n",
    "    item[\"titleVector\"] = title_embeddings.tolist()\n",
    "    item[\"contentVector\"] = content_embeddings.tolist()\n",
    "\n",
    "with open(\"./output/docVectors-e5.json\", \"w\") as f:\n",
    "    json.dump(input_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Upload data\n",
    "\n",
    "<!-- https://github.com/microsoft/rag-experiment-accelerator/blob/development/rag_experiment_accelerator/ingest_data/acs_ingest.py -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add texts and metadata from the JSON data to the vector store using Hugging Face embedded vectors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 108 documents\n"
     ]
    }
   ],
   "source": [
    "# Upload some documents to the index\n",
    "with open('./output/docVectors-e5.json', 'r') as file:  \n",
    "    documents = json.load(file)  \n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=search_index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(result):\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Score: {result['@search.score']}\")\n",
    "        print(f\"Content: {result['content']}\")\n",
    "        print(f\"Category: {result['category']}\\n\")\n",
    "\n",
    "\n",
    "search_client = SearchClient(service_endpoint, search_index_name, credential=credential)\n",
    "query_embeddings = model.encode(query, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Search\n",
    "\n",
    "<!-- https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167 -->\n",
    "\n",
    "There are two layers of execution: retrieval and ranking.\n",
    "\n",
    "- Retrieval - also called L1, has the goal to quickly find all the documents from the index that satisfy the search criteria (possibly across millions or billions of documents). These are scored to pick the top few (typically in order of 50) to return to the user or to feed the next layer. Azure AI Search supports three different models:\n",
    "\n",
    "  - Keyword: Uses traditional full-text search methods – content is broken into terms through language-specific text analysis, inverted indexes are created for fast retrieval, and the BM25 probabilistic model is used for scoring.\n",
    "\n",
    "  - Vector: Documents are converted from text to vector representations using an embedding model. Retrieval is performed by generating a query embedding and finding the documents whose vectors are closest to the query’s. We used Azure Open AI text-embedding-ada-002 (Ada-002) embeddings and cosine similarity for all our tests in this post.\n",
    "  - Hybrid: Performs both keyword and vector retrieval and applies a fusion step to select the best results from each technique. Azure AI Search currently uses Reciprocal Rank Fusion (RRF) to produce a single result set.\n",
    "\n",
    "- Ranking – also called L2, takes a subset of the top L1 results and computes higher quality relevance scores to reorder the result set. The L2 can improve the L1's ranking because it applies more computational power to each result. The L2 ranker can only reorder what the L1 already found – if the L1 missed an ideal document, the L2 can't fix that. L2 ranking is critical for RAG applications to make sure the best results are in the top positions.\n",
    "  - Semantic ranking is performed by Azure AI Search's L2 ranker which utilizes multi-lingual, deep learning models adapted from Microsoft Bing. The Semantic ranker can rank the top 50 results from the L1.\n",
    "\n",
    "https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a vector similarity search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Azure Front Door\n",
      "Score: 0.80073524\n",
      "Content: Azure Front Door is a global, scalable, and secure entry point for fast delivery of your web applications. It provides features like load balancing, SSL offloading, and web application firewall (WAF). Front Door supports various Azure services, such as Azure App Service, Azure Storage, and Azure Virtual Machines. You can use Azure Front Door to build highly available and responsive applications, optimize your users' experience, and improve the security of your infrastructure. It also integrates with other Azure services, such as Azure CDN and Azure Traffic Manager.\n",
      "Category: Networking\n",
      "\n",
      "Title: Azure Power BI Embedded\n",
      "Score: 0.7912205\n",
      "Content: Azure Power BI Embedded is a cloud-based analytics service that enables you to embed interactive visualizations and reports into your applications. It provides features like data exploration, custom visuals, and real-time data refresh. Power BI Embedded supports various data sources, such as Azure SQL Database, Azure Blob Storage, and on-premises databases. You can use Power BI Embedded to create data-driven applications, improve decision-making, and enhance user experiences. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Machine Learning.\n",
      "Category: Analytics\n",
      "\n",
      "Title: Azure API for FHIR\n",
      "Score: 0.7905315\n",
      "Content: Azure API for FHIR is a fully managed, standards-based API that enables you to store, manage, and exchange health data in the Fast Healthcare Interoperability Resources (FHIR) format. It provides features like data validation, versioning, and integration with Azure Active Directory. API for FHIR supports various data types, such as clinical, imaging, and administrative data. You can use Azure API for FHIR to build healthcare applications, ensure the security and compliance of your health data, and streamline your data management. It also integrates with other Azure services, such as Azure Machine Learning and Azure IoT Central.\n",
      "Category: Healthcare\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"tools for software development\"\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=query_embeddings.tolist(), k_nearest_neighbors=3, fields=\"contentVector\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=None,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    ")\n",
    "\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a hybrid search\n",
    "\n",
    "Hybrid Retrieval brings out the best of Keyword and Vector Search\n",
    "\n",
    "Keyword and vector retrieval tackle search from different perspectives, which yield complementary capabilities. Vector retrieval semantically matches queries to passages with similar meanings. This is powerful because embeddings are less sensitive to misspellings, synonyms, and phrasing differences and can even work in cross lingual scenarios. Keyword search is useful because it prioritizes matching specific, important words that might be diluted in an embedding.\n",
    "\n",
    "User search can take many forms. Hybrid retrieval consistently brings out the best from both retrieval methods across query types. With the most effective L1, the L2 ranking step can significantly improve the quality of results in the top positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Azure Storage\n",
      "Score: 0.03333333507180214\n",
      "Content: Azure Storage is a scalable, durable, and highly available cloud storage service that supports a variety of data types, including blobs, files, queues, and tables. It provides a massively scalable object store for unstructured data. Storage supports data redundancy and geo-replication, ensuring high durability and availability. It offers a variety of data access and management options, including REST APIs, SDKs, and Azure Portal. You can secure your data using encryption at rest and in transit.\n",
      "Category: Storage\n",
      "\n",
      "Title: Azure File Storage\n",
      "Score: 0.0320020467042923\n",
      "Content: Azure File Storage is a fully managed, scalable, and secure file sharing service that enables you to store and access your files over the Server Message Block (SMB) protocol. It provides features like snapshots, shared access signatures, and integration with Azure Backup. File Storage supports various platforms, such as Windows, Linux, and macOS. You can use Azure File Storage to build file sharing solutions, lift and shift your applications to the cloud, and simplify your data management. It also integrates with other Azure services, such as Azure Virtual Machines and Azure Kubernetes Service.\n",
      "Category: Storage\n",
      "\n",
      "Title: Azure Blob Storage\n",
      "Score: 0.03177805617451668\n",
      "Content: Azure Blob Storage is a scalable, durable, and high-performance object storage service for unstructured data. It provides features like data redundancy, geo-replication, and fine-grained access control. Blob Storage supports various data types, such as images, documents, and videos. You can use Blob Storage to store and manage your data, build data lakes, and develop big data analytics solutions. It also integrates with other Azure services, such as Azure CDN, Azure Functions, and Azure Machine Learning.\n",
      "Category: Storage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pure Vector Search\n",
    "query = \"scalable storage solution\"\n",
    "query_embeddings = model.encode(query, normalize_embeddings=True)\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=query_embeddings.tolist(), k_nearest_neighbors=3, fields=\"contentVector\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    "    top=3,\n",
    ")\n",
    "\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a semantic hybrid search - Required Semantic Ranker enabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Answer: Azure File Storage is<em> a fully managed, scalable, and secure file sharing service that enables you to store and access your files over the Server Message Block (SMB) protocol.</em> It provides features like snapshots, shared access signatures, and integration with Azure Backup. File Storage supports various platforms, such as Windows, Linux, and macOS.\n",
      "Semantic Answer Score: 0.9208984375\n",
      "\n",
      "Title: Azure Stack Edge\n",
      "Reranker Score: 2.075716972351074\n",
      "Content: Azure Stack Edge is a managed, edge computing appliance that enables you to run Azure services and AI workloads on-premises or at the edge. It provides features like hardware-accelerated machine learning, local caching, and integration with Azure IoT Hub. Azure Stack Edge supports various Azure services, such as Azure Functions, Azure Machine Learning, and Azure Kubernetes Service. You can use Azure Stack Edge to build edge computing applications, optimize your data processing, and ensure the security and compliance of your workloads. It also integrates with other Azure services, such as Azure Monitor and Azure Stack Hub.\n",
      "Category: Hybrid\n",
      "Caption: Azure Stack Edge is a managed, edge computing appliance that enables you to run Azure services and AI workloads on-premises or at the edge. It provides features like hardware-accelerated machine learning, local caching, and integration with Azure IoT Hub.\n",
      "\n",
      "Title: Azure Security Center\n",
      "Reranker Score: 1.9684149026870728\n",
      "Content: Azure Security Center is a unified security management service that helps you protect your workloads and improve your security posture across your Azure resources and on-premises infrastructure. It provides features like continuous security assessments, threat protection, and security recommendations. Security Center supports various platforms, such as Windows, Linux, and Kubernetes. You can use Azure Security Center to detect and remediate threats, manage your security policies, and ensure compliance with industry regulations. It also integrates with other Azure services, such as Azure Monitor and Azure Log Analytics.\n",
      "Category: Security\n",
      "Caption: Azure Security Center is a unified security management service that helps you protect your workloads and improve your security posture across your Azure resources and on-premises infrastructure. It provides features like continuous security assessments, threat protection, and security recommendations.\n",
      "\n",
      "Title: Azure Databricks\n",
      "Reranker Score: 1.723077416419983\n",
      "Content: Azure Databricks is an Apache Spark-based analytics platform optimized for the Azure cloud. It provides a collaborative workspace for data scientists, engineers, and business users to process, analyze, and visualize big data. Databricks supports multiple programming languages, including Python, Scala, R, and SQL. It offers built-in integration with Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can also use Databricks to train and deploy machine learning models, and integrate with Azure Machine Learning.\n",
      "Category: Analytics\n",
      "Caption: Azure Databricks is an Apache Spark-based analytics platform optimized for the Azure cloud. It provides a collaborative workspace for data scientists, engineers, and business users to process, analyze, and visualize big data. Databricks supports multiple programming languages, including Python, Scala, R, and SQL.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is azure sarch?\"\n",
    "\n",
    "query_embeddings = model.encode(query, normalize_embeddings=True)\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=query_embeddings.tolist(), k_nearest_neighbors=3, fields=\"contentVector\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name=\"my-semantic-config\",\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3,\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Category: {result['category']}\")\n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
