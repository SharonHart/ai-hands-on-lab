{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3. Chunking Experiment\n",
    "\n",
    "The search solution is comprised of both **ingestion** and **retrieval**. One does not exist without the other. While the other experiments are focused on data retrieval, ingestion plays equal importance in the effectiveness of the search solution.\n",
    "\n",
    "<!-- Certain aspects of data ingestion need to be experimented as part of the experimentation phase: -->\n",
    "\n",
    "https://github.com/microsoft/rag-openai/blob/main/topics/RAG_EnablingSearch.md#learnings-from-engagements-1\n",
    "\n",
    "When processing data, splitting the source documents into chunks requires care and expertise to ensure the resulting chunks are small enough to be effective during fact retrieval but not too small so that enough context is provided during summarization.\n",
    "\n",
    "```{note}\n",
    "Our goal here is not to identify which chunking strategy is the “best” in general but rather to demonstrate how various choices of chunking may have a non-trivial impact on the ultimate outcome from the retrieval-augmented-generation solution.\n",
    "```\n",
    "\n",
    "[Learnings fromm other engagements](https://github.com/microsoft/rag-openai/blob/main/topics/RAG_EnablingSearch.md#learnings-from-engagements-1)\n",
    "\n",
    "<!-- https://vectara.com/blog/grounded-generation-done-right-chunking/#:~:text=In%20the%20context%20of%20Grounded%20Generation%2C%20chunking%20is,find%20natural%20segments%20like%20complete%20sentences%20or%20paragraphs. -->\n",
    "\n",
    "## Why Chunking Size Matters\n",
    "\n",
    "As mentioned [here](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview), the models used to generate embedding vectors have maximum limits on the text fragments provided as input. For example, the maximum length of input text for the Azure OpenAI embedding models is **8,191** tokens. Given that each token is around 4 characters of text for common OpenAI models, this maximum limit is equivalent to around 6000 words of text. If you're using these models to generate embeddings, it's critical that the input text stays under the limit. Partitioning your content into chunks ensures that your data can be processed by the Large Language Models (LLM) used for indexing and queries.\n",
    "\n",
    "**Relevance and Granularity**: A small chunk size, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the similarity _top_k_ setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the _Faithfulness and Relevancy_ metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
    "\n",
    "**Response Generation Time**: As the chunk_size increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
    "\n",
    "In essence, determining the optimal chunk_size is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset.\n",
    "\n",
    "https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n",
    "\n",
    "Example code: https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/data-chunking/textsplit-data-chunking-example.ipynb\n",
    "\n",
    "Read [Common Chunking Technique](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview), [Content overlap considerations](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents#content-overlap-considerations), [Simple example of how to create chunks with sentences](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents#content-overlap-considerations)\n",
    "\n",
    "CODE: https://github.com/microsoft/rag-openai/blob/438999a5470bef7946fa1c8714ed1090e1ed40c3/samples/searchEvaluation/customskills/utils/chunker/text_chunker.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%pip install langchain-community==0.0.18\n",
    "# %pip install langchain-core==0.1.20\n",
    "%pip install unstructured==0.12.3\n",
    "%pip install unstructured-client==0.17.0\n",
    "%pip install langchain==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import os\n",
    "\n",
    "# Code also https://github.com/microsoft/rag-openai/blob/438999a5470bef7946fa1c8714ed1090e1ed40c3/samples/searchEvaluation/customskills/utils/chunker/text_chunker.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_folder(path, totalNumberOfDocuments=200) -> list[str]:\n",
    "    print(\"Loading documents...\")\n",
    "    markdown_documents = []\n",
    "    i = 0\n",
    "    for file in tqdm.tqdm(glob.glob(path, recursive=True)):\n",
    "        loader = UnstructuredFileLoader(file)\n",
    "        document = loader.load()\n",
    "        markdown_documents.append(document)\n",
    "        if i == totalNumberOfDocuments:\n",
    "            return markdown_documents\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def create_chunks_and_save_to_file(\n",
    "    path_to_output, totalNumberOfDocuments=200, chunk_size=300, chunk_overlap=30\n",
    ") -> list:\n",
    "    try:\n",
    "        if os.path.exists(path_to_output):\n",
    "            print(f\"Chunks already created at: {path_to_output} \")\n",
    "            return\n",
    "\n",
    "        documents = load_documents_from_folder(\n",
    "            \"..\\data\\docs\\**\\*.md\", totalNumberOfDocuments\n",
    "        )\n",
    "\n",
    "        print(\"Creating chunks...\")\n",
    "        markdown_splitter = MarkdownTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        lengths = {}\n",
    "        all_chunks = []\n",
    "        chunk_id = 0\n",
    "        for document in tqdm.tqdm(documents):\n",
    "            current_chunks_text_list = markdown_splitter.split_text(\n",
    "                document[0].page_content\n",
    "            )  # output = [\"content chunk1\", \"content chunk2\", ...]\n",
    "\n",
    "            for i, chunk in enumerate(\n",
    "                current_chunks_text_list\n",
    "            ):  # (0, \"content chunk1\"), (1, \"content chunk2\"), ...\n",
    "                current_chunk_dict = {\n",
    "                    \"chunkId\": f\"chunk{chunk_id}_{i}\",\n",
    "                    \"chunkContent\": chunk,\n",
    "                    \"source\": document[0].metadata[\"source\"],\n",
    "                }\n",
    "                all_chunks.append(current_chunk_dict)\n",
    "\n",
    "            chunk_id += 1\n",
    "\n",
    "            n_chunks = len(current_chunks_text_list)\n",
    "            # lengths = {[Number of chunks]: [number of documents with that number of chunks]}\n",
    "            if n_chunks not in lengths:\n",
    "                lengths[n_chunks] = 1\n",
    "            else:\n",
    "                lengths[n_chunks] += 1\n",
    "\n",
    "        with open(path_to_output, \"w\") as f:\n",
    "            json.dump(all_chunks, f)\n",
    "        print(f\"Chunks created: \", lengths)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating chunks: {e}\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the chunks\n",
    "\n",
    "Note:\n",
    "\n",
    "- we are only chunking the first `totalNumberOfDocuments` from `..\\data\\docs\\**\\*.md`\n",
    "- `chunk_size` is the number of tokens a chunk should have\n",
    "- `chunk_overlap` is the percentage of overlap between two chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks already created at: ./output/chunks-solution-ops-200.json \n"
     ]
    }
   ],
   "source": [
    "totalNumberOfDocuments = 200\n",
    "path_to_chunks_output = f\"./output/chunks-solution-ops-{totalNumberOfDocuments}.json\"\n",
    "chunks = create_chunks_and_save_to_file(\n",
    "    path_to_chunks_output, totalNumberOfDocuments, chunk_size=300, chunk_overlap=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, to separate our experiments, we will take the _Full Reindex_ strategy by creating a new index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Create the new index\u001b[39;00m\n\u001b[0;32m      4\u001b[0m new_index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolution-ops-chunking-300-30\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_index_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 2. Generate embeddings for the new chunks\u001b[39;00m\n\u001b[0;32m      8\u001b[0m generated_embeddings_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output/chunks-solution-ops-embedded-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotalNumberOfDocuments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9856\\391458413.py:2\u001b[0m, in \u001b[0;36mcreate_index\u001b[1;34m(search_index_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_index\u001b[39m(search_index_name):\n\u001b[1;32m----> 2\u001b[0m     client \u001b[38;5;241m=\u001b[39m SearchIndexClient(service_endpoint, AzureKeyCredential(\u001b[43mkey\u001b[49m))\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# 1. Define the fields\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m         SimpleField(\n\u001b[0;32m      7\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkId\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m         ),\n\u001b[0;32m     27\u001b[0m     ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'key' is not defined"
     ]
    }
   ],
   "source": [
    "%run -i ./helpers/search.ipynb\n",
    "\n",
    "# 1. Create the new index\n",
    "new_index_name = \"solution-ops-chunking-300-30\"\n",
    "create_index(new_index_name)\n",
    "\n",
    "# 2. Generate embeddings for the new chunks\n",
    "generated_embeddings_path = f\"./output/chunks-solution-ops-embedded-{totalNumberOfDocuments}.json\"\n",
    "generate_embeddings_for_chunks_and_save_to_file(path_to_chunks_file=path_to_chunks_output, path_to_output=generated_embeddings_path)\n",
    "\n",
    "# 3. Upload the embeddings to the new index\n",
    "upload_data(file_path=generated_embeddings_path, search_index_name=new_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Semantic chunking\n",
    "# # https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-retrieval-augumented-generation?view=doc-intel-4.0.0\n",
    "\n",
    "# # Using SDK targeting 2023-10-31-preview, make sure your resource is in one of these regions: East US, West US2, West Europe\n",
    "# # pip install azure-ai-documentintelligence==1.0.0b1\n",
    "# # pip install langchain langchain-community azure-ai-documentintelligence\n",
    "\n",
    "# from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "\n",
    "# endpoint = \"https://<my-custom-subdomain>.cognitiveservices.azure.com/\"\n",
    "# key = \"<api_key>\"\n",
    "\n",
    "# from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "# from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "# # Initiate Azure AI Document Intelligence to load the document. You can either specify file_path or url_path to load the document.\n",
    "# loader = AzureAIDocumentIntelligenceLoader(file_path=\"<path to your file>\", api_key = key, api_endpoint = endpoint, api_model=\"prebuilt-layout\")\n",
    "# docs = loader.load()\n",
    "\n",
    "# # Split the document into chunks base on markdown headers.\n",
    "# headers_to_split_on = [\n",
    "#     (\"#\", \"Header 1\"),\n",
    "#     (\"##\", \"Header 2\"),\n",
    "#     (\"###\", \"Header 3\"),\n",
    "# ]\n",
    "# text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# docs_string = docs[0].page_content\n",
    "# splits = text_splitter.split_text(docs_string)\n",
    "# splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in skillset: SplitSkill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload files to a storage account so we can create an Indexer\n",
    "https://github.com/microsoft/rag-openai/blob/438999a5470bef7946fa1c8714ed1090e1ed40c3/samples/searchEvaluation/upload_files.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
