{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import dotenv\n",
    "\n",
    "# %reload_ext dotenv\n",
    "# %dotenv\n",
    "\n",
    "file_path = \"../pre-requisites.ipynb\"\n",
    "default_file_path = True\n",
    "if(not os.path.exists(file_path)):\n",
    "    default_file_path = False\n",
    "    file_path = \"./pre-requisites.ipynb\"\n",
    "\n",
    "%run -i {file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    ScoringProfile,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearch,\n",
    "    HnswParameters,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    ")\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Index Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(search_index_name, vector_search_dimensions=1536):\n",
    "    client = SearchIndexClient(service_endpoint, credential)\n",
    "\n",
    "    # 1. Define the fields\n",
    "    fields = [\n",
    "        SimpleField(\n",
    "            name=\"chunkId\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            sortable=True,\n",
    "            filterable=True,\n",
    "            key=True,\n",
    "        ),\n",
    "        SimpleField(\n",
    "            name=\"source\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            sortable=True,\n",
    "            filterable=True,\n",
    "        ),\n",
    "        SearchableField(name=\"chunkContent\", type=SearchFieldDataType.String),\n",
    "        SearchField(\n",
    "            name=\"chunkContentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            # the dimension of the embedded vector\n",
    "            vector_search_dimensions=vector_search_dimensions,\n",
    "            vector_search_profile_name=\"my-vector-config\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # 2. Configure the vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"my-vector-config\",\n",
    "                algorithm_configuration_name=\"my-algorithms-config\",\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[\n",
    "            # Contains configuration options specific to the hnsw approximate nearest neighbors  algorithm used during indexing and querying\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"my-algorithms-config\",\n",
    "                kind=\"hnsw\",\n",
    "                # https://learn.microsoft.com/en-us/python/api/azure-search-documents/azure.search.documents.indexes.models.hnswparameters?view=azure-python-preview#variables\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    # The size of the dynamic list containing the nearest neighbors, which is used during index time.\n",
    "                    # Increasing this parameter may improve index quality, at the expense of increased indexing time.\n",
    "                    ef_construction=400,\n",
    "                    # The size of the dynamic list containing the nearest neighbors, which is used during search time.\n",
    "                    # Increasing this parameter may improve search results, at the expense of slower search.\n",
    "                    ef_search=500,\n",
    "                    # The similarity metric to use for vector comparisons.\n",
    "                    # Known values are: \"cosine\", \"euclidean\", and \"dotProduct\"\n",
    "                    metric=\"cosine\",\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=search_index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    result = client.create_or_update_index(index)\n",
    "    print(f\"Index: '{result.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Chunking Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create chunks using markdown text splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "def create_md_header_chunks_and_save_to_file(documents, path_to_output) -> list:\n",
    "    try:\n",
    "        if os.path.exists(path_to_output):\n",
    "            print(f\"Chunks already created at: {path_to_output} \")\n",
    "            return\n",
    "        lengths = {}\n",
    "        all_chunks = []\n",
    "        chunk_id = 0\n",
    "        # tqdm.tqdm(\n",
    "        headers_to_split_on = [\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "        for document in documents:\n",
    "            current_chunks_text_list = markdown_splitter.split_text(\n",
    "                document[0].page_content)\n",
    "            source = document[0].metadata[\"source\"]\n",
    "\n",
    "            for i, chunk in enumerate(current_chunks_text_list):\n",
    "\n",
    "                current_chunk_dict = {\n",
    "                    \"chunkId\": f\"chunk{chunk_id}_{i}\",\n",
    "                    \"chunkContent\": chunk.page_content,\n",
    "                    \"source\": source,\n",
    "                }\n",
    "                all_chunks.append(current_chunk_dict)\n",
    "            chunk_id += 1\n",
    "        with open(path_to_output, \"w\") as f:\n",
    "            json.dump(all_chunks, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating chunks: {e}\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_output = f\"./output/pre-generated/chunking/md-header-text-splitter-engineering-mlops.json\"\n",
    "\n",
    "# create_md_header_chunks_and_save_to_file(documents, path_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "def load_documents_from_folder(path, totalNumberOfDocuments=200) -> list[str]:\n",
    "    print(\"Loading documents...\")\n",
    "    markdown_documents = []\n",
    "    i = 0\n",
    "    for file in glob.glob(path, recursive=True):\n",
    "        loader = UnstructuredFileLoader(file)\n",
    "        document = loader.load()\n",
    "        markdown_documents.append(document)\n",
    "        if i == totalNumberOfDocuments:\n",
    "            return markdown_documents\n",
    "        i += 1\n",
    "    return markdown_documents\n",
    "\n",
    "\n",
    "def create_chunks_and_save_to_file(path_to_output, totalNumberOfDocuments=200, chunk_size=300, chunk_overlap=30) -> list:\n",
    "    if (os.path.exists(path_to_output)):\n",
    "        print(f\"Chunks already created at: {path_to_output} \")\n",
    "        return\n",
    "\n",
    "    documents = load_documents_from_folder(\n",
    "        \"..\\data\\docs\\code-with-engineering\\**\\*.md\", totalNumberOfDocuments)\n",
    "\n",
    "    print(\"Creating chunks...\")\n",
    "    markdown_splitter = MarkdownTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    lengths = {}\n",
    "    all_chunks = []\n",
    "    chunk_id = 0\n",
    "    for document in documents:\n",
    "        current_chunks_text_list = markdown_splitter.split_text(\n",
    "            document[0].page_content\n",
    "        )  # output = [\"content chunk1\", \"content chunk2\", ...]\n",
    "\n",
    "        for i, chunk in enumerate(\n",
    "            current_chunks_text_list\n",
    "        ):  # (0, \"content chunk1\"), (1, \"content chunk2\"), ...\n",
    "            current_chunk_dict = {\n",
    "                \"chunkId\": f\"chunk{chunk_id}_{i}\",\n",
    "                \"chunkContent\": chunk,\n",
    "                \"source\": document[0].metadata[\"source\"],\n",
    "            }\n",
    "            all_chunks.append(current_chunk_dict)\n",
    "\n",
    "        chunk_id += 1\n",
    "\n",
    "        n_chunks = len(current_chunks_text_list)\n",
    "        # lengths = {[Number of chunks]: [number of documents with that number of chunks]}\n",
    "        if n_chunks not in lengths:\n",
    "            lengths[n_chunks] = 1\n",
    "        else:\n",
    "            lengths[n_chunks] += 1\n",
    "\n",
    "    with open(path_to_output, \"w\") as f:\n",
    "        json.dump(all_chunks, f)\n",
    "    print(f\"Chunks created: \", lengths)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings using AOI:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "def oai_query_embedding(\n",
    "    query,\n",
    "    endpoint=azure_aoai_endpoint,\n",
    "    api_key=azure_openai_key,\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    embedding_model_deployment=azure_openai_embedding_deployment,\n",
    "):\n",
    "    \"\"\"\n",
    "    Query the OpenAI Embedding model to get the embeddings for the given query.\n",
    "\n",
    "    Args:\n",
    "    query (str): The query for which to get the embeddings.\n",
    "    endpoint (str): The endpoint for the OpenAI service.\n",
    "    api_key (str): The API key for the OpenAI service.\n",
    "    api_version (str): The API version for the OpenAI service.\n",
    "    embedding_model_deployment (str): The deployment for the OpenAI embedding model.\n",
    "    Returns:\n",
    "    list: The embeddings for the given query.\n",
    "    \"\"\"\n",
    "    # If input has more than 8,000 words, shrink it to max\n",
    "    if len(query) > 8000:\n",
    "        print(\"Shrinked!!1\")\n",
    "        query = query[:8000]\n",
    "    # print(len(query))\n",
    "    request_url = f\"{endpoint}/openai/deployments/{embedding_model_deployment}/embeddings?api-version={api_version}\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": api_key}\n",
    "    request_payload = {\"input\": query}\n",
    "    embedding_response = requests.post(\n",
    "        request_url, json=request_payload, headers=headers, timeout=None\n",
    "    )\n",
    "    # embedding_response = embed_input(query)\n",
    "    if embedding_response.status_code == 200:\n",
    "        #     time.sleep(2.5)\n",
    "        #     embedding_response = embed_input(query)\n",
    "\n",
    "        data_values = embedding_response.json()[\"data\"]\n",
    "        embeddings_vectors = [data_value[\"embedding\"]\n",
    "                              for data_value in data_values]\n",
    "        return embeddings_vectors[0]\n",
    "    else:\n",
    "        print(\"Failed to get embedding: \", embedding_response.json())\n",
    "        print(\"Length: \", len(query))\n",
    "        return []\n",
    "        # print(\"Retried\")\n",
    "        # raise Exception(\n",
    "        #     f\"failed to get embedding: {embedding_response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings using OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings using AOI in batch:\n",
    "\n",
    "Took 48s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def generate_embeddings_for_chunks_in_batch(path_to_chunked_documents, path_to_output_file):\n",
    "    \"\"\"\n",
    "    Generate embeddings for chunked data\n",
    "    Args:\n",
    "    path_to_chunked_documents: str: path to the input file\n",
    "    path_to_output_file: str: path to the output file\n",
    "    \"\"\"\n",
    "    if os.path.exists(path_to_output_file):\n",
    "        print(f\"Embeddings were already created for chunked data {path_to_chunked_documents} at: {path_to_chunked_documents}\")\n",
    "        return\n",
    "    try:\n",
    "        with open(path_to_chunked_documents, \"r\", encoding=\"utf-8\") as file:\n",
    "            input_data = json.load(file)\n",
    "            batch_size = 32\n",
    "            num_chunks = len(input_data)\n",
    "            for i in range(0, num_chunks, batch_size):\n",
    "                batch_chunks = input_data[i:i + batch_size]\n",
    "                batch_chunks_content = [chunk[\"chunkContent\"]\n",
    "                                        for chunk in batch_chunks]\n",
    "                batch_embeddings = oai_query_embedding(\n",
    "                    batch_chunks_content, batch=True)\n",
    "                for j, chunk in enumerate(batch_chunks):\n",
    "                    # print(\"j : \", j)\n",
    "                    chunk[\"chunkContentVector\"] = batch_embeddings[j]\n",
    "                    # content = chunk[\"chunkContent\"]\n",
    "                    # content_embeddings = oai_query_embedding(content)\n",
    "                    # chunk[\"chunkContentVector\"] = content_embeddings\n",
    "\n",
    "        with open(path_to_output_file, \"w\") as f:\n",
    "            json.dump(input_data, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate embeddings for chunks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings using AOI and save to file:\n",
    "\n",
    "Took 16 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_for_chunks_and_save_to_file(path_to_chunks_file, path_to_output):\n",
    "    try:\n",
    "        if (os.path.exists(path_to_output)):\n",
    "            print(\n",
    "                f\"Embeddings were already created for chunked data at: {path_to_chunks_file} \")\n",
    "            return\n",
    "        # i = 0\n",
    "        with open(path_to_chunks_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            input_data = json.load(file)\n",
    "\n",
    "            for chunk in input_data:\n",
    "                content = chunk[\"chunkContent\"]\n",
    "                # print(f\"Length: {len(content)}\")\n",
    "                # print(i)\n",
    "                # print(chunk[\"chunkId\"])\n",
    "\n",
    "                content_emebddings = oai_query_embedding(content)\n",
    "                chunk[\"chunkContentVector\"] = content_emebddings\n",
    "                # i = i+1\n",
    "        print(f\"Created {len(input_data)} chunks\")\n",
    "        print(f\"Example of one chunk: {input_data[1]}\")\n",
    "\n",
    "        with open(path_to_output, \"w\") as f:\n",
    "            json.dump(input_data, f)\n",
    "            print(f\"Saved embeddings to: {path_to_output}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate embeddings: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "\n",
    "def intfloat_e5_small_v2_query_embedding(chunk, model=SentenceTransformer(\"intfloat/e5-small-v2\")):\n",
    "    embedded_input = model.encode(\n",
    "        chunk, normalize_embeddings=True\n",
    "    )  # Note that the type is a ndarray.\n",
    "    return (\n",
    "        embedded_input.tolist()\n",
    "    )  # We need to reshape the array to be a list of floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data to the Index Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(file_path, search_index_name):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            documents = json.load(file)\n",
    "\n",
    "        search_client = SearchClient(\n",
    "            endpoint=service_endpoint,\n",
    "            index_name=search_index_name,\n",
    "            credential=credential,\n",
    "        )\n",
    "        for i in range(0, len(documents)):\n",
    "            try:\n",
    "                search_client.upload_documents(documents[i])\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading document {i}: {e}\")\n",
    "\n",
    "        print(\n",
    "            f\"Uploaded {len(documents)} documents to Index: {search_index_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading documents: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings using open source model for fixed size chunking\n",
    "\n",
    "Took 22 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings were already created for chunked data ../output/pre-generated/chunking/fixed-size-chunks-engineering-mlops-180-30.json at: ../output/pre-generated/chunking/fixed-size-chunks-engineering-mlops-180-30.json \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "\n",
    "def embed_chunk(chunk, model=SentenceTransformer(\"intfloat/e5-small-v2\")):\n",
    "    embedded_input = model.encode(\n",
    "        chunk, normalize_embeddings=True\n",
    "    )  # Note that the type is a ndarray.\n",
    "    return (\n",
    "        embedded_input.tolist()\n",
    "    )  # We need to reshape the array to be a list of floats\n",
    "\n",
    "\n",
    "def generate_embeddings_with_intfloat_e5_small_v2(\n",
    "    path_to_input_file, path_to_output_file\n",
    "):\n",
    "    if os.path.exists(path_to_output_file):\n",
    "        print(\n",
    "            f\"Embeddings were already created for chunked data {path_to_input_file} at: {path_to_input_file} \")\n",
    "        return\n",
    "    try:\n",
    "        model = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
    "        with open(path_to_input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            input_data = json.load(file)\n",
    "            for chunk in input_data:\n",
    "                content = chunk[\"chunkContent\"]\n",
    "                content_emebddings = embed_chunk(content, model)\n",
    "                chunk[\"chunkContentVector\"] = content_emebddings\n",
    "\n",
    "        with open(path_to_output_file, \"w\") as f:\n",
    "            json.dump(input_data, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate embeddings for chunks: {e}\")\n",
    "\n",
    "\n",
    "# e5_small_v2_prefix = \"fixed-size-chunks-180-30-engineering-mlops-e5-small-v2\"\n",
    "# path_to_output_file = f\"../output/pre-generated/embeddings/{\n",
    "#     e5_small_v2_prefix}.json\"\n",
    "# pregenerated_fixed_size_chunks = '../output/pre-generated/chunking/fixed-size-chunks-engineering-mlops-180-30.json'\n",
    "# generate_embeddings_with_intfloat_e5_small_v2(\n",
    "#     path_to_input_file=pregenerated_fixed_size_chunks,\n",
    "#     path_to_output_file=path_to_output_file,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings using open source model for semantic chunking\n",
    "\n",
    "Took 12 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings were already created for chunked data ../output/pre-generated/chunking/semantic-chunks-engineering-mlops.json at: ../output/pre-generated/chunking/semantic-chunks-engineering-mlops.json \n"
     ]
    }
   ],
   "source": [
    "# path_to_chunked_documents = \"../output/pre-generated/chunking/semantic-chunks-engineering-mlops.json\"\n",
    "# e5_small_v2_prefix = \"semantic-chunking-engineering-mlops-e5-small-v2\"\n",
    "# path_to_output_file = f\"../output/pre-generated/embeddings/{\n",
    "#     e5_small_v2_prefix}.json\"\n",
    "# generate_embeddings_with_intfloat_e5_small_v2(\n",
    "#     path_to_input_file=path_to_chunked_documents,\n",
    "#     path_to_output_file=path_to_output_file,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search documents Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(search_index_name, input, embedding_function):\n",
    "    search_client = SearchClient(\n",
    "        service_endpoint, search_index_name, credential=credential\n",
    "    )\n",
    "    query_embeddings = embedding_function(input)\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_embeddings, k_nearest_neighbors=3, fields=\"chunkContentVector\"\n",
    "    )\n",
    "\n",
    "    results = search_client.search(\n",
    "        search_text=None,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"chunkContent\", \"chunkId\", \"source\", \"chunkContentVector\"],\n",
    "    )\n",
    "    # print_results(results)\n",
    "\n",
    "    documents = []\n",
    "    for document in results:\n",
    "        item = {}\n",
    "        item[\"chunkContent\"] = document[\"chunkContent\"]\n",
    "        item[\"source\"] = os.path.normpath(document[\"source\"])\n",
    "        item[\"chunkId\"] = document[\"chunkId\"]\n",
    "        item[\"score\"] = document['@search.score']\n",
    "        item[\"chunkContentVector\"] = document[\"chunkContentVector\"]\n",
    "        documents.append(item)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(query, documents):\n",
    "    system_prompt = f\"\"\"\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    \"You are an AI assistant that helps users answer questions given a specific context.\n",
    "    You will be given a context (\"chunkContent\") in Retrieved Documents and will be asked a question (User Question) based on that context.\n",
    "    Your answer should be as precise as possible and should only come from the context.\n",
    "    Please add citation after each sentence when possible in a form \"(Source: source+chunkId),\n",
    "    where both 'source' and 'chunkId' are taken from the Retrieved Documents.\"\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    ## Retrieve Documents:\n",
    "    {documents}\n",
    "\n",
    "    ## User Question\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    final_message = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt + \"\\nEND OF CONTEXT\"},\n",
    "    ]\n",
    "\n",
    "    return final_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "def call_llm(messages: list[dict]):\n",
    "    client = AzureOpenAI(\n",
    "        api_key=azure_openai_key,\n",
    "        api_version=\"2023-07-01-preview\",\n",
    "        azure_endpoint=azure_aoai_endpoint\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=azure_openai_chat_deployment, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
