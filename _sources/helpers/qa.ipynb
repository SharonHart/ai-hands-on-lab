{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate answers for each question in evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "%run -i ./search.ipynb\n",
    "\n",
    "def generate_answers_for_qa(evaluation_data_path, search_index_name, embedding_function, path_to_output):\n",
    "    try:\n",
    "        if os.path.exists(path_to_output):\n",
    "            print(f\"QA already created at: {path_to_output} \")\n",
    "            return\n",
    "        with open(evaluation_data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            evaluation_data = json.load(file)\n",
    "            generated_qa = []\n",
    "            for data in evaluation_data:\n",
    "                question = data[\"user_prompt\"]\n",
    "\n",
    "                # 1. Search in the index\n",
    "                search_response = search_documents(\n",
    "                    search_index_name=search_index_name,\n",
    "                    input=question,\n",
    "                    embedding_function=embedding_function,\n",
    "                )\n",
    "                retrieved_sources = [os.path.normpath(response[\"source\"])\n",
    "                                   for response in search_response]\n",
    "                retrieved_contexts = [response[\"chunkContext\"]\n",
    "                                   for response in search_response]\n",
    "                retrieved_chunk_ids = [response[\"chunkId\"]\n",
    "                                   for response in search_response]\n",
    "                # 2. Create prompt with the query and retrieved documents\n",
    "                prompt = create_prompt(question, search_response)\n",
    "\n",
    "                # 3. Call GPT-3 model to generate an answer\n",
    "                # given the question and the retrieved documents\n",
    "                response = call_llm(prompt)\n",
    "\n",
    "                current_qa = {\n",
    "                    \"user_prompt\": question,\n",
    "                    \"output_prompt\": data[\"output_prompt\"],\n",
    "                    \"context\": data[\"context\"],\n",
    "                    \"chunk_id\": data[\"chunk_id\"],\n",
    "                    \"source\": os.path.normpath(data[\"source\"]),\n",
    "                    \"root_chunk_id\": data[\"root_chunk_id\"],\n",
    "\n",
    "                    \"generated_output\": response,\n",
    "                    \"retrieved_context\": retrieved_contexts,\n",
    "                    \"retrieved_source\": retrieved_sources,\n",
    "                    \"retrieved_chunk_id\": retrieved_chunk_ids\n",
    "                }\n",
    "\n",
    "                generated_qa.append(current_qa)\n",
    "\n",
    "            with open(path_to_output, \"w\") as f:\n",
    "                json.dump(generated_qa, f)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers using ADA + fixed size chunking \n",
    "Took 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA already created at: ../output/qa/results/fixed-size-chunks-180-30-engineering-mlops-ada.json \n"
     ]
    }
   ],
   "source": [
    "%run -i ./search.ipynb\n",
    "\n",
    "evaluation_data_path = \"../output/qa/evaluation/qa_pairs_solutionops.json\"\n",
    "\n",
    "search_index_name = \"fixed-size-chunks-180-30-batch-engineering-mlops-ada\"\n",
    "embedding_function = oai_query_embedding\n",
    "path_to_output = \"../output/qa/results/fixed-size-chunks-180-30-engineering-mlops-ada.json\"\n",
    "\n",
    "generate_answers_for_qa(evaluation_data_path, search_index_name, embedding_function, path_to_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers using semantic chunking + open source embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Create the new index\n",
    "# # TODO: Replace this with a name for your new index\n",
    "# index_name = \"semantic-chunking-eval\"\n",
    "# vector_size = 384\n",
    "# create_index(index_name, vector_size)\n",
    "\n",
    "# # 2. Upload the embeddings to the new index\n",
    "# upload_data(file_path=pregenerated_semantic_chunks_embeddings_os,\n",
    "#             search_index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA already created at: ../output/qa/results/semantic-chunking-intfloat.json \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "%run -i ./search.ipynb\n",
    "\n",
    "evaluation_data_path = \"../output/qa/evaluation/qa_pairs_solutionops.json\"\n",
    "search_index_name = \"semantic-chunking-eval\"\n",
    "embedding_function = intfloat_e5_small_v2_query_embedding\n",
    "path_to_output = \"../output/qa/results/semantic-chunking-intfloat.json\"\n",
    "\n",
    "generate_answers_for_qa(evaluation_data_path, search_index_name, embedding_function, path_to_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
