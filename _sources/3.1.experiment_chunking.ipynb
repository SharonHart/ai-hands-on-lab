{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Chunking\n",
    "\n",
    "The search solution is comprised of both **ingestion** and **retrieval**. One does not exist without the other.\n",
    "\n",
    "While the other experiments are focused on data retrieval, ingestion plays equal importance in the effectiveness of the search solution.\n",
    "\n",
    "Certain aspects of data ingestion need to be experimented as part of the experimentation phase:\n",
    "\n",
    "```{note}\n",
    "Other pre and post-processing techniques include: Optical Character Recognition, data conversation, use of Azure Form Recognizer to extract information from the documents, chunking, summarization, post-processing to make data more \"human like\", video captioning, speech to text, tagging, etc. are all methods that need to be considered and experimented with as part of the ingestion pipeline experimentation.\n",
    "```\n",
    "\n",
    "https://github.com/microsoft/rag-openai/blob/main/topics/RAG_EnablingSearch.md#learnings-from-engagements-1\n",
    "\n",
    "When processing data, splitting the source documents into chunks requires care and expertise to ensure the resulting chunks are small enough to be effective during fact retrieval but not too small so that enough context is provided during summarization.\n",
    "\n",
    "```{note}\n",
    "Our goal here is not to identify which chunking strategy is the “best” in general but rather to demonstrate how various choices of chunking may have a non-trivial impact on the ultimate outcome from the retrieval-augmented-generation solution.\n",
    "```\n",
    "\n",
    "<!-- https://vectara.com/blog/grounded-generation-done-right-chunking/#:~:text=In%20the%20context%20of%20Grounded%20Generation%2C%20chunking%20is,find%20natural%20segments%20like%20complete%20sentences%20or%20paragraphs. -->\n",
    "\n",
    "## Why Chunking Size Matters\n",
    "\n",
    "As mentioned [here](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview), the models used to generate embedding vectors have maximum limits on the text fragments provided as input. For example, the maximum length of input text for the Azure OpenAI embedding models is **8,191** tokens. Given that each token is around 4 characters of text for common OpenAI models, this maximum limit is equivalent to around 6000 words of text. If you're using these models to generate embeddings, it's critical that the input text stays under the limit. Partitioning your content into chunks ensures that your data can be processed by the Large Language Models (LLM) used for indexing and queries.\n",
    "\n",
    "**Relevance and Granularity**: A small chunk size, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the similarity _top_k_ setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the _Faithfulness and Relevancy_ metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
    "\n",
    "**Response Generation Time**: As the chunk_size increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
    "\n",
    "In essence, determining the optimal chunk_size is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset.\n",
    "\n",
    "https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n",
    "\n",
    "Example code: https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/data-chunking/textsplit-data-chunking-example.ipynb\n",
    "\n",
    "Read [Common Chunking Technique](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview), [Content overlap considerations](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents#content-overlap-considerations), [Simple example of how to create chunks with sentences](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents#content-overlap-considerations)\n",
    "\n",
    "CODE: https://github.com/microsoft/rag-openai/blob/438999a5470bef7946fa1c8714ed1090e1ed40c3/samples/searchEvaluation/customskills/utils/chunker/text_chunker.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community==0.0.18\n",
    "# %pip install langchain-core==0.1.20\n",
    "%pip install unstructured==0.12.3\n",
    "%pip install unstructured-client==0.17.0\n",
    "%pip install langchain==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter, NLTKTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Code also https://github.com/microsoft/rag-openai/blob/438999a5470bef7946fa1c8714ed1090e1ed40c3/samples/searchEvaluation/customskills/utils/chunker/text_chunker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_folder(path: str) -> list[str]:\n",
    "    print(\"Loading documents...\")\n",
    "    markdown_documents = []\n",
    "    for file in tqdm.tqdm(glob.glob(path, recursive=True)):\n",
    "        loader = UnstructuredFileLoader(file) \n",
    "        document = loader.load()\n",
    "        markdown_documents.append(document)\n",
    "    return markdown_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 777/777 [00:57<00:00, 13.57it/s]\n"
     ]
    }
   ],
   "source": [
    "markdown_documents = load_documents_from_folder(\"../data/docs/**/*.md\")\n",
    "# TODO: Move this to a Storage Account?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(documents: list) -> list:\n",
    "    print(\"Creating chunks...\")\n",
    "    markdown_splitter = MarkdownTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=300, chunk_overlap=30\n",
    "    )\n",
    "    lengths = {}\n",
    "    all_chunks = {}\n",
    "    chunk_id = 0\n",
    "    for document in tqdm.tqdm(documents):\n",
    "        current_chunks_text_list = markdown_splitter.split_text(\n",
    "            document[0].page_content\n",
    "        )  # output = [\"content chunk1\", \"content chunk2\", ...]\n",
    "\n",
    "        for i, chunk in enumerate(\n",
    "            current_chunks_text_list\n",
    "        ):  # (0, \"content chunk1\"), (1, \"content chunk2\"), ...\n",
    "            current_chunk_dict = {\n",
    "                \"chunk_id\": i,\n",
    "                \"chunk_text\": chunk,\n",
    "                \"source\": document[0].metadata[\"source\"],\n",
    "            }\n",
    "            current_id_str = f\"chunk{chunk_id}_{i}\"\n",
    "            all_chunks[current_id_str] = current_chunk_dict\n",
    "\n",
    "        chunk_id += 1\n",
    "\n",
    "        n_chunks = len(current_chunks_text_list)\n",
    "        # lengths = {[Number of chunks]: [number of documents with that number of chunks]}\n",
    "        if n_chunks not in lengths:\n",
    "            lengths[n_chunks] = 1\n",
    "        else:\n",
    "            lengths[n_chunks] += 1\n",
    "\n",
    "    print(lengths)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 777/777 [00:02<00:00, 277.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 145, 2: 137, 4: 69, 5: 74, 7: 43, 12: 8, 16: 6, 32: 1, 18: 7, 3: 98, 8: 30, 6: 58, 20: 2, 9: 20, 13: 13, 15: 10, 10: 11, 17: 2, 11: 19, 14: 12, 43: 1, 25: 1, 26: 2, 22: 3, 30: 1, 19: 2, 38: 1, 29: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = create_chunks(markdown_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will use OpenAI ada for embedding the chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload files to a storage account so we can create an Indexer\n",
    "https://github.com/microsoft/rag-openai/blob/438999a5470bef7946fa1c8714ed1090e1ed40c3/samples/searchEvaluation/upload_files.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: python-dotenv in c:\\projects\\workshop2024\\.venv\\lib\\site-packages (1.0.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'workspace_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m STORAGE_ACCOUNT_URL \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworkspace_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_upload_ops_files\u001b[39m(credential: AzureCliCredential, storage_account_name: \u001b[38;5;28mstr\u001b[39m, folder: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m      4\u001b[0m     path \u001b[38;5;241m=\u001b[39m ROOT_FILE_DIR \u001b[38;5;241m/\u001b[39m folder\n",
      "File \u001b[1;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'workspace_name'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
